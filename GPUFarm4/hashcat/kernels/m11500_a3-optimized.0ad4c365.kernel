//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: UNKNOWN
// Driver 
// Based on LLVM 3.4svn
//

.version 6.1
.target sm_61, texmode_independent
.address_size 64

	// .globl	gpu_memset
.const .align 4 .b8 c_append_helper[4096] = {255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255};
.const .align 8 .b8 crc32tab[1024] = {0, 0, 0, 0, 150, 48, 7, 119, 44, 97, 14, 238, 186, 81, 9, 153, 25, 196, 109, 7, 143, 244, 106, 112, 53, 165, 99, 233, 163, 149, 100, 158, 50, 136, 219, 14, 164, 184, 220, 121, 30, 233, 213, 224, 136, 217, 210, 151, 43, 76, 182, 9, 189, 124, 177, 126, 7, 45, 184, 231, 145, 29, 191, 144, 100, 16, 183, 29, 242, 32, 176, 106, 72, 113, 185, 243, 222, 65, 190, 132, 125, 212, 218, 26, 235, 228, 221, 109, 81, 181, 212, 244, 199, 133, 211, 131, 86, 152, 108, 19, 192, 168, 107, 100, 122, 249, 98, 253, 236, 201, 101, 138, 79, 92, 1, 20, 217, 108, 6, 99, 99, 61, 15, 250, 245, 13, 8, 141, 200, 32, 110, 59, 94, 16, 105, 76, 228, 65, 96, 213, 114, 113, 103, 162, 209, 228, 3, 60, 71, 212, 4, 75, 253, 133, 13, 210, 107, 181, 10, 165, 250, 168, 181, 53, 108, 152, 178, 66, 214, 201, 187, 219, 64, 249, 188, 172, 227, 108, 216, 50, 117, 92, 223, 69, 207, 13, 214, 220, 89, 61, 209, 171, 172, 48, 217, 38, 58, 0, 222, 81, 128, 81, 215, 200, 22, 97, 208, 191, 181, 244, 180, 33, 35, 196, 179, 86, 153, 149, 186, 207, 15, 165, 189, 184, 158, 184, 2, 40, 8, 136, 5, 95, 178, 217, 12, 198, 36, 233, 11, 177, 135, 124, 111, 47, 17, 76, 104, 88, 171, 29, 97, 193, 61, 45, 102, 182, 144, 65, 220, 118, 6, 113, 219, 1, 188, 32, 210, 152, 42, 16, 213, 239, 137, 133, 177, 113, 31, 181, 182, 6, 165, 228, 191, 159, 51, 212, 184, 232, 162, 201, 7, 120, 52, 249, 0, 15, 142, 168, 9, 150, 24, 152, 14, 225, 187, 13, 106, 127, 45, 61, 109, 8, 151, 108, 100, 145, 1, 92, 99, 230, 244, 81, 107, 107, 98, 97, 108, 28, 216, 48, 101, 133, 78, 0, 98, 242, 237, 149, 6, 108, 123, 165, 1, 27, 193, 244, 8, 130, 87, 196, 15, 245, 198, 217, 176, 101, 80, 233, 183, 18, 234, 184, 190, 139, 124, 136, 185, 252, 223, 29, 221, 98, 73, 45, 218, 21, 243, 124, 211, 140, 101, 76, 212, 251, 88, 97, 178, 77, 206, 81, 181, 58, 116, 0, 188, 163, 226, 48, 187, 212, 65, 165, 223, 74, 215, 149, 216, 61, 109, 196, 209, 164, 251, 244, 214, 211, 106, 233, 105, 67, 252, 217, 110, 52, 70, 136, 103, 173, 208, 184, 96, 218, 115, 45, 4, 68, 229, 29, 3, 51, 95, 76, 10, 170, 201, 124, 13, 221, 60, 113, 5, 80, 170, 65, 2, 39, 16, 16, 11, 190, 134, 32, 12, 201, 37, 181, 104, 87, 179, 133, 111, 32, 9, 212, 102, 185, 159, 228, 97, 206, 14, 249, 222, 94, 152, 201, 217, 41, 34, 152, 208, 176, 180, 168, 215, 199, 23, 61, 179, 89, 129, 13, 180, 46, 59, 92, 189, 183, 173, 108, 186, 192, 32, 131, 184, 237, 182, 179, 191, 154, 12, 226, 182, 3, 154, 210, 177, 116, 57, 71, 213, 234, 175, 119, 210, 157, 21, 38, 219, 4, 131, 22, 220, 115, 18, 11, 99, 227, 132, 59, 100, 148, 62, 106, 109, 13, 168, 90, 106, 122, 11, 207, 14, 228, 157, 255, 9, 147, 39, 174, 0, 10, 177, 158, 7, 125, 68, 147, 15, 240, 210, 163, 8, 135, 104, 242, 1, 30, 254, 194, 6, 105, 93, 87, 98, 247, 203, 103, 101, 128, 113, 54, 108, 25, 231, 6, 107, 110, 118, 27, 212, 254, 224, 43, 211, 137, 90, 122, 218, 16, 204, 74, 221, 103, 111, 223, 185, 249, 249, 239, 190, 142, 67, 190, 183, 23, 213, 142, 176, 96, 232, 163, 214, 214, 126, 147, 209, 161, 196, 194, 216, 56, 82, 242, 223, 79, 241, 103, 187, 209, 103, 87, 188, 166, 221, 6, 181, 63, 75, 54, 178, 72, 218, 43, 13, 216, 76, 27, 10, 175, 246, 74, 3, 54, 96, 122, 4, 65, 195, 239, 96, 223, 85, 223, 103, 168, 239, 142, 110, 49, 121, 190, 105, 70, 140, 179, 97, 203, 26, 131, 102, 188, 160, 210, 111, 37, 54, 226, 104, 82, 149, 119, 12, 204, 3, 71, 11, 187, 185, 22, 2, 34, 47, 38, 5, 85, 190, 59, 186, 197, 40, 11, 189, 178, 146, 90, 180, 43, 4, 106, 179, 92, 167, 255, 215, 194, 49, 207, 208, 181, 139, 158, 217, 44, 29, 174, 222, 91, 176, 194, 100, 155, 38, 242, 99, 236, 156, 163, 106, 117, 10, 147, 109, 2, 169, 6, 9, 156, 63, 54, 14, 235, 133, 103, 7, 114, 19, 87, 0, 5, 130, 74, 191, 149, 20, 122, 184, 226, 174, 43, 177, 123, 56, 27, 182, 12, 155, 142, 210, 146, 13, 190, 213, 229, 183, 239, 220, 124, 33, 223, 219, 11, 212, 210, 211, 134, 66, 226, 212, 241, 248, 179, 221, 104, 110, 131, 218, 31, 205, 22, 190, 129, 91, 38, 185, 246, 225, 119, 176, 111, 119, 71, 183, 24, 230, 90, 8, 136, 112, 106, 15, 255, 202, 59, 6, 102, 92, 11, 1, 17, 255, 158, 101, 143, 105, 174, 98, 248, 211, 255, 107, 97, 69, 207, 108, 22, 120, 226, 10, 160, 238, 210, 13, 215, 84, 131, 4, 78, 194, 179, 3, 57, 97, 38, 103, 167, 247, 22, 96, 208, 77, 71, 105, 73, 219, 119, 110, 62, 74, 106, 209, 174, 220, 90, 214, 217, 102, 11, 223, 64, 240, 59, 216, 55, 83, 174, 188, 169, 197, 158, 187, 222, 127, 207, 178, 71, 233, 255, 181, 48, 28, 242, 189, 189, 138, 194, 186, 202, 48, 147, 179, 83, 166, 163, 180, 36, 5, 54, 208, 186, 147, 6, 215, 205, 41, 87, 222, 84, 191, 103, 217, 35, 46, 122, 102, 179, 184, 74, 97, 196, 2, 27, 104, 93, 148, 43, 111, 42, 55, 190, 11, 180, 161, 142, 12, 195, 27, 223, 5, 90, 141, 239, 2, 45};

.entry gpu_memset(
	.param .u64 .ptr .global .align 16 gpu_memset_param_0,
	.param .u32 gpu_memset_param_1,
	.param .u64 gpu_memset_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd2, [gpu_memset_param_0];
	ld.param.u32 	%r1, [gpu_memset_param_1];
	ld.param.u64 	%rd3, [gpu_memset_param_2];
	mov.b32	%r2, %envreg3;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mad.lo.s32 	%r5, %r3, %r4, %r2;
	mov.u32 	%r6, %tid.x;
	add.s32 	%r7, %r5, %r6;
	cvt.s64.s32	%rd1, %r7;
	setp.ge.u64	%p1, %rd1, %rd3;
	@%p1 bra 	BB0_2;

	shl.b64 	%rd4, %rd1, 4;
	add.s64 	%rd5, %rd2, %rd4;
	st.global.v4.u32 	[%rd5], {%r1, %r1, %r1, %r1};

BB0_2:
	ret;
}

	// .globl	m11500_m04
.entry m11500_m04(
	.param .u64 .ptr .global .align 4 m11500_m04_param_0,
	.param .u64 .ptr .global .align 4 m11500_m04_param_1,
	.param .u64 .ptr .global .align 4 m11500_m04_param_2,
	.param .u64 .ptr .const .align 4 m11500_m04_param_3,
	.param .u64 .ptr .global .align 1 m11500_m04_param_4,
	.param .u64 .ptr .global .align 1 m11500_m04_param_5,
	.param .u64 .ptr .global .align 4 m11500_m04_param_6,
	.param .u64 .ptr .global .align 4 m11500_m04_param_7,
	.param .u64 .ptr .global .align 4 m11500_m04_param_8,
	.param .u64 .ptr .global .align 4 m11500_m04_param_9,
	.param .u64 .ptr .global .align 4 m11500_m04_param_10,
	.param .u64 .ptr .global .align 4 m11500_m04_param_11,
	.param .u64 .ptr .global .align 4 m11500_m04_param_12,
	.param .u64 .ptr .global .align 4 m11500_m04_param_13,
	.param .u64 .ptr .global .align 4 m11500_m04_param_14,
	.param .u64 .ptr .global .align 4 m11500_m04_param_15,
	.param .u64 .ptr .global .align 4 m11500_m04_param_16,
	.param .u64 .ptr .global .align 4 m11500_m04_param_17,
	.param .u64 .ptr .global .align 1 m11500_m04_param_18,
	.param .u64 .ptr .global .align 4 m11500_m04_param_19,
	.param .u64 .ptr .global .align 4 m11500_m04_param_20,
	.param .u64 .ptr .global .align 4 m11500_m04_param_21,
	.param .u64 .ptr .global .align 4 m11500_m04_param_22,
	.param .u64 .ptr .global .align 4 m11500_m04_param_23,
	.param .u32 m11500_m04_param_24,
	.param .u32 m11500_m04_param_25,
	.param .u32 m11500_m04_param_26,
	.param .u32 m11500_m04_param_27,
	.param .u32 m11500_m04_param_28,
	.param .u32 m11500_m04_param_29,
	.param .u32 m11500_m04_param_30,
	.param .u32 m11500_m04_param_31,
	.param .u32 m11500_m04_param_32,
	.param .u32 m11500_m04_param_33,
	.param .u64 m11500_m04_param_34
)
{
	.local .align 16 .b8 	__local_depot1[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<34>;
	.reg .b32 	%r<166>;
	.reg .b64 	%rd<66>;


	mov.u64 	%rd65, __local_depot1;
	cvta.local.u64 	%SP, %rd65;
	ld.param.u64 	%rd7, [m11500_m04_param_0];
	ld.param.u64 	%rd8, [m11500_m04_param_3];
	ld.param.u64 	%rd9, [m11500_m04_param_6];
	ld.param.u64 	%rd10, [m11500_m04_param_7];
	ld.param.u64 	%rd11, [m11500_m04_param_8];
	ld.param.u64 	%rd12, [m11500_m04_param_9];
	ld.param.u64 	%rd13, [m11500_m04_param_10];
	ld.param.u64 	%rd14, [m11500_m04_param_11];
	ld.param.u64 	%rd15, [m11500_m04_param_12];
	ld.param.u64 	%rd16, [m11500_m04_param_13];
	ld.param.u64 	%rd17, [m11500_m04_param_14];
	ld.param.u64 	%rd18, [m11500_m04_param_15];
	ld.param.u64 	%rd19, [m11500_m04_param_16];
	ld.param.u64 	%rd20, [m11500_m04_param_17];
	ld.param.u64 	%rd21, [m11500_m04_param_19];
	ld.param.u32 	%r50, [m11500_m04_param_24];
	ld.param.u32 	%r51, [m11500_m04_param_25];
	ld.param.u32 	%r52, [m11500_m04_param_26];
	ld.param.u32 	%r53, [m11500_m04_param_27];
	ld.param.u32 	%r54, [m11500_m04_param_30];
	ld.param.u32 	%r55, [m11500_m04_param_31];
	ld.param.u32 	%r56, [m11500_m04_param_32];
	ld.param.u64 	%rd22, [m11500_m04_param_34];
	add.u64 	%rd23, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd23;
	mov.u32 	%r57, %ctaid.x;
	mov.u32 	%r58, %ntid.x;
	mov.b32	%r59, %envreg3;
	mad.lo.s32 	%r60, %r57, %r58, %r59;
	mov.u32 	%r61, %tid.x;
	add.s32 	%r1, %r60, %r61;
	cvt.s64.s32	%rd2, %r1;
	setp.ge.u64	%p1, %rd2, %rd22;
	@%p1 bra 	BB1_43;

	mul.lo.s64 	%rd24, %rd2, 260;
	add.s64 	%rd3, %rd7, %rd24;
	ld.global.u32 	%r2, [%rd3+256];
	setp.eq.s32	%p2, %r54, 0;
	@%p2 bra 	BB1_43;

	ld.global.u32 	%r3, [%rd3];
	ld.global.u32 	%r4, [%rd3+4];
	ld.global.u32 	%r5, [%rd3+8];
	ld.global.u32 	%r6, [%rd3+12];
	mul.wide.u32 	%rd25, %r53, 564;
	add.s64 	%rd26, %rd20, %rd25;
	ld.global.u32 	%r63, [%rd26];
	not.b32 	%r7, %r63;
	and.b32  	%r8, %r51, 31;
	and.b32  	%r9, %r52, 31;
	cvt.u64.u32	%rd4, %r56;
	shr.u32 	%r10, %r7, 8;
	mov.u32 	%r62, 0;
	mov.u32 	%r150, %r62;

BB1_3:
	mul.wide.u32 	%rd27, %r150, 4;
	add.s64 	%rd28, %rd8, %rd27;
	ld.const.u32 	%r64, [%rd28];
	or.b32  	%r12, %r64, %r3;
	st.local.v4.u32 	[%rd1], {%r12, %r4, %r5, %r6};
	st.local.v4.u32 	[%rd1+16], {%r62, %r62, %r62, %r62};
	st.local.v4.u32 	[%rd1+32], {%r62, %r62, %r62, %r62};
	st.local.v4.u32 	[%rd1+48], {%r62, %r62, %r62, %r62};
	setp.eq.s32	%p3, %r2, 0;
	mov.u32 	%r157, %r7;
	@%p3 bra 	BB1_5;

	xor.b32  	%r66, %r12, %r7;
	and.b32  	%r67, %r66, 255;
	mul.wide.u32 	%rd29, %r67, 4;
	mov.u64 	%rd30, crc32tab;
	add.s64 	%rd31, %rd30, %rd29;
	ld.const.u32 	%r68, [%rd31];
	xor.b32  	%r157, %r68, %r10;

BB1_5:
	setp.lt.u32	%p4, %r2, 2;
	@%p4 bra 	BB1_7;

	shr.u32 	%r69, %r12, 8;
	xor.b32  	%r70, %r69, %r157;
	and.b32  	%r71, %r70, 255;
	mul.wide.u32 	%rd32, %r71, 4;
	mov.u64 	%rd33, crc32tab;
	add.s64 	%rd34, %rd33, %rd32;
	ld.const.u32 	%r72, [%rd34];
	shr.u32 	%r73, %r157, 8;
	xor.b32  	%r157, %r72, %r73;

BB1_7:
	setp.lt.u32	%p5, %r2, 3;
	@%p5 bra 	BB1_9;

	shr.u32 	%r74, %r12, 16;
	xor.b32  	%r75, %r74, %r157;
	and.b32  	%r76, %r75, 255;
	mul.wide.u32 	%rd35, %r76, 4;
	mov.u64 	%rd36, crc32tab;
	add.s64 	%rd37, %rd36, %rd35;
	ld.const.u32 	%r77, [%rd37];
	shr.u32 	%r78, %r157, 8;
	xor.b32  	%r157, %r77, %r78;

BB1_9:
	setp.lt.u32	%p6, %r2, 4;
	@%p6 bra 	BB1_11;

	shr.u32 	%r79, %r12, 24;
	and.b32  	%r80, %r157, 255;
	xor.b32  	%r81, %r79, %r80;
	mul.wide.u32 	%rd38, %r81, 4;
	mov.u64 	%rd39, crc32tab;
	add.s64 	%rd40, %rd39, %rd38;
	ld.const.u32 	%r82, [%rd40];
	shr.u32 	%r83, %r157, 8;
	xor.b32  	%r157, %r82, %r83;

BB1_11:
	mov.u32 	%r156, 4;
	mov.u32 	%r155, 1;
	setp.lt.u32	%p7, %r2, 5;
	@%p7 bra 	BB1_21;

BB1_12:
	add.s32 	%r86, %r156, 1;
	setp.gt.u32	%p8, %r86, %r2;
	mul.wide.u32 	%rd41, %r155, 4;
	add.s64 	%rd5, %rd1, %rd41;
	@%p8 bra 	BB1_14;

	ld.local.u32 	%r87, [%rd5];
	xor.b32  	%r88, %r87, %r157;
	and.b32  	%r89, %r88, 255;
	mul.wide.u32 	%rd42, %r89, 4;
	mov.u64 	%rd43, crc32tab;
	add.s64 	%rd44, %rd43, %rd42;
	ld.const.u32 	%r90, [%rd44];
	shr.u32 	%r91, %r157, 8;
	xor.b32  	%r157, %r90, %r91;

BB1_14:
	add.s32 	%r92, %r156, 2;
	setp.gt.u32	%p9, %r92, %r2;
	@%p9 bra 	BB1_16;

	ld.local.u32 	%r93, [%rd5];
	shr.u32 	%r94, %r93, 8;
	xor.b32  	%r95, %r94, %r157;
	and.b32  	%r96, %r95, 255;
	mul.wide.u32 	%rd45, %r96, 4;
	mov.u64 	%rd46, crc32tab;
	add.s64 	%rd47, %rd46, %rd45;
	ld.const.u32 	%r97, [%rd47];
	shr.u32 	%r98, %r157, 8;
	xor.b32  	%r157, %r97, %r98;

BB1_16:
	add.s32 	%r99, %r156, 3;
	setp.gt.u32	%p10, %r99, %r2;
	@%p10 bra 	BB1_18;

	ld.local.u16 	%r100, [%rd5+2];
	xor.b32  	%r101, %r100, %r157;
	and.b32  	%r102, %r101, 255;
	mul.wide.u32 	%rd48, %r102, 4;
	mov.u64 	%rd49, crc32tab;
	add.s64 	%rd50, %rd49, %rd48;
	ld.const.u32 	%r103, [%rd50];
	shr.u32 	%r104, %r157, 8;
	xor.b32  	%r157, %r103, %r104;

BB1_18:
	add.s32 	%r156, %r156, 4;
	setp.gt.u32	%p11, %r156, %r2;
	@%p11 bra 	BB1_20;

	ld.local.u8 	%r105, [%rd5+3];
	and.b32  	%r106, %r157, 255;
	xor.b32  	%r107, %r105, %r106;
	mul.wide.u32 	%rd51, %r107, 4;
	mov.u64 	%rd52, crc32tab;
	add.s64 	%rd53, %rd52, %rd51;
	ld.const.u32 	%r108, [%rd53];
	shr.u32 	%r109, %r157, 8;
	xor.b32  	%r157, %r108, %r109;

BB1_20:
	add.s32 	%r155, %r155, 1;
	setp.lt.u32	%p12, %r156, %r2;
	@%p12 bra 	BB1_12;

BB1_21:
	not.b32 	%r35, %r157;
	shr.u32 	%r110, %r35, %r8;
	and.b32  	%r111, %r110, %r50;
	mul.wide.u32 	%rd54, %r111, 4;
	add.s64 	%rd55, %rd9, %rd54;
	and.b32  	%r112, %r35, 31;
	mov.u32 	%r113, 1;
	shl.b32 	%r36, %r113, %r112;
	ld.global.u32 	%r114, [%rd55];
	and.b32  	%r115, %r114, %r36;
	setp.eq.s32	%p13, %r115, 0;
	@%p13 bra 	BB1_42;

	ld.global.u32 	%r116, [%rd10];
	and.b32  	%r117, %r116, 1;
	setp.eq.b32	%p14, %r117, 1;
	@!%p14 bra 	BB1_42;
	bra.uni 	BB1_23;

BB1_23:
	ld.global.u32 	%r118, [%rd11];
	and.b32  	%r119, %r118, 1;
	setp.eq.b32	%p15, %r119, 1;
	@!%p15 bra 	BB1_42;
	bra.uni 	BB1_24;

BB1_24:
	ld.global.u32 	%r120, [%rd12];
	and.b32  	%r121, %r120, 1;
	setp.eq.b32	%p16, %r121, 1;
	@!%p16 bra 	BB1_42;
	bra.uni 	BB1_25;

BB1_25:
	shr.u32 	%r122, %r35, %r9;
	and.b32  	%r123, %r122, %r50;
	mul.wide.u32 	%rd56, %r123, 4;
	add.s64 	%rd57, %rd13, %rd56;
	ld.global.u32 	%r124, [%rd57];
	and.b32  	%r125, %r124, %r36;
	setp.eq.s32	%p17, %r125, 0;
	@%p17 bra 	BB1_42;

	ld.global.u32 	%r126, [%rd14];
	and.b32  	%r127, %r126, 1;
	setp.eq.b32	%p18, %r127, 1;
	@!%p18 bra 	BB1_42;
	bra.uni 	BB1_27;

BB1_27:
	ld.global.u32 	%r128, [%rd15];
	and.b32  	%r129, %r128, 1;
	setp.eq.b32	%p19, %r129, 1;
	@!%p19 bra 	BB1_42;
	bra.uni 	BB1_28;

BB1_28:
	ld.global.u32 	%r130, [%rd16];
	and.b32  	%r131, %r130, 1;
	setp.eq.b32	%p20, %r131, 1;
	@!%p20 bra 	BB1_42;
	bra.uni 	BB1_29;

BB1_29:
	setp.eq.s32	%p21, %r55, 0;
	mov.u32 	%r163, 0;
	mov.u32 	%r165, -1;
	mov.u32 	%r162, %r55;
	@%p21 bra 	BB1_37;

BB1_30:
	shr.u32 	%r39, %r162, 1;
	add.s32 	%r165, %r39, %r163;
	cvt.u64.u32	%rd58, %r165;
	add.s64 	%rd59, %rd58, %rd4;
	shl.b64 	%rd60, %rd59, 4;
	add.s64 	%rd6, %rd18, %rd60;
	ld.global.u32 	%r135, [%rd6+12];
	mov.u32 	%r164, -1;
	setp.ne.s32	%p22, %r135, 0;
	@%p22 bra 	BB1_35;

	ld.global.u32 	%r137, [%rd6+8];
	setp.ne.s32	%p23, %r137, 0;
	@%p23 bra 	BB1_35;

	ld.global.u32 	%r139, [%rd6+4];
	setp.ne.s32	%p24, %r139, 0;
	@%p24 bra 	BB1_35;

	mov.u32 	%r164, 1;
	ld.global.u32 	%r41, [%rd6];
	setp.lt.u32	%p25, %r41, %r35;
	@%p25 bra 	BB1_35;

	setp.gt.u32	%p26, %r41, %r35;
	selp.b32	%r164, -1, 0, %p26;

BB1_35:
	add.s32 	%r141, %r39, 1;
	setp.gt.s32	%p27, %r164, 0;
	selp.b32	%r142, %r141, 0, %p27;
	add.s32 	%r163, %r142, %r163;
	selp.b32	%r143, -1, 0, %p27;
	add.s32 	%r144, %r143, %r162;
	shr.u32 	%r162, %r144, 1;
	setp.eq.s32	%p28, %r164, 0;
	@%p28 bra 	BB1_37;

	mov.u32 	%r165, -1;
	setp.ne.s32	%p29, %r162, 0;
	@%p29 bra 	BB1_30;

BB1_37:
	setp.eq.s32	%p30, %r165, -1;
	@%p30 bra 	BB1_42;

	add.s32 	%r47, %r165, %r56;
	mul.wide.u32 	%rd61, %r47, 4;
	add.s64 	%rd62, %rd19, %rd61;
	atom.global.add.u32 	%r146, [%rd62], 1;
	setp.ne.s32	%p31, %r146, 0;
	@%p31 bra 	BB1_42;

	atom.global.add.u32 	%r48, [%rd21], 1;
	setp.lt.u32	%p32, %r48, %r55;
	@%p32 bra 	BB1_41;
	bra.uni 	BB1_40;

BB1_41:
	ld.param.u32 	%r148, [m11500_m04_param_27];
	mul.wide.u32 	%rd63, %r48, 20;
	add.s64 	%rd64, %rd17, %rd63;
	st.global.u32 	[%rd64], %r148;
	st.global.u32 	[%rd64+4], %r165;
	st.global.u32 	[%rd64+8], %r47;
	st.global.u32 	[%rd64+12], %r1;
	st.global.u32 	[%rd64+16], %r150;
	bra.uni 	BB1_42;

BB1_40:
	atom.global.add.u32 	%r147, [%rd21], -1;

BB1_42:
	add.s32 	%r150, %r150, 1;
	setp.lt.u32	%p33, %r150, %r54;
	@%p33 bra 	BB1_3;

BB1_43:
	ret;
}

	// .globl	m11500_m08
.entry m11500_m08(
	.param .u64 .ptr .global .align 4 m11500_m08_param_0,
	.param .u64 .ptr .global .align 4 m11500_m08_param_1,
	.param .u64 .ptr .global .align 4 m11500_m08_param_2,
	.param .u64 .ptr .const .align 4 m11500_m08_param_3,
	.param .u64 .ptr .global .align 1 m11500_m08_param_4,
	.param .u64 .ptr .global .align 1 m11500_m08_param_5,
	.param .u64 .ptr .global .align 4 m11500_m08_param_6,
	.param .u64 .ptr .global .align 4 m11500_m08_param_7,
	.param .u64 .ptr .global .align 4 m11500_m08_param_8,
	.param .u64 .ptr .global .align 4 m11500_m08_param_9,
	.param .u64 .ptr .global .align 4 m11500_m08_param_10,
	.param .u64 .ptr .global .align 4 m11500_m08_param_11,
	.param .u64 .ptr .global .align 4 m11500_m08_param_12,
	.param .u64 .ptr .global .align 4 m11500_m08_param_13,
	.param .u64 .ptr .global .align 4 m11500_m08_param_14,
	.param .u64 .ptr .global .align 4 m11500_m08_param_15,
	.param .u64 .ptr .global .align 4 m11500_m08_param_16,
	.param .u64 .ptr .global .align 4 m11500_m08_param_17,
	.param .u64 .ptr .global .align 1 m11500_m08_param_18,
	.param .u64 .ptr .global .align 4 m11500_m08_param_19,
	.param .u64 .ptr .global .align 4 m11500_m08_param_20,
	.param .u64 .ptr .global .align 4 m11500_m08_param_21,
	.param .u64 .ptr .global .align 4 m11500_m08_param_22,
	.param .u64 .ptr .global .align 4 m11500_m08_param_23,
	.param .u32 m11500_m08_param_24,
	.param .u32 m11500_m08_param_25,
	.param .u32 m11500_m08_param_26,
	.param .u32 m11500_m08_param_27,
	.param .u32 m11500_m08_param_28,
	.param .u32 m11500_m08_param_29,
	.param .u32 m11500_m08_param_30,
	.param .u32 m11500_m08_param_31,
	.param .u32 m11500_m08_param_32,
	.param .u32 m11500_m08_param_33,
	.param .u64 m11500_m08_param_34
)
{
	.local .align 16 .b8 	__local_depot2[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<34>;
	.reg .b32 	%r<172>;
	.reg .b64 	%rd<67>;


	mov.u64 	%rd66, __local_depot2;
	cvta.local.u64 	%SP, %rd66;
	ld.param.u64 	%rd7, [m11500_m08_param_0];
	ld.param.u64 	%rd9, [m11500_m08_param_6];
	ld.param.u64 	%rd10, [m11500_m08_param_7];
	ld.param.u64 	%rd11, [m11500_m08_param_8];
	ld.param.u64 	%rd12, [m11500_m08_param_9];
	ld.param.u64 	%rd13, [m11500_m08_param_10];
	ld.param.u64 	%rd14, [m11500_m08_param_11];
	ld.param.u64 	%rd15, [m11500_m08_param_12];
	ld.param.u64 	%rd16, [m11500_m08_param_13];
	ld.param.u64 	%rd17, [m11500_m08_param_14];
	ld.param.u64 	%rd18, [m11500_m08_param_15];
	ld.param.u64 	%rd19, [m11500_m08_param_16];
	ld.param.u64 	%rd20, [m11500_m08_param_17];
	ld.param.u64 	%rd21, [m11500_m08_param_19];
	ld.param.u32 	%r54, [m11500_m08_param_24];
	ld.param.u32 	%r55, [m11500_m08_param_25];
	ld.param.u32 	%r56, [m11500_m08_param_26];
	ld.param.u32 	%r57, [m11500_m08_param_27];
	ld.param.u32 	%r58, [m11500_m08_param_30];
	ld.param.u32 	%r59, [m11500_m08_param_31];
	ld.param.u32 	%r60, [m11500_m08_param_32];
	ld.param.u64 	%rd22, [m11500_m08_param_34];
	add.u64 	%rd23, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd23;
	mov.u32 	%r61, %ctaid.x;
	mov.u32 	%r62, %ntid.x;
	mov.b32	%r63, %envreg3;
	mad.lo.s32 	%r64, %r61, %r62, %r63;
	mov.u32 	%r65, %tid.x;
	add.s32 	%r1, %r64, %r65;
	cvt.s64.s32	%rd2, %r1;
	setp.ge.u64	%p1, %rd2, %rd22;
	@%p1 bra 	BB2_43;

	mul.lo.s64 	%rd24, %rd2, 260;
	add.s64 	%rd3, %rd7, %rd24;
	ld.global.u32 	%r2, [%rd3+256];
	setp.eq.s32	%p2, %r58, 0;
	@%p2 bra 	BB2_43;

	ld.global.u32 	%r3, [%rd3];
	ld.global.u32 	%r4, [%rd3+4];
	ld.global.u32 	%r5, [%rd3+8];
	ld.global.u32 	%r6, [%rd3+12];
	ld.global.u32 	%r7, [%rd3+16];
	ld.global.u32 	%r8, [%rd3+20];
	ld.global.u32 	%r9, [%rd3+24];
	ld.global.u32 	%r10, [%rd3+28];
	mul.wide.u32 	%rd25, %r57, 564;
	add.s64 	%rd26, %rd20, %rd25;
	ld.global.u32 	%r67, [%rd26];
	not.b32 	%r11, %r67;
	and.b32  	%r12, %r55, 31;
	and.b32  	%r13, %r56, 31;
	cvt.u64.u32	%rd4, %r60;
	shr.u32 	%r14, %r11, 8;
	mov.u32 	%r156, 0;

BB2_3:
	mov.u32 	%r154, 0;
	ld.param.u64 	%rd65, [m11500_m08_param_3];
	mul.wide.u32 	%rd27, %r156, 4;
	add.s64 	%rd28, %rd65, %rd27;
	ld.const.u32 	%r68, [%rd28];
	or.b32  	%r16, %r68, %r3;
	st.local.v4.u32 	[%rd1], {%r16, %r4, %r5, %r6};
	st.local.v4.u32 	[%rd1+16], {%r7, %r8, %r9, %r10};
	st.local.v4.u32 	[%rd1+32], {%r154, %r154, %r154, %r154};
	st.local.v4.u32 	[%rd1+48], {%r154, %r154, %r154, %r154};
	setp.eq.s32	%p3, %r2, 0;
	mov.u32 	%r163, %r11;
	@%p3 bra 	BB2_5;

	xor.b32  	%r70, %r16, %r11;
	and.b32  	%r71, %r70, 255;
	mul.wide.u32 	%rd29, %r71, 4;
	mov.u64 	%rd30, crc32tab;
	add.s64 	%rd31, %rd30, %rd29;
	ld.const.u32 	%r72, [%rd31];
	xor.b32  	%r163, %r72, %r14;

BB2_5:
	setp.lt.u32	%p4, %r2, 2;
	@%p4 bra 	BB2_7;

	shr.u32 	%r73, %r16, 8;
	xor.b32  	%r74, %r73, %r163;
	and.b32  	%r75, %r74, 255;
	mul.wide.u32 	%rd32, %r75, 4;
	mov.u64 	%rd33, crc32tab;
	add.s64 	%rd34, %rd33, %rd32;
	ld.const.u32 	%r76, [%rd34];
	shr.u32 	%r77, %r163, 8;
	xor.b32  	%r163, %r76, %r77;

BB2_7:
	setp.lt.u32	%p5, %r2, 3;
	@%p5 bra 	BB2_9;

	shr.u32 	%r78, %r16, 16;
	xor.b32  	%r79, %r78, %r163;
	and.b32  	%r80, %r79, 255;
	mul.wide.u32 	%rd35, %r80, 4;
	mov.u64 	%rd36, crc32tab;
	add.s64 	%rd37, %rd36, %rd35;
	ld.const.u32 	%r81, [%rd37];
	shr.u32 	%r82, %r163, 8;
	xor.b32  	%r163, %r81, %r82;

BB2_9:
	setp.lt.u32	%p6, %r2, 4;
	@%p6 bra 	BB2_11;

	shr.u32 	%r83, %r16, 24;
	and.b32  	%r84, %r163, 255;
	xor.b32  	%r85, %r83, %r84;
	mul.wide.u32 	%rd38, %r85, 4;
	mov.u64 	%rd39, crc32tab;
	add.s64 	%rd40, %rd39, %rd38;
	ld.const.u32 	%r86, [%rd40];
	shr.u32 	%r87, %r163, 8;
	xor.b32  	%r163, %r86, %r87;

BB2_11:
	mov.u32 	%r162, 4;
	mov.u32 	%r161, 1;
	setp.lt.u32	%p7, %r2, 5;
	@%p7 bra 	BB2_21;

BB2_12:
	add.s32 	%r90, %r162, 1;
	setp.gt.u32	%p8, %r90, %r2;
	mul.wide.u32 	%rd41, %r161, 4;
	add.s64 	%rd5, %rd1, %rd41;
	@%p8 bra 	BB2_14;

	ld.local.u32 	%r91, [%rd5];
	xor.b32  	%r92, %r91, %r163;
	and.b32  	%r93, %r92, 255;
	mul.wide.u32 	%rd42, %r93, 4;
	mov.u64 	%rd43, crc32tab;
	add.s64 	%rd44, %rd43, %rd42;
	ld.const.u32 	%r94, [%rd44];
	shr.u32 	%r95, %r163, 8;
	xor.b32  	%r163, %r94, %r95;

BB2_14:
	add.s32 	%r96, %r162, 2;
	setp.gt.u32	%p9, %r96, %r2;
	@%p9 bra 	BB2_16;

	ld.local.u32 	%r97, [%rd5];
	shr.u32 	%r98, %r97, 8;
	xor.b32  	%r99, %r98, %r163;
	and.b32  	%r100, %r99, 255;
	mul.wide.u32 	%rd45, %r100, 4;
	mov.u64 	%rd46, crc32tab;
	add.s64 	%rd47, %rd46, %rd45;
	ld.const.u32 	%r101, [%rd47];
	shr.u32 	%r102, %r163, 8;
	xor.b32  	%r163, %r101, %r102;

BB2_16:
	add.s32 	%r103, %r162, 3;
	setp.gt.u32	%p10, %r103, %r2;
	@%p10 bra 	BB2_18;

	ld.local.u16 	%r104, [%rd5+2];
	xor.b32  	%r105, %r104, %r163;
	and.b32  	%r106, %r105, 255;
	mul.wide.u32 	%rd48, %r106, 4;
	mov.u64 	%rd49, crc32tab;
	add.s64 	%rd50, %rd49, %rd48;
	ld.const.u32 	%r107, [%rd50];
	shr.u32 	%r108, %r163, 8;
	xor.b32  	%r163, %r107, %r108;

BB2_18:
	add.s32 	%r162, %r162, 4;
	setp.gt.u32	%p11, %r162, %r2;
	@%p11 bra 	BB2_20;

	ld.local.u8 	%r109, [%rd5+3];
	and.b32  	%r110, %r163, 255;
	xor.b32  	%r111, %r109, %r110;
	mul.wide.u32 	%rd51, %r111, 4;
	mov.u64 	%rd52, crc32tab;
	add.s64 	%rd53, %rd52, %rd51;
	ld.const.u32 	%r112, [%rd53];
	shr.u32 	%r113, %r163, 8;
	xor.b32  	%r163, %r112, %r113;

BB2_20:
	add.s32 	%r161, %r161, 1;
	setp.lt.u32	%p12, %r162, %r2;
	@%p12 bra 	BB2_12;

BB2_21:
	not.b32 	%r39, %r163;
	shr.u32 	%r114, %r39, %r12;
	and.b32  	%r115, %r114, %r54;
	mul.wide.u32 	%rd54, %r115, 4;
	add.s64 	%rd55, %rd9, %rd54;
	and.b32  	%r116, %r39, 31;
	mov.u32 	%r117, 1;
	shl.b32 	%r40, %r117, %r116;
	ld.global.u32 	%r118, [%rd55];
	and.b32  	%r119, %r118, %r40;
	setp.eq.s32	%p13, %r119, 0;
	@%p13 bra 	BB2_42;

	ld.global.u32 	%r120, [%rd10];
	and.b32  	%r121, %r120, 1;
	setp.eq.b32	%p14, %r121, 1;
	@!%p14 bra 	BB2_42;
	bra.uni 	BB2_23;

BB2_23:
	ld.global.u32 	%r122, [%rd11];
	and.b32  	%r123, %r122, 1;
	setp.eq.b32	%p15, %r123, 1;
	@!%p15 bra 	BB2_42;
	bra.uni 	BB2_24;

BB2_24:
	ld.global.u32 	%r124, [%rd12];
	and.b32  	%r125, %r124, 1;
	setp.eq.b32	%p16, %r125, 1;
	@!%p16 bra 	BB2_42;
	bra.uni 	BB2_25;

BB2_25:
	shr.u32 	%r126, %r39, %r13;
	and.b32  	%r127, %r126, %r54;
	mul.wide.u32 	%rd56, %r127, 4;
	add.s64 	%rd57, %rd13, %rd56;
	ld.global.u32 	%r128, [%rd57];
	and.b32  	%r129, %r128, %r40;
	setp.eq.s32	%p17, %r129, 0;
	@%p17 bra 	BB2_42;

	ld.global.u32 	%r130, [%rd14];
	and.b32  	%r131, %r130, 1;
	setp.eq.b32	%p18, %r131, 1;
	@!%p18 bra 	BB2_42;
	bra.uni 	BB2_27;

BB2_27:
	ld.global.u32 	%r132, [%rd15];
	and.b32  	%r133, %r132, 1;
	setp.eq.b32	%p19, %r133, 1;
	@!%p19 bra 	BB2_42;
	bra.uni 	BB2_28;

BB2_28:
	ld.global.u32 	%r134, [%rd16];
	and.b32  	%r135, %r134, 1;
	setp.eq.b32	%p20, %r135, 1;
	@!%p20 bra 	BB2_42;
	bra.uni 	BB2_29;

BB2_29:
	setp.eq.s32	%p21, %r59, 0;
	mov.u32 	%r169, 0;
	mov.u32 	%r171, -1;
	mov.u32 	%r168, %r59;
	@%p21 bra 	BB2_37;

BB2_30:
	shr.u32 	%r43, %r168, 1;
	add.s32 	%r171, %r43, %r169;
	cvt.u64.u32	%rd58, %r171;
	add.s64 	%rd59, %rd58, %rd4;
	shl.b64 	%rd60, %rd59, 4;
	add.s64 	%rd6, %rd18, %rd60;
	ld.global.u32 	%r139, [%rd6+12];
	mov.u32 	%r170, -1;
	setp.ne.s32	%p22, %r139, 0;
	@%p22 bra 	BB2_35;

	ld.global.u32 	%r141, [%rd6+8];
	setp.ne.s32	%p23, %r141, 0;
	@%p23 bra 	BB2_35;

	ld.global.u32 	%r143, [%rd6+4];
	setp.ne.s32	%p24, %r143, 0;
	@%p24 bra 	BB2_35;

	mov.u32 	%r170, 1;
	ld.global.u32 	%r45, [%rd6];
	setp.lt.u32	%p25, %r45, %r39;
	@%p25 bra 	BB2_35;

	setp.gt.u32	%p26, %r45, %r39;
	selp.b32	%r170, -1, 0, %p26;

BB2_35:
	add.s32 	%r145, %r43, 1;
	setp.gt.s32	%p27, %r170, 0;
	selp.b32	%r146, %r145, 0, %p27;
	add.s32 	%r169, %r146, %r169;
	selp.b32	%r147, -1, 0, %p27;
	add.s32 	%r148, %r147, %r168;
	shr.u32 	%r168, %r148, 1;
	setp.eq.s32	%p28, %r170, 0;
	@%p28 bra 	BB2_37;

	mov.u32 	%r171, -1;
	setp.ne.s32	%p29, %r168, 0;
	@%p29 bra 	BB2_30;

BB2_37:
	setp.eq.s32	%p30, %r171, -1;
	@%p30 bra 	BB2_42;

	ld.param.u32 	%r152, [m11500_m08_param_32];
	add.s32 	%r51, %r171, %r152;
	mul.wide.u32 	%rd61, %r51, 4;
	add.s64 	%rd62, %rd19, %rd61;
	atom.global.add.u32 	%r150, [%rd62], 1;
	setp.ne.s32	%p31, %r150, 0;
	@%p31 bra 	BB2_42;

	atom.global.add.u32 	%r52, [%rd21], 1;
	setp.lt.u32	%p32, %r52, %r59;
	@%p32 bra 	BB2_41;
	bra.uni 	BB2_40;

BB2_41:
	ld.param.u32 	%r153, [m11500_m08_param_27];
	mul.wide.u32 	%rd63, %r52, 20;
	add.s64 	%rd64, %rd17, %rd63;
	st.global.u32 	[%rd64], %r153;
	st.global.u32 	[%rd64+4], %r171;
	st.global.u32 	[%rd64+8], %r51;
	st.global.u32 	[%rd64+12], %r1;
	st.global.u32 	[%rd64+16], %r156;
	bra.uni 	BB2_42;

BB2_40:
	atom.global.add.u32 	%r151, [%rd21], -1;

BB2_42:
	add.s32 	%r156, %r156, 1;
	setp.lt.u32	%p33, %r156, %r58;
	@%p33 bra 	BB2_3;

BB2_43:
	ret;
}

	// .globl	m11500_m16
.entry m11500_m16(
	.param .u64 .ptr .global .align 4 m11500_m16_param_0,
	.param .u64 .ptr .global .align 4 m11500_m16_param_1,
	.param .u64 .ptr .global .align 4 m11500_m16_param_2,
	.param .u64 .ptr .const .align 4 m11500_m16_param_3,
	.param .u64 .ptr .global .align 1 m11500_m16_param_4,
	.param .u64 .ptr .global .align 1 m11500_m16_param_5,
	.param .u64 .ptr .global .align 4 m11500_m16_param_6,
	.param .u64 .ptr .global .align 4 m11500_m16_param_7,
	.param .u64 .ptr .global .align 4 m11500_m16_param_8,
	.param .u64 .ptr .global .align 4 m11500_m16_param_9,
	.param .u64 .ptr .global .align 4 m11500_m16_param_10,
	.param .u64 .ptr .global .align 4 m11500_m16_param_11,
	.param .u64 .ptr .global .align 4 m11500_m16_param_12,
	.param .u64 .ptr .global .align 4 m11500_m16_param_13,
	.param .u64 .ptr .global .align 4 m11500_m16_param_14,
	.param .u64 .ptr .global .align 4 m11500_m16_param_15,
	.param .u64 .ptr .global .align 4 m11500_m16_param_16,
	.param .u64 .ptr .global .align 4 m11500_m16_param_17,
	.param .u64 .ptr .global .align 1 m11500_m16_param_18,
	.param .u64 .ptr .global .align 4 m11500_m16_param_19,
	.param .u64 .ptr .global .align 4 m11500_m16_param_20,
	.param .u64 .ptr .global .align 4 m11500_m16_param_21,
	.param .u64 .ptr .global .align 4 m11500_m16_param_22,
	.param .u64 .ptr .global .align 4 m11500_m16_param_23,
	.param .u32 m11500_m16_param_24,
	.param .u32 m11500_m16_param_25,
	.param .u32 m11500_m16_param_26,
	.param .u32 m11500_m16_param_27,
	.param .u32 m11500_m16_param_28,
	.param .u32 m11500_m16_param_29,
	.param .u32 m11500_m16_param_30,
	.param .u32 m11500_m16_param_31,
	.param .u32 m11500_m16_param_32,
	.param .u32 m11500_m16_param_33,
	.param .u64 m11500_m16_param_34
)
{
	.local .align 16 .b8 	__local_depot3[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<34>;
	.reg .b32 	%r<176>;
	.reg .b64 	%rd<75>;


	mov.u64 	%rd74, __local_depot3;
	cvta.local.u64 	%SP, %rd74;
	ld.param.u64 	%rd7, [m11500_m16_param_0];
	ld.param.u64 	%rd17, [m11500_m16_param_14];
	ld.param.u64 	%rd18, [m11500_m16_param_15];
	ld.param.u64 	%rd19, [m11500_m16_param_16];
	ld.param.u64 	%rd20, [m11500_m16_param_17];
	ld.param.u64 	%rd21, [m11500_m16_param_19];
	ld.param.u32 	%r62, [m11500_m16_param_24];
	ld.param.u32 	%r63, [m11500_m16_param_25];
	ld.param.u32 	%r64, [m11500_m16_param_26];
	ld.param.u32 	%r65, [m11500_m16_param_27];
	ld.param.u32 	%r66, [m11500_m16_param_30];
	ld.param.u32 	%r67, [m11500_m16_param_31];
	ld.param.u32 	%r68, [m11500_m16_param_32];
	ld.param.u64 	%rd22, [m11500_m16_param_34];
	add.u64 	%rd23, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd23;
	mov.u32 	%r69, %ctaid.x;
	mov.u32 	%r70, %ntid.x;
	mov.b32	%r71, %envreg3;
	mad.lo.s32 	%r72, %r69, %r70, %r71;
	mov.u32 	%r73, %tid.x;
	add.s32 	%r1, %r72, %r73;
	cvt.s64.s32	%rd2, %r1;
	setp.ge.u64	%p1, %rd2, %rd22;
	@%p1 bra 	BB3_43;

	mul.lo.s64 	%rd24, %rd2, 260;
	add.s64 	%rd3, %rd7, %rd24;
	ld.global.u32 	%r2, [%rd3+256];
	setp.eq.s32	%p2, %r66, 0;
	@%p2 bra 	BB3_43;

	ld.global.u32 	%r3, [%rd3];
	ld.global.u32 	%r4, [%rd3+4];
	ld.global.u32 	%r5, [%rd3+8];
	ld.global.u32 	%r6, [%rd3+12];
	ld.global.u32 	%r7, [%rd3+16];
	ld.global.u32 	%r8, [%rd3+20];
	ld.global.u32 	%r9, [%rd3+24];
	ld.global.u32 	%r10, [%rd3+28];
	ld.global.u32 	%r11, [%rd3+32];
	ld.global.u32 	%r12, [%rd3+36];
	ld.global.u32 	%r13, [%rd3+40];
	ld.global.u32 	%r14, [%rd3+44];
	ld.global.u32 	%r15, [%rd3+48];
	ld.global.u32 	%r16, [%rd3+52];
	ld.global.u32 	%r17, [%rd3+56];
	ld.global.u32 	%r18, [%rd3+60];
	mul.wide.u32 	%rd25, %r65, 564;
	add.s64 	%rd26, %rd20, %rd25;
	ld.global.u32 	%r75, [%rd26];
	not.b32 	%r19, %r75;
	and.b32  	%r20, %r63, 31;
	and.b32  	%r21, %r64, 31;
	cvt.u64.u32	%rd4, %r68;
	shr.u32 	%r22, %r19, 8;
	mov.u32 	%r160, 0;

BB3_3:
	ld.param.u64 	%rd71, [m11500_m16_param_3];
	mul.wide.u32 	%rd27, %r160, 4;
	add.s64 	%rd28, %rd71, %rd27;
	ld.const.u32 	%r76, [%rd28];
	or.b32  	%r24, %r76, %r3;
	st.local.v4.u32 	[%rd1], {%r24, %r4, %r5, %r6};
	st.local.v4.u32 	[%rd1+16], {%r7, %r8, %r9, %r10};
	st.local.v4.u32 	[%rd1+32], {%r11, %r12, %r13, %r14};
	st.local.v4.u32 	[%rd1+48], {%r15, %r16, %r17, %r18};
	setp.eq.s32	%p3, %r2, 0;
	mov.u32 	%r167, %r19;
	@%p3 bra 	BB3_5;

	xor.b32  	%r77, %r24, %r19;
	and.b32  	%r78, %r77, 255;
	mul.wide.u32 	%rd29, %r78, 4;
	mov.u64 	%rd30, crc32tab;
	add.s64 	%rd31, %rd30, %rd29;
	ld.const.u32 	%r79, [%rd31];
	xor.b32  	%r167, %r79, %r22;

BB3_5:
	setp.lt.u32	%p4, %r2, 2;
	@%p4 bra 	BB3_7;

	shr.u32 	%r80, %r24, 8;
	xor.b32  	%r81, %r80, %r167;
	and.b32  	%r82, %r81, 255;
	mul.wide.u32 	%rd32, %r82, 4;
	mov.u64 	%rd33, crc32tab;
	add.s64 	%rd34, %rd33, %rd32;
	ld.const.u32 	%r83, [%rd34];
	shr.u32 	%r84, %r167, 8;
	xor.b32  	%r167, %r83, %r84;

BB3_7:
	setp.lt.u32	%p5, %r2, 3;
	@%p5 bra 	BB3_9;

	shr.u32 	%r85, %r24, 16;
	xor.b32  	%r86, %r85, %r167;
	and.b32  	%r87, %r86, 255;
	mul.wide.u32 	%rd35, %r87, 4;
	mov.u64 	%rd36, crc32tab;
	add.s64 	%rd37, %rd36, %rd35;
	ld.const.u32 	%r88, [%rd37];
	shr.u32 	%r89, %r167, 8;
	xor.b32  	%r167, %r88, %r89;

BB3_9:
	setp.lt.u32	%p6, %r2, 4;
	@%p6 bra 	BB3_11;

	shr.u32 	%r90, %r24, 24;
	and.b32  	%r91, %r167, 255;
	xor.b32  	%r92, %r90, %r91;
	mul.wide.u32 	%rd38, %r92, 4;
	mov.u64 	%rd39, crc32tab;
	add.s64 	%rd40, %rd39, %rd38;
	ld.const.u32 	%r93, [%rd40];
	shr.u32 	%r94, %r167, 8;
	xor.b32  	%r167, %r93, %r94;

BB3_11:
	mov.u32 	%r166, 4;
	mov.u32 	%r165, 1;
	setp.lt.u32	%p7, %r2, 5;
	@%p7 bra 	BB3_21;

BB3_12:
	add.s32 	%r97, %r166, 1;
	setp.gt.u32	%p8, %r97, %r2;
	mul.wide.u32 	%rd41, %r165, 4;
	add.s64 	%rd5, %rd1, %rd41;
	@%p8 bra 	BB3_14;

	ld.local.u32 	%r98, [%rd5];
	xor.b32  	%r99, %r98, %r167;
	and.b32  	%r100, %r99, 255;
	mul.wide.u32 	%rd42, %r100, 4;
	mov.u64 	%rd43, crc32tab;
	add.s64 	%rd44, %rd43, %rd42;
	ld.const.u32 	%r101, [%rd44];
	shr.u32 	%r102, %r167, 8;
	xor.b32  	%r167, %r101, %r102;

BB3_14:
	add.s32 	%r103, %r166, 2;
	setp.gt.u32	%p9, %r103, %r2;
	@%p9 bra 	BB3_16;

	ld.local.u32 	%r104, [%rd5];
	shr.u32 	%r105, %r104, 8;
	xor.b32  	%r106, %r105, %r167;
	and.b32  	%r107, %r106, 255;
	mul.wide.u32 	%rd45, %r107, 4;
	mov.u64 	%rd46, crc32tab;
	add.s64 	%rd47, %rd46, %rd45;
	ld.const.u32 	%r108, [%rd47];
	shr.u32 	%r109, %r167, 8;
	xor.b32  	%r167, %r108, %r109;

BB3_16:
	add.s32 	%r110, %r166, 3;
	setp.gt.u32	%p10, %r110, %r2;
	@%p10 bra 	BB3_18;

	ld.local.u16 	%r111, [%rd5+2];
	xor.b32  	%r112, %r111, %r167;
	and.b32  	%r113, %r112, 255;
	mul.wide.u32 	%rd48, %r113, 4;
	mov.u64 	%rd49, crc32tab;
	add.s64 	%rd50, %rd49, %rd48;
	ld.const.u32 	%r114, [%rd50];
	shr.u32 	%r115, %r167, 8;
	xor.b32  	%r167, %r114, %r115;

BB3_18:
	add.s32 	%r166, %r166, 4;
	setp.gt.u32	%p11, %r166, %r2;
	@%p11 bra 	BB3_20;

	ld.local.u8 	%r116, [%rd5+3];
	and.b32  	%r117, %r167, 255;
	xor.b32  	%r118, %r116, %r117;
	mul.wide.u32 	%rd51, %r118, 4;
	mov.u64 	%rd52, crc32tab;
	add.s64 	%rd53, %rd52, %rd51;
	ld.const.u32 	%r119, [%rd53];
	shr.u32 	%r120, %r167, 8;
	xor.b32  	%r167, %r119, %r120;

BB3_20:
	add.s32 	%r165, %r165, 1;
	setp.lt.u32	%p12, %r166, %r2;
	@%p12 bra 	BB3_12;

BB3_21:
	ld.param.u64 	%rd65, [m11500_m16_param_6];
	not.b32 	%r47, %r167;
	shr.u32 	%r121, %r47, %r20;
	and.b32  	%r122, %r121, %r62;
	mul.wide.u32 	%rd54, %r122, 4;
	add.s64 	%rd55, %rd65, %rd54;
	and.b32  	%r123, %r47, 31;
	mov.u32 	%r124, 1;
	shl.b32 	%r48, %r124, %r123;
	ld.global.u32 	%r125, [%rd55];
	and.b32  	%r126, %r125, %r48;
	setp.eq.s32	%p13, %r126, 0;
	@%p13 bra 	BB3_42;

	ld.param.u64 	%rd66, [m11500_m16_param_7];
	ld.global.u32 	%r127, [%rd66];
	and.b32  	%r128, %r127, 1;
	setp.eq.b32	%p14, %r128, 1;
	@!%p14 bra 	BB3_42;
	bra.uni 	BB3_23;

BB3_23:
	ld.param.u64 	%rd67, [m11500_m16_param_8];
	ld.global.u32 	%r129, [%rd67];
	and.b32  	%r130, %r129, 1;
	setp.eq.b32	%p15, %r130, 1;
	@!%p15 bra 	BB3_42;
	bra.uni 	BB3_24;

BB3_24:
	ld.param.u64 	%rd68, [m11500_m16_param_9];
	ld.global.u32 	%r131, [%rd68];
	and.b32  	%r132, %r131, 1;
	setp.eq.b32	%p16, %r132, 1;
	@!%p16 bra 	BB3_42;
	bra.uni 	BB3_25;

BB3_25:
	ld.param.u64 	%rd69, [m11500_m16_param_10];
	shr.u32 	%r133, %r47, %r21;
	and.b32  	%r134, %r133, %r62;
	mul.wide.u32 	%rd56, %r134, 4;
	add.s64 	%rd57, %rd69, %rd56;
	ld.global.u32 	%r135, [%rd57];
	and.b32  	%r136, %r135, %r48;
	setp.eq.s32	%p17, %r136, 0;
	@%p17 bra 	BB3_42;

	ld.param.u64 	%rd70, [m11500_m16_param_11];
	ld.global.u32 	%r137, [%rd70];
	and.b32  	%r138, %r137, 1;
	setp.eq.b32	%p18, %r138, 1;
	@!%p18 bra 	BB3_42;
	bra.uni 	BB3_27;

BB3_27:
	ld.param.u64 	%rd72, [m11500_m16_param_12];
	ld.global.u32 	%r139, [%rd72];
	and.b32  	%r140, %r139, 1;
	setp.eq.b32	%p19, %r140, 1;
	@!%p19 bra 	BB3_42;
	bra.uni 	BB3_28;

BB3_28:
	ld.param.u64 	%rd73, [m11500_m16_param_13];
	ld.global.u32 	%r141, [%rd73];
	and.b32  	%r142, %r141, 1;
	setp.eq.b32	%p20, %r142, 1;
	@!%p20 bra 	BB3_42;
	bra.uni 	BB3_29;

BB3_29:
	setp.eq.s32	%p21, %r67, 0;
	mov.u32 	%r173, 0;
	mov.u32 	%r175, -1;
	mov.u32 	%r172, %r67;
	@%p21 bra 	BB3_37;

BB3_30:
	shr.u32 	%r51, %r172, 1;
	add.s32 	%r175, %r51, %r173;
	cvt.u64.u32	%rd58, %r175;
	add.s64 	%rd59, %rd58, %rd4;
	shl.b64 	%rd60, %rd59, 4;
	add.s64 	%rd6, %rd18, %rd60;
	ld.global.u32 	%r146, [%rd6+12];
	mov.u32 	%r174, -1;
	setp.ne.s32	%p22, %r146, 0;
	@%p22 bra 	BB3_35;

	ld.global.u32 	%r148, [%rd6+8];
	setp.ne.s32	%p23, %r148, 0;
	@%p23 bra 	BB3_35;

	ld.global.u32 	%r150, [%rd6+4];
	setp.ne.s32	%p24, %r150, 0;
	@%p24 bra 	BB3_35;

	mov.u32 	%r174, 1;
	ld.global.u32 	%r53, [%rd6];
	setp.lt.u32	%p25, %r53, %r47;
	@%p25 bra 	BB3_35;

	setp.gt.u32	%p26, %r53, %r47;
	selp.b32	%r174, -1, 0, %p26;

BB3_35:
	add.s32 	%r152, %r51, 1;
	setp.gt.s32	%p27, %r174, 0;
	selp.b32	%r153, %r152, 0, %p27;
	add.s32 	%r173, %r153, %r173;
	selp.b32	%r154, -1, 0, %p27;
	add.s32 	%r155, %r154, %r172;
	shr.u32 	%r172, %r155, 1;
	setp.eq.s32	%p28, %r174, 0;
	@%p28 bra 	BB3_37;

	mov.u32 	%r175, -1;
	setp.ne.s32	%p29, %r172, 0;
	@%p29 bra 	BB3_30;

BB3_37:
	setp.eq.s32	%p30, %r175, -1;
	@%p30 bra 	BB3_42;

	add.s32 	%r59, %r175, %r68;
	mul.wide.u32 	%rd61, %r59, 4;
	add.s64 	%rd62, %rd19, %rd61;
	atom.global.add.u32 	%r157, [%rd62], 1;
	setp.ne.s32	%p31, %r157, 0;
	@%p31 bra 	BB3_42;

	atom.global.add.u32 	%r60, [%rd21], 1;
	setp.lt.u32	%p32, %r60, %r67;
	@%p32 bra 	BB3_41;
	bra.uni 	BB3_40;

BB3_41:
	mul.wide.u32 	%rd63, %r60, 20;
	add.s64 	%rd64, %rd17, %rd63;
	st.global.u32 	[%rd64], %r65;
	st.global.u32 	[%rd64+4], %r175;
	st.global.u32 	[%rd64+8], %r59;
	st.global.u32 	[%rd64+12], %r1;
	st.global.u32 	[%rd64+16], %r160;
	bra.uni 	BB3_42;

BB3_40:
	atom.global.add.u32 	%r158, [%rd21], -1;

BB3_42:
	add.s32 	%r160, %r160, 1;
	setp.lt.u32	%p33, %r160, %r66;
	@%p33 bra 	BB3_3;

BB3_43:
	ret;
}

	// .globl	m11500_s04
.entry m11500_s04(
	.param .u64 .ptr .global .align 4 m11500_s04_param_0,
	.param .u64 .ptr .global .align 4 m11500_s04_param_1,
	.param .u64 .ptr .global .align 4 m11500_s04_param_2,
	.param .u64 .ptr .const .align 4 m11500_s04_param_3,
	.param .u64 .ptr .global .align 1 m11500_s04_param_4,
	.param .u64 .ptr .global .align 1 m11500_s04_param_5,
	.param .u64 .ptr .global .align 4 m11500_s04_param_6,
	.param .u64 .ptr .global .align 4 m11500_s04_param_7,
	.param .u64 .ptr .global .align 4 m11500_s04_param_8,
	.param .u64 .ptr .global .align 4 m11500_s04_param_9,
	.param .u64 .ptr .global .align 4 m11500_s04_param_10,
	.param .u64 .ptr .global .align 4 m11500_s04_param_11,
	.param .u64 .ptr .global .align 4 m11500_s04_param_12,
	.param .u64 .ptr .global .align 4 m11500_s04_param_13,
	.param .u64 .ptr .global .align 4 m11500_s04_param_14,
	.param .u64 .ptr .global .align 4 m11500_s04_param_15,
	.param .u64 .ptr .global .align 4 m11500_s04_param_16,
	.param .u64 .ptr .global .align 4 m11500_s04_param_17,
	.param .u64 .ptr .global .align 1 m11500_s04_param_18,
	.param .u64 .ptr .global .align 4 m11500_s04_param_19,
	.param .u64 .ptr .global .align 4 m11500_s04_param_20,
	.param .u64 .ptr .global .align 4 m11500_s04_param_21,
	.param .u64 .ptr .global .align 4 m11500_s04_param_22,
	.param .u64 .ptr .global .align 4 m11500_s04_param_23,
	.param .u32 m11500_s04_param_24,
	.param .u32 m11500_s04_param_25,
	.param .u32 m11500_s04_param_26,
	.param .u32 m11500_s04_param_27,
	.param .u32 m11500_s04_param_28,
	.param .u32 m11500_s04_param_29,
	.param .u32 m11500_s04_param_30,
	.param .u32 m11500_s04_param_31,
	.param .u32 m11500_s04_param_32,
	.param .u32 m11500_s04_param_33,
	.param .u64 m11500_s04_param_34
)
{
	.local .align 16 .b8 	__local_depot4[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<17>;
	.reg .b32 	%r<109>;
	.reg .b64 	%rd<51>;


	mov.u64 	%rd50, __local_depot4;
	cvta.local.u64 	%SP, %rd50;
	ld.param.u64 	%rd6, [m11500_s04_param_0];
	ld.param.u64 	%rd7, [m11500_s04_param_3];
	ld.param.u64 	%rd8, [m11500_s04_param_14];
	ld.param.u64 	%rd9, [m11500_s04_param_15];
	ld.param.u64 	%rd10, [m11500_s04_param_16];
	ld.param.u64 	%rd11, [m11500_s04_param_17];
	ld.param.u64 	%rd12, [m11500_s04_param_19];
	ld.param.u32 	%r36, [m11500_s04_param_27];
	ld.param.u32 	%r37, [m11500_s04_param_30];
	ld.param.u32 	%r38, [m11500_s04_param_31];
	ld.param.u32 	%r39, [m11500_s04_param_32];
	ld.param.u64 	%rd13, [m11500_s04_param_34];
	add.u64 	%rd14, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd14;
	mov.u32 	%r40, %ctaid.x;
	mov.u32 	%r41, %ntid.x;
	mov.b32	%r42, %envreg3;
	mad.lo.s32 	%r43, %r40, %r41, %r42;
	mov.u32 	%r44, %tid.x;
	add.s32 	%r1, %r43, %r44;
	cvt.s64.s32	%rd2, %r1;
	setp.ge.u64	%p1, %rd2, %rd13;
	@%p1 bra 	BB4_27;

	mul.lo.s64 	%rd15, %rd2, 260;
	add.s64 	%rd3, %rd6, %rd15;
	ld.global.u32 	%r2, [%rd3+256];
	setp.eq.s32	%p2, %r37, 0;
	@%p2 bra 	BB4_27;

	ld.global.u32 	%r3, [%rd3];
	ld.global.u32 	%r4, [%rd3+4];
	ld.global.u32 	%r5, [%rd3+8];
	ld.global.u32 	%r6, [%rd3+12];
	mul.wide.u32 	%rd16, %r39, 16;
	add.s64 	%rd17, %rd9, %rd16;
	ld.global.u32 	%r7, [%rd17];
	mul.wide.u32 	%rd18, %r36, 564;
	add.s64 	%rd19, %rd11, %rd18;
	ld.global.u32 	%r46, [%rd19];
	not.b32 	%r8, %r46;
	mul.wide.u32 	%rd20, %r39, 4;
	add.s64 	%rd4, %rd10, %rd20;
	shr.u32 	%r9, %r8, 8;
	mov.u32 	%r45, 0;
	mov.u32 	%r97, %r45;

BB4_3:
	mul.wide.u32 	%rd21, %r97, 4;
	add.s64 	%rd22, %rd7, %rd21;
	ld.const.u32 	%r47, [%rd22];
	or.b32  	%r11, %r47, %r3;
	st.local.v4.u32 	[%rd1], {%r11, %r4, %r5, %r6};
	st.local.v4.u32 	[%rd1+16], {%r45, %r45, %r45, %r45};
	st.local.v4.u32 	[%rd1+32], {%r45, %r45, %r45, %r45};
	st.local.v4.u32 	[%rd1+48], {%r45, %r45, %r45, %r45};
	setp.eq.s32	%p3, %r2, 0;
	mov.u32 	%r104, %r8;
	@%p3 bra 	BB4_5;

	xor.b32  	%r49, %r11, %r8;
	and.b32  	%r50, %r49, 255;
	mul.wide.u32 	%rd23, %r50, 4;
	mov.u64 	%rd24, crc32tab;
	add.s64 	%rd25, %rd24, %rd23;
	ld.const.u32 	%r51, [%rd25];
	xor.b32  	%r104, %r51, %r9;

BB4_5:
	setp.lt.u32	%p4, %r2, 2;
	@%p4 bra 	BB4_7;

	shr.u32 	%r52, %r11, 8;
	xor.b32  	%r53, %r52, %r104;
	and.b32  	%r54, %r53, 255;
	mul.wide.u32 	%rd26, %r54, 4;
	mov.u64 	%rd27, crc32tab;
	add.s64 	%rd28, %rd27, %rd26;
	ld.const.u32 	%r55, [%rd28];
	shr.u32 	%r56, %r104, 8;
	xor.b32  	%r104, %r55, %r56;

BB4_7:
	setp.lt.u32	%p5, %r2, 3;
	@%p5 bra 	BB4_9;

	shr.u32 	%r57, %r11, 16;
	xor.b32  	%r58, %r57, %r104;
	and.b32  	%r59, %r58, 255;
	mul.wide.u32 	%rd29, %r59, 4;
	mov.u64 	%rd30, crc32tab;
	add.s64 	%rd31, %rd30, %rd29;
	ld.const.u32 	%r60, [%rd31];
	shr.u32 	%r61, %r104, 8;
	xor.b32  	%r104, %r60, %r61;

BB4_9:
	setp.lt.u32	%p6, %r2, 4;
	@%p6 bra 	BB4_11;

	shr.u32 	%r62, %r11, 24;
	and.b32  	%r63, %r104, 255;
	xor.b32  	%r64, %r62, %r63;
	mul.wide.u32 	%rd32, %r64, 4;
	mov.u64 	%rd33, crc32tab;
	add.s64 	%rd34, %rd33, %rd32;
	ld.const.u32 	%r65, [%rd34];
	shr.u32 	%r66, %r104, 8;
	xor.b32  	%r104, %r65, %r66;

BB4_11:
	mov.u32 	%r103, 4;
	mov.u32 	%r102, 1;
	setp.lt.u32	%p7, %r2, 5;
	@%p7 bra 	BB4_21;

BB4_12:
	add.s32 	%r69, %r103, 1;
	setp.gt.u32	%p8, %r69, %r2;
	mul.wide.u32 	%rd35, %r102, 4;
	add.s64 	%rd5, %rd1, %rd35;
	@%p8 bra 	BB4_14;

	ld.local.u32 	%r70, [%rd5];
	xor.b32  	%r71, %r70, %r104;
	and.b32  	%r72, %r71, 255;
	mul.wide.u32 	%rd36, %r72, 4;
	mov.u64 	%rd37, crc32tab;
	add.s64 	%rd38, %rd37, %rd36;
	ld.const.u32 	%r73, [%rd38];
	shr.u32 	%r74, %r104, 8;
	xor.b32  	%r104, %r73, %r74;

BB4_14:
	add.s32 	%r75, %r103, 2;
	setp.gt.u32	%p9, %r75, %r2;
	@%p9 bra 	BB4_16;

	ld.local.u32 	%r76, [%rd5];
	shr.u32 	%r77, %r76, 8;
	xor.b32  	%r78, %r77, %r104;
	and.b32  	%r79, %r78, 255;
	mul.wide.u32 	%rd39, %r79, 4;
	mov.u64 	%rd40, crc32tab;
	add.s64 	%rd41, %rd40, %rd39;
	ld.const.u32 	%r80, [%rd41];
	shr.u32 	%r81, %r104, 8;
	xor.b32  	%r104, %r80, %r81;

BB4_16:
	add.s32 	%r82, %r103, 3;
	setp.gt.u32	%p10, %r82, %r2;
	@%p10 bra 	BB4_18;

	ld.local.u16 	%r83, [%rd5+2];
	xor.b32  	%r84, %r83, %r104;
	and.b32  	%r85, %r84, 255;
	mul.wide.u32 	%rd42, %r85, 4;
	mov.u64 	%rd43, crc32tab;
	add.s64 	%rd44, %rd43, %rd42;
	ld.const.u32 	%r86, [%rd44];
	shr.u32 	%r87, %r104, 8;
	xor.b32  	%r104, %r86, %r87;

BB4_18:
	add.s32 	%r103, %r103, 4;
	setp.gt.u32	%p11, %r103, %r2;
	@%p11 bra 	BB4_20;

	ld.local.u8 	%r88, [%rd5+3];
	and.b32  	%r89, %r104, 255;
	xor.b32  	%r90, %r88, %r89;
	mul.wide.u32 	%rd45, %r90, 4;
	mov.u64 	%rd46, crc32tab;
	add.s64 	%rd47, %rd46, %rd45;
	ld.const.u32 	%r91, [%rd47];
	shr.u32 	%r92, %r104, 8;
	xor.b32  	%r104, %r91, %r92;

BB4_20:
	add.s32 	%r102, %r102, 1;
	setp.lt.u32	%p12, %r103, %r2;
	@%p12 bra 	BB4_12;

BB4_21:
	not.b32 	%r93, %r104;
	setp.ne.s32	%p13, %r7, %r93;
	@%p13 bra 	BB4_26;

	atom.global.add.u32 	%r94, [%rd4], 1;
	setp.ne.s32	%p14, %r94, 0;
	@%p14 bra 	BB4_26;

	atom.global.add.u32 	%r34, [%rd12], 1;
	setp.lt.u32	%p15, %r34, %r38;
	@%p15 bra 	BB4_25;
	bra.uni 	BB4_24;

BB4_25:
	mul.wide.u32 	%rd48, %r34, 20;
	add.s64 	%rd49, %rd8, %rd48;
	st.global.u32 	[%rd49], %r36;
	mov.u32 	%r96, 0;
	st.global.u32 	[%rd49+4], %r96;
	st.global.u32 	[%rd49+8], %r39;
	st.global.u32 	[%rd49+12], %r1;
	st.global.u32 	[%rd49+16], %r97;
	bra.uni 	BB4_26;

BB4_24:
	atom.global.add.u32 	%r95, [%rd12], -1;

BB4_26:
	add.s32 	%r97, %r97, 1;
	setp.lt.u32	%p16, %r97, %r37;
	@%p16 bra 	BB4_3;

BB4_27:
	ret;
}

	// .globl	m11500_s08
.entry m11500_s08(
	.param .u64 .ptr .global .align 4 m11500_s08_param_0,
	.param .u64 .ptr .global .align 4 m11500_s08_param_1,
	.param .u64 .ptr .global .align 4 m11500_s08_param_2,
	.param .u64 .ptr .const .align 4 m11500_s08_param_3,
	.param .u64 .ptr .global .align 1 m11500_s08_param_4,
	.param .u64 .ptr .global .align 1 m11500_s08_param_5,
	.param .u64 .ptr .global .align 4 m11500_s08_param_6,
	.param .u64 .ptr .global .align 4 m11500_s08_param_7,
	.param .u64 .ptr .global .align 4 m11500_s08_param_8,
	.param .u64 .ptr .global .align 4 m11500_s08_param_9,
	.param .u64 .ptr .global .align 4 m11500_s08_param_10,
	.param .u64 .ptr .global .align 4 m11500_s08_param_11,
	.param .u64 .ptr .global .align 4 m11500_s08_param_12,
	.param .u64 .ptr .global .align 4 m11500_s08_param_13,
	.param .u64 .ptr .global .align 4 m11500_s08_param_14,
	.param .u64 .ptr .global .align 4 m11500_s08_param_15,
	.param .u64 .ptr .global .align 4 m11500_s08_param_16,
	.param .u64 .ptr .global .align 4 m11500_s08_param_17,
	.param .u64 .ptr .global .align 1 m11500_s08_param_18,
	.param .u64 .ptr .global .align 4 m11500_s08_param_19,
	.param .u64 .ptr .global .align 4 m11500_s08_param_20,
	.param .u64 .ptr .global .align 4 m11500_s08_param_21,
	.param .u64 .ptr .global .align 4 m11500_s08_param_22,
	.param .u64 .ptr .global .align 4 m11500_s08_param_23,
	.param .u32 m11500_s08_param_24,
	.param .u32 m11500_s08_param_25,
	.param .u32 m11500_s08_param_26,
	.param .u32 m11500_s08_param_27,
	.param .u32 m11500_s08_param_28,
	.param .u32 m11500_s08_param_29,
	.param .u32 m11500_s08_param_30,
	.param .u32 m11500_s08_param_31,
	.param .u32 m11500_s08_param_32,
	.param .u32 m11500_s08_param_33,
	.param .u64 m11500_s08_param_34
)
{
	.local .align 16 .b8 	__local_depot5[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<17>;
	.reg .b32 	%r<113>;
	.reg .b64 	%rd<51>;


	mov.u64 	%rd50, __local_depot5;
	cvta.local.u64 	%SP, %rd50;
	ld.param.u64 	%rd6, [m11500_s08_param_0];
	ld.param.u64 	%rd7, [m11500_s08_param_3];
	ld.param.u64 	%rd8, [m11500_s08_param_14];
	ld.param.u64 	%rd9, [m11500_s08_param_15];
	ld.param.u64 	%rd10, [m11500_s08_param_16];
	ld.param.u64 	%rd11, [m11500_s08_param_17];
	ld.param.u64 	%rd12, [m11500_s08_param_19];
	ld.param.u32 	%r40, [m11500_s08_param_27];
	ld.param.u32 	%r41, [m11500_s08_param_30];
	ld.param.u32 	%r42, [m11500_s08_param_31];
	ld.param.u32 	%r43, [m11500_s08_param_32];
	ld.param.u64 	%rd13, [m11500_s08_param_34];
	add.u64 	%rd14, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd14;
	mov.u32 	%r44, %ctaid.x;
	mov.u32 	%r45, %ntid.x;
	mov.b32	%r46, %envreg3;
	mad.lo.s32 	%r47, %r44, %r45, %r46;
	mov.u32 	%r48, %tid.x;
	add.s32 	%r1, %r47, %r48;
	cvt.s64.s32	%rd2, %r1;
	setp.ge.u64	%p1, %rd2, %rd13;
	@%p1 bra 	BB5_27;

	mul.lo.s64 	%rd15, %rd2, 260;
	add.s64 	%rd3, %rd6, %rd15;
	ld.global.u32 	%r2, [%rd3+256];
	setp.eq.s32	%p2, %r41, 0;
	@%p2 bra 	BB5_27;

	ld.global.u32 	%r3, [%rd3];
	ld.global.u32 	%r4, [%rd3+4];
	ld.global.u32 	%r5, [%rd3+8];
	ld.global.u32 	%r6, [%rd3+12];
	ld.global.u32 	%r7, [%rd3+16];
	ld.global.u32 	%r8, [%rd3+20];
	ld.global.u32 	%r9, [%rd3+24];
	ld.global.u32 	%r10, [%rd3+28];
	mul.wide.u32 	%rd16, %r43, 16;
	add.s64 	%rd17, %rd9, %rd16;
	ld.global.u32 	%r11, [%rd17];
	mul.wide.u32 	%rd18, %r40, 564;
	add.s64 	%rd19, %rd11, %rd18;
	ld.global.u32 	%r50, [%rd19];
	not.b32 	%r12, %r50;
	mul.wide.u32 	%rd20, %r43, 4;
	add.s64 	%rd4, %rd10, %rd20;
	shr.u32 	%r13, %r12, 8;
	mov.u32 	%r49, 0;
	mov.u32 	%r101, %r49;

BB5_3:
	mul.wide.u32 	%rd21, %r101, 4;
	add.s64 	%rd22, %rd7, %rd21;
	ld.const.u32 	%r51, [%rd22];
	or.b32  	%r15, %r51, %r3;
	st.local.v4.u32 	[%rd1], {%r15, %r4, %r5, %r6};
	st.local.v4.u32 	[%rd1+16], {%r7, %r8, %r9, %r10};
	st.local.v4.u32 	[%rd1+32], {%r49, %r49, %r49, %r49};
	st.local.v4.u32 	[%rd1+48], {%r49, %r49, %r49, %r49};
	setp.eq.s32	%p3, %r2, 0;
	mov.u32 	%r108, %r12;
	@%p3 bra 	BB5_5;

	xor.b32  	%r53, %r15, %r12;
	and.b32  	%r54, %r53, 255;
	mul.wide.u32 	%rd23, %r54, 4;
	mov.u64 	%rd24, crc32tab;
	add.s64 	%rd25, %rd24, %rd23;
	ld.const.u32 	%r55, [%rd25];
	xor.b32  	%r108, %r55, %r13;

BB5_5:
	setp.lt.u32	%p4, %r2, 2;
	@%p4 bra 	BB5_7;

	shr.u32 	%r56, %r15, 8;
	xor.b32  	%r57, %r56, %r108;
	and.b32  	%r58, %r57, 255;
	mul.wide.u32 	%rd26, %r58, 4;
	mov.u64 	%rd27, crc32tab;
	add.s64 	%rd28, %rd27, %rd26;
	ld.const.u32 	%r59, [%rd28];
	shr.u32 	%r60, %r108, 8;
	xor.b32  	%r108, %r59, %r60;

BB5_7:
	setp.lt.u32	%p5, %r2, 3;
	@%p5 bra 	BB5_9;

	shr.u32 	%r61, %r15, 16;
	xor.b32  	%r62, %r61, %r108;
	and.b32  	%r63, %r62, 255;
	mul.wide.u32 	%rd29, %r63, 4;
	mov.u64 	%rd30, crc32tab;
	add.s64 	%rd31, %rd30, %rd29;
	ld.const.u32 	%r64, [%rd31];
	shr.u32 	%r65, %r108, 8;
	xor.b32  	%r108, %r64, %r65;

BB5_9:
	setp.lt.u32	%p6, %r2, 4;
	@%p6 bra 	BB5_11;

	shr.u32 	%r66, %r15, 24;
	and.b32  	%r67, %r108, 255;
	xor.b32  	%r68, %r66, %r67;
	mul.wide.u32 	%rd32, %r68, 4;
	mov.u64 	%rd33, crc32tab;
	add.s64 	%rd34, %rd33, %rd32;
	ld.const.u32 	%r69, [%rd34];
	shr.u32 	%r70, %r108, 8;
	xor.b32  	%r108, %r69, %r70;

BB5_11:
	mov.u32 	%r107, 4;
	mov.u32 	%r106, 1;
	setp.lt.u32	%p7, %r2, 5;
	@%p7 bra 	BB5_21;

BB5_12:
	add.s32 	%r73, %r107, 1;
	setp.gt.u32	%p8, %r73, %r2;
	mul.wide.u32 	%rd35, %r106, 4;
	add.s64 	%rd5, %rd1, %rd35;
	@%p8 bra 	BB5_14;

	ld.local.u32 	%r74, [%rd5];
	xor.b32  	%r75, %r74, %r108;
	and.b32  	%r76, %r75, 255;
	mul.wide.u32 	%rd36, %r76, 4;
	mov.u64 	%rd37, crc32tab;
	add.s64 	%rd38, %rd37, %rd36;
	ld.const.u32 	%r77, [%rd38];
	shr.u32 	%r78, %r108, 8;
	xor.b32  	%r108, %r77, %r78;

BB5_14:
	add.s32 	%r79, %r107, 2;
	setp.gt.u32	%p9, %r79, %r2;
	@%p9 bra 	BB5_16;

	ld.local.u32 	%r80, [%rd5];
	shr.u32 	%r81, %r80, 8;
	xor.b32  	%r82, %r81, %r108;
	and.b32  	%r83, %r82, 255;
	mul.wide.u32 	%rd39, %r83, 4;
	mov.u64 	%rd40, crc32tab;
	add.s64 	%rd41, %rd40, %rd39;
	ld.const.u32 	%r84, [%rd41];
	shr.u32 	%r85, %r108, 8;
	xor.b32  	%r108, %r84, %r85;

BB5_16:
	add.s32 	%r86, %r107, 3;
	setp.gt.u32	%p10, %r86, %r2;
	@%p10 bra 	BB5_18;

	ld.local.u16 	%r87, [%rd5+2];
	xor.b32  	%r88, %r87, %r108;
	and.b32  	%r89, %r88, 255;
	mul.wide.u32 	%rd42, %r89, 4;
	mov.u64 	%rd43, crc32tab;
	add.s64 	%rd44, %rd43, %rd42;
	ld.const.u32 	%r90, [%rd44];
	shr.u32 	%r91, %r108, 8;
	xor.b32  	%r108, %r90, %r91;

BB5_18:
	add.s32 	%r107, %r107, 4;
	setp.gt.u32	%p11, %r107, %r2;
	@%p11 bra 	BB5_20;

	ld.local.u8 	%r92, [%rd5+3];
	and.b32  	%r93, %r108, 255;
	xor.b32  	%r94, %r92, %r93;
	mul.wide.u32 	%rd45, %r94, 4;
	mov.u64 	%rd46, crc32tab;
	add.s64 	%rd47, %rd46, %rd45;
	ld.const.u32 	%r95, [%rd47];
	shr.u32 	%r96, %r108, 8;
	xor.b32  	%r108, %r95, %r96;

BB5_20:
	add.s32 	%r106, %r106, 1;
	setp.lt.u32	%p12, %r107, %r2;
	@%p12 bra 	BB5_12;

BB5_21:
	not.b32 	%r97, %r108;
	setp.ne.s32	%p13, %r11, %r97;
	@%p13 bra 	BB5_26;

	atom.global.add.u32 	%r98, [%rd4], 1;
	setp.ne.s32	%p14, %r98, 0;
	@%p14 bra 	BB5_26;

	atom.global.add.u32 	%r38, [%rd12], 1;
	setp.lt.u32	%p15, %r38, %r42;
	@%p15 bra 	BB5_25;
	bra.uni 	BB5_24;

BB5_25:
	mul.wide.u32 	%rd48, %r38, 20;
	add.s64 	%rd49, %rd8, %rd48;
	st.global.u32 	[%rd49], %r40;
	mov.u32 	%r100, 0;
	st.global.u32 	[%rd49+4], %r100;
	st.global.u32 	[%rd49+8], %r43;
	st.global.u32 	[%rd49+12], %r1;
	st.global.u32 	[%rd49+16], %r101;
	bra.uni 	BB5_26;

BB5_24:
	atom.global.add.u32 	%r99, [%rd12], -1;

BB5_26:
	add.s32 	%r101, %r101, 1;
	setp.lt.u32	%p16, %r101, %r41;
	@%p16 bra 	BB5_3;

BB5_27:
	ret;
}

	// .globl	m11500_s16
.entry m11500_s16(
	.param .u64 .ptr .global .align 4 m11500_s16_param_0,
	.param .u64 .ptr .global .align 4 m11500_s16_param_1,
	.param .u64 .ptr .global .align 4 m11500_s16_param_2,
	.param .u64 .ptr .const .align 4 m11500_s16_param_3,
	.param .u64 .ptr .global .align 1 m11500_s16_param_4,
	.param .u64 .ptr .global .align 1 m11500_s16_param_5,
	.param .u64 .ptr .global .align 4 m11500_s16_param_6,
	.param .u64 .ptr .global .align 4 m11500_s16_param_7,
	.param .u64 .ptr .global .align 4 m11500_s16_param_8,
	.param .u64 .ptr .global .align 4 m11500_s16_param_9,
	.param .u64 .ptr .global .align 4 m11500_s16_param_10,
	.param .u64 .ptr .global .align 4 m11500_s16_param_11,
	.param .u64 .ptr .global .align 4 m11500_s16_param_12,
	.param .u64 .ptr .global .align 4 m11500_s16_param_13,
	.param .u64 .ptr .global .align 4 m11500_s16_param_14,
	.param .u64 .ptr .global .align 4 m11500_s16_param_15,
	.param .u64 .ptr .global .align 4 m11500_s16_param_16,
	.param .u64 .ptr .global .align 4 m11500_s16_param_17,
	.param .u64 .ptr .global .align 1 m11500_s16_param_18,
	.param .u64 .ptr .global .align 4 m11500_s16_param_19,
	.param .u64 .ptr .global .align 4 m11500_s16_param_20,
	.param .u64 .ptr .global .align 4 m11500_s16_param_21,
	.param .u64 .ptr .global .align 4 m11500_s16_param_22,
	.param .u64 .ptr .global .align 4 m11500_s16_param_23,
	.param .u32 m11500_s16_param_24,
	.param .u32 m11500_s16_param_25,
	.param .u32 m11500_s16_param_26,
	.param .u32 m11500_s16_param_27,
	.param .u32 m11500_s16_param_28,
	.param .u32 m11500_s16_param_29,
	.param .u32 m11500_s16_param_30,
	.param .u32 m11500_s16_param_31,
	.param .u32 m11500_s16_param_32,
	.param .u32 m11500_s16_param_33,
	.param .u64 m11500_s16_param_34
)
{
	.local .align 16 .b8 	__local_depot6[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<17>;
	.reg .b32 	%r<120>;
	.reg .b64 	%rd<51>;


	mov.u64 	%rd50, __local_depot6;
	cvta.local.u64 	%SP, %rd50;
	ld.param.u64 	%rd6, [m11500_s16_param_0];
	ld.param.u64 	%rd7, [m11500_s16_param_3];
	ld.param.u64 	%rd8, [m11500_s16_param_14];
	ld.param.u64 	%rd9, [m11500_s16_param_15];
	ld.param.u64 	%rd10, [m11500_s16_param_16];
	ld.param.u64 	%rd11, [m11500_s16_param_17];
	ld.param.u64 	%rd12, [m11500_s16_param_19];
	ld.param.u32 	%r48, [m11500_s16_param_27];
	ld.param.u32 	%r49, [m11500_s16_param_30];
	ld.param.u32 	%r50, [m11500_s16_param_31];
	ld.param.u32 	%r51, [m11500_s16_param_32];
	ld.param.u64 	%rd13, [m11500_s16_param_34];
	add.u64 	%rd14, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd14;
	mov.u32 	%r52, %ctaid.x;
	mov.u32 	%r53, %ntid.x;
	mov.b32	%r54, %envreg3;
	mad.lo.s32 	%r55, %r52, %r53, %r54;
	mov.u32 	%r56, %tid.x;
	add.s32 	%r1, %r55, %r56;
	cvt.s64.s32	%rd2, %r1;
	setp.ge.u64	%p1, %rd2, %rd13;
	@%p1 bra 	BB6_27;

	mul.lo.s64 	%rd15, %rd2, 260;
	add.s64 	%rd3, %rd6, %rd15;
	ld.global.u32 	%r2, [%rd3+256];
	setp.eq.s32	%p2, %r49, 0;
	@%p2 bra 	BB6_27;

	ld.global.u32 	%r3, [%rd3];
	ld.global.u32 	%r4, [%rd3+4];
	ld.global.u32 	%r5, [%rd3+8];
	ld.global.u32 	%r6, [%rd3+12];
	ld.global.u32 	%r7, [%rd3+16];
	ld.global.u32 	%r8, [%rd3+20];
	ld.global.u32 	%r9, [%rd3+24];
	ld.global.u32 	%r10, [%rd3+28];
	ld.global.u32 	%r11, [%rd3+32];
	ld.global.u32 	%r12, [%rd3+36];
	ld.global.u32 	%r13, [%rd3+40];
	ld.global.u32 	%r14, [%rd3+44];
	ld.global.u32 	%r15, [%rd3+48];
	ld.global.u32 	%r16, [%rd3+52];
	ld.global.u32 	%r17, [%rd3+56];
	ld.global.u32 	%r18, [%rd3+60];
	mul.wide.u32 	%rd16, %r51, 16;
	add.s64 	%rd17, %rd9, %rd16;
	ld.global.u32 	%r19, [%rd17];
	mul.wide.u32 	%rd18, %r48, 564;
	add.s64 	%rd19, %rd11, %rd18;
	ld.global.u32 	%r58, [%rd19];
	not.b32 	%r20, %r58;
	mul.wide.u32 	%rd20, %r51, 4;
	add.s64 	%rd4, %rd10, %rd20;
	shr.u32 	%r21, %r20, 8;
	mov.u32 	%r108, 0;

BB6_3:
	mul.wide.u32 	%rd21, %r108, 4;
	add.s64 	%rd22, %rd7, %rd21;
	ld.const.u32 	%r59, [%rd22];
	or.b32  	%r23, %r59, %r3;
	st.local.v4.u32 	[%rd1], {%r23, %r4, %r5, %r6};
	st.local.v4.u32 	[%rd1+16], {%r7, %r8, %r9, %r10};
	st.local.v4.u32 	[%rd1+32], {%r11, %r12, %r13, %r14};
	st.local.v4.u32 	[%rd1+48], {%r15, %r16, %r17, %r18};
	setp.eq.s32	%p3, %r2, 0;
	mov.u32 	%r115, %r20;
	@%p3 bra 	BB6_5;

	xor.b32  	%r60, %r23, %r20;
	and.b32  	%r61, %r60, 255;
	mul.wide.u32 	%rd23, %r61, 4;
	mov.u64 	%rd24, crc32tab;
	add.s64 	%rd25, %rd24, %rd23;
	ld.const.u32 	%r62, [%rd25];
	xor.b32  	%r115, %r62, %r21;

BB6_5:
	setp.lt.u32	%p4, %r2, 2;
	@%p4 bra 	BB6_7;

	shr.u32 	%r63, %r23, 8;
	xor.b32  	%r64, %r63, %r115;
	and.b32  	%r65, %r64, 255;
	mul.wide.u32 	%rd26, %r65, 4;
	mov.u64 	%rd27, crc32tab;
	add.s64 	%rd28, %rd27, %rd26;
	ld.const.u32 	%r66, [%rd28];
	shr.u32 	%r67, %r115, 8;
	xor.b32  	%r115, %r66, %r67;

BB6_7:
	setp.lt.u32	%p5, %r2, 3;
	@%p5 bra 	BB6_9;

	shr.u32 	%r68, %r23, 16;
	xor.b32  	%r69, %r68, %r115;
	and.b32  	%r70, %r69, 255;
	mul.wide.u32 	%rd29, %r70, 4;
	mov.u64 	%rd30, crc32tab;
	add.s64 	%rd31, %rd30, %rd29;
	ld.const.u32 	%r71, [%rd31];
	shr.u32 	%r72, %r115, 8;
	xor.b32  	%r115, %r71, %r72;

BB6_9:
	setp.lt.u32	%p6, %r2, 4;
	@%p6 bra 	BB6_11;

	shr.u32 	%r73, %r23, 24;
	and.b32  	%r74, %r115, 255;
	xor.b32  	%r75, %r73, %r74;
	mul.wide.u32 	%rd32, %r75, 4;
	mov.u64 	%rd33, crc32tab;
	add.s64 	%rd34, %rd33, %rd32;
	ld.const.u32 	%r76, [%rd34];
	shr.u32 	%r77, %r115, 8;
	xor.b32  	%r115, %r76, %r77;

BB6_11:
	mov.u32 	%r114, 4;
	mov.u32 	%r113, 1;
	setp.lt.u32	%p7, %r2, 5;
	@%p7 bra 	BB6_21;

BB6_12:
	add.s32 	%r80, %r114, 1;
	setp.gt.u32	%p8, %r80, %r2;
	mul.wide.u32 	%rd35, %r113, 4;
	add.s64 	%rd5, %rd1, %rd35;
	@%p8 bra 	BB6_14;

	ld.local.u32 	%r81, [%rd5];
	xor.b32  	%r82, %r81, %r115;
	and.b32  	%r83, %r82, 255;
	mul.wide.u32 	%rd36, %r83, 4;
	mov.u64 	%rd37, crc32tab;
	add.s64 	%rd38, %rd37, %rd36;
	ld.const.u32 	%r84, [%rd38];
	shr.u32 	%r85, %r115, 8;
	xor.b32  	%r115, %r84, %r85;

BB6_14:
	add.s32 	%r86, %r114, 2;
	setp.gt.u32	%p9, %r86, %r2;
	@%p9 bra 	BB6_16;

	ld.local.u32 	%r87, [%rd5];
	shr.u32 	%r88, %r87, 8;
	xor.b32  	%r89, %r88, %r115;
	and.b32  	%r90, %r89, 255;
	mul.wide.u32 	%rd39, %r90, 4;
	mov.u64 	%rd40, crc32tab;
	add.s64 	%rd41, %rd40, %rd39;
	ld.const.u32 	%r91, [%rd41];
	shr.u32 	%r92, %r115, 8;
	xor.b32  	%r115, %r91, %r92;

BB6_16:
	add.s32 	%r93, %r114, 3;
	setp.gt.u32	%p10, %r93, %r2;
	@%p10 bra 	BB6_18;

	ld.local.u16 	%r94, [%rd5+2];
	xor.b32  	%r95, %r94, %r115;
	and.b32  	%r96, %r95, 255;
	mul.wide.u32 	%rd42, %r96, 4;
	mov.u64 	%rd43, crc32tab;
	add.s64 	%rd44, %rd43, %rd42;
	ld.const.u32 	%r97, [%rd44];
	shr.u32 	%r98, %r115, 8;
	xor.b32  	%r115, %r97, %r98;

BB6_18:
	add.s32 	%r114, %r114, 4;
	setp.gt.u32	%p11, %r114, %r2;
	@%p11 bra 	BB6_20;

	ld.local.u8 	%r99, [%rd5+3];
	and.b32  	%r100, %r115, 255;
	xor.b32  	%r101, %r99, %r100;
	mul.wide.u32 	%rd45, %r101, 4;
	mov.u64 	%rd46, crc32tab;
	add.s64 	%rd47, %rd46, %rd45;
	ld.const.u32 	%r102, [%rd47];
	shr.u32 	%r103, %r115, 8;
	xor.b32  	%r115, %r102, %r103;

BB6_20:
	add.s32 	%r113, %r113, 1;
	setp.lt.u32	%p12, %r114, %r2;
	@%p12 bra 	BB6_12;

BB6_21:
	not.b32 	%r104, %r115;
	setp.ne.s32	%p13, %r19, %r104;
	@%p13 bra 	BB6_26;

	atom.global.add.u32 	%r105, [%rd4], 1;
	setp.ne.s32	%p14, %r105, 0;
	@%p14 bra 	BB6_26;

	atom.global.add.u32 	%r46, [%rd12], 1;
	setp.lt.u32	%p15, %r46, %r50;
	@%p15 bra 	BB6_25;
	bra.uni 	BB6_24;

BB6_25:
	mul.wide.u32 	%rd48, %r46, 20;
	add.s64 	%rd49, %rd8, %rd48;
	st.global.u32 	[%rd49], %r48;
	mov.u32 	%r107, 0;
	st.global.u32 	[%rd49+4], %r107;
	st.global.u32 	[%rd49+8], %r51;
	st.global.u32 	[%rd49+12], %r1;
	st.global.u32 	[%rd49+16], %r108;
	bra.uni 	BB6_26;

BB6_24:
	atom.global.add.u32 	%r106, [%rd12], -1;

BB6_26:
	add.s32 	%r108, %r108, 1;
	setp.lt.u32	%p16, %r108, %r49;
	@%p16 bra 	BB6_3;

BB6_27:
	ret;
}


  