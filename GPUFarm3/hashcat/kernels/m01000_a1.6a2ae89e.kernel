//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: UNKNOWN
// Driver 
// Based on LLVM 3.4svn
//

.version 6.1
.target sm_61, texmode_independent
.address_size 64

	// .globl	gpu_memset
.const .align 4 .b8 c_append_helper[4096] = {255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255};

.entry gpu_memset(
	.param .u64 .ptr .global .align 16 gpu_memset_param_0,
	.param .u32 gpu_memset_param_1,
	.param .u64 gpu_memset_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [gpu_memset_param_0];
	ld.param.u32 	%r2, [gpu_memset_param_1];
	ld.param.u64 	%rd2, [gpu_memset_param_2];
	mov.b32	%r3, %envreg3;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mad.lo.s32 	%r6, %r4, %r5, %r3;
	mov.u32 	%r7, %tid.x;
	add.s32 	%r1, %r6, %r7;
	cvt.s64.s32	%rd3, %r1;
	setp.ge.u64	%p1, %rd3, %rd2;
	@%p1 bra 	BB0_2;

	mul.wide.s32 	%rd4, %r1, 16;
	add.s64 	%rd5, %rd1, %rd4;
	st.global.v4.u32 	[%rd5], {%r2, %r2, %r2, %r2};

BB0_2:
	ret;
}

	// .globl	m01000_mxx
.entry m01000_mxx(
	.param .u64 .ptr .global .align 4 m01000_mxx_param_0,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_1,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_2,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_3,
	.param .u64 .ptr .global .align 1 m01000_mxx_param_4,
	.param .u64 .ptr .global .align 1 m01000_mxx_param_5,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_6,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_7,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_8,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_9,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_10,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_11,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_12,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_13,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_14,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_15,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_16,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_17,
	.param .u64 .ptr .global .align 1 m01000_mxx_param_18,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_19,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_20,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_21,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_22,
	.param .u64 .ptr .global .align 4 m01000_mxx_param_23,
	.param .u32 m01000_mxx_param_24,
	.param .u32 m01000_mxx_param_25,
	.param .u32 m01000_mxx_param_26,
	.param .u32 m01000_mxx_param_27,
	.param .u32 m01000_mxx_param_28,
	.param .u32 m01000_mxx_param_29,
	.param .u32 m01000_mxx_param_30,
	.param .u32 m01000_mxx_param_31,
	.param .u32 m01000_mxx_param_32,
	.param .u32 m01000_mxx_param_33,
	.param .u64 m01000_mxx_param_34
)
{
	.reg .pred 	%p<133>;
	.reg .b32 	%r<7152>;
	.reg .b64 	%rd<70>;


	ld.param.u64 	%rd4, [m01000_mxx_param_0];
	ld.param.u64 	%rd5, [m01000_mxx_param_2];
	ld.param.u64 	%rd17, [m01000_mxx_param_19];
	ld.param.u32 	%r1023, [m01000_mxx_param_24];
	ld.param.u32 	%r1028, [m01000_mxx_param_31];
	ld.param.u64 	%rd18, [m01000_mxx_param_34];
	mov.b32	%r1030, %envreg3;
	mov.u32 	%r1031, %ctaid.x;
	mov.u32 	%r1032, %ntid.x;
	mad.lo.s32 	%r1033, %r1031, %r1032, %r1030;
	mov.u32 	%r1034, %tid.x;
	add.s32 	%r1, %r1033, %r1034;
	cvt.s64.s32	%rd19, %r1;
	setp.ge.u64	%p1, %rd19, %rd18;
	@%p1 bra 	BB1_141;

	mul.wide.s32 	%rd20, %r1, 260;
	add.s64 	%rd21, %rd4, %rd20;
	ld.global.u32 	%r2, [%rd21+256];
	mov.u32 	%r6979, 0;
	mov.u32 	%r7, 1732584193;
	mov.u32 	%r6, -271733879;
	mov.u32 	%r5, -1732584194;
	mov.u32 	%r4, 271733878;
	mov.u32 	%r6984, %r6979;
	mov.u32 	%r6985, %r6979;
	bra.uni 	BB1_2;

BB1_189:
	mov.u32 	%r6641, 0;
	mov.u32 	%r6569, 29554;
	// inline asm
	prmt.b32 %r6510, %r17, %r6641, %r6569;
	// inline asm
	mov.u32 	%r6573, 29040;
	// inline asm
	prmt.b32 %r6514, %r17, %r6641, %r6573;
	// inline asm
	// inline asm
	prmt.b32 %r6518, %r16, %r6641, %r6569;
	// inline asm
	// inline asm
	prmt.b32 %r6522, %r16, %r6641, %r6573;
	// inline asm
	// inline asm
	prmt.b32 %r6526, %r15, %r6641, %r6569;
	// inline asm
	// inline asm
	prmt.b32 %r6530, %r15, %r6641, %r6573;
	// inline asm
	// inline asm
	prmt.b32 %r6534, %r14, %r6641, %r6569;
	// inline asm
	// inline asm
	prmt.b32 %r6538, %r14, %r6641, %r6573;
	// inline asm
	// inline asm
	prmt.b32 %r6542, %r13, %r6641, %r6569;
	// inline asm
	// inline asm
	prmt.b32 %r6546, %r13, %r6641, %r6573;
	// inline asm
	// inline asm
	prmt.b32 %r6550, %r12, %r6641, %r6569;
	// inline asm
	// inline asm
	prmt.b32 %r6554, %r12, %r6641, %r6573;
	// inline asm
	// inline asm
	prmt.b32 %r6558, %r11, %r6641, %r6569;
	// inline asm
	// inline asm
	prmt.b32 %r6562, %r11, %r6641, %r6573;
	// inline asm
	// inline asm
	prmt.b32 %r6566, %r10, %r6641, %r6569;
	// inline asm
	// inline asm
	prmt.b32 %r6570, %r10, %r6641, %r6573;
	// inline asm
	add.s32 	%r6979, %r6979, 64;
	// inline asm
	shf.r.wrap.b32 %r6574, %r6510, %r6641, %r6641;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6578, %r6514, %r6510, %r6641;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6582, %r6518, %r6514, %r6641;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6586, %r6522, %r6518, %r6641;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6590, %r6526, %r6522, %r6641;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6594, %r6530, %r6526, %r6641;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6598, %r6534, %r6530, %r6641;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6602, %r6538, %r6534, %r6641;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6606, %r6542, %r6538, %r6641;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6610, %r6546, %r6542, %r6641;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6614, %r6550, %r6546, %r6641;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6618, %r6554, %r6550, %r6641;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6622, %r6558, %r6554, %r6641;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6626, %r6562, %r6558, %r6641;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6630, %r6566, %r6562, %r6641;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6634, %r6570, %r6566, %r6641;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6638, %r6641, %r6570, %r6641;
	// inline asm
	xor.b32  	%r6642, %r5, %r4;
	and.b32  	%r6643, %r6642, %r6;
	xor.b32  	%r6644, %r6643, %r4;
	add.s32 	%r6645, %r6644, %r7;
	add.s32 	%r6646, %r6645, %r6634;
	shf.l.wrap.b32 	%r6647, %r6646, %r6646, 3;
	xor.b32  	%r6648, %r6, %r5;
	and.b32  	%r6649, %r6647, %r6648;
	xor.b32  	%r6650, %r6649, %r5;
	add.s32 	%r6651, %r6630, %r4;
	add.s32 	%r6652, %r6651, %r6650;
	shf.l.wrap.b32 	%r6653, %r6652, %r6652, 7;
	xor.b32  	%r6654, %r6647, %r6;
	and.b32  	%r6655, %r6654, %r6653;
	xor.b32  	%r6656, %r6655, %r6;
	add.s32 	%r6657, %r6626, %r5;
	add.s32 	%r6658, %r6657, %r6656;
	shf.l.wrap.b32 	%r6659, %r6658, %r6658, 11;
	xor.b32  	%r6660, %r6653, %r6647;
	and.b32  	%r6661, %r6660, %r6659;
	xor.b32  	%r6662, %r6661, %r6647;
	add.s32 	%r6663, %r6622, %r6;
	add.s32 	%r6664, %r6663, %r6662;
	shf.l.wrap.b32 	%r6665, %r6664, %r6664, 19;
	xor.b32  	%r6666, %r6659, %r6653;
	and.b32  	%r6667, %r6666, %r6665;
	xor.b32  	%r6668, %r6667, %r6653;
	add.s32 	%r6669, %r6647, %r6618;
	add.s32 	%r6670, %r6669, %r6668;
	shf.l.wrap.b32 	%r6671, %r6670, %r6670, 3;
	xor.b32  	%r6672, %r6665, %r6659;
	and.b32  	%r6673, %r6672, %r6671;
	xor.b32  	%r6674, %r6673, %r6659;
	add.s32 	%r6675, %r6653, %r6614;
	add.s32 	%r6676, %r6675, %r6674;
	shf.l.wrap.b32 	%r6677, %r6676, %r6676, 7;
	xor.b32  	%r6678, %r6671, %r6665;
	and.b32  	%r6679, %r6678, %r6677;
	xor.b32  	%r6680, %r6679, %r6665;
	add.s32 	%r6681, %r6659, %r6610;
	add.s32 	%r6682, %r6681, %r6680;
	shf.l.wrap.b32 	%r6683, %r6682, %r6682, 11;
	xor.b32  	%r6684, %r6677, %r6671;
	and.b32  	%r6685, %r6684, %r6683;
	xor.b32  	%r6686, %r6685, %r6671;
	add.s32 	%r6687, %r6665, %r6606;
	add.s32 	%r6688, %r6687, %r6686;
	shf.l.wrap.b32 	%r6689, %r6688, %r6688, 19;
	xor.b32  	%r6690, %r6683, %r6677;
	and.b32  	%r6691, %r6690, %r6689;
	xor.b32  	%r6692, %r6691, %r6677;
	add.s32 	%r6693, %r6671, %r6602;
	add.s32 	%r6694, %r6693, %r6692;
	shf.l.wrap.b32 	%r6695, %r6694, %r6694, 3;
	xor.b32  	%r6696, %r6689, %r6683;
	and.b32  	%r6697, %r6696, %r6695;
	xor.b32  	%r6698, %r6697, %r6683;
	add.s32 	%r6699, %r6677, %r6598;
	add.s32 	%r6700, %r6699, %r6698;
	shf.l.wrap.b32 	%r6701, %r6700, %r6700, 7;
	xor.b32  	%r6702, %r6695, %r6689;
	and.b32  	%r6703, %r6702, %r6701;
	xor.b32  	%r6704, %r6703, %r6689;
	add.s32 	%r6705, %r6683, %r6594;
	add.s32 	%r6706, %r6705, %r6704;
	shf.l.wrap.b32 	%r6707, %r6706, %r6706, 11;
	xor.b32  	%r6708, %r6701, %r6695;
	and.b32  	%r6709, %r6708, %r6707;
	xor.b32  	%r6710, %r6709, %r6695;
	add.s32 	%r6711, %r6689, %r6590;
	add.s32 	%r6712, %r6711, %r6710;
	shf.l.wrap.b32 	%r6713, %r6712, %r6712, 19;
	xor.b32  	%r6714, %r6707, %r6701;
	and.b32  	%r6715, %r6714, %r6713;
	xor.b32  	%r6716, %r6715, %r6701;
	add.s32 	%r6717, %r6695, %r6586;
	add.s32 	%r6718, %r6717, %r6716;
	shf.l.wrap.b32 	%r6719, %r6718, %r6718, 3;
	xor.b32  	%r6720, %r6713, %r6707;
	and.b32  	%r6721, %r6720, %r6719;
	xor.b32  	%r6722, %r6721, %r6707;
	add.s32 	%r6723, %r6701, %r6582;
	add.s32 	%r6724, %r6723, %r6722;
	shf.l.wrap.b32 	%r6725, %r6724, %r6724, 7;
	xor.b32  	%r6726, %r6719, %r6713;
	and.b32  	%r6727, %r6726, %r6725;
	xor.b32  	%r6728, %r6727, %r6713;
	add.s32 	%r6729, %r6707, %r6578;
	add.s32 	%r6730, %r6729, %r6728;
	shf.l.wrap.b32 	%r6731, %r6730, %r6730, 11;
	xor.b32  	%r6732, %r6725, %r6719;
	and.b32  	%r6733, %r6732, %r6731;
	xor.b32  	%r6734, %r6733, %r6719;
	add.s32 	%r6735, %r6713, %r6574;
	add.s32 	%r6736, %r6735, %r6734;
	shf.l.wrap.b32 	%r6737, %r6736, %r6736, 19;
	xor.b32  	%r6738, %r6737, %r6725;
	xor.b32  	%r6739, %r6737, %r6731;
	and.b32  	%r6740, %r6739, %r6738;
	xor.b32  	%r6741, %r6740, %r6737;
	add.s32 	%r6742, %r6634, %r6719;
	add.s32 	%r6743, %r6742, %r6741;
	add.s32 	%r6744, %r6743, 1518500249;
	shf.l.wrap.b32 	%r6745, %r6744, %r6744, 3;
	xor.b32  	%r6746, %r6745, %r6731;
	xor.b32  	%r6747, %r6745, %r6737;
	and.b32  	%r6748, %r6747, %r6746;
	xor.b32  	%r6749, %r6748, %r6745;
	add.s32 	%r6750, %r6618, %r6725;
	add.s32 	%r6751, %r6750, %r6749;
	add.s32 	%r6752, %r6751, 1518500249;
	shf.l.wrap.b32 	%r6753, %r6752, %r6752, 5;
	xor.b32  	%r6754, %r6753, %r6737;
	xor.b32  	%r6755, %r6753, %r6745;
	and.b32  	%r6756, %r6755, %r6754;
	xor.b32  	%r6757, %r6756, %r6753;
	add.s32 	%r6758, %r6602, %r6731;
	add.s32 	%r6759, %r6758, %r6757;
	add.s32 	%r6760, %r6759, 1518500249;
	shf.l.wrap.b32 	%r6761, %r6760, %r6760, 9;
	xor.b32  	%r6762, %r6761, %r6745;
	xor.b32  	%r6763, %r6761, %r6753;
	and.b32  	%r6764, %r6763, %r6762;
	xor.b32  	%r6765, %r6764, %r6761;
	add.s32 	%r6766, %r6586, %r6737;
	add.s32 	%r6767, %r6766, %r6765;
	add.s32 	%r6768, %r6767, 1518500249;
	shf.l.wrap.b32 	%r6769, %r6768, %r6768, 13;
	xor.b32  	%r6770, %r6769, %r6753;
	xor.b32  	%r6771, %r6769, %r6761;
	and.b32  	%r6772, %r6771, %r6770;
	xor.b32  	%r6773, %r6772, %r6769;
	add.s32 	%r6774, %r6630, %r6745;
	add.s32 	%r6775, %r6774, %r6773;
	add.s32 	%r6776, %r6775, 1518500249;
	shf.l.wrap.b32 	%r6777, %r6776, %r6776, 3;
	xor.b32  	%r6778, %r6777, %r6761;
	xor.b32  	%r6779, %r6777, %r6769;
	and.b32  	%r6780, %r6779, %r6778;
	xor.b32  	%r6781, %r6780, %r6777;
	add.s32 	%r6782, %r6614, %r6753;
	add.s32 	%r6783, %r6782, %r6781;
	add.s32 	%r6784, %r6783, 1518500249;
	shf.l.wrap.b32 	%r6785, %r6784, %r6784, 5;
	xor.b32  	%r6786, %r6785, %r6769;
	xor.b32  	%r6787, %r6785, %r6777;
	and.b32  	%r6788, %r6787, %r6786;
	xor.b32  	%r6789, %r6788, %r6785;
	add.s32 	%r6790, %r6598, %r6761;
	add.s32 	%r6791, %r6790, %r6789;
	add.s32 	%r6792, %r6791, 1518500249;
	shf.l.wrap.b32 	%r6793, %r6792, %r6792, 9;
	xor.b32  	%r6794, %r6793, %r6777;
	xor.b32  	%r6795, %r6793, %r6785;
	and.b32  	%r6796, %r6795, %r6794;
	xor.b32  	%r6797, %r6796, %r6793;
	add.s32 	%r6798, %r6582, %r6769;
	add.s32 	%r6799, %r6798, %r6797;
	add.s32 	%r6800, %r6799, 1518500249;
	shf.l.wrap.b32 	%r6801, %r6800, %r6800, 13;
	xor.b32  	%r6802, %r6801, %r6785;
	xor.b32  	%r6803, %r6801, %r6793;
	and.b32  	%r6804, %r6803, %r6802;
	xor.b32  	%r6805, %r6804, %r6801;
	add.s32 	%r6806, %r6626, %r6777;
	add.s32 	%r6807, %r6806, %r6805;
	add.s32 	%r6808, %r6807, 1518500249;
	shf.l.wrap.b32 	%r6809, %r6808, %r6808, 3;
	xor.b32  	%r6810, %r6809, %r6793;
	xor.b32  	%r6811, %r6809, %r6801;
	and.b32  	%r6812, %r6811, %r6810;
	xor.b32  	%r6813, %r6812, %r6809;
	add.s32 	%r6814, %r6610, %r6785;
	add.s32 	%r6815, %r6814, %r6813;
	add.s32 	%r6816, %r6815, 1518500249;
	shf.l.wrap.b32 	%r6817, %r6816, %r6816, 5;
	xor.b32  	%r6818, %r6817, %r6801;
	xor.b32  	%r6819, %r6817, %r6809;
	and.b32  	%r6820, %r6819, %r6818;
	xor.b32  	%r6821, %r6820, %r6817;
	add.s32 	%r6822, %r6594, %r6793;
	add.s32 	%r6823, %r6822, %r6821;
	add.s32 	%r6824, %r6823, 1518500249;
	shf.l.wrap.b32 	%r6825, %r6824, %r6824, 9;
	xor.b32  	%r6826, %r6825, %r6809;
	xor.b32  	%r6827, %r6825, %r6817;
	and.b32  	%r6828, %r6827, %r6826;
	xor.b32  	%r6829, %r6828, %r6825;
	add.s32 	%r6830, %r6578, %r6801;
	add.s32 	%r6831, %r6830, %r6829;
	add.s32 	%r6832, %r6831, 1518500249;
	shf.l.wrap.b32 	%r6833, %r6832, %r6832, 13;
	xor.b32  	%r6834, %r6833, %r6817;
	xor.b32  	%r6835, %r6833, %r6825;
	and.b32  	%r6836, %r6835, %r6834;
	xor.b32  	%r6837, %r6836, %r6833;
	add.s32 	%r6838, %r6622, %r6809;
	add.s32 	%r6839, %r6838, %r6837;
	add.s32 	%r6840, %r6839, 1518500249;
	shf.l.wrap.b32 	%r6841, %r6840, %r6840, 3;
	xor.b32  	%r6842, %r6841, %r6825;
	xor.b32  	%r6843, %r6841, %r6833;
	and.b32  	%r6844, %r6843, %r6842;
	xor.b32  	%r6845, %r6844, %r6841;
	add.s32 	%r6846, %r6606, %r6817;
	add.s32 	%r6847, %r6846, %r6845;
	add.s32 	%r6848, %r6847, 1518500249;
	shf.l.wrap.b32 	%r6849, %r6848, %r6848, 5;
	xor.b32  	%r6850, %r6849, %r6833;
	xor.b32  	%r6851, %r6849, %r6841;
	and.b32  	%r6852, %r6851, %r6850;
	xor.b32  	%r6853, %r6852, %r6849;
	add.s32 	%r6854, %r6590, %r6825;
	add.s32 	%r6855, %r6854, %r6853;
	add.s32 	%r6856, %r6855, 1518500249;
	shf.l.wrap.b32 	%r6857, %r6856, %r6856, 9;
	xor.b32  	%r6858, %r6857, %r6841;
	xor.b32  	%r6859, %r6857, %r6849;
	and.b32  	%r6860, %r6859, %r6858;
	xor.b32  	%r6861, %r6860, %r6857;
	add.s32 	%r6862, %r6574, %r6833;
	add.s32 	%r6863, %r6862, %r6861;
	add.s32 	%r6864, %r6863, 1518500249;
	shf.l.wrap.b32 	%r6865, %r6864, %r6864, 13;
	xor.b32  	%r6866, %r6859, %r6865;
	add.s32 	%r6867, %r6634, %r6841;
	add.s32 	%r6868, %r6867, %r6866;
	add.s32 	%r6869, %r6868, 1859775393;
	shf.l.wrap.b32 	%r6870, %r6869, %r6869, 3;
	xor.b32  	%r6871, %r6865, %r6857;
	xor.b32  	%r6872, %r6871, %r6870;
	add.s32 	%r6873, %r6602, %r6849;
	add.s32 	%r6874, %r6873, %r6872;
	add.s32 	%r6875, %r6874, 1859775393;
	shf.l.wrap.b32 	%r6876, %r6875, %r6875, 9;
	xor.b32  	%r6877, %r6870, %r6865;
	xor.b32  	%r6878, %r6877, %r6876;
	add.s32 	%r6879, %r6618, %r6857;
	add.s32 	%r6880, %r6879, %r6878;
	add.s32 	%r6881, %r6880, 1859775393;
	shf.l.wrap.b32 	%r6882, %r6881, %r6881, 11;
	xor.b32  	%r6883, %r6876, %r6870;
	xor.b32  	%r6884, %r6883, %r6882;
	add.s32 	%r6885, %r6586, %r6865;
	add.s32 	%r6886, %r6885, %r6884;
	add.s32 	%r6887, %r6886, 1859775393;
	shf.l.wrap.b32 	%r6888, %r6887, %r6887, 15;
	xor.b32  	%r6889, %r6882, %r6876;
	xor.b32  	%r6890, %r6889, %r6888;
	add.s32 	%r6891, %r6626, %r6870;
	add.s32 	%r6892, %r6891, %r6890;
	add.s32 	%r6893, %r6892, 1859775393;
	shf.l.wrap.b32 	%r6894, %r6893, %r6893, 3;
	xor.b32  	%r6895, %r6888, %r6882;
	xor.b32  	%r6896, %r6895, %r6894;
	add.s32 	%r6897, %r6594, %r6876;
	add.s32 	%r6898, %r6897, %r6896;
	add.s32 	%r6899, %r6898, 1859775393;
	shf.l.wrap.b32 	%r6900, %r6899, %r6899, 9;
	xor.b32  	%r6901, %r6894, %r6888;
	xor.b32  	%r6902, %r6901, %r6900;
	add.s32 	%r6903, %r6610, %r6882;
	add.s32 	%r6904, %r6903, %r6902;
	add.s32 	%r6905, %r6904, 1859775393;
	shf.l.wrap.b32 	%r6906, %r6905, %r6905, 11;
	xor.b32  	%r6907, %r6900, %r6894;
	xor.b32  	%r6908, %r6907, %r6906;
	add.s32 	%r6909, %r6578, %r6888;
	add.s32 	%r6910, %r6909, %r6908;
	add.s32 	%r6911, %r6910, 1859775393;
	shf.l.wrap.b32 	%r6912, %r6911, %r6911, 15;
	xor.b32  	%r6913, %r6906, %r6900;
	xor.b32  	%r6914, %r6913, %r6912;
	add.s32 	%r6915, %r6630, %r6894;
	add.s32 	%r6916, %r6915, %r6914;
	add.s32 	%r6917, %r6916, 1859775393;
	shf.l.wrap.b32 	%r6918, %r6917, %r6917, 3;
	xor.b32  	%r6919, %r6912, %r6906;
	xor.b32  	%r6920, %r6919, %r6918;
	add.s32 	%r6921, %r6598, %r6900;
	add.s32 	%r6922, %r6921, %r6920;
	add.s32 	%r6923, %r6922, 1859775393;
	shf.l.wrap.b32 	%r6924, %r6923, %r6923, 9;
	xor.b32  	%r6925, %r6918, %r6912;
	xor.b32  	%r6926, %r6925, %r6924;
	add.s32 	%r6927, %r6614, %r6906;
	add.s32 	%r6928, %r6927, %r6926;
	add.s32 	%r6929, %r6928, 1859775393;
	shf.l.wrap.b32 	%r6930, %r6929, %r6929, 11;
	xor.b32  	%r6931, %r6924, %r6918;
	xor.b32  	%r6932, %r6931, %r6930;
	add.s32 	%r6933, %r6582, %r6912;
	add.s32 	%r6934, %r6933, %r6932;
	add.s32 	%r6935, %r6934, 1859775393;
	shf.l.wrap.b32 	%r6936, %r6935, %r6935, 15;
	xor.b32  	%r6937, %r6930, %r6924;
	xor.b32  	%r6938, %r6937, %r6936;
	add.s32 	%r6939, %r6622, %r6918;
	add.s32 	%r6940, %r6939, %r6938;
	add.s32 	%r6941, %r6940, 1859775393;
	shf.l.wrap.b32 	%r6942, %r6941, %r6941, 3;
	xor.b32  	%r6943, %r6936, %r6930;
	xor.b32  	%r6944, %r6943, %r6942;
	add.s32 	%r6945, %r6590, %r6924;
	add.s32 	%r6946, %r6945, %r6944;
	add.s32 	%r6947, %r6946, 1859775393;
	shf.l.wrap.b32 	%r6948, %r6947, %r6947, 9;
	xor.b32  	%r6949, %r6942, %r6936;
	xor.b32  	%r6950, %r6949, %r6948;
	add.s32 	%r6951, %r6606, %r6930;
	add.s32 	%r6952, %r6951, %r6950;
	add.s32 	%r6953, %r6952, 1859775393;
	shf.l.wrap.b32 	%r6954, %r6953, %r6953, 11;
	xor.b32  	%r6955, %r6948, %r6942;
	xor.b32  	%r6956, %r6955, %r6954;
	add.s32 	%r6957, %r6574, %r6936;
	add.s32 	%r6958, %r6957, %r6956;
	add.s32 	%r6959, %r6958, 1859775393;
	shf.l.wrap.b32 	%r6960, %r6959, %r6959, 15;
	add.s32 	%r7, %r6942, %r7;
	add.s32 	%r6, %r6960, %r6;
	add.s32 	%r5, %r6954, %r5;
	add.s32 	%r4, %r6948, %r4;
	add.s32 	%r6984, %r6984, 32;
	add.s32 	%r6985, %r6985, 8;

BB1_2:
	add.s32 	%r1042, %r2, -32;
	setp.lt.s32	%p2, %r6984, %r1042;
	mul.wide.s32 	%rd24, %r6985, 4;
	add.s64 	%rd25, %rd21, %rd24;
	ld.global.u32 	%r10, [%rd25];
	ld.global.u32 	%r11, [%rd25+4];
	ld.global.u32 	%r12, [%rd25+8];
	ld.global.u32 	%r13, [%rd25+12];
	ld.global.u32 	%r14, [%rd25+16];
	ld.global.u32 	%r15, [%rd25+20];
	ld.global.u32 	%r16, [%rd25+24];
	ld.global.u32 	%r17, [%rd25+28];
	@%p2 bra 	BB1_189;

	mov.u32 	%r6986, 0;
	mov.u32 	%r1102, 29554;
	// inline asm
	prmt.b32 %r1043, %r17, %r6986, %r1102;
	// inline asm
	mov.u32 	%r1106, 29040;
	// inline asm
	prmt.b32 %r1047, %r17, %r6986, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r1051, %r16, %r6986, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r1055, %r16, %r6986, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r1059, %r15, %r6986, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r1063, %r15, %r6986, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r1067, %r14, %r6986, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r1071, %r14, %r6986, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r1075, %r13, %r6986, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r1079, %r13, %r6986, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r1083, %r12, %r6986, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r1087, %r12, %r6986, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r1091, %r11, %r6986, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r1095, %r11, %r6986, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r1099, %r10, %r6986, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r1103, %r10, %r6986, %r1106;
	// inline asm
	sub.s32 	%r1107, %r2, %r6984;
	shl.b32 	%r1108, %r1107, 1;
	add.s32 	%r34, %r1108, %r6979;
	setp.lt.s32	%p3, %r1108, 64;
	@%p3 bra 	BB1_5;
	bra.uni 	BB1_4;

BB1_5:
	mov.u32 	%r1575, 30292;
	// inline asm
	prmt.b32 %r6986, %r1047, %r1043, %r1575;
	// inline asm
	// inline asm
	prmt.b32 %r6987, %r1051, %r1047, %r1575;
	// inline asm
	// inline asm
	prmt.b32 %r6988, %r1055, %r1051, %r1575;
	// inline asm
	// inline asm
	prmt.b32 %r6989, %r1059, %r1055, %r1575;
	// inline asm
	// inline asm
	prmt.b32 %r6990, %r1063, %r1059, %r1575;
	// inline asm
	// inline asm
	prmt.b32 %r6991, %r1067, %r1063, %r1575;
	// inline asm
	// inline asm
	prmt.b32 %r6992, %r1071, %r1067, %r1575;
	// inline asm
	// inline asm
	prmt.b32 %r6993, %r1075, %r1071, %r1575;
	// inline asm
	// inline asm
	prmt.b32 %r6994, %r1079, %r1075, %r1575;
	// inline asm
	// inline asm
	prmt.b32 %r6995, %r1083, %r1079, %r1575;
	// inline asm
	// inline asm
	prmt.b32 %r6996, %r1087, %r1083, %r1575;
	// inline asm
	// inline asm
	prmt.b32 %r6997, %r1091, %r1087, %r1575;
	// inline asm
	// inline asm
	prmt.b32 %r6998, %r1095, %r1091, %r1575;
	// inline asm
	// inline asm
	prmt.b32 %r6999, %r1099, %r1095, %r1575;
	// inline asm
	// inline asm
	prmt.b32 %r7000, %r1103, %r1099, %r1575;
	// inline asm
	mov.u32 	%r1573, 0;
	// inline asm
	prmt.b32 %r7001, %r1573, %r1103, %r1575;
	// inline asm
	bra.uni 	BB1_6;

BB1_4:
	// inline asm
	shf.r.wrap.b32 %r1109, %r1043, %r6986, %r6986;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1113, %r1047, %r1043, %r6986;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1117, %r1051, %r1047, %r6986;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1121, %r1055, %r1051, %r6986;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1125, %r1059, %r1055, %r6986;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1129, %r1063, %r1059, %r6986;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1133, %r1067, %r1063, %r6986;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1137, %r1071, %r1067, %r6986;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1141, %r1075, %r1071, %r6986;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1145, %r1079, %r1075, %r6986;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1149, %r1083, %r1079, %r6986;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1153, %r1087, %r1083, %r6986;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1157, %r1091, %r1087, %r6986;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1161, %r1095, %r1091, %r6986;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1165, %r1099, %r1095, %r6986;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1169, %r1103, %r1099, %r6986;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1173, %r6986, %r1103, %r6986;
	// inline asm
	xor.b32  	%r1193, %r5, %r4;
	and.b32  	%r1194, %r1193, %r6;
	xor.b32  	%r1195, %r1194, %r4;
	add.s32 	%r1196, %r1195, %r7;
	add.s32 	%r1197, %r1196, %r1169;
	shf.l.wrap.b32 	%r1198, %r1197, %r1197, 3;
	xor.b32  	%r1199, %r6, %r5;
	and.b32  	%r1200, %r1198, %r1199;
	xor.b32  	%r1201, %r1200, %r5;
	add.s32 	%r1202, %r1165, %r4;
	add.s32 	%r1203, %r1202, %r1201;
	shf.l.wrap.b32 	%r1204, %r1203, %r1203, 7;
	xor.b32  	%r1205, %r1198, %r6;
	and.b32  	%r1206, %r1205, %r1204;
	xor.b32  	%r1207, %r1206, %r6;
	add.s32 	%r1208, %r1161, %r5;
	add.s32 	%r1209, %r1208, %r1207;
	shf.l.wrap.b32 	%r1210, %r1209, %r1209, 11;
	xor.b32  	%r1211, %r1204, %r1198;
	and.b32  	%r1212, %r1211, %r1210;
	xor.b32  	%r1213, %r1212, %r1198;
	add.s32 	%r1214, %r1157, %r6;
	add.s32 	%r1215, %r1214, %r1213;
	shf.l.wrap.b32 	%r1216, %r1215, %r1215, 19;
	xor.b32  	%r1217, %r1210, %r1204;
	and.b32  	%r1218, %r1217, %r1216;
	xor.b32  	%r1219, %r1218, %r1204;
	add.s32 	%r1220, %r1198, %r1153;
	add.s32 	%r1221, %r1220, %r1219;
	shf.l.wrap.b32 	%r1222, %r1221, %r1221, 3;
	xor.b32  	%r1223, %r1216, %r1210;
	and.b32  	%r1224, %r1223, %r1222;
	xor.b32  	%r1225, %r1224, %r1210;
	add.s32 	%r1226, %r1204, %r1149;
	add.s32 	%r1227, %r1226, %r1225;
	shf.l.wrap.b32 	%r1228, %r1227, %r1227, 7;
	xor.b32  	%r1229, %r1222, %r1216;
	and.b32  	%r1230, %r1229, %r1228;
	xor.b32  	%r1231, %r1230, %r1216;
	add.s32 	%r1232, %r1210, %r1145;
	add.s32 	%r1233, %r1232, %r1231;
	shf.l.wrap.b32 	%r1234, %r1233, %r1233, 11;
	xor.b32  	%r1235, %r1228, %r1222;
	and.b32  	%r1236, %r1235, %r1234;
	xor.b32  	%r1237, %r1236, %r1222;
	add.s32 	%r1238, %r1216, %r1141;
	add.s32 	%r1239, %r1238, %r1237;
	shf.l.wrap.b32 	%r1240, %r1239, %r1239, 19;
	xor.b32  	%r1241, %r1234, %r1228;
	and.b32  	%r1242, %r1241, %r1240;
	xor.b32  	%r1243, %r1242, %r1228;
	add.s32 	%r1244, %r1222, %r1137;
	add.s32 	%r1245, %r1244, %r1243;
	shf.l.wrap.b32 	%r1246, %r1245, %r1245, 3;
	xor.b32  	%r1247, %r1240, %r1234;
	and.b32  	%r1248, %r1247, %r1246;
	xor.b32  	%r1249, %r1248, %r1234;
	add.s32 	%r1250, %r1228, %r1133;
	add.s32 	%r1251, %r1250, %r1249;
	shf.l.wrap.b32 	%r1252, %r1251, %r1251, 7;
	xor.b32  	%r1253, %r1246, %r1240;
	and.b32  	%r1254, %r1253, %r1252;
	xor.b32  	%r1255, %r1254, %r1240;
	add.s32 	%r1256, %r1234, %r1129;
	add.s32 	%r1257, %r1256, %r1255;
	shf.l.wrap.b32 	%r1258, %r1257, %r1257, 11;
	xor.b32  	%r1259, %r1252, %r1246;
	and.b32  	%r1260, %r1259, %r1258;
	xor.b32  	%r1261, %r1260, %r1246;
	add.s32 	%r1262, %r1240, %r1125;
	add.s32 	%r1263, %r1262, %r1261;
	shf.l.wrap.b32 	%r1264, %r1263, %r1263, 19;
	xor.b32  	%r1265, %r1258, %r1252;
	and.b32  	%r1266, %r1265, %r1264;
	xor.b32  	%r1267, %r1266, %r1252;
	add.s32 	%r1268, %r1246, %r1121;
	add.s32 	%r1269, %r1268, %r1267;
	shf.l.wrap.b32 	%r1270, %r1269, %r1269, 3;
	xor.b32  	%r1271, %r1264, %r1258;
	and.b32  	%r1272, %r1271, %r1270;
	xor.b32  	%r1273, %r1272, %r1258;
	add.s32 	%r1274, %r1252, %r1117;
	add.s32 	%r1275, %r1274, %r1273;
	shf.l.wrap.b32 	%r1276, %r1275, %r1275, 7;
	xor.b32  	%r1277, %r1270, %r1264;
	and.b32  	%r1278, %r1277, %r1276;
	xor.b32  	%r1279, %r1278, %r1264;
	add.s32 	%r1280, %r1258, %r1113;
	add.s32 	%r1281, %r1280, %r1279;
	shf.l.wrap.b32 	%r1282, %r1281, %r1281, 11;
	xor.b32  	%r1283, %r1276, %r1270;
	and.b32  	%r1284, %r1283, %r1282;
	xor.b32  	%r1285, %r1284, %r1270;
	add.s32 	%r1286, %r1264, %r1109;
	add.s32 	%r1287, %r1286, %r1285;
	shf.l.wrap.b32 	%r1288, %r1287, %r1287, 19;
	xor.b32  	%r1289, %r1288, %r1276;
	xor.b32  	%r1290, %r1288, %r1282;
	and.b32  	%r1291, %r1290, %r1289;
	xor.b32  	%r1292, %r1291, %r1288;
	add.s32 	%r1293, %r1169, %r1270;
	add.s32 	%r1294, %r1293, %r1292;
	add.s32 	%r1295, %r1294, 1518500249;
	shf.l.wrap.b32 	%r1296, %r1295, %r1295, 3;
	xor.b32  	%r1297, %r1296, %r1282;
	xor.b32  	%r1298, %r1296, %r1288;
	and.b32  	%r1299, %r1298, %r1297;
	xor.b32  	%r1300, %r1299, %r1296;
	add.s32 	%r1301, %r1153, %r1276;
	add.s32 	%r1302, %r1301, %r1300;
	add.s32 	%r1303, %r1302, 1518500249;
	shf.l.wrap.b32 	%r1304, %r1303, %r1303, 5;
	xor.b32  	%r1305, %r1304, %r1288;
	xor.b32  	%r1306, %r1304, %r1296;
	and.b32  	%r1307, %r1306, %r1305;
	xor.b32  	%r1308, %r1307, %r1304;
	add.s32 	%r1309, %r1137, %r1282;
	add.s32 	%r1310, %r1309, %r1308;
	add.s32 	%r1311, %r1310, 1518500249;
	shf.l.wrap.b32 	%r1312, %r1311, %r1311, 9;
	xor.b32  	%r1313, %r1312, %r1296;
	xor.b32  	%r1314, %r1312, %r1304;
	and.b32  	%r1315, %r1314, %r1313;
	xor.b32  	%r1316, %r1315, %r1312;
	add.s32 	%r1317, %r1121, %r1288;
	add.s32 	%r1318, %r1317, %r1316;
	add.s32 	%r1319, %r1318, 1518500249;
	shf.l.wrap.b32 	%r1320, %r1319, %r1319, 13;
	xor.b32  	%r1321, %r1320, %r1304;
	xor.b32  	%r1322, %r1320, %r1312;
	and.b32  	%r1323, %r1322, %r1321;
	xor.b32  	%r1324, %r1323, %r1320;
	add.s32 	%r1325, %r1165, %r1296;
	add.s32 	%r1326, %r1325, %r1324;
	add.s32 	%r1327, %r1326, 1518500249;
	shf.l.wrap.b32 	%r1328, %r1327, %r1327, 3;
	xor.b32  	%r1329, %r1328, %r1312;
	xor.b32  	%r1330, %r1328, %r1320;
	and.b32  	%r1331, %r1330, %r1329;
	xor.b32  	%r1332, %r1331, %r1328;
	add.s32 	%r1333, %r1149, %r1304;
	add.s32 	%r1334, %r1333, %r1332;
	add.s32 	%r1335, %r1334, 1518500249;
	shf.l.wrap.b32 	%r1336, %r1335, %r1335, 5;
	xor.b32  	%r1337, %r1336, %r1320;
	xor.b32  	%r1338, %r1336, %r1328;
	and.b32  	%r1339, %r1338, %r1337;
	xor.b32  	%r1340, %r1339, %r1336;
	add.s32 	%r1341, %r1133, %r1312;
	add.s32 	%r1342, %r1341, %r1340;
	add.s32 	%r1343, %r1342, 1518500249;
	shf.l.wrap.b32 	%r1344, %r1343, %r1343, 9;
	xor.b32  	%r1345, %r1344, %r1328;
	xor.b32  	%r1346, %r1344, %r1336;
	and.b32  	%r1347, %r1346, %r1345;
	xor.b32  	%r1348, %r1347, %r1344;
	add.s32 	%r1349, %r1117, %r1320;
	add.s32 	%r1350, %r1349, %r1348;
	add.s32 	%r1351, %r1350, 1518500249;
	shf.l.wrap.b32 	%r1352, %r1351, %r1351, 13;
	xor.b32  	%r1353, %r1352, %r1336;
	xor.b32  	%r1354, %r1352, %r1344;
	and.b32  	%r1355, %r1354, %r1353;
	xor.b32  	%r1356, %r1355, %r1352;
	add.s32 	%r1357, %r1161, %r1328;
	add.s32 	%r1358, %r1357, %r1356;
	add.s32 	%r1359, %r1358, 1518500249;
	shf.l.wrap.b32 	%r1360, %r1359, %r1359, 3;
	xor.b32  	%r1361, %r1360, %r1344;
	xor.b32  	%r1362, %r1360, %r1352;
	and.b32  	%r1363, %r1362, %r1361;
	xor.b32  	%r1364, %r1363, %r1360;
	add.s32 	%r1365, %r1145, %r1336;
	add.s32 	%r1366, %r1365, %r1364;
	add.s32 	%r1367, %r1366, 1518500249;
	shf.l.wrap.b32 	%r1368, %r1367, %r1367, 5;
	xor.b32  	%r1369, %r1368, %r1352;
	xor.b32  	%r1370, %r1368, %r1360;
	and.b32  	%r1371, %r1370, %r1369;
	xor.b32  	%r1372, %r1371, %r1368;
	add.s32 	%r1373, %r1129, %r1344;
	add.s32 	%r1374, %r1373, %r1372;
	add.s32 	%r1375, %r1374, 1518500249;
	shf.l.wrap.b32 	%r1376, %r1375, %r1375, 9;
	xor.b32  	%r1377, %r1376, %r1360;
	xor.b32  	%r1378, %r1376, %r1368;
	and.b32  	%r1379, %r1378, %r1377;
	xor.b32  	%r1380, %r1379, %r1376;
	add.s32 	%r1381, %r1113, %r1352;
	add.s32 	%r1382, %r1381, %r1380;
	add.s32 	%r1383, %r1382, 1518500249;
	shf.l.wrap.b32 	%r1384, %r1383, %r1383, 13;
	xor.b32  	%r1385, %r1384, %r1368;
	xor.b32  	%r1386, %r1384, %r1376;
	and.b32  	%r1387, %r1386, %r1385;
	xor.b32  	%r1388, %r1387, %r1384;
	add.s32 	%r1389, %r1157, %r1360;
	add.s32 	%r1390, %r1389, %r1388;
	add.s32 	%r1391, %r1390, 1518500249;
	shf.l.wrap.b32 	%r1392, %r1391, %r1391, 3;
	xor.b32  	%r1393, %r1392, %r1376;
	xor.b32  	%r1394, %r1392, %r1384;
	and.b32  	%r1395, %r1394, %r1393;
	xor.b32  	%r1396, %r1395, %r1392;
	add.s32 	%r1397, %r1141, %r1368;
	add.s32 	%r1398, %r1397, %r1396;
	add.s32 	%r1399, %r1398, 1518500249;
	shf.l.wrap.b32 	%r1400, %r1399, %r1399, 5;
	xor.b32  	%r1401, %r1400, %r1384;
	xor.b32  	%r1402, %r1400, %r1392;
	and.b32  	%r1403, %r1402, %r1401;
	xor.b32  	%r1404, %r1403, %r1400;
	add.s32 	%r1405, %r1125, %r1376;
	add.s32 	%r1406, %r1405, %r1404;
	add.s32 	%r1407, %r1406, 1518500249;
	shf.l.wrap.b32 	%r1408, %r1407, %r1407, 9;
	xor.b32  	%r1409, %r1408, %r1392;
	xor.b32  	%r1410, %r1408, %r1400;
	and.b32  	%r1411, %r1410, %r1409;
	xor.b32  	%r1412, %r1411, %r1408;
	add.s32 	%r1413, %r1109, %r1384;
	add.s32 	%r1414, %r1413, %r1412;
	add.s32 	%r1415, %r1414, 1518500249;
	shf.l.wrap.b32 	%r1416, %r1415, %r1415, 13;
	xor.b32  	%r1417, %r1410, %r1416;
	add.s32 	%r1418, %r1169, %r1392;
	add.s32 	%r1419, %r1418, %r1417;
	add.s32 	%r1420, %r1419, 1859775393;
	shf.l.wrap.b32 	%r1421, %r1420, %r1420, 3;
	xor.b32  	%r1422, %r1416, %r1408;
	xor.b32  	%r1423, %r1422, %r1421;
	add.s32 	%r1424, %r1137, %r1400;
	add.s32 	%r1425, %r1424, %r1423;
	add.s32 	%r1426, %r1425, 1859775393;
	shf.l.wrap.b32 	%r1427, %r1426, %r1426, 9;
	xor.b32  	%r1428, %r1421, %r1416;
	xor.b32  	%r1429, %r1428, %r1427;
	add.s32 	%r1430, %r1153, %r1408;
	add.s32 	%r1431, %r1430, %r1429;
	add.s32 	%r1432, %r1431, 1859775393;
	shf.l.wrap.b32 	%r1433, %r1432, %r1432, 11;
	xor.b32  	%r1434, %r1427, %r1421;
	xor.b32  	%r1435, %r1434, %r1433;
	add.s32 	%r1436, %r1121, %r1416;
	add.s32 	%r1437, %r1436, %r1435;
	add.s32 	%r1438, %r1437, 1859775393;
	shf.l.wrap.b32 	%r1439, %r1438, %r1438, 15;
	xor.b32  	%r1440, %r1433, %r1427;
	xor.b32  	%r1441, %r1440, %r1439;
	add.s32 	%r1442, %r1161, %r1421;
	add.s32 	%r1443, %r1442, %r1441;
	add.s32 	%r1444, %r1443, 1859775393;
	shf.l.wrap.b32 	%r1445, %r1444, %r1444, 3;
	xor.b32  	%r1446, %r1439, %r1433;
	xor.b32  	%r1447, %r1446, %r1445;
	add.s32 	%r1448, %r1129, %r1427;
	add.s32 	%r1449, %r1448, %r1447;
	add.s32 	%r1450, %r1449, 1859775393;
	shf.l.wrap.b32 	%r1451, %r1450, %r1450, 9;
	xor.b32  	%r1452, %r1445, %r1439;
	xor.b32  	%r1453, %r1452, %r1451;
	add.s32 	%r1454, %r1145, %r1433;
	add.s32 	%r1455, %r1454, %r1453;
	add.s32 	%r1456, %r1455, 1859775393;
	shf.l.wrap.b32 	%r1457, %r1456, %r1456, 11;
	xor.b32  	%r1458, %r1451, %r1445;
	xor.b32  	%r1459, %r1458, %r1457;
	add.s32 	%r1460, %r1113, %r1439;
	add.s32 	%r1461, %r1460, %r1459;
	add.s32 	%r1462, %r1461, 1859775393;
	shf.l.wrap.b32 	%r1463, %r1462, %r1462, 15;
	xor.b32  	%r1464, %r1457, %r1451;
	xor.b32  	%r1465, %r1464, %r1463;
	add.s32 	%r1466, %r1165, %r1445;
	add.s32 	%r1467, %r1466, %r1465;
	add.s32 	%r1468, %r1467, 1859775393;
	shf.l.wrap.b32 	%r1469, %r1468, %r1468, 3;
	xor.b32  	%r1470, %r1463, %r1457;
	xor.b32  	%r1471, %r1470, %r1469;
	add.s32 	%r1472, %r1133, %r1451;
	add.s32 	%r1473, %r1472, %r1471;
	add.s32 	%r1474, %r1473, 1859775393;
	shf.l.wrap.b32 	%r1475, %r1474, %r1474, 9;
	xor.b32  	%r1476, %r1469, %r1463;
	xor.b32  	%r1477, %r1476, %r1475;
	add.s32 	%r1478, %r1149, %r1457;
	add.s32 	%r1479, %r1478, %r1477;
	add.s32 	%r1480, %r1479, 1859775393;
	shf.l.wrap.b32 	%r1481, %r1480, %r1480, 11;
	xor.b32  	%r1482, %r1475, %r1469;
	xor.b32  	%r1483, %r1482, %r1481;
	add.s32 	%r1484, %r1117, %r1463;
	add.s32 	%r1485, %r1484, %r1483;
	add.s32 	%r1486, %r1485, 1859775393;
	shf.l.wrap.b32 	%r1487, %r1486, %r1486, 15;
	xor.b32  	%r1488, %r1481, %r1475;
	xor.b32  	%r1489, %r1488, %r1487;
	add.s32 	%r1490, %r1157, %r1469;
	add.s32 	%r1491, %r1490, %r1489;
	add.s32 	%r1492, %r1491, 1859775393;
	shf.l.wrap.b32 	%r1493, %r1492, %r1492, 3;
	xor.b32  	%r1494, %r1487, %r1481;
	xor.b32  	%r1495, %r1494, %r1493;
	add.s32 	%r1496, %r1125, %r1475;
	add.s32 	%r1497, %r1496, %r1495;
	add.s32 	%r1498, %r1497, 1859775393;
	shf.l.wrap.b32 	%r1499, %r1498, %r1498, 9;
	xor.b32  	%r1500, %r1493, %r1487;
	xor.b32  	%r1501, %r1500, %r1499;
	add.s32 	%r1502, %r1141, %r1481;
	add.s32 	%r1503, %r1502, %r1501;
	add.s32 	%r1504, %r1503, 1859775393;
	shf.l.wrap.b32 	%r1505, %r1504, %r1504, 11;
	xor.b32  	%r1506, %r1499, %r1493;
	xor.b32  	%r1507, %r1506, %r1505;
	add.s32 	%r1508, %r1109, %r1487;
	add.s32 	%r1509, %r1508, %r1507;
	add.s32 	%r1510, %r1509, 1859775393;
	shf.l.wrap.b32 	%r1511, %r1510, %r1510, 15;
	add.s32 	%r7, %r1493, %r7;
	add.s32 	%r6, %r1511, %r6;
	add.s32 	%r5, %r1505, %r5;
	add.s32 	%r4, %r1499, %r4;
	mov.u32 	%r6987, %r6986;
	mov.u32 	%r6988, %r6986;
	mov.u32 	%r6989, %r6986;
	mov.u32 	%r6990, %r6986;
	mov.u32 	%r6991, %r6986;
	mov.u32 	%r6992, %r6986;
	mov.u32 	%r6993, %r6986;
	mov.u32 	%r6994, %r6986;
	mov.u32 	%r6995, %r6986;
	mov.u32 	%r6996, %r6986;
	mov.u32 	%r6997, %r6986;
	mov.u32 	%r6998, %r6986;
	mov.u32 	%r6999, %r6986;
	mov.u32 	%r7000, %r6986;
	mov.u32 	%r7001, %r6986;

BB1_6:
	ld.param.u32 	%r6975, [m01000_mxx_param_30];
	setp.eq.s32	%p4, %r6975, 0;
	@%p4 bra 	BB1_141;

	ld.param.u32 	%r6978, [m01000_mxx_param_32];
	ld.param.u32 	%r6977, [m01000_mxx_param_26];
	ld.param.u32 	%r6976, [m01000_mxx_param_25];
	and.b32  	%r75, %r6976, 31;
	and.b32  	%r76, %r6977, 31;
	cvt.u64.u32	%rd1, %r6978;
	mov.u32 	%r7006, 0;

BB1_8:
	mov.u32 	%r7028, 0;
	mul.wide.u32 	%rd26, %r7006, 260;
	add.s64 	%rd27, %rd5, %rd26;
	ld.global.u32 	%r78, [%rd27+256];
	mov.u32 	%r7007, %r34;
	mov.u32 	%r7128, %r6986;
	mov.u32 	%r7129, %r6987;
	mov.u32 	%r7130, %r6988;
	mov.u32 	%r7131, %r6989;
	mov.u32 	%r7124, %r6990;
	mov.u32 	%r7125, %r6991;
	mov.u32 	%r7126, %r6992;
	mov.u32 	%r7127, %r6993;
	mov.u32 	%r7120, %r6994;
	mov.u32 	%r7121, %r6995;
	mov.u32 	%r7122, %r6996;
	mov.u32 	%r7123, %r6997;
	mov.u32 	%r7151, %r6998;
	mov.u32 	%r7150, %r6999;
	mov.u32 	%r7149, %r7000;
	mov.u32 	%r7148, %r7001;
	mov.u32 	%r96, %r4;
	mov.u32 	%r97, %r5;
	mov.u32 	%r98, %r6;
	mov.u32 	%r99, %r7;
	mov.u32 	%r7029, %r7028;
	bra.uni 	BB1_9;

BB1_188:
	xor.b32  	%r6175, %r97, %r96;
	and.b32  	%r6176, %r6175, %r98;
	xor.b32  	%r6177, %r6176, %r96;
	add.s32 	%r6178, %r6177, %r99;
	or.b32  	%r6179, %r7135, %r95;
	add.s32 	%r6180, %r6178, %r6179;
	shf.l.wrap.b32 	%r6181, %r6180, %r6180, 3;
	xor.b32  	%r6182, %r98, %r97;
	and.b32  	%r6183, %r6181, %r6182;
	xor.b32  	%r6184, %r6183, %r97;
	or.b32  	%r6185, %r7134, %r94;
	add.s32 	%r6186, %r6185, %r96;
	add.s32 	%r6187, %r6186, %r6184;
	shf.l.wrap.b32 	%r6188, %r6187, %r6187, 7;
	xor.b32  	%r6189, %r6181, %r98;
	and.b32  	%r6190, %r6189, %r6188;
	xor.b32  	%r6191, %r6190, %r98;
	or.b32  	%r6192, %r7133, %r93;
	add.s32 	%r6193, %r6192, %r97;
	add.s32 	%r6194, %r6193, %r6191;
	shf.l.wrap.b32 	%r6195, %r6194, %r6194, 11;
	xor.b32  	%r6196, %r6188, %r6181;
	and.b32  	%r6197, %r6196, %r6195;
	xor.b32  	%r6198, %r6197, %r6181;
	or.b32  	%r6199, %r7132, %r92;
	add.s32 	%r6200, %r6199, %r98;
	add.s32 	%r6201, %r6200, %r6198;
	shf.l.wrap.b32 	%r6202, %r6201, %r6201, 19;
	xor.b32  	%r6203, %r6195, %r6188;
	and.b32  	%r6204, %r6203, %r6202;
	xor.b32  	%r6205, %r6204, %r6188;
	or.b32  	%r6206, %r7139, %r91;
	add.s32 	%r6207, %r6181, %r6206;
	add.s32 	%r6208, %r6207, %r6205;
	shf.l.wrap.b32 	%r6209, %r6208, %r6208, 3;
	xor.b32  	%r6210, %r6202, %r6195;
	and.b32  	%r6211, %r6210, %r6209;
	xor.b32  	%r6212, %r6211, %r6195;
	or.b32  	%r6213, %r7138, %r90;
	add.s32 	%r6214, %r6188, %r6213;
	add.s32 	%r6215, %r6214, %r6212;
	shf.l.wrap.b32 	%r6216, %r6215, %r6215, 7;
	xor.b32  	%r6217, %r6209, %r6202;
	and.b32  	%r6218, %r6217, %r6216;
	xor.b32  	%r6219, %r6218, %r6202;
	or.b32  	%r6220, %r7137, %r89;
	add.s32 	%r6221, %r6195, %r6220;
	add.s32 	%r6222, %r6221, %r6219;
	shf.l.wrap.b32 	%r6223, %r6222, %r6222, 11;
	xor.b32  	%r6224, %r6216, %r6209;
	and.b32  	%r6225, %r6224, %r6223;
	xor.b32  	%r6226, %r6225, %r6209;
	or.b32  	%r6227, %r7136, %r88;
	add.s32 	%r6228, %r6202, %r6227;
	add.s32 	%r6229, %r6228, %r6226;
	shf.l.wrap.b32 	%r6230, %r6229, %r6229, 19;
	xor.b32  	%r6231, %r6223, %r6216;
	and.b32  	%r6232, %r6231, %r6230;
	xor.b32  	%r6233, %r6232, %r6216;
	or.b32  	%r6234, %r7143, %r87;
	add.s32 	%r6235, %r6209, %r6234;
	add.s32 	%r6236, %r6235, %r6233;
	shf.l.wrap.b32 	%r6237, %r6236, %r6236, 3;
	xor.b32  	%r6238, %r6230, %r6223;
	and.b32  	%r6239, %r6238, %r6237;
	xor.b32  	%r6240, %r6239, %r6223;
	or.b32  	%r6241, %r7142, %r86;
	add.s32 	%r6242, %r6216, %r6241;
	add.s32 	%r6243, %r6242, %r6240;
	shf.l.wrap.b32 	%r6244, %r6243, %r6243, 7;
	xor.b32  	%r6245, %r6237, %r6230;
	and.b32  	%r6246, %r6245, %r6244;
	xor.b32  	%r6247, %r6246, %r6230;
	or.b32  	%r6248, %r7141, %r85;
	add.s32 	%r6249, %r6223, %r6248;
	add.s32 	%r6250, %r6249, %r6247;
	shf.l.wrap.b32 	%r6251, %r6250, %r6250, 11;
	xor.b32  	%r6252, %r6244, %r6237;
	and.b32  	%r6253, %r6252, %r6251;
	xor.b32  	%r6254, %r6253, %r6237;
	or.b32  	%r6255, %r7140, %r84;
	add.s32 	%r6256, %r6230, %r6255;
	add.s32 	%r6257, %r6256, %r6254;
	shf.l.wrap.b32 	%r6258, %r6257, %r6257, 19;
	xor.b32  	%r6259, %r6251, %r6244;
	and.b32  	%r6260, %r6259, %r6258;
	xor.b32  	%r6261, %r6260, %r6244;
	or.b32  	%r6262, %r7147, %r83;
	add.s32 	%r6263, %r6237, %r6262;
	add.s32 	%r6264, %r6263, %r6261;
	shf.l.wrap.b32 	%r6265, %r6264, %r6264, 3;
	xor.b32  	%r6266, %r6258, %r6251;
	and.b32  	%r6267, %r6266, %r6265;
	xor.b32  	%r6268, %r6267, %r6251;
	or.b32  	%r6269, %r7146, %r82;
	add.s32 	%r6270, %r6244, %r6269;
	add.s32 	%r6271, %r6270, %r6268;
	shf.l.wrap.b32 	%r6272, %r6271, %r6271, 7;
	xor.b32  	%r6273, %r6265, %r6258;
	and.b32  	%r6274, %r6273, %r6272;
	xor.b32  	%r6275, %r6274, %r6258;
	or.b32  	%r6276, %r7145, %r81;
	add.s32 	%r6277, %r6251, %r6276;
	add.s32 	%r6278, %r6277, %r6275;
	shf.l.wrap.b32 	%r6279, %r6278, %r6278, 11;
	xor.b32  	%r6280, %r6272, %r6265;
	and.b32  	%r6281, %r6280, %r6279;
	xor.b32  	%r6282, %r6281, %r6265;
	or.b32  	%r6283, %r7144, %r80;
	add.s32 	%r6284, %r6258, %r6283;
	add.s32 	%r6285, %r6284, %r6282;
	shf.l.wrap.b32 	%r6286, %r6285, %r6285, 19;
	xor.b32  	%r6287, %r6286, %r6272;
	xor.b32  	%r6288, %r6286, %r6279;
	and.b32  	%r6289, %r6288, %r6287;
	xor.b32  	%r6290, %r6289, %r6286;
	add.s32 	%r6291, %r6179, %r6265;
	add.s32 	%r6292, %r6291, %r6290;
	add.s32 	%r6293, %r6292, 1518500249;
	shf.l.wrap.b32 	%r6294, %r6293, %r6293, 3;
	xor.b32  	%r6295, %r6294, %r6279;
	xor.b32  	%r6296, %r6294, %r6286;
	and.b32  	%r6297, %r6296, %r6295;
	xor.b32  	%r6298, %r6297, %r6294;
	add.s32 	%r6299, %r6206, %r6272;
	add.s32 	%r6300, %r6299, %r6298;
	add.s32 	%r6301, %r6300, 1518500249;
	shf.l.wrap.b32 	%r6302, %r6301, %r6301, 5;
	xor.b32  	%r6303, %r6302, %r6286;
	xor.b32  	%r6304, %r6302, %r6294;
	and.b32  	%r6305, %r6304, %r6303;
	xor.b32  	%r6306, %r6305, %r6302;
	add.s32 	%r6307, %r6234, %r6279;
	add.s32 	%r6308, %r6307, %r6306;
	add.s32 	%r6309, %r6308, 1518500249;
	shf.l.wrap.b32 	%r6310, %r6309, %r6309, 9;
	xor.b32  	%r6311, %r6310, %r6294;
	xor.b32  	%r6312, %r6310, %r6302;
	and.b32  	%r6313, %r6312, %r6311;
	xor.b32  	%r6314, %r6313, %r6310;
	add.s32 	%r6315, %r6262, %r6286;
	add.s32 	%r6316, %r6315, %r6314;
	add.s32 	%r6317, %r6316, 1518500249;
	shf.l.wrap.b32 	%r6318, %r6317, %r6317, 13;
	xor.b32  	%r6319, %r6318, %r6302;
	xor.b32  	%r6320, %r6318, %r6310;
	and.b32  	%r6321, %r6320, %r6319;
	xor.b32  	%r6322, %r6321, %r6318;
	add.s32 	%r6323, %r6185, %r6294;
	add.s32 	%r6324, %r6323, %r6322;
	add.s32 	%r6325, %r6324, 1518500249;
	shf.l.wrap.b32 	%r6326, %r6325, %r6325, 3;
	xor.b32  	%r6327, %r6326, %r6310;
	xor.b32  	%r6328, %r6326, %r6318;
	and.b32  	%r6329, %r6328, %r6327;
	xor.b32  	%r6330, %r6329, %r6326;
	add.s32 	%r6331, %r6213, %r6302;
	add.s32 	%r6332, %r6331, %r6330;
	add.s32 	%r6333, %r6332, 1518500249;
	shf.l.wrap.b32 	%r6334, %r6333, %r6333, 5;
	xor.b32  	%r6335, %r6334, %r6318;
	xor.b32  	%r6336, %r6334, %r6326;
	and.b32  	%r6337, %r6336, %r6335;
	xor.b32  	%r6338, %r6337, %r6334;
	add.s32 	%r6339, %r6241, %r6310;
	add.s32 	%r6340, %r6339, %r6338;
	add.s32 	%r6341, %r6340, 1518500249;
	shf.l.wrap.b32 	%r6342, %r6341, %r6341, 9;
	xor.b32  	%r6343, %r6342, %r6326;
	xor.b32  	%r6344, %r6342, %r6334;
	and.b32  	%r6345, %r6344, %r6343;
	xor.b32  	%r6346, %r6345, %r6342;
	add.s32 	%r6347, %r6269, %r6318;
	add.s32 	%r6348, %r6347, %r6346;
	add.s32 	%r6349, %r6348, 1518500249;
	shf.l.wrap.b32 	%r6350, %r6349, %r6349, 13;
	xor.b32  	%r6351, %r6350, %r6334;
	xor.b32  	%r6352, %r6350, %r6342;
	and.b32  	%r6353, %r6352, %r6351;
	xor.b32  	%r6354, %r6353, %r6350;
	add.s32 	%r6355, %r6192, %r6326;
	add.s32 	%r6356, %r6355, %r6354;
	add.s32 	%r6357, %r6356, 1518500249;
	shf.l.wrap.b32 	%r6358, %r6357, %r6357, 3;
	xor.b32  	%r6359, %r6358, %r6342;
	xor.b32  	%r6360, %r6358, %r6350;
	and.b32  	%r6361, %r6360, %r6359;
	xor.b32  	%r6362, %r6361, %r6358;
	add.s32 	%r6363, %r6220, %r6334;
	add.s32 	%r6364, %r6363, %r6362;
	add.s32 	%r6365, %r6364, 1518500249;
	shf.l.wrap.b32 	%r6366, %r6365, %r6365, 5;
	xor.b32  	%r6367, %r6366, %r6350;
	xor.b32  	%r6368, %r6366, %r6358;
	and.b32  	%r6369, %r6368, %r6367;
	xor.b32  	%r6370, %r6369, %r6366;
	add.s32 	%r6371, %r6248, %r6342;
	add.s32 	%r6372, %r6371, %r6370;
	add.s32 	%r6373, %r6372, 1518500249;
	shf.l.wrap.b32 	%r6374, %r6373, %r6373, 9;
	xor.b32  	%r6375, %r6374, %r6358;
	xor.b32  	%r6376, %r6374, %r6366;
	and.b32  	%r6377, %r6376, %r6375;
	xor.b32  	%r6378, %r6377, %r6374;
	add.s32 	%r6379, %r6276, %r6350;
	add.s32 	%r6380, %r6379, %r6378;
	add.s32 	%r6381, %r6380, 1518500249;
	shf.l.wrap.b32 	%r6382, %r6381, %r6381, 13;
	xor.b32  	%r6383, %r6382, %r6366;
	xor.b32  	%r6384, %r6382, %r6374;
	and.b32  	%r6385, %r6384, %r6383;
	xor.b32  	%r6386, %r6385, %r6382;
	add.s32 	%r6387, %r6199, %r6358;
	add.s32 	%r6388, %r6387, %r6386;
	add.s32 	%r6389, %r6388, 1518500249;
	shf.l.wrap.b32 	%r6390, %r6389, %r6389, 3;
	xor.b32  	%r6391, %r6390, %r6374;
	xor.b32  	%r6392, %r6390, %r6382;
	and.b32  	%r6393, %r6392, %r6391;
	xor.b32  	%r6394, %r6393, %r6390;
	add.s32 	%r6395, %r6227, %r6366;
	add.s32 	%r6396, %r6395, %r6394;
	add.s32 	%r6397, %r6396, 1518500249;
	shf.l.wrap.b32 	%r6398, %r6397, %r6397, 5;
	xor.b32  	%r6399, %r6398, %r6382;
	xor.b32  	%r6400, %r6398, %r6390;
	and.b32  	%r6401, %r6400, %r6399;
	xor.b32  	%r6402, %r6401, %r6398;
	add.s32 	%r6403, %r6255, %r6374;
	add.s32 	%r6404, %r6403, %r6402;
	add.s32 	%r6405, %r6404, 1518500249;
	shf.l.wrap.b32 	%r6406, %r6405, %r6405, 9;
	xor.b32  	%r6407, %r6406, %r6390;
	xor.b32  	%r6408, %r6406, %r6398;
	and.b32  	%r6409, %r6408, %r6407;
	xor.b32  	%r6410, %r6409, %r6406;
	add.s32 	%r6411, %r6283, %r6382;
	add.s32 	%r6412, %r6411, %r6410;
	add.s32 	%r6413, %r6412, 1518500249;
	shf.l.wrap.b32 	%r6414, %r6413, %r6413, 13;
	xor.b32  	%r6415, %r6408, %r6414;
	add.s32 	%r6416, %r6179, %r6390;
	add.s32 	%r6417, %r6416, %r6415;
	add.s32 	%r6418, %r6417, 1859775393;
	shf.l.wrap.b32 	%r6419, %r6418, %r6418, 3;
	xor.b32  	%r6420, %r6414, %r6406;
	xor.b32  	%r6421, %r6420, %r6419;
	add.s32 	%r6422, %r6234, %r6398;
	add.s32 	%r6423, %r6422, %r6421;
	add.s32 	%r6424, %r6423, 1859775393;
	shf.l.wrap.b32 	%r6425, %r6424, %r6424, 9;
	xor.b32  	%r6426, %r6419, %r6414;
	xor.b32  	%r6427, %r6426, %r6425;
	add.s32 	%r6428, %r6206, %r6406;
	add.s32 	%r6429, %r6428, %r6427;
	add.s32 	%r6430, %r6429, 1859775393;
	shf.l.wrap.b32 	%r6431, %r6430, %r6430, 11;
	xor.b32  	%r6432, %r6425, %r6419;
	xor.b32  	%r6433, %r6432, %r6431;
	add.s32 	%r6434, %r6262, %r6414;
	add.s32 	%r6435, %r6434, %r6433;
	add.s32 	%r6436, %r6435, 1859775393;
	shf.l.wrap.b32 	%r6437, %r6436, %r6436, 15;
	xor.b32  	%r6438, %r6431, %r6425;
	xor.b32  	%r6439, %r6438, %r6437;
	add.s32 	%r6440, %r6192, %r6419;
	add.s32 	%r6441, %r6440, %r6439;
	add.s32 	%r6442, %r6441, 1859775393;
	shf.l.wrap.b32 	%r6443, %r6442, %r6442, 3;
	xor.b32  	%r6444, %r6437, %r6431;
	xor.b32  	%r6445, %r6444, %r6443;
	add.s32 	%r6446, %r6248, %r6425;
	add.s32 	%r6447, %r6446, %r6445;
	add.s32 	%r6448, %r6447, 1859775393;
	shf.l.wrap.b32 	%r6449, %r6448, %r6448, 9;
	xor.b32  	%r6450, %r6443, %r6437;
	xor.b32  	%r6451, %r6450, %r6449;
	add.s32 	%r6452, %r6220, %r6431;
	add.s32 	%r6453, %r6452, %r6451;
	add.s32 	%r6454, %r6453, 1859775393;
	shf.l.wrap.b32 	%r6455, %r6454, %r6454, 11;
	xor.b32  	%r6456, %r6449, %r6443;
	xor.b32  	%r6457, %r6456, %r6455;
	add.s32 	%r6458, %r6276, %r6437;
	add.s32 	%r6459, %r6458, %r6457;
	add.s32 	%r6460, %r6459, 1859775393;
	shf.l.wrap.b32 	%r6461, %r6460, %r6460, 15;
	xor.b32  	%r6462, %r6455, %r6449;
	xor.b32  	%r6463, %r6462, %r6461;
	add.s32 	%r6464, %r6185, %r6443;
	add.s32 	%r6465, %r6464, %r6463;
	add.s32 	%r6466, %r6465, 1859775393;
	shf.l.wrap.b32 	%r6467, %r6466, %r6466, 3;
	xor.b32  	%r6468, %r6461, %r6455;
	xor.b32  	%r6469, %r6468, %r6467;
	add.s32 	%r6470, %r6241, %r6449;
	add.s32 	%r6471, %r6470, %r6469;
	add.s32 	%r6472, %r6471, 1859775393;
	shf.l.wrap.b32 	%r6473, %r6472, %r6472, 9;
	xor.b32  	%r6474, %r6467, %r6461;
	xor.b32  	%r6475, %r6474, %r6473;
	add.s32 	%r6476, %r6213, %r6455;
	add.s32 	%r6477, %r6476, %r6475;
	add.s32 	%r6478, %r6477, 1859775393;
	shf.l.wrap.b32 	%r6479, %r6478, %r6478, 11;
	xor.b32  	%r6480, %r6473, %r6467;
	xor.b32  	%r6481, %r6480, %r6479;
	add.s32 	%r6482, %r6269, %r6461;
	add.s32 	%r6483, %r6482, %r6481;
	add.s32 	%r6484, %r6483, 1859775393;
	shf.l.wrap.b32 	%r6485, %r6484, %r6484, 15;
	xor.b32  	%r6486, %r6479, %r6473;
	xor.b32  	%r6487, %r6486, %r6485;
	add.s32 	%r6488, %r6199, %r6467;
	add.s32 	%r6489, %r6488, %r6487;
	add.s32 	%r6490, %r6489, 1859775393;
	shf.l.wrap.b32 	%r6491, %r6490, %r6490, 3;
	xor.b32  	%r6492, %r6485, %r6479;
	xor.b32  	%r6493, %r6492, %r6491;
	add.s32 	%r6494, %r6255, %r6473;
	add.s32 	%r6495, %r6494, %r6493;
	add.s32 	%r6496, %r6495, 1859775393;
	shf.l.wrap.b32 	%r6497, %r6496, %r6496, 9;
	xor.b32  	%r6498, %r6491, %r6485;
	xor.b32  	%r6499, %r6498, %r6497;
	add.s32 	%r6500, %r6227, %r6479;
	add.s32 	%r6501, %r6500, %r6499;
	add.s32 	%r6502, %r6501, 1859775393;
	shf.l.wrap.b32 	%r6503, %r6502, %r6502, 11;
	xor.b32  	%r6504, %r6497, %r6491;
	xor.b32  	%r6505, %r6504, %r6503;
	add.s32 	%r6506, %r6283, %r6485;
	add.s32 	%r6507, %r6506, %r6505;
	add.s32 	%r6508, %r6507, 1859775393;
	shf.l.wrap.b32 	%r6509, %r6508, %r6508, 15;
	add.s32 	%r99, %r6491, %r99;
	add.s32 	%r98, %r6509, %r98;
	add.s32 	%r97, %r6503, %r97;
	add.s32 	%r96, %r6497, %r96;
	add.s32 	%r7028, %r7028, 32;
	add.s32 	%r7029, %r7029, 8;
	add.s32 	%r7007, %r7007, 64;

BB1_9:
	mov.u32 	%r95, %r7148;
	mov.u32 	%r94, %r7149;
	mov.u32 	%r93, %r7150;
	mov.u32 	%r92, %r7151;
	mov.u32 	%r91, %r7123;
	mov.u32 	%r90, %r7122;
	mov.u32 	%r89, %r7121;
	mov.u32 	%r88, %r7120;
	mov.u32 	%r87, %r7127;
	mov.u32 	%r86, %r7126;
	mov.u32 	%r85, %r7125;
	mov.u32 	%r84, %r7124;
	mov.u32 	%r83, %r7131;
	mov.u32 	%r82, %r7130;
	mov.u32 	%r81, %r7129;
	mov.u32 	%r80, %r7128;
	cvt.u64.u32	%rd69, %r7006;
	add.s32 	%r1579, %r78, -32;
	setp.lt.s32	%p5, %r7028, %r1579;
	mul.lo.s64 	%rd28, %rd69, 260;
	add.s64 	%rd29, %rd5, %rd28;
	mul.wide.s32 	%rd30, %r7029, 4;
	add.s64 	%rd31, %rd29, %rd30;
	ld.global.u32 	%r102, [%rd31];
	ld.global.u32 	%r103, [%rd31+4];
	ld.global.u32 	%r104, [%rd31+8];
	ld.global.u32 	%r105, [%rd31+12];
	ld.global.u32 	%r106, [%rd31+16];
	ld.global.u32 	%r107, [%rd31+20];
	ld.global.u32 	%r108, [%rd31+24];
	ld.global.u32 	%r109, [%rd31+28];
	and.b32  	%r110, %r7007, 3;
	mov.u32 	%r1580, 4;
	sub.s32 	%r111, %r1580, %r110;
	@%p5 bra 	BB1_142;
	bra.uni 	BB1_10;

BB1_142:
	mov.u32 	%r7120, 0;
	// inline asm
	prmt.b32 %r7144, %r109, %r7120, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r7145, %r109, %r7120, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r7146, %r108, %r7120, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r7147, %r108, %r7120, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r7140, %r107, %r7120, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r7141, %r107, %r7120, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r7142, %r106, %r7120, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r7143, %r106, %r7120, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r7136, %r105, %r7120, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r7137, %r105, %r7120, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r7138, %r104, %r7120, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r7139, %r104, %r7120, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r4798, %r103, %r7120, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r7133, %r103, %r7120, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r7134, %r102, %r7120, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r7135, %r102, %r7120, %r1106;
	// inline asm
	bfe.u32 	%r4830, %r7007, 2, 4;
	setp.gt.s32	%p94, %r4830, 7;
	@%p94 bra 	BB1_158;

	setp.gt.s32	%p106, %r4830, 3;
	@%p106 bra 	BB1_151;

	setp.gt.s32	%p112, %r4830, 1;
	@%p112 bra 	BB1_148;

	setp.eq.s32	%p115, %r4830, 0;
	@%p115 bra 	BB1_184;
	bra.uni 	BB1_146;

BB1_184:
	and.b32  	%r6174, %r111, 3;
	shl.b32 	%r6158, %r6174, 3;
	mov.u32 	%r7120, 0;
	// inline asm
	shf.r.wrap.b32 %r6091, %r7144, %r7120, %r6158;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6095, %r7145, %r7144, %r6158;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6099, %r7146, %r7145, %r6158;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6103, %r7147, %r7146, %r6158;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6107, %r7140, %r7147, %r6158;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6111, %r7141, %r7140, %r6158;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6115, %r7142, %r7141, %r6158;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6119, %r7143, %r7142, %r6158;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6123, %r7136, %r7143, %r6158;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6127, %r7137, %r7136, %r6158;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6131, %r7138, %r7137, %r6158;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6135, %r7139, %r7138, %r6158;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6139, %r4798, %r7139, %r6158;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6143, %r7133, %r4798, %r6158;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6147, %r7134, %r7133, %r6158;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6151, %r7135, %r7134, %r6158;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6155, %r7120, %r7135, %r6158;
	// inline asm
	setp.eq.s32	%p132, %r110, 0;
	selp.b32	%r7132, %r6139, %r6143, %p132;
	selp.b32	%r7133, %r6143, %r6147, %p132;
	selp.b32	%r7134, %r6147, %r6151, %p132;
	selp.b32	%r7135, %r6151, %r6155, %p132;
	selp.b32	%r7136, %r6123, %r6127, %p132;
	selp.b32	%r7137, %r6127, %r6131, %p132;
	selp.b32	%r7138, %r6131, %r6135, %p132;
	selp.b32	%r7139, %r6135, %r6139, %p132;
	selp.b32	%r7140, %r6107, %r6111, %p132;
	selp.b32	%r7141, %r6111, %r6115, %p132;
	selp.b32	%r7142, %r6115, %r6119, %p132;
	selp.b32	%r7143, %r6119, %r6123, %p132;
	selp.b32	%r7144, %r6091, %r6095, %p132;
	selp.b32	%r7145, %r6095, %r6099, %p132;
	selp.b32	%r7146, %r6099, %r6103, %p132;
	selp.b32	%r7147, %r6103, %r6107, %p132;
	selp.b32	%r7148, 0, %r6091, %p132;
	mov.u32 	%r7121, %r7120;
	mov.u32 	%r7122, %r7120;
	mov.u32 	%r7123, %r7120;
	mov.u32 	%r7124, %r7120;
	mov.u32 	%r7125, %r7120;
	mov.u32 	%r7126, %r7120;
	mov.u32 	%r7127, %r7120;
	mov.u32 	%r7128, %r7120;
	mov.u32 	%r7129, %r7120;
	mov.u32 	%r7130, %r7120;
	mov.u32 	%r7131, %r7120;
	bra.uni 	BB1_185;

BB1_158:
	setp.gt.s32	%p95, %r4830, 11;
	@%p95 bra 	BB1_166;

	setp.gt.s32	%p101, %r4830, 9;
	@%p101 bra 	BB1_163;

	setp.eq.s32	%p104, %r4830, 8;
	@%p104 bra 	BB1_178;
	bra.uni 	BB1_161;

BB1_178:
	and.b32  	%r5502, %r111, 3;
	shl.b32 	%r5486, %r5502, 3;
	mov.u32 	%r7124, 0;
	// inline asm
	shf.r.wrap.b32 %r5419, %r7144, %r7124, %r5486;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5423, %r7145, %r7144, %r5486;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5427, %r7146, %r7145, %r5486;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5431, %r7147, %r7146, %r5486;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5435, %r7140, %r7147, %r5486;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5439, %r7141, %r7140, %r5486;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5443, %r7142, %r7141, %r5486;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5447, %r7143, %r7142, %r5486;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5451, %r7136, %r7143, %r5486;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5455, %r7137, %r7136, %r5486;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5459, %r7138, %r7137, %r5486;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5463, %r7139, %r7138, %r5486;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5467, %r4798, %r7139, %r5486;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5471, %r7133, %r4798, %r5486;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5475, %r7134, %r7133, %r5486;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5479, %r7135, %r7134, %r5486;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5483, %r7124, %r7135, %r5486;
	// inline asm
	setp.eq.s32	%p124, %r110, 0;
	selp.b32	%r7120, %r5419, %r5423, %p124;
	selp.b32	%r7121, %r5423, %r5427, %p124;
	selp.b32	%r7122, %r5427, %r5431, %p124;
	selp.b32	%r7123, %r5431, %r5435, %p124;
	selp.b32	%r7127, 0, %r5419, %p124;
	selp.b32	%r7140, %r5467, %r5471, %p124;
	selp.b32	%r7141, %r5471, %r5475, %p124;
	selp.b32	%r7142, %r5475, %r5479, %p124;
	selp.b32	%r7143, %r5479, %r5483, %p124;
	selp.b32	%r7144, %r5451, %r5455, %p124;
	selp.b32	%r7145, %r5455, %r5459, %p124;
	selp.b32	%r7146, %r5459, %r5463, %p124;
	selp.b32	%r7147, %r5463, %r5467, %p124;
	selp.b32	%r7148, %r5447, %r5451, %p124;
	selp.b32	%r7149, %r5443, %r5447, %p124;
	selp.b32	%r7150, %r5439, %r5443, %p124;
	selp.b32	%r7151, %r5435, %r5439, %p124;
	mov.u32 	%r7125, %r7124;
	mov.u32 	%r7126, %r7124;
	mov.u32 	%r7128, %r7124;
	mov.u32 	%r7129, %r7124;
	mov.u32 	%r7130, %r7124;
	mov.u32 	%r7131, %r7124;
	mov.u32 	%r7132, %r7124;
	mov.u32 	%r7133, %r7124;
	mov.u32 	%r7134, %r7124;
	mov.u32 	%r7135, %r7124;
	mov.u32 	%r7136, %r7124;
	bra.uni 	BB1_179;

BB1_151:
	setp.gt.s32	%p107, %r4830, 5;
	@%p107 bra 	BB1_155;

	setp.eq.s32	%p110, %r4830, 4;
	@%p110 bra 	BB1_181;
	bra.uni 	BB1_153;

BB1_181:
	and.b32  	%r5838, %r111, 3;
	shl.b32 	%r5822, %r5838, 3;
	mov.u32 	%r7120, 0;
	// inline asm
	shf.r.wrap.b32 %r5755, %r7144, %r7120, %r5822;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5759, %r7145, %r7144, %r5822;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5763, %r7146, %r7145, %r5822;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5767, %r7147, %r7146, %r5822;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5771, %r7140, %r7147, %r5822;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5775, %r7141, %r7140, %r5822;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5779, %r7142, %r7141, %r5822;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5783, %r7143, %r7142, %r5822;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5787, %r7136, %r7143, %r5822;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5791, %r7137, %r7136, %r5822;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5795, %r7138, %r7137, %r5822;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5799, %r7139, %r7138, %r5822;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5803, %r4798, %r7139, %r5822;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5807, %r7133, %r4798, %r5822;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5811, %r7134, %r7133, %r5822;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5815, %r7135, %r7134, %r5822;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5819, %r7120, %r7135, %r5822;
	// inline asm
	setp.eq.s32	%p128, %r110, 0;
	selp.b32	%r7123, 0, %r5755, %p128;
	selp.b32	%r7136, %r5803, %r5807, %p128;
	selp.b32	%r7137, %r5807, %r5811, %p128;
	selp.b32	%r7138, %r5811, %r5815, %p128;
	selp.b32	%r7139, %r5815, %r5819, %p128;
	selp.b32	%r7140, %r5787, %r5791, %p128;
	selp.b32	%r7141, %r5791, %r5795, %p128;
	selp.b32	%r7142, %r5795, %r5799, %p128;
	selp.b32	%r7143, %r5799, %r5803, %p128;
	selp.b32	%r7144, %r5771, %r5775, %p128;
	selp.b32	%r7145, %r5775, %r5779, %p128;
	selp.b32	%r7146, %r5779, %r5783, %p128;
	selp.b32	%r7147, %r5783, %r5787, %p128;
	selp.b32	%r7148, %r5767, %r5771, %p128;
	selp.b32	%r7149, %r5763, %r5767, %p128;
	selp.b32	%r7150, %r5759, %r5763, %p128;
	selp.b32	%r7151, %r5755, %r5759, %p128;
	mov.u32 	%r7121, %r7120;
	mov.u32 	%r7122, %r7120;
	mov.u32 	%r7124, %r7120;
	mov.u32 	%r7125, %r7120;
	mov.u32 	%r7126, %r7120;
	mov.u32 	%r7127, %r7120;
	mov.u32 	%r7128, %r7120;
	mov.u32 	%r7129, %r7120;
	mov.u32 	%r7130, %r7120;
	mov.u32 	%r7131, %r7120;
	mov.u32 	%r7132, %r7120;
	bra.uni 	BB1_182;

BB1_166:
	setp.gt.s32	%p96, %r4830, 13;
	@%p96 bra 	BB1_170;

	setp.eq.s32	%p99, %r4830, 12;
	@%p99 bra 	BB1_175;
	bra.uni 	BB1_168;

BB1_175:
	and.b32  	%r5166, %r111, 3;
	shl.b32 	%r5150, %r5166, 3;
	mov.u32 	%r7128, 0;
	// inline asm
	shf.r.wrap.b32 %r5083, %r7144, %r7128, %r5150;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5087, %r7145, %r7144, %r5150;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5091, %r7146, %r7145, %r5150;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5095, %r7147, %r7146, %r5150;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5099, %r7140, %r7147, %r5150;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5103, %r7141, %r7140, %r5150;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5107, %r7142, %r7141, %r5150;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5111, %r7143, %r7142, %r5150;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5115, %r7136, %r7143, %r5150;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5119, %r7137, %r7136, %r5150;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5123, %r7138, %r7137, %r5150;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5127, %r7139, %r7138, %r5150;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5131, %r4798, %r7139, %r5150;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5135, %r7133, %r4798, %r5150;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5139, %r7134, %r7133, %r5150;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5143, %r7135, %r7134, %r5150;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5147, %r7128, %r7135, %r5150;
	// inline asm
	setp.eq.s32	%p120, %r110, 0;
	selp.b32	%r7120, %r5099, %r5103, %p120;
	selp.b32	%r7121, %r5103, %r5107, %p120;
	selp.b32	%r7122, %r5107, %r5111, %p120;
	selp.b32	%r7123, %r5111, %r5115, %p120;
	selp.b32	%r7124, %r5083, %r5087, %p120;
	selp.b32	%r7125, %r5087, %r5091, %p120;
	selp.b32	%r7126, %r5091, %r5095, %p120;
	selp.b32	%r7127, %r5095, %r5099, %p120;
	selp.b32	%r7131, 0, %r5083, %p120;
	selp.b32	%r7144, %r5131, %r5135, %p120;
	selp.b32	%r7145, %r5135, %r5139, %p120;
	selp.b32	%r7146, %r5139, %r5143, %p120;
	selp.b32	%r7147, %r5143, %r5147, %p120;
	selp.b32	%r7148, %r5127, %r5131, %p120;
	selp.b32	%r7149, %r5123, %r5127, %p120;
	selp.b32	%r7150, %r5119, %r5123, %p120;
	selp.b32	%r7151, %r5115, %r5119, %p120;
	mov.u32 	%r7129, %r7128;
	mov.u32 	%r7130, %r7128;
	mov.u32 	%r7132, %r7128;
	mov.u32 	%r7133, %r7128;
	mov.u32 	%r7134, %r7128;
	mov.u32 	%r7135, %r7128;
	mov.u32 	%r7136, %r7128;
	mov.u32 	%r7137, %r7128;
	mov.u32 	%r7138, %r7128;
	mov.u32 	%r7139, %r7128;
	mov.u32 	%r7140, %r7128;
	bra.uni 	BB1_176;

BB1_148:
	setp.eq.s32	%p113, %r4830, 2;
	@%p113 bra 	BB1_183;
	bra.uni 	BB1_149;

BB1_183:
	and.b32  	%r6006, %r111, 3;
	shl.b32 	%r5990, %r6006, 3;
	mov.u32 	%r7120, 0;
	// inline asm
	shf.r.wrap.b32 %r5923, %r7144, %r7120, %r5990;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5927, %r7145, %r7144, %r5990;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5931, %r7146, %r7145, %r5990;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5935, %r7147, %r7146, %r5990;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5939, %r7140, %r7147, %r5990;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5943, %r7141, %r7140, %r5990;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5947, %r7142, %r7141, %r5990;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5951, %r7143, %r7142, %r5990;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5955, %r7136, %r7143, %r5990;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5959, %r7137, %r7136, %r5990;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5963, %r7138, %r7137, %r5990;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5967, %r7139, %r7138, %r5990;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5971, %r4798, %r7139, %r5990;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5975, %r7133, %r4798, %r5990;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5979, %r7134, %r7133, %r5990;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5983, %r7135, %r7134, %r5990;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5987, %r7120, %r7135, %r5990;
	// inline asm
	setp.eq.s32	%p130, %r110, 0;
	selp.b32	%r7132, %r5979, %r5983, %p130;
	selp.b32	%r7133, %r5983, %r5987, %p130;
	selp.b32	%r7136, %r5963, %r5967, %p130;
	selp.b32	%r7137, %r5967, %r5971, %p130;
	selp.b32	%r7138, %r5971, %r5975, %p130;
	selp.b32	%r7139, %r5975, %r5979, %p130;
	selp.b32	%r7140, %r5947, %r5951, %p130;
	selp.b32	%r7141, %r5951, %r5955, %p130;
	selp.b32	%r7142, %r5955, %r5959, %p130;
	selp.b32	%r7143, %r5959, %r5963, %p130;
	selp.b32	%r7144, %r5931, %r5935, %p130;
	selp.b32	%r7145, %r5935, %r5939, %p130;
	selp.b32	%r7146, %r5939, %r5943, %p130;
	selp.b32	%r7147, %r5943, %r5947, %p130;
	selp.b32	%r7148, %r5927, %r5931, %p130;
	selp.b32	%r7149, %r5923, %r5927, %p130;
	selp.b32	%r7150, 0, %r5923, %p130;
	mov.u32 	%r7121, %r7120;
	mov.u32 	%r7122, %r7120;
	mov.u32 	%r7123, %r7120;
	mov.u32 	%r7124, %r7120;
	mov.u32 	%r7125, %r7120;
	mov.u32 	%r7126, %r7120;
	mov.u32 	%r7127, %r7120;
	mov.u32 	%r7128, %r7120;
	mov.u32 	%r7129, %r7120;
	mov.u32 	%r7130, %r7120;
	mov.u32 	%r7131, %r7120;
	mov.u32 	%r7134, %r7120;
	mov.u32 	%r7135, %r7120;
	bra.uni 	BB1_187;

BB1_163:
	setp.eq.s32	%p102, %r4830, 10;
	@%p102 bra 	BB1_177;
	bra.uni 	BB1_164;

BB1_177:
	and.b32  	%r5334, %r111, 3;
	shl.b32 	%r5318, %r5334, 3;
	mov.u32 	%r7124, 0;
	// inline asm
	shf.r.wrap.b32 %r5251, %r7144, %r7124, %r5318;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5255, %r7145, %r7144, %r5318;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5259, %r7146, %r7145, %r5318;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5263, %r7147, %r7146, %r5318;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5267, %r7140, %r7147, %r5318;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5271, %r7141, %r7140, %r5318;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5275, %r7142, %r7141, %r5318;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5279, %r7143, %r7142, %r5318;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5283, %r7136, %r7143, %r5318;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5287, %r7137, %r7136, %r5318;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5291, %r7138, %r7137, %r5318;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5295, %r7139, %r7138, %r5318;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5299, %r4798, %r7139, %r5318;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5303, %r7133, %r4798, %r5318;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5307, %r7134, %r7133, %r5318;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5311, %r7135, %r7134, %r5318;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5315, %r7124, %r7135, %r5318;
	// inline asm
	setp.eq.s32	%p122, %r110, 0;
	selp.b32	%r7120, %r5259, %r5263, %p122;
	selp.b32	%r7121, %r5263, %r5267, %p122;
	selp.b32	%r7122, %r5267, %r5271, %p122;
	selp.b32	%r7123, %r5271, %r5275, %p122;
	selp.b32	%r7125, 0, %r5251, %p122;
	selp.b32	%r7126, %r5251, %r5255, %p122;
	selp.b32	%r7127, %r5255, %r5259, %p122;
	selp.b32	%r7140, %r5307, %r5311, %p122;
	selp.b32	%r7141, %r5311, %r5315, %p122;
	selp.b32	%r7144, %r5291, %r5295, %p122;
	selp.b32	%r7145, %r5295, %r5299, %p122;
	selp.b32	%r7146, %r5299, %r5303, %p122;
	selp.b32	%r7147, %r5303, %r5307, %p122;
	selp.b32	%r7148, %r5287, %r5291, %p122;
	selp.b32	%r7149, %r5283, %r5287, %p122;
	selp.b32	%r7150, %r5279, %r5283, %p122;
	selp.b32	%r7151, %r5275, %r5279, %p122;
	mov.u32 	%r7128, %r7124;
	mov.u32 	%r7129, %r7124;
	mov.u32 	%r7130, %r7124;
	mov.u32 	%r7131, %r7124;
	mov.u32 	%r7132, %r7124;
	mov.u32 	%r7133, %r7124;
	mov.u32 	%r7134, %r7124;
	mov.u32 	%r7135, %r7124;
	mov.u32 	%r7136, %r7124;
	mov.u32 	%r7137, %r7124;
	mov.u32 	%r7138, %r7124;
	mov.u32 	%r7139, %r7124;
	mov.u32 	%r7142, %r7124;
	mov.u32 	%r7143, %r7124;
	bra.uni 	BB1_188;

BB1_155:
	setp.eq.s32	%p108, %r4830, 6;
	@%p108 bra 	BB1_180;
	bra.uni 	BB1_156;

BB1_180:
	and.b32  	%r5670, %r111, 3;
	shl.b32 	%r5654, %r5670, 3;
	mov.u32 	%r7120, 0;
	// inline asm
	shf.r.wrap.b32 %r5587, %r7144, %r7120, %r5654;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5591, %r7145, %r7144, %r5654;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5595, %r7146, %r7145, %r5654;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5599, %r7147, %r7146, %r5654;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5603, %r7140, %r7147, %r5654;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5607, %r7141, %r7140, %r5654;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5611, %r7142, %r7141, %r5654;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5615, %r7143, %r7142, %r5654;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5619, %r7136, %r7143, %r5654;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5623, %r7137, %r7136, %r5654;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5627, %r7138, %r7137, %r5654;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5631, %r7139, %r7138, %r5654;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5635, %r4798, %r7139, %r5654;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5639, %r7133, %r4798, %r5654;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5643, %r7134, %r7133, %r5654;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5647, %r7135, %r7134, %r5654;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5651, %r7120, %r7135, %r5654;
	// inline asm
	setp.eq.s32	%p126, %r110, 0;
	selp.b32	%r7121, 0, %r5587, %p126;
	selp.b32	%r7122, %r5587, %r5591, %p126;
	selp.b32	%r7123, %r5591, %r5595, %p126;
	selp.b32	%r7136, %r5643, %r5647, %p126;
	selp.b32	%r7137, %r5647, %r5651, %p126;
	selp.b32	%r7140, %r5627, %r5631, %p126;
	selp.b32	%r7141, %r5631, %r5635, %p126;
	selp.b32	%r7142, %r5635, %r5639, %p126;
	selp.b32	%r7143, %r5639, %r5643, %p126;
	selp.b32	%r7144, %r5611, %r5615, %p126;
	selp.b32	%r7145, %r5615, %r5619, %p126;
	selp.b32	%r7146, %r5619, %r5623, %p126;
	selp.b32	%r7147, %r5623, %r5627, %p126;
	selp.b32	%r7148, %r5607, %r5611, %p126;
	selp.b32	%r7149, %r5603, %r5607, %p126;
	selp.b32	%r7150, %r5599, %r5603, %p126;
	selp.b32	%r7151, %r5595, %r5599, %p126;
	mov.u32 	%r7124, %r7120;
	mov.u32 	%r7125, %r7120;
	mov.u32 	%r7126, %r7120;
	mov.u32 	%r7127, %r7120;
	mov.u32 	%r7128, %r7120;
	mov.u32 	%r7129, %r7120;
	mov.u32 	%r7130, %r7120;
	mov.u32 	%r7131, %r7120;
	mov.u32 	%r7132, %r7120;
	mov.u32 	%r7133, %r7120;
	mov.u32 	%r7134, %r7120;
	mov.u32 	%r7135, %r7120;
	mov.u32 	%r7138, %r7120;
	mov.u32 	%r7139, %r7120;
	bra.uni 	BB1_188;

BB1_170:
	setp.eq.s32	%p97, %r4830, 14;
	@%p97 bra 	BB1_174;
	bra.uni 	BB1_171;

BB1_174:
	and.b32  	%r4998, %r111, 3;
	shl.b32 	%r4982, %r4998, 3;
	mov.u32 	%r7128, 0;
	// inline asm
	shf.r.wrap.b32 %r4915, %r7144, %r7128, %r4982;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4919, %r7145, %r7144, %r4982;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4923, %r7146, %r7145, %r4982;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4927, %r7147, %r7146, %r4982;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4931, %r7140, %r7147, %r4982;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4935, %r7141, %r7140, %r4982;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4939, %r7142, %r7141, %r4982;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4943, %r7143, %r7142, %r4982;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4947, %r7136, %r7143, %r4982;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4951, %r7137, %r7136, %r4982;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4955, %r7138, %r7137, %r4982;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4959, %r7139, %r7138, %r4982;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4963, %r4798, %r7139, %r4982;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4967, %r7133, %r4798, %r4982;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4971, %r7134, %r7133, %r4982;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4975, %r7135, %r7134, %r4982;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4979, %r7128, %r7135, %r4982;
	// inline asm
	setp.eq.s32	%p118, %r110, 0;
	selp.b32	%r7120, %r4939, %r4943, %p118;
	selp.b32	%r7121, %r4943, %r4947, %p118;
	selp.b32	%r7122, %r4947, %r4951, %p118;
	selp.b32	%r7123, %r4951, %r4955, %p118;
	selp.b32	%r7124, %r4923, %r4927, %p118;
	selp.b32	%r7125, %r4927, %r4931, %p118;
	selp.b32	%r7126, %r4931, %r4935, %p118;
	selp.b32	%r7127, %r4935, %r4939, %p118;
	selp.b32	%r7129, 0, %r4915, %p118;
	selp.b32	%r7130, %r4915, %r4919, %p118;
	selp.b32	%r7131, %r4919, %r4923, %p118;
	selp.b32	%r7144, %r4971, %r4975, %p118;
	selp.b32	%r7145, %r4975, %r4979, %p118;
	selp.b32	%r7148, %r4967, %r4971, %p118;
	selp.b32	%r7149, %r4963, %r4967, %p118;
	selp.b32	%r7150, %r4959, %r4963, %p118;
	selp.b32	%r7151, %r4955, %r4959, %p118;
	mov.u32 	%r7132, %r7128;
	mov.u32 	%r7133, %r7128;
	mov.u32 	%r7134, %r7128;
	mov.u32 	%r7135, %r7128;
	mov.u32 	%r7136, %r7128;
	mov.u32 	%r7137, %r7128;
	mov.u32 	%r7138, %r7128;
	mov.u32 	%r7139, %r7128;
	mov.u32 	%r7140, %r7128;
	mov.u32 	%r7141, %r7128;
	mov.u32 	%r7142, %r7128;
	mov.u32 	%r7143, %r7128;
	mov.u32 	%r7146, %r7128;
	mov.u32 	%r7147, %r7128;
	bra.uni 	BB1_188;

BB1_146:
	setp.eq.s32	%p116, %r4830, 1;
	@%p116 bra 	BB1_147;
	bra.uni 	BB1_172;

BB1_147:
	and.b32  	%r6090, %r111, 3;
	shl.b32 	%r6074, %r6090, 3;
	mov.u32 	%r7120, 0;
	// inline asm
	shf.r.wrap.b32 %r6007, %r7144, %r7120, %r6074;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6011, %r7145, %r7144, %r6074;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6015, %r7146, %r7145, %r6074;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6019, %r7147, %r7146, %r6074;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6023, %r7140, %r7147, %r6074;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6027, %r7141, %r7140, %r6074;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6031, %r7142, %r7141, %r6074;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6035, %r7143, %r7142, %r6074;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6039, %r7136, %r7143, %r6074;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6043, %r7137, %r7136, %r6074;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6047, %r7138, %r7137, %r6074;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6051, %r7139, %r7138, %r6074;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6055, %r4798, %r7139, %r6074;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6059, %r7133, %r4798, %r6074;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6063, %r7134, %r7133, %r6074;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6067, %r7135, %r7134, %r6074;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6071, %r7120, %r7135, %r6074;
	// inline asm
	setp.eq.s32	%p131, %r110, 0;
	selp.b32	%r7132, %r6059, %r6063, %p131;
	selp.b32	%r7133, %r6063, %r6067, %p131;
	selp.b32	%r7134, %r6067, %r6071, %p131;
	selp.b32	%r7136, %r6043, %r6047, %p131;
	selp.b32	%r7137, %r6047, %r6051, %p131;
	selp.b32	%r7138, %r6051, %r6055, %p131;
	selp.b32	%r7139, %r6055, %r6059, %p131;
	selp.b32	%r7140, %r6027, %r6031, %p131;
	selp.b32	%r7141, %r6031, %r6035, %p131;
	selp.b32	%r7142, %r6035, %r6039, %p131;
	selp.b32	%r7143, %r6039, %r6043, %p131;
	selp.b32	%r7144, %r6011, %r6015, %p131;
	selp.b32	%r7145, %r6015, %r6019, %p131;
	selp.b32	%r7146, %r6019, %r6023, %p131;
	selp.b32	%r7147, %r6023, %r6027, %p131;
	selp.b32	%r7148, %r6007, %r6011, %p131;
	selp.b32	%r7149, 0, %r6007, %p131;
	mov.u32 	%r7121, %r7120;
	mov.u32 	%r7122, %r7120;
	mov.u32 	%r7123, %r7120;
	mov.u32 	%r7124, %r7120;
	mov.u32 	%r7125, %r7120;
	mov.u32 	%r7126, %r7120;
	mov.u32 	%r7127, %r7120;
	mov.u32 	%r7128, %r7120;
	mov.u32 	%r7129, %r7120;
	mov.u32 	%r7130, %r7120;
	mov.u32 	%r7131, %r7120;
	mov.u32 	%r7135, %r7120;
	bra.uni 	BB1_186;

BB1_161:
	setp.eq.s32	%p105, %r4830, 9;
	@%p105 bra 	BB1_162;
	bra.uni 	BB1_172;

BB1_162:
	and.b32  	%r5418, %r111, 3;
	shl.b32 	%r5402, %r5418, 3;
	mov.u32 	%r7124, 0;
	// inline asm
	shf.r.wrap.b32 %r5335, %r7144, %r7124, %r5402;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5339, %r7145, %r7144, %r5402;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5343, %r7146, %r7145, %r5402;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5347, %r7147, %r7146, %r5402;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5351, %r7140, %r7147, %r5402;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5355, %r7141, %r7140, %r5402;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5359, %r7142, %r7141, %r5402;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5363, %r7143, %r7142, %r5402;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5367, %r7136, %r7143, %r5402;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5371, %r7137, %r7136, %r5402;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5375, %r7138, %r7137, %r5402;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5379, %r7139, %r7138, %r5402;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5383, %r4798, %r7139, %r5402;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5387, %r7133, %r4798, %r5402;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5391, %r7134, %r7133, %r5402;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5395, %r7135, %r7134, %r5402;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5399, %r7124, %r7135, %r5402;
	// inline asm
	setp.eq.s32	%p123, %r110, 0;
	selp.b32	%r7120, %r5339, %r5343, %p123;
	selp.b32	%r7121, %r5343, %r5347, %p123;
	selp.b32	%r7122, %r5347, %r5351, %p123;
	selp.b32	%r7123, %r5351, %r5355, %p123;
	selp.b32	%r7126, 0, %r5335, %p123;
	selp.b32	%r7127, %r5335, %r5339, %p123;
	selp.b32	%r7140, %r5387, %r5391, %p123;
	selp.b32	%r7141, %r5391, %r5395, %p123;
	selp.b32	%r7142, %r5395, %r5399, %p123;
	selp.b32	%r7144, %r5371, %r5375, %p123;
	selp.b32	%r7145, %r5375, %r5379, %p123;
	selp.b32	%r7146, %r5379, %r5383, %p123;
	selp.b32	%r7147, %r5383, %r5387, %p123;
	selp.b32	%r7148, %r5367, %r5371, %p123;
	selp.b32	%r7149, %r5363, %r5367, %p123;
	selp.b32	%r7150, %r5359, %r5363, %p123;
	selp.b32	%r7151, %r5355, %r5359, %p123;
	mov.u32 	%r7125, %r7124;
	mov.u32 	%r7128, %r7124;
	mov.u32 	%r7129, %r7124;
	mov.u32 	%r7130, %r7124;
	mov.u32 	%r7131, %r7124;
	mov.u32 	%r7132, %r7124;
	mov.u32 	%r7133, %r7124;
	mov.u32 	%r7134, %r7124;
	mov.u32 	%r7135, %r7124;
	mov.u32 	%r7136, %r7124;
	mov.u32 	%r7137, %r7124;
	mov.u32 	%r7138, %r7124;
	mov.u32 	%r7139, %r7124;
	mov.u32 	%r7143, %r7124;
	bra.uni 	BB1_188;

BB1_153:
	setp.eq.s32	%p111, %r4830, 5;
	@%p111 bra 	BB1_154;
	bra.uni 	BB1_172;

BB1_154:
	and.b32  	%r5754, %r111, 3;
	shl.b32 	%r5738, %r5754, 3;
	mov.u32 	%r7120, 0;
	// inline asm
	shf.r.wrap.b32 %r5671, %r7144, %r7120, %r5738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5675, %r7145, %r7144, %r5738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5679, %r7146, %r7145, %r5738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5683, %r7147, %r7146, %r5738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5687, %r7140, %r7147, %r5738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5691, %r7141, %r7140, %r5738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5695, %r7142, %r7141, %r5738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5699, %r7143, %r7142, %r5738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5703, %r7136, %r7143, %r5738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5707, %r7137, %r7136, %r5738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5711, %r7138, %r7137, %r5738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5715, %r7139, %r7138, %r5738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5719, %r4798, %r7139, %r5738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5723, %r7133, %r4798, %r5738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5727, %r7134, %r7133, %r5738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5731, %r7135, %r7134, %r5738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5735, %r7120, %r7135, %r5738;
	// inline asm
	setp.eq.s32	%p127, %r110, 0;
	selp.b32	%r7122, 0, %r5671, %p127;
	selp.b32	%r7123, %r5671, %r5675, %p127;
	selp.b32	%r7136, %r5723, %r5727, %p127;
	selp.b32	%r7137, %r5727, %r5731, %p127;
	selp.b32	%r7138, %r5731, %r5735, %p127;
	selp.b32	%r7140, %r5707, %r5711, %p127;
	selp.b32	%r7141, %r5711, %r5715, %p127;
	selp.b32	%r7142, %r5715, %r5719, %p127;
	selp.b32	%r7143, %r5719, %r5723, %p127;
	selp.b32	%r7144, %r5691, %r5695, %p127;
	selp.b32	%r7145, %r5695, %r5699, %p127;
	selp.b32	%r7146, %r5699, %r5703, %p127;
	selp.b32	%r7147, %r5703, %r5707, %p127;
	selp.b32	%r7148, %r5687, %r5691, %p127;
	selp.b32	%r7149, %r5683, %r5687, %p127;
	selp.b32	%r7150, %r5679, %r5683, %p127;
	selp.b32	%r7151, %r5675, %r5679, %p127;
	mov.u32 	%r7121, %r7120;
	mov.u32 	%r7124, %r7120;
	mov.u32 	%r7125, %r7120;
	mov.u32 	%r7126, %r7120;
	mov.u32 	%r7127, %r7120;
	mov.u32 	%r7128, %r7120;
	mov.u32 	%r7129, %r7120;
	mov.u32 	%r7130, %r7120;
	mov.u32 	%r7131, %r7120;
	mov.u32 	%r7132, %r7120;
	mov.u32 	%r7133, %r7120;
	mov.u32 	%r7134, %r7120;
	mov.u32 	%r7135, %r7120;
	mov.u32 	%r7139, %r7120;
	bra.uni 	BB1_188;

BB1_168:
	setp.eq.s32	%p100, %r4830, 13;
	@%p100 bra 	BB1_169;
	bra.uni 	BB1_172;

BB1_169:
	and.b32  	%r5082, %r111, 3;
	shl.b32 	%r5066, %r5082, 3;
	mov.u32 	%r7128, 0;
	// inline asm
	shf.r.wrap.b32 %r4999, %r7144, %r7128, %r5066;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5003, %r7145, %r7144, %r5066;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5007, %r7146, %r7145, %r5066;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5011, %r7147, %r7146, %r5066;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5015, %r7140, %r7147, %r5066;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5019, %r7141, %r7140, %r5066;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5023, %r7142, %r7141, %r5066;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5027, %r7143, %r7142, %r5066;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5031, %r7136, %r7143, %r5066;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5035, %r7137, %r7136, %r5066;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5039, %r7138, %r7137, %r5066;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5043, %r7139, %r7138, %r5066;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5047, %r4798, %r7139, %r5066;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5051, %r7133, %r4798, %r5066;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5055, %r7134, %r7133, %r5066;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5059, %r7135, %r7134, %r5066;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5063, %r7128, %r7135, %r5066;
	// inline asm
	setp.eq.s32	%p119, %r110, 0;
	selp.b32	%r7120, %r5019, %r5023, %p119;
	selp.b32	%r7121, %r5023, %r5027, %p119;
	selp.b32	%r7122, %r5027, %r5031, %p119;
	selp.b32	%r7123, %r5031, %r5035, %p119;
	selp.b32	%r7124, %r5003, %r5007, %p119;
	selp.b32	%r7125, %r5007, %r5011, %p119;
	selp.b32	%r7126, %r5011, %r5015, %p119;
	selp.b32	%r7127, %r5015, %r5019, %p119;
	selp.b32	%r7130, 0, %r4999, %p119;
	selp.b32	%r7131, %r4999, %r5003, %p119;
	selp.b32	%r7144, %r5051, %r5055, %p119;
	selp.b32	%r7145, %r5055, %r5059, %p119;
	selp.b32	%r7146, %r5059, %r5063, %p119;
	selp.b32	%r7148, %r5047, %r5051, %p119;
	selp.b32	%r7149, %r5043, %r5047, %p119;
	selp.b32	%r7150, %r5039, %r5043, %p119;
	selp.b32	%r7151, %r5035, %r5039, %p119;
	mov.u32 	%r7129, %r7128;
	mov.u32 	%r7132, %r7128;
	mov.u32 	%r7133, %r7128;
	mov.u32 	%r7134, %r7128;
	mov.u32 	%r7135, %r7128;
	mov.u32 	%r7136, %r7128;
	mov.u32 	%r7137, %r7128;
	mov.u32 	%r7138, %r7128;
	mov.u32 	%r7139, %r7128;
	mov.u32 	%r7140, %r7128;
	mov.u32 	%r7141, %r7128;
	mov.u32 	%r7142, %r7128;
	mov.u32 	%r7143, %r7128;
	mov.u32 	%r7147, %r7128;
	bra.uni 	BB1_188;

BB1_149:
	setp.eq.s32	%p114, %r4830, 3;
	@%p114 bra 	BB1_150;
	bra.uni 	BB1_172;

BB1_150:
	and.b32  	%r5922, %r111, 3;
	shl.b32 	%r5906, %r5922, 3;
	mov.u32 	%r7120, 0;
	// inline asm
	shf.r.wrap.b32 %r5839, %r7144, %r7120, %r5906;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5843, %r7145, %r7144, %r5906;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5847, %r7146, %r7145, %r5906;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5851, %r7147, %r7146, %r5906;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5855, %r7140, %r7147, %r5906;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5859, %r7141, %r7140, %r5906;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5863, %r7142, %r7141, %r5906;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5867, %r7143, %r7142, %r5906;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5871, %r7136, %r7143, %r5906;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5875, %r7137, %r7136, %r5906;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5879, %r7138, %r7137, %r5906;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5883, %r7139, %r7138, %r5906;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5887, %r4798, %r7139, %r5906;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5891, %r7133, %r4798, %r5906;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5895, %r7134, %r7133, %r5906;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5899, %r7135, %r7134, %r5906;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5903, %r7120, %r7135, %r5906;
	// inline asm
	setp.eq.s32	%p129, %r110, 0;
	selp.b32	%r7132, %r5899, %r5903, %p129;
	selp.b32	%r7136, %r5883, %r5887, %p129;
	selp.b32	%r7137, %r5887, %r5891, %p129;
	selp.b32	%r7138, %r5891, %r5895, %p129;
	selp.b32	%r7139, %r5895, %r5899, %p129;
	selp.b32	%r7140, %r5867, %r5871, %p129;
	selp.b32	%r7141, %r5871, %r5875, %p129;
	selp.b32	%r7142, %r5875, %r5879, %p129;
	selp.b32	%r7143, %r5879, %r5883, %p129;
	selp.b32	%r7144, %r5851, %r5855, %p129;
	selp.b32	%r7145, %r5855, %r5859, %p129;
	selp.b32	%r7146, %r5859, %r5863, %p129;
	selp.b32	%r7147, %r5863, %r5867, %p129;
	selp.b32	%r7148, %r5847, %r5851, %p129;
	selp.b32	%r7149, %r5843, %r5847, %p129;
	selp.b32	%r7150, %r5839, %r5843, %p129;
	selp.b32	%r7151, 0, %r5839, %p129;
	mov.u32 	%r7121, %r7120;
	mov.u32 	%r7122, %r7120;
	mov.u32 	%r7123, %r7120;
	mov.u32 	%r7124, %r7120;
	mov.u32 	%r7125, %r7120;
	mov.u32 	%r7126, %r7120;
	mov.u32 	%r7127, %r7120;
	mov.u32 	%r7128, %r7120;
	mov.u32 	%r7129, %r7120;
	mov.u32 	%r7130, %r7120;
	mov.u32 	%r7131, %r7120;

BB1_182:
	mov.u32 	%r7133, %r7120;
	mov.u32 	%r7134, %r7120;
	mov.u32 	%r7135, %r7120;
	bra.uni 	BB1_188;

BB1_164:
	setp.eq.s32	%p103, %r4830, 11;
	@%p103 bra 	BB1_165;
	bra.uni 	BB1_172;

BB1_165:
	and.b32  	%r5250, %r111, 3;
	shl.b32 	%r5234, %r5250, 3;
	mov.u32 	%r7128, 0;
	// inline asm
	shf.r.wrap.b32 %r5167, %r7144, %r7128, %r5234;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5171, %r7145, %r7144, %r5234;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5175, %r7146, %r7145, %r5234;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5179, %r7147, %r7146, %r5234;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5183, %r7140, %r7147, %r5234;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5187, %r7141, %r7140, %r5234;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5191, %r7142, %r7141, %r5234;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5195, %r7143, %r7142, %r5234;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5199, %r7136, %r7143, %r5234;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5203, %r7137, %r7136, %r5234;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5207, %r7138, %r7137, %r5234;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5211, %r7139, %r7138, %r5234;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5215, %r4798, %r7139, %r5234;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5219, %r7133, %r4798, %r5234;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5223, %r7134, %r7133, %r5234;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5227, %r7135, %r7134, %r5234;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5231, %r7128, %r7135, %r5234;
	// inline asm
	setp.eq.s32	%p121, %r110, 0;
	selp.b32	%r7120, %r5179, %r5183, %p121;
	selp.b32	%r7121, %r5183, %r5187, %p121;
	selp.b32	%r7122, %r5187, %r5191, %p121;
	selp.b32	%r7123, %r5191, %r5195, %p121;
	selp.b32	%r7124, 0, %r5167, %p121;
	selp.b32	%r7125, %r5167, %r5171, %p121;
	selp.b32	%r7126, %r5171, %r5175, %p121;
	selp.b32	%r7127, %r5175, %r5179, %p121;
	selp.b32	%r7140, %r5227, %r5231, %p121;
	selp.b32	%r7144, %r5211, %r5215, %p121;
	selp.b32	%r7145, %r5215, %r5219, %p121;
	selp.b32	%r7146, %r5219, %r5223, %p121;
	selp.b32	%r7147, %r5223, %r5227, %p121;
	selp.b32	%r7148, %r5207, %r5211, %p121;
	selp.b32	%r7149, %r5203, %r5207, %p121;
	selp.b32	%r7150, %r5199, %r5203, %p121;
	selp.b32	%r7151, %r5195, %r5199, %p121;
	mov.u32 	%r7129, %r7128;
	mov.u32 	%r7130, %r7128;
	mov.u32 	%r7131, %r7128;
	mov.u32 	%r7132, %r7128;
	mov.u32 	%r7133, %r7128;
	mov.u32 	%r7134, %r7128;
	mov.u32 	%r7135, %r7128;
	mov.u32 	%r7136, %r7128;
	mov.u32 	%r7137, %r7128;
	mov.u32 	%r7138, %r7128;
	mov.u32 	%r7139, %r7128;

BB1_176:
	mov.u32 	%r7141, %r7128;
	mov.u32 	%r7142, %r7128;
	mov.u32 	%r7143, %r7128;
	bra.uni 	BB1_188;

BB1_156:
	setp.eq.s32	%p109, %r4830, 7;
	@%p109 bra 	BB1_157;
	bra.uni 	BB1_172;

BB1_157:
	and.b32  	%r5586, %r111, 3;
	shl.b32 	%r5570, %r5586, 3;
	mov.u32 	%r7124, 0;
	// inline asm
	shf.r.wrap.b32 %r5503, %r7144, %r7124, %r5570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5507, %r7145, %r7144, %r5570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5511, %r7146, %r7145, %r5570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5515, %r7147, %r7146, %r5570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5519, %r7140, %r7147, %r5570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5523, %r7141, %r7140, %r5570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5527, %r7142, %r7141, %r5570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5531, %r7143, %r7142, %r5570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5535, %r7136, %r7143, %r5570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5539, %r7137, %r7136, %r5570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5543, %r7138, %r7137, %r5570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5547, %r7139, %r7138, %r5570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5551, %r4798, %r7139, %r5570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5555, %r7133, %r4798, %r5570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5559, %r7134, %r7133, %r5570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5563, %r7135, %r7134, %r5570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5567, %r7124, %r7135, %r5570;
	// inline asm
	setp.eq.s32	%p125, %r110, 0;
	selp.b32	%r7120, 0, %r5503, %p125;
	selp.b32	%r7121, %r5503, %r5507, %p125;
	selp.b32	%r7122, %r5507, %r5511, %p125;
	selp.b32	%r7123, %r5511, %r5515, %p125;
	selp.b32	%r7136, %r5563, %r5567, %p125;
	selp.b32	%r7140, %r5547, %r5551, %p125;
	selp.b32	%r7141, %r5551, %r5555, %p125;
	selp.b32	%r7142, %r5555, %r5559, %p125;
	selp.b32	%r7143, %r5559, %r5563, %p125;
	selp.b32	%r7144, %r5531, %r5535, %p125;
	selp.b32	%r7145, %r5535, %r5539, %p125;
	selp.b32	%r7146, %r5539, %r5543, %p125;
	selp.b32	%r7147, %r5543, %r5547, %p125;
	selp.b32	%r7148, %r5527, %r5531, %p125;
	selp.b32	%r7149, %r5523, %r5527, %p125;
	selp.b32	%r7150, %r5519, %r5523, %p125;
	selp.b32	%r7151, %r5515, %r5519, %p125;
	mov.u32 	%r7125, %r7124;
	mov.u32 	%r7126, %r7124;
	mov.u32 	%r7127, %r7124;
	mov.u32 	%r7128, %r7124;
	mov.u32 	%r7129, %r7124;
	mov.u32 	%r7130, %r7124;
	mov.u32 	%r7131, %r7124;
	mov.u32 	%r7132, %r7124;
	mov.u32 	%r7133, %r7124;
	mov.u32 	%r7134, %r7124;
	mov.u32 	%r7135, %r7124;

BB1_179:
	mov.u32 	%r7137, %r7124;
	mov.u32 	%r7138, %r7124;
	mov.u32 	%r7139, %r7124;
	bra.uni 	BB1_188;

BB1_171:
	setp.ne.s32	%p98, %r4830, 15;
	@%p98 bra 	BB1_172;

	and.b32  	%r4914, %r111, 3;
	shl.b32 	%r4898, %r4914, 3;
	mov.u32 	%r7132, 0;
	// inline asm
	shf.r.wrap.b32 %r4831, %r7144, %r7132, %r4898;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4835, %r7145, %r7144, %r4898;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4839, %r7146, %r7145, %r4898;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4843, %r7147, %r7146, %r4898;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4847, %r7140, %r7147, %r4898;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4851, %r7141, %r7140, %r4898;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4855, %r7142, %r7141, %r4898;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4859, %r7143, %r7142, %r4898;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4863, %r7136, %r7143, %r4898;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4867, %r7137, %r7136, %r4898;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4871, %r7138, %r7137, %r4898;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4875, %r7139, %r7138, %r4898;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4879, %r4798, %r7139, %r4898;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4883, %r7133, %r4798, %r4898;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4887, %r7134, %r7133, %r4898;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4891, %r7135, %r7134, %r4898;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4895, %r7132, %r7135, %r4898;
	// inline asm
	setp.eq.s32	%p117, %r110, 0;
	selp.b32	%r7120, %r4859, %r4863, %p117;
	selp.b32	%r7121, %r4863, %r4867, %p117;
	selp.b32	%r7122, %r4867, %r4871, %p117;
	selp.b32	%r7123, %r4871, %r4875, %p117;
	selp.b32	%r7124, %r4843, %r4847, %p117;
	selp.b32	%r7125, %r4847, %r4851, %p117;
	selp.b32	%r7126, %r4851, %r4855, %p117;
	selp.b32	%r7127, %r4855, %r4859, %p117;
	selp.b32	%r7128, 0, %r4831, %p117;
	selp.b32	%r7129, %r4831, %r4835, %p117;
	selp.b32	%r7130, %r4835, %r4839, %p117;
	selp.b32	%r7131, %r4839, %r4843, %p117;
	selp.b32	%r7144, %r4891, %r4895, %p117;
	selp.b32	%r7148, %r4887, %r4891, %p117;
	selp.b32	%r7149, %r4883, %r4887, %p117;
	selp.b32	%r7150, %r4879, %r4883, %p117;
	selp.b32	%r7151, %r4875, %r4879, %p117;
	mov.u32 	%r7133, %r7132;
	mov.u32 	%r7134, %r7132;
	mov.u32 	%r7135, %r7132;
	mov.u32 	%r7136, %r7132;
	mov.u32 	%r7137, %r7132;
	mov.u32 	%r7138, %r7132;
	mov.u32 	%r7139, %r7132;
	mov.u32 	%r7140, %r7132;
	mov.u32 	%r7141, %r7132;
	mov.u32 	%r7142, %r7132;
	mov.u32 	%r7143, %r7132;
	mov.u32 	%r7145, %r7132;
	mov.u32 	%r7146, %r7132;
	mov.u32 	%r7147, %r7132;
	bra.uni 	BB1_188;

BB1_172:
	mov.u32 	%r7121, %r7120;
	mov.u32 	%r7122, %r7120;
	mov.u32 	%r7123, %r7120;
	mov.u32 	%r7124, %r7120;
	mov.u32 	%r7125, %r7120;
	mov.u32 	%r7126, %r7120;
	mov.u32 	%r7127, %r7120;
	mov.u32 	%r7128, %r7120;
	mov.u32 	%r7129, %r7120;
	mov.u32 	%r7130, %r7120;
	mov.u32 	%r7131, %r7120;
	mov.u32 	%r7132, %r4798;
	mov.u32 	%r7148, %r7120;

BB1_185:
	mov.u32 	%r7149, %r7120;

BB1_186:
	mov.u32 	%r7150, %r7120;

BB1_187:
	mov.u32 	%r7151, %r7120;
	bra.uni 	BB1_188;

BB1_10:
	mov.u32 	%r7030, 0;
	// inline asm
	prmt.b32 %r1581, %r109, %r7030, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r1585, %r109, %r7030, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r1589, %r108, %r7030, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r1593, %r108, %r7030, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r1597, %r107, %r7030, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r1601, %r107, %r7030, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r1605, %r106, %r7030, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r1609, %r106, %r7030, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r1613, %r105, %r7030, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r1617, %r105, %r7030, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r1621, %r104, %r7030, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r1625, %r104, %r7030, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r1629, %r103, %r7030, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r1633, %r103, %r7030, %r1106;
	// inline asm
	// inline asm
	prmt.b32 %r1637, %r102, %r7030, %r1102;
	// inline asm
	// inline asm
	prmt.b32 %r1641, %r102, %r7030, %r1106;
	// inline asm
	sub.s32 	%r1645, %r78, %r7028;
	shl.b32 	%r1646, %r1645, 1;
	add.s32 	%r128, %r1646, %r7007;
	and.b32  	%r1647, %r7007, 63;
	add.s32 	%r1648, %r1646, %r1647;
	setp.lt.s32	%p6, %r1648, 64;
	bfe.u32 	%r129, %r7007, 2, 4;
	@%p6 bra 	BB1_58;
	bra.uni 	BB1_11;

BB1_58:
	shl.b32 	%r3344, %r111, 2;
	mov.u32 	%r3345, 1985229328;
	shr.u32 	%r3346, %r3345, %r3344;
	and.b32  	%r438, %r3346, 65535;
	setp.gt.s32	%p46, %r129, 7;
	@%p46 bra 	BB1_74;

	setp.gt.s32	%p58, %r129, 3;
	@%p58 bra 	BB1_67;

	setp.gt.s32	%p64, %r129, 1;
	@%p64 bra 	BB1_64;

	setp.eq.s32	%p67, %r129, 0;
	@%p67 bra 	BB1_109;
	bra.uni 	BB1_62;

BB1_109:
	// inline asm
	prmt.b32 %r1581, %r1585, %r1581, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1585, %r1589, %r1585, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1589, %r1593, %r1589, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1593, %r1597, %r1593, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1597, %r1601, %r1597, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1601, %r1605, %r1601, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1605, %r1609, %r1605, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1609, %r1613, %r1609, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1613, %r1617, %r1613, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1617, %r1621, %r1617, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1621, %r1625, %r1621, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1625, %r1629, %r1625, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1629, %r1633, %r1629, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1633, %r1637, %r1633, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1637, %r1641, %r1637, %r438;
	// inline asm
	mov.u32 	%r4008, 0;
	// inline asm
	prmt.b32 %r7065, %r4008, %r1641, %r438;
	// inline asm
	bra.uni 	BB1_110;

BB1_11:
	setp.gt.s32	%p7, %r129, 7;
	@%p7 bra 	BB1_27;

	setp.gt.s32	%p19, %r129, 3;
	@%p19 bra 	BB1_20;

	setp.gt.s32	%p25, %r129, 1;
	@%p25 bra 	BB1_17;

	setp.eq.s32	%p28, %r129, 0;
	@%p28 bra 	BB1_53;
	bra.uni 	BB1_15;

BB1_53:
	and.b32  	%r3008, %r111, 3;
	shl.b32 	%r2992, %r3008, 3;
	mov.u32 	%r7030, 0;
	// inline asm
	shf.r.wrap.b32 %r2925, %r1581, %r7030, %r2992;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2929, %r1585, %r1581, %r2992;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2933, %r1589, %r1585, %r2992;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2937, %r1593, %r1589, %r2992;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2941, %r1597, %r1593, %r2992;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2945, %r1601, %r1597, %r2992;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2949, %r1605, %r1601, %r2992;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2953, %r1609, %r1605, %r2992;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2957, %r1613, %r1609, %r2992;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2961, %r1617, %r1613, %r2992;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2965, %r1621, %r1617, %r2992;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2969, %r1625, %r1621, %r2992;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2973, %r1629, %r1625, %r2992;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2977, %r1633, %r1629, %r2992;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2981, %r1637, %r1633, %r2992;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2985, %r1641, %r1637, %r2992;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2989, %r7030, %r1641, %r2992;
	// inline asm
	setp.eq.s32	%p45, %r110, 0;
	selp.b32	%r7042, %r2973, %r2977, %p45;
	selp.b32	%r1633, %r2977, %r2981, %p45;
	selp.b32	%r1637, %r2981, %r2985, %p45;
	selp.b32	%r1641, %r2985, %r2989, %p45;
	selp.b32	%r1613, %r2957, %r2961, %p45;
	selp.b32	%r1617, %r2961, %r2965, %p45;
	selp.b32	%r1621, %r2965, %r2969, %p45;
	selp.b32	%r1625, %r2969, %r2973, %p45;
	selp.b32	%r1597, %r2941, %r2945, %p45;
	selp.b32	%r1601, %r2945, %r2949, %p45;
	selp.b32	%r1605, %r2949, %r2953, %p45;
	selp.b32	%r1609, %r2953, %r2957, %p45;
	selp.b32	%r1581, %r2925, %r2929, %p45;
	selp.b32	%r1585, %r2929, %r2933, %p45;
	selp.b32	%r1589, %r2933, %r2937, %p45;
	selp.b32	%r1593, %r2937, %r2941, %p45;
	selp.b32	%r7058, 0, %r2925, %p45;
	mov.u32 	%r7031, %r7030;
	mov.u32 	%r7032, %r7030;
	mov.u32 	%r7033, %r7030;
	mov.u32 	%r7034, %r7030;
	mov.u32 	%r7035, %r7030;
	mov.u32 	%r7036, %r7030;
	mov.u32 	%r7037, %r7030;
	mov.u32 	%r7038, %r7030;
	mov.u32 	%r7039, %r7030;
	mov.u32 	%r7040, %r7030;
	mov.u32 	%r7041, %r7030;
	bra.uni 	BB1_54;

BB1_74:
	setp.gt.s32	%p47, %r129, 11;
	@%p47 bra 	BB1_82;

	setp.gt.s32	%p53, %r129, 9;
	@%p53 bra 	BB1_79;

	setp.eq.s32	%p56, %r129, 8;
	@%p56 bra 	BB1_99;
	bra.uni 	BB1_77;

BB1_99:
	// inline asm
	prmt.b32 %r1581, %r1617, %r1613, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1585, %r1621, %r1617, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1589, %r1625, %r1621, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1593, %r1629, %r1625, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1597, %r1633, %r1629, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1601, %r1637, %r1633, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1605, %r1641, %r1637, %r438;
	// inline asm
	mov.u32 	%r1629, 0;
	// inline asm
	prmt.b32 %r1609, %r1629, %r1641, %r438;
	// inline asm
	mov.u32 	%r1633, %r1629;
	mov.u32 	%r1637, %r1629;
	mov.u32 	%r7065, %r1629;
	mov.u32 	%r1613, %r1629;
	bra.uni 	BB1_100;

BB1_27:
	setp.gt.s32	%p8, %r129, 11;
	@%p8 bra 	BB1_35;

	setp.gt.s32	%p14, %r129, 9;
	@%p14 bra 	BB1_32;

	setp.eq.s32	%p17, %r129, 8;
	@%p17 bra 	BB1_47;
	bra.uni 	BB1_30;

BB1_47:
	and.b32  	%r2336, %r111, 3;
	shl.b32 	%r2320, %r2336, 3;
	mov.u32 	%r7034, 0;
	// inline asm
	shf.r.wrap.b32 %r2253, %r1581, %r7034, %r2320;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2257, %r1585, %r1581, %r2320;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2261, %r1589, %r1585, %r2320;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2265, %r1593, %r1589, %r2320;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2269, %r1597, %r1593, %r2320;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2273, %r1601, %r1597, %r2320;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2277, %r1605, %r1601, %r2320;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2281, %r1609, %r1605, %r2320;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2285, %r1613, %r1609, %r2320;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2289, %r1617, %r1613, %r2320;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2293, %r1621, %r1617, %r2320;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2297, %r1625, %r1621, %r2320;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2301, %r1629, %r1625, %r2320;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2305, %r1633, %r1629, %r2320;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2309, %r1637, %r1633, %r2320;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2313, %r1641, %r1637, %r2320;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2317, %r7034, %r1641, %r2320;
	// inline asm
	setp.eq.s32	%p37, %r110, 0;
	selp.b32	%r7030, %r2253, %r2257, %p37;
	selp.b32	%r7031, %r2257, %r2261, %p37;
	selp.b32	%r7032, %r2261, %r2265, %p37;
	selp.b32	%r7033, %r2265, %r2269, %p37;
	selp.b32	%r7037, 0, %r2253, %p37;
	selp.b32	%r1597, %r2301, %r2305, %p37;
	selp.b32	%r1601, %r2305, %r2309, %p37;
	selp.b32	%r1605, %r2309, %r2313, %p37;
	selp.b32	%r1609, %r2313, %r2317, %p37;
	selp.b32	%r1581, %r2285, %r2289, %p37;
	selp.b32	%r1585, %r2289, %r2293, %p37;
	selp.b32	%r1589, %r2293, %r2297, %p37;
	selp.b32	%r1593, %r2297, %r2301, %p37;
	selp.b32	%r7058, %r2281, %r2285, %p37;
	selp.b32	%r7059, %r2277, %r2281, %p37;
	selp.b32	%r7060, %r2273, %r2277, %p37;
	selp.b32	%r7061, %r2269, %r2273, %p37;
	mov.u32 	%r7035, %r7034;
	mov.u32 	%r7036, %r7034;
	mov.u32 	%r7038, %r7034;
	mov.u32 	%r7039, %r7034;
	mov.u32 	%r7040, %r7034;
	mov.u32 	%r7041, %r7034;
	mov.u32 	%r7042, %r7034;
	mov.u32 	%r1633, %r7034;
	mov.u32 	%r1637, %r7034;
	mov.u32 	%r1641, %r7034;
	mov.u32 	%r1613, %r7034;
	bra.uni 	BB1_48;

BB1_67:
	setp.gt.s32	%p59, %r129, 5;
	@%p59 bra 	BB1_71;

	setp.eq.s32	%p62, %r129, 4;
	@%p62 bra 	BB1_105;
	bra.uni 	BB1_69;

BB1_105:
	// inline asm
	prmt.b32 %r1581, %r1601, %r1597, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1585, %r1605, %r1601, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1589, %r1609, %r1605, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1593, %r1613, %r1609, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1597, %r1617, %r1613, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1601, %r1621, %r1617, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1605, %r1625, %r1621, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1609, %r1629, %r1625, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1613, %r1633, %r1629, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1617, %r1637, %r1633, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1621, %r1641, %r1637, %r438;
	// inline asm
	mov.u32 	%r1629, 0;
	// inline asm
	prmt.b32 %r1625, %r1629, %r1641, %r438;
	// inline asm
	mov.u32 	%r1633, %r1629;
	mov.u32 	%r1637, %r1629;
	mov.u32 	%r7065, %r1629;
	bra.uni 	BB1_110;

BB1_20:
	setp.gt.s32	%p20, %r129, 5;
	@%p20 bra 	BB1_24;

	setp.eq.s32	%p23, %r129, 4;
	@%p23 bra 	BB1_50;
	bra.uni 	BB1_22;

BB1_50:
	and.b32  	%r2672, %r111, 3;
	shl.b32 	%r2656, %r2672, 3;
	mov.u32 	%r7030, 0;
	// inline asm
	shf.r.wrap.b32 %r2589, %r1581, %r7030, %r2656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2593, %r1585, %r1581, %r2656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2597, %r1589, %r1585, %r2656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2601, %r1593, %r1589, %r2656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2605, %r1597, %r1593, %r2656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2609, %r1601, %r1597, %r2656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2613, %r1605, %r1601, %r2656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2617, %r1609, %r1605, %r2656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2621, %r1613, %r1609, %r2656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2625, %r1617, %r1613, %r2656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2629, %r1621, %r1617, %r2656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2633, %r1625, %r1621, %r2656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2637, %r1629, %r1625, %r2656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2641, %r1633, %r1629, %r2656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2645, %r1637, %r1633, %r2656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2649, %r1641, %r1637, %r2656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2653, %r7030, %r1641, %r2656;
	// inline asm
	setp.eq.s32	%p41, %r110, 0;
	selp.b32	%r7033, 0, %r2589, %p41;
	selp.b32	%r1613, %r2637, %r2641, %p41;
	selp.b32	%r1617, %r2641, %r2645, %p41;
	selp.b32	%r1621, %r2645, %r2649, %p41;
	selp.b32	%r1625, %r2649, %r2653, %p41;
	selp.b32	%r1597, %r2621, %r2625, %p41;
	selp.b32	%r1601, %r2625, %r2629, %p41;
	selp.b32	%r1605, %r2629, %r2633, %p41;
	selp.b32	%r1609, %r2633, %r2637, %p41;
	selp.b32	%r1581, %r2605, %r2609, %p41;
	selp.b32	%r1585, %r2609, %r2613, %p41;
	selp.b32	%r1589, %r2613, %r2617, %p41;
	selp.b32	%r1593, %r2617, %r2621, %p41;
	selp.b32	%r7058, %r2601, %r2605, %p41;
	selp.b32	%r7059, %r2597, %r2601, %p41;
	selp.b32	%r7060, %r2593, %r2597, %p41;
	selp.b32	%r7061, %r2589, %r2593, %p41;
	mov.u32 	%r7031, %r7030;
	mov.u32 	%r7032, %r7030;
	mov.u32 	%r7034, %r7030;
	mov.u32 	%r7035, %r7030;
	mov.u32 	%r7036, %r7030;
	mov.u32 	%r7037, %r7030;
	mov.u32 	%r7038, %r7030;
	mov.u32 	%r7039, %r7030;
	mov.u32 	%r7040, %r7030;
	mov.u32 	%r7041, %r7030;
	mov.u32 	%r7042, %r7030;
	bra.uni 	BB1_51;

BB1_82:
	setp.gt.s32	%p48, %r129, 13;
	@%p48 bra 	BB1_86;

	setp.eq.s32	%p51, %r129, 12;
	@%p51 bra 	BB1_93;
	bra.uni 	BB1_84;

BB1_93:
	// inline asm
	prmt.b32 %r1581, %r1633, %r1629, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1585, %r1637, %r1633, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1589, %r1641, %r1637, %r438;
	// inline asm
	mov.u32 	%r1629, 0;
	// inline asm
	prmt.b32 %r1593, %r1629, %r1641, %r438;
	// inline asm
	mov.u32 	%r1633, %r1629;
	mov.u32 	%r1637, %r1629;
	mov.u32 	%r7065, %r1629;
	mov.u32 	%r1613, %r1629;
	mov.u32 	%r1617, %r1629;
	mov.u32 	%r1621, %r1629;
	mov.u32 	%r1625, %r1629;
	mov.u32 	%r1597, %r1629;
	bra.uni 	BB1_94;

BB1_35:
	setp.gt.s32	%p9, %r129, 13;
	@%p9 bra 	BB1_39;

	setp.eq.s32	%p12, %r129, 12;
	@%p12 bra 	BB1_44;
	bra.uni 	BB1_37;

BB1_44:
	and.b32  	%r2000, %r111, 3;
	shl.b32 	%r1984, %r2000, 3;
	mov.u32 	%r7038, 0;
	// inline asm
	shf.r.wrap.b32 %r1917, %r1581, %r7038, %r1984;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1921, %r1585, %r1581, %r1984;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1925, %r1589, %r1585, %r1984;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1929, %r1593, %r1589, %r1984;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1933, %r1597, %r1593, %r1984;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1937, %r1601, %r1597, %r1984;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1941, %r1605, %r1601, %r1984;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1945, %r1609, %r1605, %r1984;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1949, %r1613, %r1609, %r1984;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1953, %r1617, %r1613, %r1984;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1957, %r1621, %r1617, %r1984;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1961, %r1625, %r1621, %r1984;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1965, %r1629, %r1625, %r1984;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1969, %r1633, %r1629, %r1984;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1973, %r1637, %r1633, %r1984;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1977, %r1641, %r1637, %r1984;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1981, %r7038, %r1641, %r1984;
	// inline asm
	setp.eq.s32	%p33, %r110, 0;
	selp.b32	%r7030, %r1933, %r1937, %p33;
	selp.b32	%r7031, %r1937, %r1941, %p33;
	selp.b32	%r7032, %r1941, %r1945, %p33;
	selp.b32	%r7033, %r1945, %r1949, %p33;
	selp.b32	%r7034, %r1917, %r1921, %p33;
	selp.b32	%r7035, %r1921, %r1925, %p33;
	selp.b32	%r7036, %r1925, %r1929, %p33;
	selp.b32	%r7037, %r1929, %r1933, %p33;
	selp.b32	%r7041, 0, %r1917, %p33;
	selp.b32	%r1581, %r1965, %r1969, %p33;
	selp.b32	%r1585, %r1969, %r1973, %p33;
	selp.b32	%r1589, %r1973, %r1977, %p33;
	selp.b32	%r1593, %r1977, %r1981, %p33;
	selp.b32	%r7058, %r1961, %r1965, %p33;
	selp.b32	%r7059, %r1957, %r1961, %p33;
	selp.b32	%r7060, %r1953, %r1957, %p33;
	selp.b32	%r7061, %r1949, %r1953, %p33;
	mov.u32 	%r7039, %r7038;
	mov.u32 	%r7040, %r7038;
	mov.u32 	%r7042, %r7038;
	mov.u32 	%r1633, %r7038;
	mov.u32 	%r1637, %r7038;
	mov.u32 	%r1641, %r7038;
	mov.u32 	%r1613, %r7038;
	mov.u32 	%r1617, %r7038;
	mov.u32 	%r1621, %r7038;
	mov.u32 	%r1625, %r7038;
	mov.u32 	%r1597, %r7038;
	bra.uni 	BB1_45;

BB1_64:
	setp.eq.s32	%p65, %r129, 2;
	@%p65 bra 	BB1_107;
	bra.uni 	BB1_65;

BB1_107:
	// inline asm
	prmt.b32 %r1581, %r1593, %r1589, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1585, %r1597, %r1593, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1589, %r1601, %r1597, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1593, %r1605, %r1601, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1597, %r1609, %r1605, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1601, %r1613, %r1609, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1605, %r1617, %r1613, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1609, %r1621, %r1617, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1613, %r1625, %r1621, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1617, %r1629, %r1625, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1621, %r1633, %r1629, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1625, %r1637, %r1633, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1629, %r1641, %r1637, %r438;
	// inline asm
	mov.u32 	%r1637, 0;
	// inline asm
	prmt.b32 %r1633, %r1637, %r1641, %r438;
	// inline asm
	mov.u32 	%r7065, %r1637;
	bra.uni 	BB1_110;

BB1_17:
	setp.eq.s32	%p26, %r129, 2;
	@%p26 bra 	BB1_52;
	bra.uni 	BB1_18;

BB1_52:
	and.b32  	%r2840, %r111, 3;
	shl.b32 	%r2824, %r2840, 3;
	mov.u32 	%r7030, 0;
	// inline asm
	shf.r.wrap.b32 %r2757, %r1581, %r7030, %r2824;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2761, %r1585, %r1581, %r2824;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2765, %r1589, %r1585, %r2824;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2769, %r1593, %r1589, %r2824;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2773, %r1597, %r1593, %r2824;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2777, %r1601, %r1597, %r2824;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2781, %r1605, %r1601, %r2824;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2785, %r1609, %r1605, %r2824;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2789, %r1613, %r1609, %r2824;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2793, %r1617, %r1613, %r2824;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2797, %r1621, %r1617, %r2824;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2801, %r1625, %r1621, %r2824;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2805, %r1629, %r1625, %r2824;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2809, %r1633, %r1629, %r2824;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2813, %r1637, %r1633, %r2824;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2817, %r1641, %r1637, %r2824;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2821, %r7030, %r1641, %r2824;
	// inline asm
	setp.eq.s32	%p43, %r110, 0;
	selp.b32	%r7042, %r2813, %r2817, %p43;
	selp.b32	%r1633, %r2817, %r2821, %p43;
	selp.b32	%r1613, %r2797, %r2801, %p43;
	selp.b32	%r1617, %r2801, %r2805, %p43;
	selp.b32	%r1621, %r2805, %r2809, %p43;
	selp.b32	%r1625, %r2809, %r2813, %p43;
	selp.b32	%r1597, %r2781, %r2785, %p43;
	selp.b32	%r1601, %r2785, %r2789, %p43;
	selp.b32	%r1605, %r2789, %r2793, %p43;
	selp.b32	%r1609, %r2793, %r2797, %p43;
	selp.b32	%r1581, %r2765, %r2769, %p43;
	selp.b32	%r1585, %r2769, %r2773, %p43;
	selp.b32	%r1589, %r2773, %r2777, %p43;
	selp.b32	%r1593, %r2777, %r2781, %p43;
	selp.b32	%r7058, %r2761, %r2765, %p43;
	selp.b32	%r7059, %r2757, %r2761, %p43;
	selp.b32	%r7060, 0, %r2757, %p43;
	mov.u32 	%r7031, %r7030;
	mov.u32 	%r7032, %r7030;
	mov.u32 	%r7033, %r7030;
	mov.u32 	%r7034, %r7030;
	mov.u32 	%r7035, %r7030;
	mov.u32 	%r7036, %r7030;
	mov.u32 	%r7037, %r7030;
	mov.u32 	%r7038, %r7030;
	mov.u32 	%r7039, %r7030;
	mov.u32 	%r7040, %r7030;
	mov.u32 	%r7041, %r7030;
	mov.u32 	%r1637, %r7030;
	mov.u32 	%r1641, %r7030;
	bra.uni 	BB1_56;

BB1_79:
	setp.eq.s32	%p54, %r129, 10;
	@%p54 bra 	BB1_97;
	bra.uni 	BB1_80;

BB1_97:
	// inline asm
	prmt.b32 %r1581, %r1625, %r1621, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1585, %r1629, %r1625, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1589, %r1633, %r1629, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1593, %r1637, %r1633, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1597, %r1641, %r1637, %r438;
	// inline asm
	mov.u32 	%r1629, 0;
	// inline asm
	prmt.b32 %r1601, %r1629, %r1641, %r438;
	// inline asm
	mov.u32 	%r1633, %r1629;
	mov.u32 	%r1637, %r1629;
	mov.u32 	%r7065, %r1629;
	mov.u32 	%r1613, %r1629;
	mov.u32 	%r1617, %r1629;
	mov.u32 	%r1621, %r1629;
	mov.u32 	%r1625, %r1629;
	bra.uni 	BB1_95;

BB1_32:
	setp.eq.s32	%p15, %r129, 10;
	@%p15 bra 	BB1_46;
	bra.uni 	BB1_33;

BB1_46:
	and.b32  	%r2168, %r111, 3;
	shl.b32 	%r2152, %r2168, 3;
	mov.u32 	%r7034, 0;
	// inline asm
	shf.r.wrap.b32 %r2085, %r1581, %r7034, %r2152;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2089, %r1585, %r1581, %r2152;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2093, %r1589, %r1585, %r2152;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2097, %r1593, %r1589, %r2152;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2101, %r1597, %r1593, %r2152;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2105, %r1601, %r1597, %r2152;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2109, %r1605, %r1601, %r2152;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2113, %r1609, %r1605, %r2152;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2117, %r1613, %r1609, %r2152;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2121, %r1617, %r1613, %r2152;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2125, %r1621, %r1617, %r2152;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2129, %r1625, %r1621, %r2152;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2133, %r1629, %r1625, %r2152;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2137, %r1633, %r1629, %r2152;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2141, %r1637, %r1633, %r2152;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2145, %r1641, %r1637, %r2152;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2149, %r7034, %r1641, %r2152;
	// inline asm
	setp.eq.s32	%p35, %r110, 0;
	selp.b32	%r7030, %r2093, %r2097, %p35;
	selp.b32	%r7031, %r2097, %r2101, %p35;
	selp.b32	%r7032, %r2101, %r2105, %p35;
	selp.b32	%r7033, %r2105, %r2109, %p35;
	selp.b32	%r7035, 0, %r2085, %p35;
	selp.b32	%r7036, %r2085, %r2089, %p35;
	selp.b32	%r7037, %r2089, %r2093, %p35;
	selp.b32	%r1597, %r2141, %r2145, %p35;
	selp.b32	%r1601, %r2145, %r2149, %p35;
	selp.b32	%r1581, %r2125, %r2129, %p35;
	selp.b32	%r1585, %r2129, %r2133, %p35;
	selp.b32	%r1589, %r2133, %r2137, %p35;
	selp.b32	%r1593, %r2137, %r2141, %p35;
	selp.b32	%r7058, %r2121, %r2125, %p35;
	selp.b32	%r7059, %r2117, %r2121, %p35;
	selp.b32	%r7060, %r2113, %r2117, %p35;
	selp.b32	%r7061, %r2109, %r2113, %p35;
	mov.u32 	%r7038, %r7034;
	mov.u32 	%r7039, %r7034;
	mov.u32 	%r7040, %r7034;
	mov.u32 	%r7041, %r7034;
	mov.u32 	%r7042, %r7034;
	mov.u32 	%r1633, %r7034;
	mov.u32 	%r1637, %r7034;
	mov.u32 	%r1641, %r7034;
	mov.u32 	%r1613, %r7034;
	mov.u32 	%r1617, %r7034;
	mov.u32 	%r1621, %r7034;
	mov.u32 	%r1625, %r7034;
	mov.u32 	%r1605, %r7034;
	mov.u32 	%r1609, %r7034;
	bra.uni 	BB1_57;

BB1_71:
	setp.eq.s32	%p60, %r129, 6;
	@%p60 bra 	BB1_103;
	bra.uni 	BB1_72;

BB1_103:
	// inline asm
	prmt.b32 %r1581, %r1609, %r1605, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1585, %r1613, %r1609, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1589, %r1617, %r1613, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1593, %r1621, %r1617, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1597, %r1625, %r1621, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1601, %r1629, %r1625, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1605, %r1633, %r1629, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1609, %r1637, %r1633, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1613, %r1641, %r1637, %r438;
	// inline asm
	mov.u32 	%r1629, 0;
	// inline asm
	prmt.b32 %r1617, %r1629, %r1641, %r438;
	// inline asm
	mov.u32 	%r1633, %r1629;
	mov.u32 	%r1637, %r1629;
	mov.u32 	%r7065, %r1629;
	bra.uni 	BB1_101;

BB1_24:
	setp.eq.s32	%p21, %r129, 6;
	@%p21 bra 	BB1_49;
	bra.uni 	BB1_25;

BB1_49:
	and.b32  	%r2504, %r111, 3;
	shl.b32 	%r2488, %r2504, 3;
	mov.u32 	%r7030, 0;
	// inline asm
	shf.r.wrap.b32 %r2421, %r1581, %r7030, %r2488;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2425, %r1585, %r1581, %r2488;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2429, %r1589, %r1585, %r2488;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2433, %r1593, %r1589, %r2488;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2437, %r1597, %r1593, %r2488;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2441, %r1601, %r1597, %r2488;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2445, %r1605, %r1601, %r2488;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2449, %r1609, %r1605, %r2488;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2453, %r1613, %r1609, %r2488;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2457, %r1617, %r1613, %r2488;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2461, %r1621, %r1617, %r2488;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2465, %r1625, %r1621, %r2488;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2469, %r1629, %r1625, %r2488;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2473, %r1633, %r1629, %r2488;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2477, %r1637, %r1633, %r2488;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2481, %r1641, %r1637, %r2488;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2485, %r7030, %r1641, %r2488;
	// inline asm
	setp.eq.s32	%p39, %r110, 0;
	selp.b32	%r7031, 0, %r2421, %p39;
	selp.b32	%r7032, %r2421, %r2425, %p39;
	selp.b32	%r7033, %r2425, %r2429, %p39;
	selp.b32	%r1613, %r2477, %r2481, %p39;
	selp.b32	%r1617, %r2481, %r2485, %p39;
	selp.b32	%r1597, %r2461, %r2465, %p39;
	selp.b32	%r1601, %r2465, %r2469, %p39;
	selp.b32	%r1605, %r2469, %r2473, %p39;
	selp.b32	%r1609, %r2473, %r2477, %p39;
	selp.b32	%r1581, %r2445, %r2449, %p39;
	selp.b32	%r1585, %r2449, %r2453, %p39;
	selp.b32	%r1589, %r2453, %r2457, %p39;
	selp.b32	%r1593, %r2457, %r2461, %p39;
	selp.b32	%r7058, %r2441, %r2445, %p39;
	selp.b32	%r7059, %r2437, %r2441, %p39;
	selp.b32	%r7060, %r2433, %r2437, %p39;
	selp.b32	%r7061, %r2429, %r2433, %p39;
	mov.u32 	%r7034, %r7030;
	mov.u32 	%r7035, %r7030;
	mov.u32 	%r7036, %r7030;
	mov.u32 	%r7037, %r7030;
	mov.u32 	%r7038, %r7030;
	mov.u32 	%r7039, %r7030;
	mov.u32 	%r7040, %r7030;
	mov.u32 	%r7041, %r7030;
	mov.u32 	%r7042, %r7030;
	mov.u32 	%r1633, %r7030;
	mov.u32 	%r1637, %r7030;
	mov.u32 	%r1641, %r7030;
	mov.u32 	%r1621, %r7030;
	mov.u32 	%r1625, %r7030;
	bra.uni 	BB1_57;

BB1_86:
	setp.eq.s32	%p49, %r129, 14;
	@%p49 bra 	BB1_91;
	bra.uni 	BB1_87;

BB1_91:
	// inline asm
	prmt.b32 %r1581, %r1641, %r1637, %r438;
	// inline asm
	mov.u32 	%r1629, 0;
	// inline asm
	prmt.b32 %r1585, %r1629, %r1641, %r438;
	// inline asm
	mov.u32 	%r1633, %r1629;
	mov.u32 	%r1637, %r1629;
	mov.u32 	%r7065, %r1629;
	mov.u32 	%r1613, %r1629;
	mov.u32 	%r1617, %r1629;
	mov.u32 	%r1621, %r1629;
	mov.u32 	%r1625, %r1629;
	mov.u32 	%r1597, %r1629;
	mov.u32 	%r1601, %r1629;
	mov.u32 	%r1605, %r1629;
	mov.u32 	%r1609, %r1629;
	bra.uni 	BB1_90;

BB1_39:
	setp.eq.s32	%p10, %r129, 14;
	@%p10 bra 	BB1_43;
	bra.uni 	BB1_40;

BB1_43:
	and.b32  	%r1832, %r111, 3;
	shl.b32 	%r1816, %r1832, 3;
	mov.u32 	%r7038, 0;
	// inline asm
	shf.r.wrap.b32 %r1749, %r1581, %r7038, %r1816;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1753, %r1585, %r1581, %r1816;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1757, %r1589, %r1585, %r1816;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1761, %r1593, %r1589, %r1816;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1765, %r1597, %r1593, %r1816;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1769, %r1601, %r1597, %r1816;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1773, %r1605, %r1601, %r1816;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1777, %r1609, %r1605, %r1816;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1781, %r1613, %r1609, %r1816;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1785, %r1617, %r1613, %r1816;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1789, %r1621, %r1617, %r1816;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1793, %r1625, %r1621, %r1816;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1797, %r1629, %r1625, %r1816;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1801, %r1633, %r1629, %r1816;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1805, %r1637, %r1633, %r1816;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1809, %r1641, %r1637, %r1816;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1813, %r7038, %r1641, %r1816;
	// inline asm
	setp.eq.s32	%p31, %r110, 0;
	selp.b32	%r7030, %r1773, %r1777, %p31;
	selp.b32	%r7031, %r1777, %r1781, %p31;
	selp.b32	%r7032, %r1781, %r1785, %p31;
	selp.b32	%r7033, %r1785, %r1789, %p31;
	selp.b32	%r7034, %r1757, %r1761, %p31;
	selp.b32	%r7035, %r1761, %r1765, %p31;
	selp.b32	%r7036, %r1765, %r1769, %p31;
	selp.b32	%r7037, %r1769, %r1773, %p31;
	selp.b32	%r7039, 0, %r1749, %p31;
	selp.b32	%r7040, %r1749, %r1753, %p31;
	selp.b32	%r7041, %r1753, %r1757, %p31;
	selp.b32	%r1581, %r1805, %r1809, %p31;
	selp.b32	%r1585, %r1809, %r1813, %p31;
	selp.b32	%r7058, %r1801, %r1805, %p31;
	selp.b32	%r7059, %r1797, %r1801, %p31;
	selp.b32	%r7060, %r1793, %r1797, %p31;
	selp.b32	%r7061, %r1789, %r1793, %p31;
	mov.u32 	%r7042, %r7038;
	mov.u32 	%r1633, %r7038;
	mov.u32 	%r1637, %r7038;
	mov.u32 	%r1641, %r7038;
	mov.u32 	%r1613, %r7038;
	mov.u32 	%r1617, %r7038;
	mov.u32 	%r1621, %r7038;
	mov.u32 	%r1625, %r7038;
	mov.u32 	%r1597, %r7038;
	mov.u32 	%r1601, %r7038;
	mov.u32 	%r1605, %r7038;
	mov.u32 	%r1609, %r7038;
	mov.u32 	%r1589, %r7038;
	mov.u32 	%r1593, %r7038;
	bra.uni 	BB1_57;

BB1_62:
	setp.eq.s32	%p68, %r129, 1;
	@%p68 bra 	BB1_108;
	bra.uni 	BB1_63;

BB1_108:
	// inline asm
	prmt.b32 %r1581, %r1589, %r1585, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1585, %r1593, %r1589, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1589, %r1597, %r1593, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1593, %r1601, %r1597, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1597, %r1605, %r1601, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1601, %r1609, %r1605, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1605, %r1613, %r1609, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1609, %r1617, %r1613, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1613, %r1621, %r1617, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1617, %r1625, %r1621, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1621, %r1629, %r1625, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1625, %r1633, %r1629, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1629, %r1637, %r1633, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1633, %r1641, %r1637, %r438;
	// inline asm
	mov.u32 	%r7065, 0;
	// inline asm
	prmt.b32 %r1637, %r7065, %r1641, %r438;
	// inline asm
	bra.uni 	BB1_110;

BB1_15:
	setp.eq.s32	%p29, %r129, 1;
	@%p29 bra 	BB1_16;
	bra.uni 	BB1_41;

BB1_16:
	and.b32  	%r2924, %r111, 3;
	shl.b32 	%r2908, %r2924, 3;
	mov.u32 	%r7030, 0;
	// inline asm
	shf.r.wrap.b32 %r2841, %r1581, %r7030, %r2908;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2845, %r1585, %r1581, %r2908;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2849, %r1589, %r1585, %r2908;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2853, %r1593, %r1589, %r2908;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2857, %r1597, %r1593, %r2908;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2861, %r1601, %r1597, %r2908;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2865, %r1605, %r1601, %r2908;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2869, %r1609, %r1605, %r2908;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2873, %r1613, %r1609, %r2908;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2877, %r1617, %r1613, %r2908;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2881, %r1621, %r1617, %r2908;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2885, %r1625, %r1621, %r2908;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2889, %r1629, %r1625, %r2908;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2893, %r1633, %r1629, %r2908;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2897, %r1637, %r1633, %r2908;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2901, %r1641, %r1637, %r2908;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2905, %r7030, %r1641, %r2908;
	// inline asm
	setp.eq.s32	%p44, %r110, 0;
	selp.b32	%r7042, %r2893, %r2897, %p44;
	selp.b32	%r1633, %r2897, %r2901, %p44;
	selp.b32	%r1637, %r2901, %r2905, %p44;
	selp.b32	%r1613, %r2877, %r2881, %p44;
	selp.b32	%r1617, %r2881, %r2885, %p44;
	selp.b32	%r1621, %r2885, %r2889, %p44;
	selp.b32	%r1625, %r2889, %r2893, %p44;
	selp.b32	%r1597, %r2861, %r2865, %p44;
	selp.b32	%r1601, %r2865, %r2869, %p44;
	selp.b32	%r1605, %r2869, %r2873, %p44;
	selp.b32	%r1609, %r2873, %r2877, %p44;
	selp.b32	%r1581, %r2845, %r2849, %p44;
	selp.b32	%r1585, %r2849, %r2853, %p44;
	selp.b32	%r1589, %r2853, %r2857, %p44;
	selp.b32	%r1593, %r2857, %r2861, %p44;
	selp.b32	%r7058, %r2841, %r2845, %p44;
	selp.b32	%r7059, 0, %r2841, %p44;
	mov.u32 	%r7031, %r7030;
	mov.u32 	%r7032, %r7030;
	mov.u32 	%r7033, %r7030;
	mov.u32 	%r7034, %r7030;
	mov.u32 	%r7035, %r7030;
	mov.u32 	%r7036, %r7030;
	mov.u32 	%r7037, %r7030;
	mov.u32 	%r7038, %r7030;
	mov.u32 	%r7039, %r7030;
	mov.u32 	%r7040, %r7030;
	mov.u32 	%r7041, %r7030;
	mov.u32 	%r1641, %r7030;
	bra.uni 	BB1_55;

BB1_77:
	setp.eq.s32	%p57, %r129, 9;
	@%p57 bra 	BB1_98;
	bra.uni 	BB1_78;

BB1_98:
	// inline asm
	prmt.b32 %r1581, %r1621, %r1617, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1585, %r1625, %r1621, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1589, %r1629, %r1625, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1593, %r1633, %r1629, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1597, %r1637, %r1633, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1601, %r1641, %r1637, %r438;
	// inline asm
	mov.u32 	%r1629, 0;
	// inline asm
	prmt.b32 %r1605, %r1629, %r1641, %r438;
	// inline asm
	mov.u32 	%r1633, %r1629;
	mov.u32 	%r1637, %r1629;
	mov.u32 	%r7065, %r1629;
	mov.u32 	%r1613, %r1629;
	mov.u32 	%r1617, %r1629;
	mov.u32 	%r1621, %r1629;
	mov.u32 	%r1625, %r1629;
	mov.u32 	%r1609, %r1629;
	bra.uni 	BB1_110;

BB1_30:
	setp.eq.s32	%p18, %r129, 9;
	@%p18 bra 	BB1_31;
	bra.uni 	BB1_41;

BB1_31:
	and.b32  	%r2252, %r111, 3;
	shl.b32 	%r2236, %r2252, 3;
	mov.u32 	%r7034, 0;
	// inline asm
	shf.r.wrap.b32 %r2169, %r1581, %r7034, %r2236;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2173, %r1585, %r1581, %r2236;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2177, %r1589, %r1585, %r2236;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2181, %r1593, %r1589, %r2236;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2185, %r1597, %r1593, %r2236;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2189, %r1601, %r1597, %r2236;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2193, %r1605, %r1601, %r2236;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2197, %r1609, %r1605, %r2236;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2201, %r1613, %r1609, %r2236;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2205, %r1617, %r1613, %r2236;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2209, %r1621, %r1617, %r2236;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2213, %r1625, %r1621, %r2236;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2217, %r1629, %r1625, %r2236;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2221, %r1633, %r1629, %r2236;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2225, %r1637, %r1633, %r2236;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2229, %r1641, %r1637, %r2236;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2233, %r7034, %r1641, %r2236;
	// inline asm
	setp.eq.s32	%p36, %r110, 0;
	selp.b32	%r7030, %r2173, %r2177, %p36;
	selp.b32	%r7031, %r2177, %r2181, %p36;
	selp.b32	%r7032, %r2181, %r2185, %p36;
	selp.b32	%r7033, %r2185, %r2189, %p36;
	selp.b32	%r7036, 0, %r2169, %p36;
	selp.b32	%r7037, %r2169, %r2173, %p36;
	selp.b32	%r1597, %r2221, %r2225, %p36;
	selp.b32	%r1601, %r2225, %r2229, %p36;
	selp.b32	%r1605, %r2229, %r2233, %p36;
	selp.b32	%r1581, %r2205, %r2209, %p36;
	selp.b32	%r1585, %r2209, %r2213, %p36;
	selp.b32	%r1589, %r2213, %r2217, %p36;
	selp.b32	%r1593, %r2217, %r2221, %p36;
	selp.b32	%r7058, %r2201, %r2205, %p36;
	selp.b32	%r7059, %r2197, %r2201, %p36;
	selp.b32	%r7060, %r2193, %r2197, %p36;
	selp.b32	%r7061, %r2189, %r2193, %p36;
	mov.u32 	%r7035, %r7034;
	mov.u32 	%r7038, %r7034;
	mov.u32 	%r7039, %r7034;
	mov.u32 	%r7040, %r7034;
	mov.u32 	%r7041, %r7034;
	mov.u32 	%r7042, %r7034;
	mov.u32 	%r1633, %r7034;
	mov.u32 	%r1637, %r7034;
	mov.u32 	%r1641, %r7034;
	mov.u32 	%r1613, %r7034;
	mov.u32 	%r1617, %r7034;
	mov.u32 	%r1621, %r7034;
	mov.u32 	%r1625, %r7034;
	mov.u32 	%r1609, %r7034;
	bra.uni 	BB1_57;

BB1_69:
	setp.eq.s32	%p63, %r129, 5;
	@%p63 bra 	BB1_104;
	bra.uni 	BB1_70;

BB1_104:
	// inline asm
	prmt.b32 %r1581, %r1605, %r1601, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1585, %r1609, %r1605, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1589, %r1613, %r1609, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1593, %r1617, %r1613, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1597, %r1621, %r1617, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1601, %r1625, %r1621, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1605, %r1629, %r1625, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1609, %r1633, %r1629, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1613, %r1637, %r1633, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1617, %r1641, %r1637, %r438;
	// inline asm
	mov.u32 	%r1629, 0;
	// inline asm
	prmt.b32 %r1621, %r1629, %r1641, %r438;
	// inline asm
	mov.u32 	%r1633, %r1629;
	mov.u32 	%r1637, %r1629;
	mov.u32 	%r7065, %r1629;
	mov.u32 	%r1625, %r1629;
	bra.uni 	BB1_110;

BB1_22:
	setp.eq.s32	%p24, %r129, 5;
	@%p24 bra 	BB1_23;
	bra.uni 	BB1_41;

BB1_23:
	and.b32  	%r2588, %r111, 3;
	shl.b32 	%r2572, %r2588, 3;
	mov.u32 	%r7030, 0;
	// inline asm
	shf.r.wrap.b32 %r2505, %r1581, %r7030, %r2572;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2509, %r1585, %r1581, %r2572;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2513, %r1589, %r1585, %r2572;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2517, %r1593, %r1589, %r2572;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2521, %r1597, %r1593, %r2572;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2525, %r1601, %r1597, %r2572;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2529, %r1605, %r1601, %r2572;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2533, %r1609, %r1605, %r2572;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2537, %r1613, %r1609, %r2572;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2541, %r1617, %r1613, %r2572;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2545, %r1621, %r1617, %r2572;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2549, %r1625, %r1621, %r2572;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2553, %r1629, %r1625, %r2572;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2557, %r1633, %r1629, %r2572;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2561, %r1637, %r1633, %r2572;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2565, %r1641, %r1637, %r2572;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2569, %r7030, %r1641, %r2572;
	// inline asm
	setp.eq.s32	%p40, %r110, 0;
	selp.b32	%r7032, 0, %r2505, %p40;
	selp.b32	%r7033, %r2505, %r2509, %p40;
	selp.b32	%r1613, %r2557, %r2561, %p40;
	selp.b32	%r1617, %r2561, %r2565, %p40;
	selp.b32	%r1621, %r2565, %r2569, %p40;
	selp.b32	%r1597, %r2541, %r2545, %p40;
	selp.b32	%r1601, %r2545, %r2549, %p40;
	selp.b32	%r1605, %r2549, %r2553, %p40;
	selp.b32	%r1609, %r2553, %r2557, %p40;
	selp.b32	%r1581, %r2525, %r2529, %p40;
	selp.b32	%r1585, %r2529, %r2533, %p40;
	selp.b32	%r1589, %r2533, %r2537, %p40;
	selp.b32	%r1593, %r2537, %r2541, %p40;
	selp.b32	%r7058, %r2521, %r2525, %p40;
	selp.b32	%r7059, %r2517, %r2521, %p40;
	selp.b32	%r7060, %r2513, %r2517, %p40;
	selp.b32	%r7061, %r2509, %r2513, %p40;
	mov.u32 	%r7031, %r7030;
	mov.u32 	%r7034, %r7030;
	mov.u32 	%r7035, %r7030;
	mov.u32 	%r7036, %r7030;
	mov.u32 	%r7037, %r7030;
	mov.u32 	%r7038, %r7030;
	mov.u32 	%r7039, %r7030;
	mov.u32 	%r7040, %r7030;
	mov.u32 	%r7041, %r7030;
	mov.u32 	%r7042, %r7030;
	mov.u32 	%r1633, %r7030;
	mov.u32 	%r1637, %r7030;
	mov.u32 	%r1641, %r7030;
	mov.u32 	%r1625, %r7030;
	bra.uni 	BB1_57;

BB1_84:
	setp.eq.s32	%p52, %r129, 13;
	@%p52 bra 	BB1_92;
	bra.uni 	BB1_85;

BB1_92:
	// inline asm
	prmt.b32 %r1581, %r1637, %r1633, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1585, %r1641, %r1637, %r438;
	// inline asm
	mov.u32 	%r1629, 0;
	// inline asm
	prmt.b32 %r1589, %r1629, %r1641, %r438;
	// inline asm
	mov.u32 	%r1633, %r1629;
	mov.u32 	%r1637, %r1629;
	mov.u32 	%r7065, %r1629;
	mov.u32 	%r1613, %r1629;
	mov.u32 	%r1617, %r1629;
	mov.u32 	%r1621, %r1629;
	mov.u32 	%r1625, %r1629;
	mov.u32 	%r1597, %r1629;
	mov.u32 	%r1601, %r1629;
	mov.u32 	%r1605, %r1629;
	mov.u32 	%r1609, %r1629;
	mov.u32 	%r1593, %r1629;
	bra.uni 	BB1_110;

BB1_37:
	setp.eq.s32	%p13, %r129, 13;
	@%p13 bra 	BB1_38;
	bra.uni 	BB1_41;

BB1_38:
	and.b32  	%r1916, %r111, 3;
	shl.b32 	%r1900, %r1916, 3;
	mov.u32 	%r7038, 0;
	// inline asm
	shf.r.wrap.b32 %r1833, %r1581, %r7038, %r1900;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1837, %r1585, %r1581, %r1900;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1841, %r1589, %r1585, %r1900;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1845, %r1593, %r1589, %r1900;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1849, %r1597, %r1593, %r1900;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1853, %r1601, %r1597, %r1900;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1857, %r1605, %r1601, %r1900;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1861, %r1609, %r1605, %r1900;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1865, %r1613, %r1609, %r1900;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1869, %r1617, %r1613, %r1900;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1873, %r1621, %r1617, %r1900;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1877, %r1625, %r1621, %r1900;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1881, %r1629, %r1625, %r1900;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1885, %r1633, %r1629, %r1900;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1889, %r1637, %r1633, %r1900;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1893, %r1641, %r1637, %r1900;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1897, %r7038, %r1641, %r1900;
	// inline asm
	setp.eq.s32	%p32, %r110, 0;
	selp.b32	%r7030, %r1853, %r1857, %p32;
	selp.b32	%r7031, %r1857, %r1861, %p32;
	selp.b32	%r7032, %r1861, %r1865, %p32;
	selp.b32	%r7033, %r1865, %r1869, %p32;
	selp.b32	%r7034, %r1837, %r1841, %p32;
	selp.b32	%r7035, %r1841, %r1845, %p32;
	selp.b32	%r7036, %r1845, %r1849, %p32;
	selp.b32	%r7037, %r1849, %r1853, %p32;
	selp.b32	%r7040, 0, %r1833, %p32;
	selp.b32	%r7041, %r1833, %r1837, %p32;
	selp.b32	%r1581, %r1885, %r1889, %p32;
	selp.b32	%r1585, %r1889, %r1893, %p32;
	selp.b32	%r1589, %r1893, %r1897, %p32;
	selp.b32	%r7058, %r1881, %r1885, %p32;
	selp.b32	%r7059, %r1877, %r1881, %p32;
	selp.b32	%r7060, %r1873, %r1877, %p32;
	selp.b32	%r7061, %r1869, %r1873, %p32;
	mov.u32 	%r7039, %r7038;
	mov.u32 	%r7042, %r7038;
	mov.u32 	%r1633, %r7038;
	mov.u32 	%r1637, %r7038;
	mov.u32 	%r1641, %r7038;
	mov.u32 	%r1613, %r7038;
	mov.u32 	%r1617, %r7038;
	mov.u32 	%r1621, %r7038;
	mov.u32 	%r1625, %r7038;
	mov.u32 	%r1597, %r7038;
	mov.u32 	%r1601, %r7038;
	mov.u32 	%r1605, %r7038;
	mov.u32 	%r1609, %r7038;
	mov.u32 	%r1593, %r7038;
	bra.uni 	BB1_57;

BB1_65:
	setp.eq.s32	%p66, %r129, 3;
	@%p66 bra 	BB1_106;
	bra.uni 	BB1_66;

BB1_106:
	// inline asm
	prmt.b32 %r1581, %r1597, %r1593, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1585, %r1601, %r1597, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1589, %r1605, %r1601, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1593, %r1609, %r1605, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1597, %r1613, %r1609, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1601, %r1617, %r1613, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1605, %r1621, %r1617, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1609, %r1625, %r1621, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1613, %r1629, %r1625, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1617, %r1633, %r1629, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1621, %r1637, %r1633, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1625, %r1641, %r1637, %r438;
	// inline asm
	mov.u32 	%r1633, 0;
	// inline asm
	prmt.b32 %r1629, %r1633, %r1641, %r438;
	// inline asm
	mov.u32 	%r1637, %r1633;
	mov.u32 	%r7065, %r1633;
	bra.uni 	BB1_110;

BB1_18:
	setp.eq.s32	%p27, %r129, 3;
	@%p27 bra 	BB1_19;
	bra.uni 	BB1_41;

BB1_19:
	and.b32  	%r2756, %r111, 3;
	shl.b32 	%r2740, %r2756, 3;
	mov.u32 	%r7030, 0;
	// inline asm
	shf.r.wrap.b32 %r2673, %r1581, %r7030, %r2740;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2677, %r1585, %r1581, %r2740;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2681, %r1589, %r1585, %r2740;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2685, %r1593, %r1589, %r2740;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2689, %r1597, %r1593, %r2740;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2693, %r1601, %r1597, %r2740;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2697, %r1605, %r1601, %r2740;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2701, %r1609, %r1605, %r2740;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2705, %r1613, %r1609, %r2740;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2709, %r1617, %r1613, %r2740;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2713, %r1621, %r1617, %r2740;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2717, %r1625, %r1621, %r2740;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2721, %r1629, %r1625, %r2740;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2725, %r1633, %r1629, %r2740;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2729, %r1637, %r1633, %r2740;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2733, %r1641, %r1637, %r2740;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2737, %r7030, %r1641, %r2740;
	// inline asm
	setp.eq.s32	%p42, %r110, 0;
	selp.b32	%r7042, %r2733, %r2737, %p42;
	selp.b32	%r1613, %r2717, %r2721, %p42;
	selp.b32	%r1617, %r2721, %r2725, %p42;
	selp.b32	%r1621, %r2725, %r2729, %p42;
	selp.b32	%r1625, %r2729, %r2733, %p42;
	selp.b32	%r1597, %r2701, %r2705, %p42;
	selp.b32	%r1601, %r2705, %r2709, %p42;
	selp.b32	%r1605, %r2709, %r2713, %p42;
	selp.b32	%r1609, %r2713, %r2717, %p42;
	selp.b32	%r1581, %r2685, %r2689, %p42;
	selp.b32	%r1585, %r2689, %r2693, %p42;
	selp.b32	%r1589, %r2693, %r2697, %p42;
	selp.b32	%r1593, %r2697, %r2701, %p42;
	selp.b32	%r7058, %r2681, %r2685, %p42;
	selp.b32	%r7059, %r2677, %r2681, %p42;
	selp.b32	%r7060, %r2673, %r2677, %p42;
	selp.b32	%r7061, 0, %r2673, %p42;
	mov.u32 	%r7031, %r7030;
	mov.u32 	%r7032, %r7030;
	mov.u32 	%r7033, %r7030;
	mov.u32 	%r7034, %r7030;
	mov.u32 	%r7035, %r7030;
	mov.u32 	%r7036, %r7030;
	mov.u32 	%r7037, %r7030;
	mov.u32 	%r7038, %r7030;
	mov.u32 	%r7039, %r7030;
	mov.u32 	%r7040, %r7030;
	mov.u32 	%r7041, %r7030;

BB1_51:
	mov.u32 	%r1633, %r7030;
	mov.u32 	%r1637, %r7030;
	mov.u32 	%r1641, %r7030;
	bra.uni 	BB1_57;

BB1_80:
	setp.eq.s32	%p55, %r129, 11;
	@%p55 bra 	BB1_96;
	bra.uni 	BB1_81;

BB1_96:
	// inline asm
	prmt.b32 %r1581, %r1629, %r1625, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1585, %r1633, %r1629, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1589, %r1637, %r1633, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1593, %r1641, %r1637, %r438;
	// inline asm
	mov.u32 	%r1629, 0;
	// inline asm
	prmt.b32 %r1597, %r1629, %r1641, %r438;
	// inline asm
	mov.u32 	%r1633, %r1629;
	mov.u32 	%r1637, %r1629;
	mov.u32 	%r7065, %r1629;
	mov.u32 	%r1613, %r1629;
	mov.u32 	%r1617, %r1629;
	mov.u32 	%r1621, %r1629;
	mov.u32 	%r1625, %r1629;

BB1_94:
	mov.u32 	%r1601, %r1629;

BB1_95:
	mov.u32 	%r1605, %r1629;
	mov.u32 	%r1609, %r1629;
	bra.uni 	BB1_110;

BB1_33:
	setp.eq.s32	%p16, %r129, 11;
	@%p16 bra 	BB1_34;
	bra.uni 	BB1_41;

BB1_34:
	and.b32  	%r2084, %r111, 3;
	shl.b32 	%r2068, %r2084, 3;
	mov.u32 	%r7038, 0;
	// inline asm
	shf.r.wrap.b32 %r2001, %r1581, %r7038, %r2068;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2005, %r1585, %r1581, %r2068;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2009, %r1589, %r1585, %r2068;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2013, %r1593, %r1589, %r2068;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2017, %r1597, %r1593, %r2068;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2021, %r1601, %r1597, %r2068;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2025, %r1605, %r1601, %r2068;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2029, %r1609, %r1605, %r2068;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2033, %r1613, %r1609, %r2068;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2037, %r1617, %r1613, %r2068;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2041, %r1621, %r1617, %r2068;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2045, %r1625, %r1621, %r2068;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2049, %r1629, %r1625, %r2068;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2053, %r1633, %r1629, %r2068;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2057, %r1637, %r1633, %r2068;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2061, %r1641, %r1637, %r2068;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2065, %r7038, %r1641, %r2068;
	// inline asm
	setp.eq.s32	%p34, %r110, 0;
	selp.b32	%r7030, %r2013, %r2017, %p34;
	selp.b32	%r7031, %r2017, %r2021, %p34;
	selp.b32	%r7032, %r2021, %r2025, %p34;
	selp.b32	%r7033, %r2025, %r2029, %p34;
	selp.b32	%r7034, 0, %r2001, %p34;
	selp.b32	%r7035, %r2001, %r2005, %p34;
	selp.b32	%r7036, %r2005, %r2009, %p34;
	selp.b32	%r7037, %r2009, %r2013, %p34;
	selp.b32	%r1597, %r2061, %r2065, %p34;
	selp.b32	%r1581, %r2045, %r2049, %p34;
	selp.b32	%r1585, %r2049, %r2053, %p34;
	selp.b32	%r1589, %r2053, %r2057, %p34;
	selp.b32	%r1593, %r2057, %r2061, %p34;
	selp.b32	%r7058, %r2041, %r2045, %p34;
	selp.b32	%r7059, %r2037, %r2041, %p34;
	selp.b32	%r7060, %r2033, %r2037, %p34;
	selp.b32	%r7061, %r2029, %r2033, %p34;
	mov.u32 	%r7039, %r7038;
	mov.u32 	%r7040, %r7038;
	mov.u32 	%r7041, %r7038;
	mov.u32 	%r7042, %r7038;
	mov.u32 	%r1633, %r7038;
	mov.u32 	%r1637, %r7038;
	mov.u32 	%r1641, %r7038;
	mov.u32 	%r1613, %r7038;
	mov.u32 	%r1617, %r7038;
	mov.u32 	%r1621, %r7038;
	mov.u32 	%r1625, %r7038;

BB1_45:
	mov.u32 	%r1601, %r7038;
	mov.u32 	%r1605, %r7038;
	mov.u32 	%r1609, %r7038;
	bra.uni 	BB1_57;

BB1_72:
	setp.eq.s32	%p61, %r129, 7;
	@%p61 bra 	BB1_102;
	bra.uni 	BB1_73;

BB1_102:
	// inline asm
	prmt.b32 %r1581, %r1613, %r1609, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1585, %r1617, %r1613, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1589, %r1621, %r1617, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1593, %r1625, %r1621, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1597, %r1629, %r1625, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1601, %r1633, %r1629, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1605, %r1637, %r1633, %r438;
	// inline asm
	// inline asm
	prmt.b32 %r1609, %r1641, %r1637, %r438;
	// inline asm
	mov.u32 	%r1629, 0;
	// inline asm
	prmt.b32 %r1613, %r1629, %r1641, %r438;
	// inline asm
	mov.u32 	%r1633, %r1629;
	mov.u32 	%r1637, %r1629;
	mov.u32 	%r7065, %r1629;

BB1_100:
	mov.u32 	%r1617, %r1629;

BB1_101:
	mov.u32 	%r1621, %r1629;
	mov.u32 	%r1625, %r1629;
	bra.uni 	BB1_110;

BB1_25:
	setp.eq.s32	%p22, %r129, 7;
	@%p22 bra 	BB1_26;
	bra.uni 	BB1_41;

BB1_26:
	and.b32  	%r2420, %r111, 3;
	shl.b32 	%r2404, %r2420, 3;
	mov.u32 	%r7034, 0;
	// inline asm
	shf.r.wrap.b32 %r2337, %r1581, %r7034, %r2404;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2341, %r1585, %r1581, %r2404;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2345, %r1589, %r1585, %r2404;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2349, %r1593, %r1589, %r2404;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2353, %r1597, %r1593, %r2404;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2357, %r1601, %r1597, %r2404;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2361, %r1605, %r1601, %r2404;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2365, %r1609, %r1605, %r2404;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2369, %r1613, %r1609, %r2404;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2373, %r1617, %r1613, %r2404;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2377, %r1621, %r1617, %r2404;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2381, %r1625, %r1621, %r2404;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2385, %r1629, %r1625, %r2404;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2389, %r1633, %r1629, %r2404;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2393, %r1637, %r1633, %r2404;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2397, %r1641, %r1637, %r2404;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2401, %r7034, %r1641, %r2404;
	// inline asm
	setp.eq.s32	%p38, %r110, 0;
	selp.b32	%r7030, 0, %r2337, %p38;
	selp.b32	%r7031, %r2337, %r2341, %p38;
	selp.b32	%r7032, %r2341, %r2345, %p38;
	selp.b32	%r7033, %r2345, %r2349, %p38;
	selp.b32	%r1613, %r2397, %r2401, %p38;
	selp.b32	%r1597, %r2381, %r2385, %p38;
	selp.b32	%r1601, %r2385, %r2389, %p38;
	selp.b32	%r1605, %r2389, %r2393, %p38;
	selp.b32	%r1609, %r2393, %r2397, %p38;
	selp.b32	%r1581, %r2365, %r2369, %p38;
	selp.b32	%r1585, %r2369, %r2373, %p38;
	selp.b32	%r1589, %r2373, %r2377, %p38;
	selp.b32	%r1593, %r2377, %r2381, %p38;
	selp.b32	%r7058, %r2361, %r2365, %p38;
	selp.b32	%r7059, %r2357, %r2361, %p38;
	selp.b32	%r7060, %r2353, %r2357, %p38;
	selp.b32	%r7061, %r2349, %r2353, %p38;
	mov.u32 	%r7035, %r7034;
	mov.u32 	%r7036, %r7034;
	mov.u32 	%r7037, %r7034;
	mov.u32 	%r7038, %r7034;
	mov.u32 	%r7039, %r7034;
	mov.u32 	%r7040, %r7034;
	mov.u32 	%r7041, %r7034;
	mov.u32 	%r7042, %r7034;
	mov.u32 	%r1633, %r7034;
	mov.u32 	%r1637, %r7034;
	mov.u32 	%r1641, %r7034;

BB1_48:
	mov.u32 	%r1617, %r7034;
	mov.u32 	%r1621, %r7034;
	mov.u32 	%r1625, %r7034;
	bra.uni 	BB1_57;

BB1_87:
	setp.ne.s32	%p50, %r129, 15;
	@%p50 bra 	BB1_88;

	mov.u32 	%r1629, 0;
	// inline asm
	prmt.b32 %r1581, %r1629, %r1641, %r438;
	// inline asm
	mov.u32 	%r1633, %r1629;
	mov.u32 	%r1637, %r1629;
	mov.u32 	%r7065, %r1629;
	mov.u32 	%r1613, %r1629;
	mov.u32 	%r1617, %r1629;
	mov.u32 	%r1621, %r1629;
	mov.u32 	%r1625, %r1629;
	mov.u32 	%r1597, %r1629;
	mov.u32 	%r1601, %r1629;
	mov.u32 	%r1605, %r1629;
	mov.u32 	%r1609, %r1629;
	mov.u32 	%r1585, %r1629;

BB1_90:
	mov.u32 	%r1589, %r1629;
	mov.u32 	%r1593, %r1629;
	bra.uni 	BB1_110;

BB1_40:
	setp.ne.s32	%p11, %r129, 15;
	@%p11 bra 	BB1_41;

	and.b32  	%r1748, %r111, 3;
	shl.b32 	%r1732, %r1748, 3;
	mov.u32 	%r7042, 0;
	// inline asm
	shf.r.wrap.b32 %r1665, %r1581, %r7042, %r1732;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1669, %r1585, %r1581, %r1732;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1673, %r1589, %r1585, %r1732;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1677, %r1593, %r1589, %r1732;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1681, %r1597, %r1593, %r1732;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1685, %r1601, %r1597, %r1732;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1689, %r1605, %r1601, %r1732;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1693, %r1609, %r1605, %r1732;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1697, %r1613, %r1609, %r1732;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1701, %r1617, %r1613, %r1732;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1705, %r1621, %r1617, %r1732;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1709, %r1625, %r1621, %r1732;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1713, %r1629, %r1625, %r1732;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1717, %r1633, %r1629, %r1732;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1721, %r1637, %r1633, %r1732;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1725, %r1641, %r1637, %r1732;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1729, %r7042, %r1641, %r1732;
	// inline asm
	setp.eq.s32	%p30, %r110, 0;
	selp.b32	%r7030, %r1693, %r1697, %p30;
	selp.b32	%r7031, %r1697, %r1701, %p30;
	selp.b32	%r7032, %r1701, %r1705, %p30;
	selp.b32	%r7033, %r1705, %r1709, %p30;
	selp.b32	%r7034, %r1677, %r1681, %p30;
	selp.b32	%r7035, %r1681, %r1685, %p30;
	selp.b32	%r7036, %r1685, %r1689, %p30;
	selp.b32	%r7037, %r1689, %r1693, %p30;
	selp.b32	%r7038, 0, %r1665, %p30;
	selp.b32	%r7039, %r1665, %r1669, %p30;
	selp.b32	%r7040, %r1669, %r1673, %p30;
	selp.b32	%r7041, %r1673, %r1677, %p30;
	selp.b32	%r1581, %r1725, %r1729, %p30;
	selp.b32	%r7058, %r1721, %r1725, %p30;
	selp.b32	%r7059, %r1717, %r1721, %p30;
	selp.b32	%r7060, %r1713, %r1717, %p30;
	selp.b32	%r7061, %r1709, %r1713, %p30;
	mov.u32 	%r1633, %r7042;
	mov.u32 	%r1637, %r7042;
	mov.u32 	%r1641, %r7042;
	mov.u32 	%r1613, %r7042;
	mov.u32 	%r1617, %r7042;
	mov.u32 	%r1621, %r7042;
	mov.u32 	%r1625, %r7042;
	mov.u32 	%r1597, %r7042;
	mov.u32 	%r1601, %r7042;
	mov.u32 	%r1605, %r7042;
	mov.u32 	%r1609, %r7042;
	mov.u32 	%r1585, %r7042;
	mov.u32 	%r1589, %r7042;
	mov.u32 	%r1593, %r7042;
	bra.uni 	BB1_57;

BB1_41:
	mov.u32 	%r7031, %r7030;
	mov.u32 	%r7032, %r7030;
	mov.u32 	%r7033, %r7030;
	mov.u32 	%r7034, %r7030;
	mov.u32 	%r7035, %r7030;
	mov.u32 	%r7036, %r7030;
	mov.u32 	%r7037, %r7030;
	mov.u32 	%r7038, %r7030;
	mov.u32 	%r7039, %r7030;
	mov.u32 	%r7040, %r7030;
	mov.u32 	%r7041, %r7030;
	mov.u32 	%r7042, %r1629;
	mov.u32 	%r7058, %r7030;

BB1_54:
	mov.u32 	%r7059, %r7030;

BB1_55:
	mov.u32 	%r7060, %r7030;

BB1_56:
	mov.u32 	%r7061, %r7030;

BB1_57:
	xor.b32  	%r3009, %r97, %r96;
	and.b32  	%r3010, %r3009, %r98;
	xor.b32  	%r3011, %r3010, %r96;
	add.s32 	%r3012, %r3011, %r99;
	or.b32  	%r3013, %r1641, %r95;
	add.s32 	%r3014, %r3012, %r3013;
	shf.l.wrap.b32 	%r3015, %r3014, %r3014, 3;
	xor.b32  	%r3016, %r98, %r97;
	and.b32  	%r3017, %r3015, %r3016;
	xor.b32  	%r3018, %r3017, %r97;
	or.b32  	%r3019, %r1637, %r94;
	add.s32 	%r3020, %r3019, %r96;
	add.s32 	%r3021, %r3020, %r3018;
	shf.l.wrap.b32 	%r3022, %r3021, %r3021, 7;
	xor.b32  	%r3023, %r3015, %r98;
	and.b32  	%r3024, %r3023, %r3022;
	xor.b32  	%r3025, %r3024, %r98;
	or.b32  	%r3026, %r1633, %r93;
	add.s32 	%r3027, %r3026, %r97;
	add.s32 	%r3028, %r3027, %r3025;
	shf.l.wrap.b32 	%r3029, %r3028, %r3028, 11;
	xor.b32  	%r3030, %r3022, %r3015;
	and.b32  	%r3031, %r3030, %r3029;
	xor.b32  	%r3032, %r3031, %r3015;
	or.b32  	%r3033, %r7042, %r92;
	add.s32 	%r3034, %r3033, %r98;
	add.s32 	%r3035, %r3034, %r3032;
	shf.l.wrap.b32 	%r3036, %r3035, %r3035, 19;
	xor.b32  	%r3037, %r3029, %r3022;
	and.b32  	%r3038, %r3037, %r3036;
	xor.b32  	%r3039, %r3038, %r3022;
	or.b32  	%r3040, %r1625, %r91;
	add.s32 	%r3041, %r3015, %r3040;
	add.s32 	%r3042, %r3041, %r3039;
	shf.l.wrap.b32 	%r3043, %r3042, %r3042, 3;
	xor.b32  	%r3044, %r3036, %r3029;
	and.b32  	%r3045, %r3044, %r3043;
	xor.b32  	%r3046, %r3045, %r3029;
	or.b32  	%r3047, %r1621, %r90;
	add.s32 	%r3048, %r3022, %r3047;
	add.s32 	%r3049, %r3048, %r3046;
	shf.l.wrap.b32 	%r3050, %r3049, %r3049, 7;
	xor.b32  	%r3051, %r3043, %r3036;
	and.b32  	%r3052, %r3051, %r3050;
	xor.b32  	%r3053, %r3052, %r3036;
	or.b32  	%r3054, %r1617, %r89;
	add.s32 	%r3055, %r3029, %r3054;
	add.s32 	%r3056, %r3055, %r3053;
	shf.l.wrap.b32 	%r3057, %r3056, %r3056, 11;
	xor.b32  	%r3058, %r3050, %r3043;
	and.b32  	%r3059, %r3058, %r3057;
	xor.b32  	%r3060, %r3059, %r3043;
	or.b32  	%r3061, %r1613, %r88;
	add.s32 	%r3062, %r3036, %r3061;
	add.s32 	%r3063, %r3062, %r3060;
	shf.l.wrap.b32 	%r3064, %r3063, %r3063, 19;
	xor.b32  	%r3065, %r3057, %r3050;
	and.b32  	%r3066, %r3065, %r3064;
	xor.b32  	%r3067, %r3066, %r3050;
	or.b32  	%r3068, %r1609, %r87;
	add.s32 	%r3069, %r3043, %r3068;
	add.s32 	%r3070, %r3069, %r3067;
	shf.l.wrap.b32 	%r3071, %r3070, %r3070, 3;
	xor.b32  	%r3072, %r3064, %r3057;
	and.b32  	%r3073, %r3072, %r3071;
	xor.b32  	%r3074, %r3073, %r3057;
	or.b32  	%r3075, %r1605, %r86;
	add.s32 	%r3076, %r3050, %r3075;
	add.s32 	%r3077, %r3076, %r3074;
	shf.l.wrap.b32 	%r3078, %r3077, %r3077, 7;
	xor.b32  	%r3079, %r3071, %r3064;
	and.b32  	%r3080, %r3079, %r3078;
	xor.b32  	%r3081, %r3080, %r3064;
	or.b32  	%r3082, %r1601, %r85;
	add.s32 	%r3083, %r3057, %r3082;
	add.s32 	%r3084, %r3083, %r3081;
	shf.l.wrap.b32 	%r3085, %r3084, %r3084, 11;
	xor.b32  	%r3086, %r3078, %r3071;
	and.b32  	%r3087, %r3086, %r3085;
	xor.b32  	%r3088, %r3087, %r3071;
	or.b32  	%r3089, %r1597, %r84;
	add.s32 	%r3090, %r3064, %r3089;
	add.s32 	%r3091, %r3090, %r3088;
	shf.l.wrap.b32 	%r3092, %r3091, %r3091, 19;
	xor.b32  	%r3093, %r3085, %r3078;
	and.b32  	%r3094, %r3093, %r3092;
	xor.b32  	%r3095, %r3094, %r3078;
	or.b32  	%r3096, %r1593, %r83;
	add.s32 	%r3097, %r3071, %r3096;
	add.s32 	%r3098, %r3097, %r3095;
	shf.l.wrap.b32 	%r3099, %r3098, %r3098, 3;
	xor.b32  	%r3100, %r3092, %r3085;
	and.b32  	%r3101, %r3100, %r3099;
	xor.b32  	%r3102, %r3101, %r3085;
	or.b32  	%r3103, %r1589, %r82;
	add.s32 	%r3104, %r3078, %r3103;
	add.s32 	%r3105, %r3104, %r3102;
	shf.l.wrap.b32 	%r3106, %r3105, %r3105, 7;
	xor.b32  	%r3107, %r3099, %r3092;
	and.b32  	%r3108, %r3107, %r3106;
	xor.b32  	%r3109, %r3108, %r3092;
	or.b32  	%r3110, %r1585, %r81;
	add.s32 	%r3111, %r3085, %r3110;
	add.s32 	%r3112, %r3111, %r3109;
	shf.l.wrap.b32 	%r3113, %r3112, %r3112, 11;
	xor.b32  	%r3114, %r3106, %r3099;
	and.b32  	%r3115, %r3114, %r3113;
	xor.b32  	%r3116, %r3115, %r3099;
	or.b32  	%r3117, %r1581, %r80;
	add.s32 	%r3118, %r3092, %r3117;
	add.s32 	%r3119, %r3118, %r3116;
	shf.l.wrap.b32 	%r3120, %r3119, %r3119, 19;
	xor.b32  	%r3121, %r3120, %r3106;
	xor.b32  	%r3122, %r3120, %r3113;
	and.b32  	%r3123, %r3122, %r3121;
	xor.b32  	%r3124, %r3123, %r3120;
	add.s32 	%r3125, %r3013, %r3099;
	add.s32 	%r3126, %r3125, %r3124;
	add.s32 	%r3127, %r3126, 1518500249;
	shf.l.wrap.b32 	%r3128, %r3127, %r3127, 3;
	xor.b32  	%r3129, %r3128, %r3113;
	xor.b32  	%r3130, %r3128, %r3120;
	and.b32  	%r3131, %r3130, %r3129;
	xor.b32  	%r3132, %r3131, %r3128;
	add.s32 	%r3133, %r3040, %r3106;
	add.s32 	%r3134, %r3133, %r3132;
	add.s32 	%r3135, %r3134, 1518500249;
	shf.l.wrap.b32 	%r3136, %r3135, %r3135, 5;
	xor.b32  	%r3137, %r3136, %r3120;
	xor.b32  	%r3138, %r3136, %r3128;
	and.b32  	%r3139, %r3138, %r3137;
	xor.b32  	%r3140, %r3139, %r3136;
	add.s32 	%r3141, %r3068, %r3113;
	add.s32 	%r3142, %r3141, %r3140;
	add.s32 	%r3143, %r3142, 1518500249;
	shf.l.wrap.b32 	%r3144, %r3143, %r3143, 9;
	xor.b32  	%r3145, %r3144, %r3128;
	xor.b32  	%r3146, %r3144, %r3136;
	and.b32  	%r3147, %r3146, %r3145;
	xor.b32  	%r3148, %r3147, %r3144;
	add.s32 	%r3149, %r3096, %r3120;
	add.s32 	%r3150, %r3149, %r3148;
	add.s32 	%r3151, %r3150, 1518500249;
	shf.l.wrap.b32 	%r3152, %r3151, %r3151, 13;
	xor.b32  	%r3153, %r3152, %r3136;
	xor.b32  	%r3154, %r3152, %r3144;
	and.b32  	%r3155, %r3154, %r3153;
	xor.b32  	%r3156, %r3155, %r3152;
	add.s32 	%r3157, %r3019, %r3128;
	add.s32 	%r3158, %r3157, %r3156;
	add.s32 	%r3159, %r3158, 1518500249;
	shf.l.wrap.b32 	%r3160, %r3159, %r3159, 3;
	xor.b32  	%r3161, %r3160, %r3144;
	xor.b32  	%r3162, %r3160, %r3152;
	and.b32  	%r3163, %r3162, %r3161;
	xor.b32  	%r3164, %r3163, %r3160;
	add.s32 	%r3165, %r3047, %r3136;
	add.s32 	%r3166, %r3165, %r3164;
	add.s32 	%r3167, %r3166, 1518500249;
	shf.l.wrap.b32 	%r3168, %r3167, %r3167, 5;
	xor.b32  	%r3169, %r3168, %r3152;
	xor.b32  	%r3170, %r3168, %r3160;
	and.b32  	%r3171, %r3170, %r3169;
	xor.b32  	%r3172, %r3171, %r3168;
	add.s32 	%r3173, %r3075, %r3144;
	add.s32 	%r3174, %r3173, %r3172;
	add.s32 	%r3175, %r3174, 1518500249;
	shf.l.wrap.b32 	%r3176, %r3175, %r3175, 9;
	xor.b32  	%r3177, %r3176, %r3160;
	xor.b32  	%r3178, %r3176, %r3168;
	and.b32  	%r3179, %r3178, %r3177;
	xor.b32  	%r3180, %r3179, %r3176;
	add.s32 	%r3181, %r3103, %r3152;
	add.s32 	%r3182, %r3181, %r3180;
	add.s32 	%r3183, %r3182, 1518500249;
	shf.l.wrap.b32 	%r3184, %r3183, %r3183, 13;
	xor.b32  	%r3185, %r3184, %r3168;
	xor.b32  	%r3186, %r3184, %r3176;
	and.b32  	%r3187, %r3186, %r3185;
	xor.b32  	%r3188, %r3187, %r3184;
	add.s32 	%r3189, %r3026, %r3160;
	add.s32 	%r3190, %r3189, %r3188;
	add.s32 	%r3191, %r3190, 1518500249;
	shf.l.wrap.b32 	%r3192, %r3191, %r3191, 3;
	xor.b32  	%r3193, %r3192, %r3176;
	xor.b32  	%r3194, %r3192, %r3184;
	and.b32  	%r3195, %r3194, %r3193;
	xor.b32  	%r3196, %r3195, %r3192;
	add.s32 	%r3197, %r3054, %r3168;
	add.s32 	%r3198, %r3197, %r3196;
	add.s32 	%r3199, %r3198, 1518500249;
	shf.l.wrap.b32 	%r3200, %r3199, %r3199, 5;
	xor.b32  	%r3201, %r3200, %r3184;
	xor.b32  	%r3202, %r3200, %r3192;
	and.b32  	%r3203, %r3202, %r3201;
	xor.b32  	%r3204, %r3203, %r3200;
	add.s32 	%r3205, %r3082, %r3176;
	add.s32 	%r3206, %r3205, %r3204;
	add.s32 	%r3207, %r3206, 1518500249;
	shf.l.wrap.b32 	%r3208, %r3207, %r3207, 9;
	xor.b32  	%r3209, %r3208, %r3192;
	xor.b32  	%r3210, %r3208, %r3200;
	and.b32  	%r3211, %r3210, %r3209;
	xor.b32  	%r3212, %r3211, %r3208;
	add.s32 	%r3213, %r3110, %r3184;
	add.s32 	%r3214, %r3213, %r3212;
	add.s32 	%r3215, %r3214, 1518500249;
	shf.l.wrap.b32 	%r3216, %r3215, %r3215, 13;
	xor.b32  	%r3217, %r3216, %r3200;
	xor.b32  	%r3218, %r3216, %r3208;
	and.b32  	%r3219, %r3218, %r3217;
	xor.b32  	%r3220, %r3219, %r3216;
	add.s32 	%r3221, %r3033, %r3192;
	add.s32 	%r3222, %r3221, %r3220;
	add.s32 	%r3223, %r3222, 1518500249;
	shf.l.wrap.b32 	%r3224, %r3223, %r3223, 3;
	xor.b32  	%r3225, %r3224, %r3208;
	xor.b32  	%r3226, %r3224, %r3216;
	and.b32  	%r3227, %r3226, %r3225;
	xor.b32  	%r3228, %r3227, %r3224;
	add.s32 	%r3229, %r3061, %r3200;
	add.s32 	%r3230, %r3229, %r3228;
	add.s32 	%r3231, %r3230, 1518500249;
	shf.l.wrap.b32 	%r3232, %r3231, %r3231, 5;
	xor.b32  	%r3233, %r3232, %r3216;
	xor.b32  	%r3234, %r3232, %r3224;
	and.b32  	%r3235, %r3234, %r3233;
	xor.b32  	%r3236, %r3235, %r3232;
	add.s32 	%r3237, %r3089, %r3208;
	add.s32 	%r3238, %r3237, %r3236;
	add.s32 	%r3239, %r3238, 1518500249;
	shf.l.wrap.b32 	%r3240, %r3239, %r3239, 9;
	xor.b32  	%r3241, %r3240, %r3224;
	xor.b32  	%r3242, %r3240, %r3232;
	and.b32  	%r3243, %r3242, %r3241;
	xor.b32  	%r3244, %r3243, %r3240;
	add.s32 	%r3245, %r3117, %r3216;
	add.s32 	%r3246, %r3245, %r3244;
	add.s32 	%r3247, %r3246, 1518500249;
	shf.l.wrap.b32 	%r3248, %r3247, %r3247, 13;
	xor.b32  	%r3249, %r3242, %r3248;
	add.s32 	%r3250, %r3013, %r3224;
	add.s32 	%r3251, %r3250, %r3249;
	add.s32 	%r3252, %r3251, 1859775393;
	shf.l.wrap.b32 	%r3253, %r3252, %r3252, 3;
	xor.b32  	%r3254, %r3248, %r3240;
	xor.b32  	%r3255, %r3254, %r3253;
	add.s32 	%r3256, %r3068, %r3232;
	add.s32 	%r3257, %r3256, %r3255;
	add.s32 	%r3258, %r3257, 1859775393;
	shf.l.wrap.b32 	%r3259, %r3258, %r3258, 9;
	xor.b32  	%r3260, %r3253, %r3248;
	xor.b32  	%r3261, %r3260, %r3259;
	add.s32 	%r3262, %r3040, %r3240;
	add.s32 	%r3263, %r3262, %r3261;
	add.s32 	%r3264, %r3263, 1859775393;
	shf.l.wrap.b32 	%r3265, %r3264, %r3264, 11;
	xor.b32  	%r3266, %r3259, %r3253;
	xor.b32  	%r3267, %r3266, %r3265;
	add.s32 	%r3268, %r3096, %r3248;
	add.s32 	%r3269, %r3268, %r3267;
	add.s32 	%r3270, %r3269, 1859775393;
	shf.l.wrap.b32 	%r3271, %r3270, %r3270, 15;
	xor.b32  	%r3272, %r3265, %r3259;
	xor.b32  	%r3273, %r3272, %r3271;
	add.s32 	%r3274, %r3026, %r3253;
	add.s32 	%r3275, %r3274, %r3273;
	add.s32 	%r3276, %r3275, 1859775393;
	shf.l.wrap.b32 	%r3277, %r3276, %r3276, 3;
	xor.b32  	%r3278, %r3271, %r3265;
	xor.b32  	%r3279, %r3278, %r3277;
	add.s32 	%r3280, %r3082, %r3259;
	add.s32 	%r3281, %r3280, %r3279;
	add.s32 	%r3282, %r3281, 1859775393;
	shf.l.wrap.b32 	%r3283, %r3282, %r3282, 9;
	xor.b32  	%r3284, %r3277, %r3271;
	xor.b32  	%r3285, %r3284, %r3283;
	add.s32 	%r3286, %r3054, %r3265;
	add.s32 	%r3287, %r3286, %r3285;
	add.s32 	%r3288, %r3287, 1859775393;
	shf.l.wrap.b32 	%r3289, %r3288, %r3288, 11;
	xor.b32  	%r3290, %r3283, %r3277;
	xor.b32  	%r3291, %r3290, %r3289;
	add.s32 	%r3292, %r3110, %r3271;
	add.s32 	%r3293, %r3292, %r3291;
	add.s32 	%r3294, %r3293, 1859775393;
	shf.l.wrap.b32 	%r3295, %r3294, %r3294, 15;
	xor.b32  	%r3296, %r3289, %r3283;
	xor.b32  	%r3297, %r3296, %r3295;
	add.s32 	%r3298, %r3019, %r3277;
	add.s32 	%r3299, %r3298, %r3297;
	add.s32 	%r3300, %r3299, 1859775393;
	shf.l.wrap.b32 	%r3301, %r3300, %r3300, 3;
	xor.b32  	%r3302, %r3295, %r3289;
	xor.b32  	%r3303, %r3302, %r3301;
	add.s32 	%r3304, %r3075, %r3283;
	add.s32 	%r3305, %r3304, %r3303;
	add.s32 	%r3306, %r3305, 1859775393;
	shf.l.wrap.b32 	%r3307, %r3306, %r3306, 9;
	xor.b32  	%r3308, %r3301, %r3295;
	xor.b32  	%r3309, %r3308, %r3307;
	add.s32 	%r3310, %r3047, %r3289;
	add.s32 	%r3311, %r3310, %r3309;
	add.s32 	%r3312, %r3311, 1859775393;
	shf.l.wrap.b32 	%r3313, %r3312, %r3312, 11;
	xor.b32  	%r3314, %r3307, %r3301;
	xor.b32  	%r3315, %r3314, %r3313;
	add.s32 	%r3316, %r3103, %r3295;
	add.s32 	%r3317, %r3316, %r3315;
	add.s32 	%r3318, %r3317, 1859775393;
	shf.l.wrap.b32 	%r3319, %r3318, %r3318, 15;
	xor.b32  	%r3320, %r3313, %r3307;
	xor.b32  	%r3321, %r3320, %r3319;
	add.s32 	%r3322, %r3033, %r3301;
	add.s32 	%r3323, %r3322, %r3321;
	add.s32 	%r3324, %r3323, 1859775393;
	shf.l.wrap.b32 	%r3325, %r3324, %r3324, 3;
	xor.b32  	%r3326, %r3319, %r3313;
	xor.b32  	%r3327, %r3326, %r3325;
	add.s32 	%r3328, %r3089, %r3307;
	add.s32 	%r3329, %r3328, %r3327;
	add.s32 	%r3330, %r3329, 1859775393;
	shf.l.wrap.b32 	%r3331, %r3330, %r3330, 9;
	xor.b32  	%r3332, %r3325, %r3319;
	xor.b32  	%r3333, %r3332, %r3331;
	add.s32 	%r3334, %r3061, %r3313;
	add.s32 	%r3335, %r3334, %r3333;
	add.s32 	%r3336, %r3335, 1859775393;
	shf.l.wrap.b32 	%r3337, %r3336, %r3336, 11;
	xor.b32  	%r3338, %r3331, %r3325;
	xor.b32  	%r3339, %r3338, %r3337;
	add.s32 	%r3340, %r3117, %r3319;
	add.s32 	%r3341, %r3340, %r3339;
	add.s32 	%r3342, %r3341, 1859775393;
	shf.l.wrap.b32 	%r3343, %r3342, %r3342, 15;
	add.s32 	%r99, %r3325, %r99;
	add.s32 	%r98, %r3343, %r98;
	add.s32 	%r97, %r3337, %r97;
	add.s32 	%r96, %r3331, %r96;
	bra.uni 	BB1_111;

BB1_63:
	mov.u32 	%r7065, %r1641;
	bra.uni 	BB1_110;

BB1_78:
	mov.u32 	%r7065, %r1641;
	bra.uni 	BB1_110;

BB1_70:
	mov.u32 	%r7065, %r1641;
	bra.uni 	BB1_110;

BB1_85:
	mov.u32 	%r7065, %r1641;
	bra.uni 	BB1_110;

BB1_66:
	mov.u32 	%r7065, %r1641;
	bra.uni 	BB1_110;

BB1_81:
	mov.u32 	%r7065, %r1641;
	bra.uni 	BB1_110;

BB1_73:
	mov.u32 	%r7065, %r1641;
	bra.uni 	BB1_110;

BB1_88:
	mov.u32 	%r7065, %r1641;

BB1_110:
	or.b32  	%r7058, %r7065, %r95;
	or.b32  	%r7059, %r1637, %r94;
	or.b32  	%r7060, %r1633, %r93;
	or.b32  	%r7061, %r1629, %r92;
	or.b32  	%r7033, %r1625, %r91;
	or.b32  	%r7032, %r1621, %r90;
	or.b32  	%r7031, %r1617, %r89;
	or.b32  	%r7030, %r1613, %r88;
	or.b32  	%r7037, %r1609, %r87;
	or.b32  	%r7036, %r1605, %r86;
	or.b32  	%r7035, %r1601, %r85;
	or.b32  	%r7034, %r1597, %r84;
	or.b32  	%r7041, %r1593, %r83;
	or.b32  	%r7040, %r1589, %r82;
	or.b32  	%r7039, %r1585, %r81;
	or.b32  	%r7038, %r1581, %r80;

BB1_111:
	and.b32  	%r4011, %r128, 63;
	mul.wide.u32 	%rd32, %r4011, 64;
	mov.u64 	%rd33, c_append_helper;
	add.s64 	%rd34, %rd33, %rd32;
	ld.const.u32 	%r4012, [%rd34];
	and.b32  	%r4013, %r4012, -2139062144;
	or.b32  	%r7111, %r4013, %r7058;
	ld.const.u32 	%r4014, [%rd34+4];
	and.b32  	%r4015, %r4014, -2139062144;
	or.b32  	%r7110, %r4015, %r7059;
	ld.const.u32 	%r4016, [%rd34+8];
	and.b32  	%r4017, %r4016, -2139062144;
	or.b32  	%r7109, %r4017, %r7060;
	ld.const.u32 	%r4018, [%rd34+12];
	and.b32  	%r4019, %r4018, -2139062144;
	or.b32  	%r7108, %r4019, %r7061;
	ld.const.u32 	%r4020, [%rd34+16];
	and.b32  	%r4021, %r4020, -2139062144;
	or.b32  	%r7107, %r4021, %r7033;
	ld.const.u32 	%r4022, [%rd34+20];
	and.b32  	%r4023, %r4022, -2139062144;
	or.b32  	%r7106, %r4023, %r7032;
	ld.const.u32 	%r4024, [%rd34+24];
	and.b32  	%r4025, %r4024, -2139062144;
	or.b32  	%r7105, %r4025, %r7031;
	ld.const.u32 	%r4026, [%rd34+28];
	and.b32  	%r4027, %r4026, -2139062144;
	or.b32  	%r7104, %r4027, %r7030;
	ld.const.u32 	%r4028, [%rd34+32];
	and.b32  	%r4029, %r4028, -2139062144;
	or.b32  	%r7103, %r4029, %r7037;
	ld.const.u32 	%r4030, [%rd34+36];
	and.b32  	%r4031, %r4030, -2139062144;
	or.b32  	%r7102, %r4031, %r7036;
	ld.const.u32 	%r4032, [%rd34+40];
	and.b32  	%r4033, %r4032, -2139062144;
	or.b32  	%r7101, %r4033, %r7035;
	ld.const.u32 	%r4034, [%rd34+44];
	and.b32  	%r4035, %r4034, -2139062144;
	or.b32  	%r7100, %r4035, %r7034;
	ld.const.u32 	%r4036, [%rd34+48];
	and.b32  	%r4037, %r4036, -2139062144;
	or.b32  	%r7099, %r4037, %r7041;
	ld.const.u32 	%r4038, [%rd34+52];
	and.b32  	%r4039, %r4038, -2139062144;
	or.b32  	%r7098, %r4039, %r7040;
	ld.const.u32 	%r4040, [%rd34+56];
	and.b32  	%r4041, %r4040, -2139062144;
	or.b32  	%r641, %r4041, %r7039;
	ld.const.u32 	%r4042, [%rd34+60];
	and.b32  	%r4043, %r4042, -2139062144;
	or.b32  	%r642, %r4043, %r7038;
	setp.lt.u32	%p69, %r4011, 56;
	@%p69 bra 	BB1_113;

	xor.b32  	%r4058, %r97, %r96;
	and.b32  	%r4059, %r4058, %r98;
	xor.b32  	%r4060, %r4059, %r96;
	add.s32 	%r4061, %r4060, %r99;
	add.s32 	%r4062, %r4061, %r7111;
	shf.l.wrap.b32 	%r4063, %r4062, %r4062, 3;
	xor.b32  	%r4064, %r98, %r97;
	and.b32  	%r4065, %r4063, %r4064;
	xor.b32  	%r4066, %r4065, %r97;
	add.s32 	%r4067, %r7110, %r96;
	add.s32 	%r4068, %r4067, %r4066;
	shf.l.wrap.b32 	%r4069, %r4068, %r4068, 7;
	xor.b32  	%r4070, %r4063, %r98;
	and.b32  	%r4071, %r4070, %r4069;
	xor.b32  	%r4072, %r4071, %r98;
	add.s32 	%r4073, %r7109, %r97;
	add.s32 	%r4074, %r4073, %r4072;
	shf.l.wrap.b32 	%r4075, %r4074, %r4074, 11;
	xor.b32  	%r4076, %r4069, %r4063;
	and.b32  	%r4077, %r4076, %r4075;
	xor.b32  	%r4078, %r4077, %r4063;
	add.s32 	%r4079, %r7108, %r98;
	add.s32 	%r4080, %r4079, %r4078;
	shf.l.wrap.b32 	%r4081, %r4080, %r4080, 19;
	xor.b32  	%r4082, %r4075, %r4069;
	and.b32  	%r4083, %r4082, %r4081;
	xor.b32  	%r4084, %r4083, %r4069;
	add.s32 	%r4085, %r4063, %r7107;
	add.s32 	%r4086, %r4085, %r4084;
	shf.l.wrap.b32 	%r4087, %r4086, %r4086, 3;
	xor.b32  	%r4088, %r4081, %r4075;
	and.b32  	%r4089, %r4088, %r4087;
	xor.b32  	%r4090, %r4089, %r4075;
	add.s32 	%r4091, %r4069, %r7106;
	add.s32 	%r4092, %r4091, %r4090;
	shf.l.wrap.b32 	%r4093, %r4092, %r4092, 7;
	xor.b32  	%r4094, %r4087, %r4081;
	and.b32  	%r4095, %r4094, %r4093;
	xor.b32  	%r4096, %r4095, %r4081;
	add.s32 	%r4097, %r4075, %r7105;
	add.s32 	%r4098, %r4097, %r4096;
	shf.l.wrap.b32 	%r4099, %r4098, %r4098, 11;
	xor.b32  	%r4100, %r4093, %r4087;
	and.b32  	%r4101, %r4100, %r4099;
	xor.b32  	%r4102, %r4101, %r4087;
	add.s32 	%r4103, %r4081, %r7104;
	add.s32 	%r4104, %r4103, %r4102;
	shf.l.wrap.b32 	%r4105, %r4104, %r4104, 19;
	xor.b32  	%r4106, %r4099, %r4093;
	and.b32  	%r4107, %r4106, %r4105;
	xor.b32  	%r4108, %r4107, %r4093;
	add.s32 	%r4109, %r4087, %r7103;
	add.s32 	%r4110, %r4109, %r4108;
	shf.l.wrap.b32 	%r4111, %r4110, %r4110, 3;
	xor.b32  	%r4112, %r4105, %r4099;
	and.b32  	%r4113, %r4112, %r4111;
	xor.b32  	%r4114, %r4113, %r4099;
	add.s32 	%r4115, %r4093, %r7102;
	add.s32 	%r4116, %r4115, %r4114;
	shf.l.wrap.b32 	%r4117, %r4116, %r4116, 7;
	xor.b32  	%r4118, %r4111, %r4105;
	and.b32  	%r4119, %r4118, %r4117;
	xor.b32  	%r4120, %r4119, %r4105;
	add.s32 	%r4121, %r4099, %r7101;
	add.s32 	%r4122, %r4121, %r4120;
	shf.l.wrap.b32 	%r4123, %r4122, %r4122, 11;
	xor.b32  	%r4124, %r4117, %r4111;
	and.b32  	%r4125, %r4124, %r4123;
	xor.b32  	%r4126, %r4125, %r4111;
	add.s32 	%r4127, %r4105, %r7100;
	add.s32 	%r4128, %r4127, %r4126;
	shf.l.wrap.b32 	%r4129, %r4128, %r4128, 19;
	xor.b32  	%r4130, %r4123, %r4117;
	and.b32  	%r4131, %r4130, %r4129;
	xor.b32  	%r4132, %r4131, %r4117;
	add.s32 	%r4133, %r4111, %r7099;
	add.s32 	%r4134, %r4133, %r4132;
	shf.l.wrap.b32 	%r4135, %r4134, %r4134, 3;
	xor.b32  	%r4136, %r4129, %r4123;
	and.b32  	%r4137, %r4136, %r4135;
	xor.b32  	%r4138, %r4137, %r4123;
	add.s32 	%r4139, %r4117, %r7098;
	add.s32 	%r4140, %r4139, %r4138;
	shf.l.wrap.b32 	%r4141, %r4140, %r4140, 7;
	xor.b32  	%r4142, %r4135, %r4129;
	and.b32  	%r4143, %r4142, %r4141;
	xor.b32  	%r4144, %r4143, %r4129;
	add.s32 	%r4145, %r4123, %r641;
	add.s32 	%r4146, %r4145, %r4144;
	shf.l.wrap.b32 	%r4147, %r4146, %r4146, 11;
	xor.b32  	%r4148, %r4141, %r4135;
	and.b32  	%r4149, %r4148, %r4147;
	xor.b32  	%r4150, %r4149, %r4135;
	add.s32 	%r4151, %r4129, %r642;
	add.s32 	%r4152, %r4151, %r4150;
	shf.l.wrap.b32 	%r4153, %r4152, %r4152, 19;
	xor.b32  	%r4154, %r4153, %r4141;
	xor.b32  	%r4155, %r4153, %r4147;
	and.b32  	%r4156, %r4155, %r4154;
	xor.b32  	%r4157, %r4156, %r4153;
	add.s32 	%r4158, %r7111, %r4135;
	add.s32 	%r4159, %r4158, %r4157;
	add.s32 	%r4160, %r4159, 1518500249;
	shf.l.wrap.b32 	%r4161, %r4160, %r4160, 3;
	xor.b32  	%r4162, %r4161, %r4147;
	xor.b32  	%r4163, %r4161, %r4153;
	and.b32  	%r4164, %r4163, %r4162;
	xor.b32  	%r4165, %r4164, %r4161;
	add.s32 	%r4166, %r7107, %r4141;
	add.s32 	%r4167, %r4166, %r4165;
	add.s32 	%r4168, %r4167, 1518500249;
	shf.l.wrap.b32 	%r4169, %r4168, %r4168, 5;
	xor.b32  	%r4170, %r4169, %r4153;
	xor.b32  	%r4171, %r4169, %r4161;
	and.b32  	%r4172, %r4171, %r4170;
	xor.b32  	%r4173, %r4172, %r4169;
	add.s32 	%r4174, %r7103, %r4147;
	add.s32 	%r4175, %r4174, %r4173;
	add.s32 	%r4176, %r4175, 1518500249;
	shf.l.wrap.b32 	%r4177, %r4176, %r4176, 9;
	xor.b32  	%r4178, %r4177, %r4161;
	xor.b32  	%r4179, %r4177, %r4169;
	and.b32  	%r4180, %r4179, %r4178;
	xor.b32  	%r4181, %r4180, %r4177;
	add.s32 	%r4182, %r7099, %r4153;
	add.s32 	%r4183, %r4182, %r4181;
	add.s32 	%r4184, %r4183, 1518500249;
	shf.l.wrap.b32 	%r4185, %r4184, %r4184, 13;
	xor.b32  	%r4186, %r4185, %r4169;
	xor.b32  	%r4187, %r4185, %r4177;
	and.b32  	%r4188, %r4187, %r4186;
	xor.b32  	%r4189, %r4188, %r4185;
	add.s32 	%r4190, %r7110, %r4161;
	add.s32 	%r4191, %r4190, %r4189;
	add.s32 	%r4192, %r4191, 1518500249;
	shf.l.wrap.b32 	%r4193, %r4192, %r4192, 3;
	xor.b32  	%r4194, %r4193, %r4177;
	xor.b32  	%r4195, %r4193, %r4185;
	and.b32  	%r4196, %r4195, %r4194;
	xor.b32  	%r4197, %r4196, %r4193;
	add.s32 	%r4198, %r7106, %r4169;
	add.s32 	%r4199, %r4198, %r4197;
	add.s32 	%r4200, %r4199, 1518500249;
	shf.l.wrap.b32 	%r4201, %r4200, %r4200, 5;
	xor.b32  	%r4202, %r4201, %r4185;
	xor.b32  	%r4203, %r4201, %r4193;
	and.b32  	%r4204, %r4203, %r4202;
	xor.b32  	%r4205, %r4204, %r4201;
	add.s32 	%r4206, %r7102, %r4177;
	add.s32 	%r4207, %r4206, %r4205;
	add.s32 	%r4208, %r4207, 1518500249;
	shf.l.wrap.b32 	%r4209, %r4208, %r4208, 9;
	xor.b32  	%r4210, %r4209, %r4193;
	xor.b32  	%r4211, %r4209, %r4201;
	and.b32  	%r4212, %r4211, %r4210;
	xor.b32  	%r4213, %r4212, %r4209;
	add.s32 	%r4214, %r7098, %r4185;
	add.s32 	%r4215, %r4214, %r4213;
	add.s32 	%r4216, %r4215, 1518500249;
	shf.l.wrap.b32 	%r4217, %r4216, %r4216, 13;
	xor.b32  	%r4218, %r4217, %r4201;
	xor.b32  	%r4219, %r4217, %r4209;
	and.b32  	%r4220, %r4219, %r4218;
	xor.b32  	%r4221, %r4220, %r4217;
	add.s32 	%r4222, %r7109, %r4193;
	add.s32 	%r4223, %r4222, %r4221;
	add.s32 	%r4224, %r4223, 1518500249;
	shf.l.wrap.b32 	%r4225, %r4224, %r4224, 3;
	xor.b32  	%r4226, %r4225, %r4209;
	xor.b32  	%r4227, %r4225, %r4217;
	and.b32  	%r4228, %r4227, %r4226;
	xor.b32  	%r4229, %r4228, %r4225;
	add.s32 	%r4230, %r7105, %r4201;
	add.s32 	%r4231, %r4230, %r4229;
	add.s32 	%r4232, %r4231, 1518500249;
	shf.l.wrap.b32 	%r4233, %r4232, %r4232, 5;
	xor.b32  	%r4234, %r4233, %r4217;
	xor.b32  	%r4235, %r4233, %r4225;
	and.b32  	%r4236, %r4235, %r4234;
	xor.b32  	%r4237, %r4236, %r4233;
	add.s32 	%r4238, %r7101, %r4209;
	add.s32 	%r4239, %r4238, %r4237;
	add.s32 	%r4240, %r4239, 1518500249;
	shf.l.wrap.b32 	%r4241, %r4240, %r4240, 9;
	xor.b32  	%r4242, %r4241, %r4225;
	xor.b32  	%r4243, %r4241, %r4233;
	and.b32  	%r4244, %r4243, %r4242;
	xor.b32  	%r4245, %r4244, %r4241;
	add.s32 	%r4246, %r641, %r4217;
	add.s32 	%r4247, %r4246, %r4245;
	add.s32 	%r4248, %r4247, 1518500249;
	shf.l.wrap.b32 	%r4249, %r4248, %r4248, 13;
	xor.b32  	%r4250, %r4249, %r4233;
	xor.b32  	%r4251, %r4249, %r4241;
	and.b32  	%r4252, %r4251, %r4250;
	xor.b32  	%r4253, %r4252, %r4249;
	add.s32 	%r4254, %r7108, %r4225;
	add.s32 	%r4255, %r4254, %r4253;
	add.s32 	%r4256, %r4255, 1518500249;
	shf.l.wrap.b32 	%r4257, %r4256, %r4256, 3;
	xor.b32  	%r4258, %r4257, %r4241;
	xor.b32  	%r4259, %r4257, %r4249;
	and.b32  	%r4260, %r4259, %r4258;
	xor.b32  	%r4261, %r4260, %r4257;
	add.s32 	%r4262, %r7104, %r4233;
	add.s32 	%r4263, %r4262, %r4261;
	add.s32 	%r4264, %r4263, 1518500249;
	shf.l.wrap.b32 	%r4265, %r4264, %r4264, 5;
	xor.b32  	%r4266, %r4265, %r4249;
	xor.b32  	%r4267, %r4265, %r4257;
	and.b32  	%r4268, %r4267, %r4266;
	xor.b32  	%r4269, %r4268, %r4265;
	add.s32 	%r4270, %r7100, %r4241;
	add.s32 	%r4271, %r4270, %r4269;
	add.s32 	%r4272, %r4271, 1518500249;
	shf.l.wrap.b32 	%r4273, %r4272, %r4272, 9;
	xor.b32  	%r4274, %r4273, %r4257;
	xor.b32  	%r4275, %r4273, %r4265;
	and.b32  	%r4276, %r4275, %r4274;
	xor.b32  	%r4277, %r4276, %r4273;
	add.s32 	%r4278, %r642, %r4249;
	add.s32 	%r4279, %r4278, %r4277;
	add.s32 	%r4280, %r4279, 1518500249;
	shf.l.wrap.b32 	%r4281, %r4280, %r4280, 13;
	xor.b32  	%r4282, %r4275, %r4281;
	add.s32 	%r4283, %r7111, %r4257;
	add.s32 	%r4284, %r4283, %r4282;
	add.s32 	%r4285, %r4284, 1859775393;
	shf.l.wrap.b32 	%r4286, %r4285, %r4285, 3;
	xor.b32  	%r4287, %r4281, %r4273;
	xor.b32  	%r4288, %r4287, %r4286;
	add.s32 	%r4289, %r7103, %r4265;
	add.s32 	%r4290, %r4289, %r4288;
	add.s32 	%r4291, %r4290, 1859775393;
	shf.l.wrap.b32 	%r4292, %r4291, %r4291, 9;
	xor.b32  	%r4293, %r4286, %r4281;
	xor.b32  	%r4294, %r4293, %r4292;
	add.s32 	%r4295, %r7107, %r4273;
	add.s32 	%r4296, %r4295, %r4294;
	add.s32 	%r4297, %r4296, 1859775393;
	shf.l.wrap.b32 	%r4298, %r4297, %r4297, 11;
	xor.b32  	%r4299, %r4292, %r4286;
	xor.b32  	%r4300, %r4299, %r4298;
	add.s32 	%r4301, %r7099, %r4281;
	add.s32 	%r4302, %r4301, %r4300;
	add.s32 	%r4303, %r4302, 1859775393;
	shf.l.wrap.b32 	%r4304, %r4303, %r4303, 15;
	xor.b32  	%r4305, %r4298, %r4292;
	xor.b32  	%r4306, %r4305, %r4304;
	add.s32 	%r4307, %r7109, %r4286;
	add.s32 	%r4308, %r4307, %r4306;
	add.s32 	%r4309, %r4308, 1859775393;
	shf.l.wrap.b32 	%r4310, %r4309, %r4309, 3;
	xor.b32  	%r4311, %r4304, %r4298;
	xor.b32  	%r4312, %r4311, %r4310;
	add.s32 	%r4313, %r7101, %r4292;
	add.s32 	%r4314, %r4313, %r4312;
	add.s32 	%r4315, %r4314, 1859775393;
	shf.l.wrap.b32 	%r4316, %r4315, %r4315, 9;
	xor.b32  	%r4317, %r4310, %r4304;
	xor.b32  	%r4318, %r4317, %r4316;
	add.s32 	%r4319, %r7105, %r4298;
	add.s32 	%r4320, %r4319, %r4318;
	add.s32 	%r4321, %r4320, 1859775393;
	shf.l.wrap.b32 	%r4322, %r4321, %r4321, 11;
	xor.b32  	%r4323, %r4316, %r4310;
	xor.b32  	%r4324, %r4323, %r4322;
	add.s32 	%r4325, %r641, %r4304;
	add.s32 	%r4326, %r4325, %r4324;
	add.s32 	%r4327, %r4326, 1859775393;
	shf.l.wrap.b32 	%r4328, %r4327, %r4327, 15;
	xor.b32  	%r4329, %r4322, %r4316;
	xor.b32  	%r4330, %r4329, %r4328;
	add.s32 	%r4331, %r7110, %r4310;
	add.s32 	%r4332, %r4331, %r4330;
	add.s32 	%r4333, %r4332, 1859775393;
	shf.l.wrap.b32 	%r4334, %r4333, %r4333, 3;
	xor.b32  	%r4335, %r4328, %r4322;
	xor.b32  	%r4336, %r4335, %r4334;
	add.s32 	%r4337, %r7102, %r4316;
	add.s32 	%r4338, %r4337, %r4336;
	add.s32 	%r4339, %r4338, 1859775393;
	shf.l.wrap.b32 	%r4340, %r4339, %r4339, 9;
	xor.b32  	%r4341, %r4334, %r4328;
	xor.b32  	%r4342, %r4341, %r4340;
	add.s32 	%r4343, %r7106, %r4322;
	add.s32 	%r4344, %r4343, %r4342;
	add.s32 	%r4345, %r4344, 1859775393;
	shf.l.wrap.b32 	%r4346, %r4345, %r4345, 11;
	xor.b32  	%r4347, %r4340, %r4334;
	xor.b32  	%r4348, %r4347, %r4346;
	add.s32 	%r4349, %r7098, %r4328;
	add.s32 	%r4350, %r4349, %r4348;
	add.s32 	%r4351, %r4350, 1859775393;
	shf.l.wrap.b32 	%r4352, %r4351, %r4351, 15;
	xor.b32  	%r4353, %r4346, %r4340;
	xor.b32  	%r4354, %r4353, %r4352;
	add.s32 	%r4355, %r7108, %r4334;
	add.s32 	%r4356, %r4355, %r4354;
	add.s32 	%r4357, %r4356, 1859775393;
	shf.l.wrap.b32 	%r4358, %r4357, %r4357, 3;
	xor.b32  	%r4359, %r4352, %r4346;
	xor.b32  	%r4360, %r4359, %r4358;
	add.s32 	%r4361, %r7100, %r4340;
	add.s32 	%r4362, %r4361, %r4360;
	add.s32 	%r4363, %r4362, 1859775393;
	shf.l.wrap.b32 	%r4364, %r4363, %r4363, 9;
	xor.b32  	%r4365, %r4358, %r4352;
	xor.b32  	%r4366, %r4365, %r4364;
	add.s32 	%r4367, %r7104, %r4346;
	add.s32 	%r4368, %r4367, %r4366;
	add.s32 	%r4369, %r4368, 1859775393;
	shf.l.wrap.b32 	%r4370, %r4369, %r4369, 11;
	xor.b32  	%r4371, %r4364, %r4358;
	xor.b32  	%r4372, %r4371, %r4370;
	add.s32 	%r4373, %r642, %r4352;
	add.s32 	%r4374, %r4373, %r4372;
	add.s32 	%r4375, %r4374, 1859775393;
	shf.l.wrap.b32 	%r4376, %r4375, %r4375, 15;
	add.s32 	%r99, %r4358, %r99;
	add.s32 	%r98, %r4376, %r98;
	add.s32 	%r97, %r4370, %r97;
	add.s32 	%r96, %r4364, %r96;
	mov.u32 	%r7098, 0;
	mov.u32 	%r7099, %r7098;
	mov.u32 	%r7100, %r7098;
	mov.u32 	%r7101, %r7098;
	mov.u32 	%r7102, %r7098;
	mov.u32 	%r7103, %r7098;
	mov.u32 	%r7104, %r7098;
	mov.u32 	%r7105, %r7098;
	mov.u32 	%r7106, %r7098;
	mov.u32 	%r7107, %r7098;
	mov.u32 	%r7108, %r7098;
	mov.u32 	%r7109, %r7098;
	mov.u32 	%r7110, %r7098;
	mov.u32 	%r7111, %r7098;

BB1_113:
	ld.param.u64 	%rd65, [m01000_mxx_param_6];
	xor.b32  	%r4377, %r97, %r96;
	and.b32  	%r4378, %r4377, %r98;
	xor.b32  	%r4379, %r4378, %r96;
	add.s32 	%r4380, %r99, %r7111;
	add.s32 	%r4381, %r4380, %r4379;
	shf.l.wrap.b32 	%r4382, %r4381, %r4381, 3;
	xor.b32  	%r4383, %r98, %r97;
	and.b32  	%r4384, %r4382, %r4383;
	xor.b32  	%r4385, %r4384, %r97;
	add.s32 	%r4386, %r96, %r7110;
	add.s32 	%r4387, %r4386, %r4385;
	shf.l.wrap.b32 	%r4388, %r4387, %r4387, 7;
	xor.b32  	%r4389, %r4382, %r98;
	and.b32  	%r4390, %r4389, %r4388;
	xor.b32  	%r4391, %r4390, %r98;
	add.s32 	%r4392, %r97, %r7109;
	add.s32 	%r4393, %r4392, %r4391;
	shf.l.wrap.b32 	%r4394, %r4393, %r4393, 11;
	xor.b32  	%r4395, %r4388, %r4382;
	and.b32  	%r4396, %r4395, %r4394;
	xor.b32  	%r4397, %r4396, %r4382;
	add.s32 	%r4398, %r98, %r7108;
	add.s32 	%r4399, %r4398, %r4397;
	shf.l.wrap.b32 	%r4400, %r4399, %r4399, 19;
	xor.b32  	%r4401, %r4394, %r4388;
	and.b32  	%r4402, %r4401, %r4400;
	xor.b32  	%r4403, %r4402, %r4388;
	add.s32 	%r4404, %r4382, %r7107;
	add.s32 	%r4405, %r4404, %r4403;
	shf.l.wrap.b32 	%r4406, %r4405, %r4405, 3;
	xor.b32  	%r4407, %r4400, %r4394;
	and.b32  	%r4408, %r4407, %r4406;
	xor.b32  	%r4409, %r4408, %r4394;
	add.s32 	%r4410, %r4388, %r7106;
	add.s32 	%r4411, %r4410, %r4409;
	shf.l.wrap.b32 	%r4412, %r4411, %r4411, 7;
	xor.b32  	%r4413, %r4406, %r4400;
	and.b32  	%r4414, %r4413, %r4412;
	xor.b32  	%r4415, %r4414, %r4400;
	add.s32 	%r4416, %r4394, %r7105;
	add.s32 	%r4417, %r4416, %r4415;
	shf.l.wrap.b32 	%r4418, %r4417, %r4417, 11;
	xor.b32  	%r4419, %r4412, %r4406;
	and.b32  	%r4420, %r4419, %r4418;
	xor.b32  	%r4421, %r4420, %r4406;
	add.s32 	%r4422, %r4400, %r7104;
	add.s32 	%r4423, %r4422, %r4421;
	shf.l.wrap.b32 	%r4424, %r4423, %r4423, 19;
	xor.b32  	%r4425, %r4418, %r4412;
	and.b32  	%r4426, %r4425, %r4424;
	xor.b32  	%r4427, %r4426, %r4412;
	add.s32 	%r4428, %r4406, %r7103;
	add.s32 	%r4429, %r4428, %r4427;
	shf.l.wrap.b32 	%r4430, %r4429, %r4429, 3;
	xor.b32  	%r4431, %r4424, %r4418;
	and.b32  	%r4432, %r4431, %r4430;
	xor.b32  	%r4433, %r4432, %r4418;
	add.s32 	%r4434, %r4412, %r7102;
	add.s32 	%r4435, %r4434, %r4433;
	shf.l.wrap.b32 	%r4436, %r4435, %r4435, 7;
	xor.b32  	%r4437, %r4430, %r4424;
	and.b32  	%r4438, %r4437, %r4436;
	xor.b32  	%r4439, %r4438, %r4424;
	add.s32 	%r4440, %r4418, %r7101;
	add.s32 	%r4441, %r4440, %r4439;
	shf.l.wrap.b32 	%r4442, %r4441, %r4441, 11;
	xor.b32  	%r4443, %r4436, %r4430;
	and.b32  	%r4444, %r4443, %r4442;
	xor.b32  	%r4445, %r4444, %r4430;
	add.s32 	%r4446, %r4424, %r7100;
	add.s32 	%r4447, %r4446, %r4445;
	shf.l.wrap.b32 	%r4448, %r4447, %r4447, 19;
	xor.b32  	%r4449, %r4442, %r4436;
	and.b32  	%r4450, %r4449, %r4448;
	xor.b32  	%r4451, %r4450, %r4436;
	add.s32 	%r4452, %r4430, %r7099;
	add.s32 	%r4453, %r4452, %r4451;
	shf.l.wrap.b32 	%r4454, %r4453, %r4453, 3;
	xor.b32  	%r4455, %r4448, %r4442;
	and.b32  	%r4456, %r4455, %r4454;
	xor.b32  	%r4457, %r4456, %r4442;
	add.s32 	%r4458, %r4436, %r7098;
	add.s32 	%r4459, %r4458, %r4457;
	shf.l.wrap.b32 	%r4460, %r4459, %r4459, 7;
	xor.b32  	%r4461, %r4454, %r4448;
	and.b32  	%r4462, %r4461, %r4460;
	xor.b32  	%r4463, %r4462, %r4448;
	shl.b32 	%r4464, %r128, 3;
	add.s32 	%r4465, %r4442, %r4464;
	add.s32 	%r4466, %r4465, %r4463;
	shf.l.wrap.b32 	%r4467, %r4466, %r4466, 11;
	xor.b32  	%r4468, %r4460, %r4454;
	and.b32  	%r4469, %r4468, %r4467;
	xor.b32  	%r4470, %r4469, %r4454;
	add.s32 	%r4471, %r4470, %r4448;
	shf.l.wrap.b32 	%r4472, %r4471, %r4471, 19;
	xor.b32  	%r4473, %r4472, %r4460;
	xor.b32  	%r4474, %r4472, %r4467;
	and.b32  	%r4475, %r4474, %r4473;
	xor.b32  	%r4476, %r4475, %r4472;
	add.s32 	%r4477, %r7111, %r4454;
	add.s32 	%r4478, %r4477, %r4476;
	add.s32 	%r4479, %r4478, 1518500249;
	shf.l.wrap.b32 	%r4480, %r4479, %r4479, 3;
	xor.b32  	%r4481, %r4480, %r4467;
	xor.b32  	%r4482, %r4480, %r4472;
	and.b32  	%r4483, %r4482, %r4481;
	xor.b32  	%r4484, %r4483, %r4480;
	add.s32 	%r4485, %r7107, %r4460;
	add.s32 	%r4486, %r4485, %r4484;
	add.s32 	%r4487, %r4486, 1518500249;
	shf.l.wrap.b32 	%r4488, %r4487, %r4487, 5;
	xor.b32  	%r4489, %r4488, %r4472;
	xor.b32  	%r4490, %r4488, %r4480;
	and.b32  	%r4491, %r4490, %r4489;
	xor.b32  	%r4492, %r4491, %r4488;
	add.s32 	%r4493, %r7103, %r4467;
	add.s32 	%r4494, %r4493, %r4492;
	add.s32 	%r4495, %r4494, 1518500249;
	shf.l.wrap.b32 	%r4496, %r4495, %r4495, 9;
	xor.b32  	%r4497, %r4496, %r4480;
	xor.b32  	%r4498, %r4496, %r4488;
	and.b32  	%r4499, %r4498, %r4497;
	xor.b32  	%r4500, %r4499, %r4496;
	add.s32 	%r4501, %r7099, %r4472;
	add.s32 	%r4502, %r4501, %r4500;
	add.s32 	%r4503, %r4502, 1518500249;
	shf.l.wrap.b32 	%r4504, %r4503, %r4503, 13;
	xor.b32  	%r4505, %r4504, %r4488;
	xor.b32  	%r4506, %r4504, %r4496;
	and.b32  	%r4507, %r4506, %r4505;
	xor.b32  	%r4508, %r4507, %r4504;
	add.s32 	%r4509, %r7110, %r4480;
	add.s32 	%r4510, %r4509, %r4508;
	add.s32 	%r4511, %r4510, 1518500249;
	shf.l.wrap.b32 	%r4512, %r4511, %r4511, 3;
	xor.b32  	%r4513, %r4512, %r4496;
	xor.b32  	%r4514, %r4512, %r4504;
	and.b32  	%r4515, %r4514, %r4513;
	xor.b32  	%r4516, %r4515, %r4512;
	add.s32 	%r4517, %r7106, %r4488;
	add.s32 	%r4518, %r4517, %r4516;
	add.s32 	%r4519, %r4518, 1518500249;
	shf.l.wrap.b32 	%r4520, %r4519, %r4519, 5;
	xor.b32  	%r4521, %r4520, %r4504;
	xor.b32  	%r4522, %r4520, %r4512;
	and.b32  	%r4523, %r4522, %r4521;
	xor.b32  	%r4524, %r4523, %r4520;
	add.s32 	%r4525, %r7102, %r4496;
	add.s32 	%r4526, %r4525, %r4524;
	add.s32 	%r4527, %r4526, 1518500249;
	shf.l.wrap.b32 	%r4528, %r4527, %r4527, 9;
	xor.b32  	%r4529, %r4528, %r4512;
	xor.b32  	%r4530, %r4528, %r4520;
	and.b32  	%r4531, %r4530, %r4529;
	xor.b32  	%r4532, %r4531, %r4528;
	add.s32 	%r4533, %r7098, %r4504;
	add.s32 	%r4534, %r4533, %r4532;
	add.s32 	%r4535, %r4534, 1518500249;
	shf.l.wrap.b32 	%r4536, %r4535, %r4535, 13;
	xor.b32  	%r4537, %r4536, %r4520;
	xor.b32  	%r4538, %r4536, %r4528;
	and.b32  	%r4539, %r4538, %r4537;
	xor.b32  	%r4540, %r4539, %r4536;
	add.s32 	%r4541, %r7109, %r4512;
	add.s32 	%r4542, %r4541, %r4540;
	add.s32 	%r4543, %r4542, 1518500249;
	shf.l.wrap.b32 	%r4544, %r4543, %r4543, 3;
	xor.b32  	%r4545, %r4544, %r4528;
	xor.b32  	%r4546, %r4544, %r4536;
	and.b32  	%r4547, %r4546, %r4545;
	xor.b32  	%r4548, %r4547, %r4544;
	add.s32 	%r4549, %r7105, %r4520;
	add.s32 	%r4550, %r4549, %r4548;
	add.s32 	%r4551, %r4550, 1518500249;
	shf.l.wrap.b32 	%r4552, %r4551, %r4551, 5;
	xor.b32  	%r4553, %r4552, %r4536;
	xor.b32  	%r4554, %r4552, %r4544;
	and.b32  	%r4555, %r4554, %r4553;
	xor.b32  	%r4556, %r4555, %r4552;
	add.s32 	%r4557, %r7101, %r4528;
	add.s32 	%r4558, %r4557, %r4556;
	add.s32 	%r4559, %r4558, 1518500249;
	shf.l.wrap.b32 	%r4560, %r4559, %r4559, 9;
	xor.b32  	%r4561, %r4560, %r4544;
	xor.b32  	%r4562, %r4560, %r4552;
	and.b32  	%r4563, %r4562, %r4561;
	xor.b32  	%r4564, %r4563, %r4560;
	add.s32 	%r4565, %r4464, %r4536;
	add.s32 	%r4566, %r4565, %r4564;
	add.s32 	%r4567, %r4566, 1518500249;
	shf.l.wrap.b32 	%r4568, %r4567, %r4567, 13;
	xor.b32  	%r4569, %r4568, %r4552;
	xor.b32  	%r4570, %r4568, %r4560;
	and.b32  	%r4571, %r4570, %r4569;
	xor.b32  	%r4572, %r4571, %r4568;
	add.s32 	%r4573, %r7108, %r4544;
	add.s32 	%r4574, %r4573, %r4572;
	add.s32 	%r4575, %r4574, 1518500249;
	shf.l.wrap.b32 	%r4576, %r4575, %r4575, 3;
	xor.b32  	%r4577, %r4576, %r4560;
	xor.b32  	%r4578, %r4576, %r4568;
	and.b32  	%r4579, %r4578, %r4577;
	xor.b32  	%r4580, %r4579, %r4576;
	add.s32 	%r4581, %r7104, %r4552;
	add.s32 	%r4582, %r4581, %r4580;
	add.s32 	%r4583, %r4582, 1518500249;
	shf.l.wrap.b32 	%r4584, %r4583, %r4583, 5;
	xor.b32  	%r4585, %r4584, %r4568;
	xor.b32  	%r4586, %r4584, %r4576;
	and.b32  	%r4587, %r4586, %r4585;
	xor.b32  	%r4588, %r4587, %r4584;
	add.s32 	%r4589, %r7100, %r4560;
	add.s32 	%r4590, %r4589, %r4588;
	add.s32 	%r4591, %r4590, 1518500249;
	shf.l.wrap.b32 	%r4592, %r4591, %r4591, 9;
	xor.b32  	%r4593, %r4592, %r4576;
	xor.b32  	%r4594, %r4592, %r4584;
	and.b32  	%r4595, %r4594, %r4593;
	xor.b32  	%r4596, %r4595, %r4592;
	add.s32 	%r4597, %r4568, %r4596;
	add.s32 	%r4598, %r4597, 1518500249;
	shf.l.wrap.b32 	%r4599, %r4598, %r4598, 13;
	xor.b32  	%r4600, %r4594, %r4599;
	add.s32 	%r4601, %r7111, %r4576;
	add.s32 	%r4602, %r4601, %r4600;
	add.s32 	%r4603, %r4602, 1859775393;
	shf.l.wrap.b32 	%r4604, %r4603, %r4603, 3;
	xor.b32  	%r4605, %r4599, %r4592;
	xor.b32  	%r4606, %r4605, %r4604;
	add.s32 	%r4607, %r7103, %r4584;
	add.s32 	%r4608, %r4607, %r4606;
	add.s32 	%r4609, %r4608, 1859775393;
	shf.l.wrap.b32 	%r4610, %r4609, %r4609, 9;
	xor.b32  	%r4611, %r4604, %r4599;
	xor.b32  	%r4612, %r4611, %r4610;
	add.s32 	%r4613, %r7107, %r4592;
	add.s32 	%r4614, %r4613, %r4612;
	add.s32 	%r4615, %r4614, 1859775393;
	shf.l.wrap.b32 	%r4616, %r4615, %r4615, 11;
	xor.b32  	%r4617, %r4610, %r4604;
	xor.b32  	%r4618, %r4617, %r4616;
	add.s32 	%r4619, %r7099, %r4599;
	add.s32 	%r4620, %r4619, %r4618;
	add.s32 	%r4621, %r4620, 1859775393;
	shf.l.wrap.b32 	%r4622, %r4621, %r4621, 15;
	xor.b32  	%r4623, %r4616, %r4610;
	xor.b32  	%r4624, %r4623, %r4622;
	add.s32 	%r4625, %r7109, %r4604;
	add.s32 	%r4626, %r4625, %r4624;
	add.s32 	%r4627, %r4626, 1859775393;
	shf.l.wrap.b32 	%r4628, %r4627, %r4627, 3;
	xor.b32  	%r4629, %r4622, %r4616;
	xor.b32  	%r4630, %r4629, %r4628;
	add.s32 	%r4631, %r7101, %r4610;
	add.s32 	%r4632, %r4631, %r4630;
	add.s32 	%r4633, %r4632, 1859775393;
	shf.l.wrap.b32 	%r4634, %r4633, %r4633, 9;
	xor.b32  	%r4635, %r4628, %r4622;
	xor.b32  	%r4636, %r4635, %r4634;
	add.s32 	%r4637, %r7105, %r4616;
	add.s32 	%r4638, %r4637, %r4636;
	add.s32 	%r4639, %r4638, 1859775393;
	shf.l.wrap.b32 	%r4640, %r4639, %r4639, 11;
	xor.b32  	%r4641, %r4634, %r4628;
	xor.b32  	%r4642, %r4641, %r4640;
	add.s32 	%r4643, %r4464, %r4622;
	add.s32 	%r4644, %r4643, %r4642;
	add.s32 	%r4645, %r4644, 1859775393;
	shf.l.wrap.b32 	%r4646, %r4645, %r4645, 15;
	xor.b32  	%r4647, %r4640, %r4634;
	xor.b32  	%r4648, %r4647, %r4646;
	add.s32 	%r4649, %r7110, %r4628;
	add.s32 	%r4650, %r4649, %r4648;
	add.s32 	%r4651, %r4650, 1859775393;
	shf.l.wrap.b32 	%r4652, %r4651, %r4651, 3;
	xor.b32  	%r4653, %r4646, %r4640;
	xor.b32  	%r4654, %r4653, %r4652;
	add.s32 	%r4655, %r7102, %r4634;
	add.s32 	%r4656, %r4655, %r4654;
	add.s32 	%r4657, %r4656, 1859775393;
	shf.l.wrap.b32 	%r4658, %r4657, %r4657, 9;
	xor.b32  	%r4659, %r4652, %r4646;
	xor.b32  	%r4660, %r4659, %r4658;
	add.s32 	%r4661, %r7106, %r4640;
	add.s32 	%r4662, %r4661, %r4660;
	add.s32 	%r4663, %r4662, 1859775393;
	shf.l.wrap.b32 	%r4664, %r4663, %r4663, 11;
	xor.b32  	%r4665, %r4658, %r4652;
	xor.b32  	%r4666, %r4665, %r4664;
	add.s32 	%r4667, %r7098, %r4646;
	add.s32 	%r4668, %r4667, %r4666;
	add.s32 	%r4669, %r4668, 1859775393;
	shf.l.wrap.b32 	%r4670, %r4669, %r4669, 15;
	xor.b32  	%r4671, %r4664, %r4658;
	xor.b32  	%r4672, %r4671, %r4670;
	add.s32 	%r4673, %r7108, %r4652;
	add.s32 	%r4674, %r4673, %r4672;
	add.s32 	%r4675, %r4674, 1859775393;
	shf.l.wrap.b32 	%r4676, %r4675, %r4675, 3;
	xor.b32  	%r4677, %r4670, %r4664;
	xor.b32  	%r4678, %r4677, %r4676;
	add.s32 	%r4679, %r7100, %r4658;
	add.s32 	%r4680, %r4679, %r4678;
	add.s32 	%r4681, %r4680, 1859775393;
	shf.l.wrap.b32 	%r4682, %r4681, %r4681, 9;
	xor.b32  	%r4683, %r4676, %r4670;
	xor.b32  	%r4684, %r4683, %r4682;
	add.s32 	%r4685, %r7104, %r4664;
	add.s32 	%r4686, %r4685, %r4684;
	add.s32 	%r4687, %r4686, 1859775393;
	shf.l.wrap.b32 	%r4688, %r4687, %r4687, 11;
	xor.b32  	%r4689, %r4682, %r4676;
	xor.b32  	%r4690, %r4689, %r4688;
	add.s32 	%r4691, %r4670, %r4690;
	add.s32 	%r4692, %r4691, 1859775393;
	shf.l.wrap.b32 	%r4693, %r4692, %r4692, 15;
	add.s32 	%r665, %r4676, %r99;
	add.s32 	%r666, %r4693, %r98;
	add.s32 	%r667, %r4688, %r97;
	add.s32 	%r668, %r4682, %r96;
	shr.u32 	%r4694, %r665, %r75;
	and.b32  	%r4695, %r4694, %r1023;
	mul.wide.u32 	%rd35, %r4695, 4;
	add.s64 	%rd36, %rd65, %rd35;
	and.b32  	%r4696, %r665, 31;
	mov.u32 	%r4697, 1;
	shl.b32 	%r669, %r4697, %r4696;
	ld.global.u32 	%r4698, [%rd36];
	and.b32  	%r4699, %r4698, %r669;
	setp.eq.s32	%p70, %r4699, 0;
	@%p70 bra 	BB1_140;

	mov.u32 	%r6965, 1;
	ld.param.u64 	%rd58, [m01000_mxx_param_7];
	shr.u32 	%r4700, %r668, %r75;
	and.b32  	%r4701, %r4700, %r1023;
	mul.wide.u32 	%rd37, %r4701, 4;
	add.s64 	%rd38, %rd58, %rd37;
	and.b32  	%r4702, %r668, 31;
	shl.b32 	%r670, %r6965, %r4702;
	ld.global.u32 	%r4704, [%rd38];
	and.b32  	%r4705, %r4704, %r670;
	setp.eq.s32	%p71, %r4705, 0;
	@%p71 bra 	BB1_140;

	mov.u32 	%r6966, 1;
	ld.param.u64 	%rd59, [m01000_mxx_param_8];
	shr.u32 	%r4706, %r667, %r75;
	and.b32  	%r4707, %r4706, %r1023;
	mul.wide.u32 	%rd39, %r4707, 4;
	add.s64 	%rd40, %rd59, %rd39;
	and.b32  	%r4708, %r667, 31;
	shl.b32 	%r671, %r6966, %r4708;
	ld.global.u32 	%r4710, [%rd40];
	and.b32  	%r4711, %r4710, %r671;
	setp.eq.s32	%p72, %r4711, 0;
	@%p72 bra 	BB1_140;

	mov.u32 	%r6967, 1;
	ld.param.u64 	%rd60, [m01000_mxx_param_9];
	shr.u32 	%r4712, %r666, %r75;
	and.b32  	%r4713, %r4712, %r1023;
	mul.wide.u32 	%rd41, %r4713, 4;
	add.s64 	%rd42, %rd60, %rd41;
	and.b32  	%r4714, %r666, 31;
	shl.b32 	%r672, %r6967, %r4714;
	ld.global.u32 	%r4716, [%rd42];
	and.b32  	%r4717, %r4716, %r672;
	setp.eq.s32	%p73, %r4717, 0;
	@%p73 bra 	BB1_140;

	and.b32  	%r6970, %r665, 31;
	mov.u32 	%r6969, 1;
	shl.b32 	%r6968, %r6969, %r6970;
	ld.param.u64 	%rd61, [m01000_mxx_param_10];
	shr.u32 	%r4718, %r665, %r76;
	and.b32  	%r4719, %r4718, %r1023;
	mul.wide.u32 	%rd43, %r4719, 4;
	add.s64 	%rd44, %rd61, %rd43;
	ld.global.u32 	%r4720, [%rd44];
	and.b32  	%r4721, %r4720, %r6968;
	setp.eq.s32	%p74, %r4721, 0;
	@%p74 bra 	BB1_140;

	ld.param.u64 	%rd62, [m01000_mxx_param_11];
	shr.u32 	%r4722, %r668, %r76;
	and.b32  	%r4723, %r4722, %r1023;
	mul.wide.u32 	%rd45, %r4723, 4;
	add.s64 	%rd46, %rd62, %rd45;
	ld.global.u32 	%r4724, [%rd46];
	and.b32  	%r4725, %r4724, %r670;
	setp.eq.s32	%p75, %r4725, 0;
	@%p75 bra 	BB1_140;

	ld.param.u64 	%rd63, [m01000_mxx_param_12];
	shr.u32 	%r4726, %r667, %r76;
	and.b32  	%r4727, %r4726, %r1023;
	mul.wide.u32 	%rd47, %r4727, 4;
	add.s64 	%rd48, %rd63, %rd47;
	ld.global.u32 	%r4728, [%rd48];
	and.b32  	%r4729, %r4728, %r671;
	setp.eq.s32	%p76, %r4729, 0;
	@%p76 bra 	BB1_140;

	ld.param.u64 	%rd64, [m01000_mxx_param_13];
	shr.u32 	%r4730, %r666, %r76;
	and.b32  	%r4731, %r4730, %r1023;
	mul.wide.u32 	%rd49, %r4731, 4;
	add.s64 	%rd50, %rd64, %rd49;
	ld.global.u32 	%r4732, [%rd50];
	and.b32  	%r4733, %r4732, %r672;
	setp.eq.s32	%p77, %r4733, 0;
	@%p77 bra 	BB1_140;

	setp.eq.s32	%p78, %r1028, 0;
	mov.u32 	%r7117, 0;
	mov.u32 	%r4734, -1;
	mov.u32 	%r7116, %r1028;
	@%p78 bra 	BB1_134;

BB1_122:
	mov.u32 	%r7118, 1;
	ld.param.u64 	%rd66, [m01000_mxx_param_15];
	shr.u32 	%r675, %r7116, 1;
	add.s32 	%r7119, %r675, %r7117;
	cvt.u64.u32	%rd51, %r7119;
	add.s64 	%rd52, %rd51, %rd1;
	shl.b64 	%rd53, %rd52, 4;
	add.s64 	%rd3, %rd66, %rd53;
	ld.global.u32 	%r677, [%rd3+4];
	setp.gt.u32	%p79, %r666, %r677;
	@%p79 bra 	BB1_132;

	setp.lt.u32	%p80, %r666, %r677;
	mov.u32 	%r4737, -1;
	@%p80 bra 	BB1_124;
	bra.uni 	BB1_125;

BB1_124:
	mov.u32 	%r7118, %r4737;
	bra.uni 	BB1_132;

BB1_125:
	mov.u32 	%r7118, 1;
	ld.global.u32 	%r678, [%rd3+8];
	setp.gt.u32	%p81, %r667, %r678;
	@%p81 bra 	BB1_132;

	setp.lt.u32	%p82, %r667, %r678;
	@%p82 bra 	BB1_127;
	bra.uni 	BB1_128;

BB1_127:
	mov.u32 	%r7118, %r4737;
	bra.uni 	BB1_132;

BB1_128:
	mov.u32 	%r7118, 1;
	ld.global.u32 	%r679, [%rd3+12];
	setp.gt.u32	%p83, %r668, %r679;
	@%p83 bra 	BB1_132;

	setp.lt.u32	%p84, %r668, %r679;
	mov.u32 	%r7118, %r4737;
	@%p84 bra 	BB1_132;

	mov.u32 	%r7118, 1;
	ld.global.u32 	%r680, [%rd3];
	setp.gt.u32	%p85, %r665, %r680;
	@%p85 bra 	BB1_132;

	setp.lt.u32	%p86, %r665, %r680;
	selp.b32	%r7118, -1, 0, %p86;

BB1_132:
	add.s32 	%r4743, %r675, 1;
	setp.gt.s32	%p87, %r7118, 0;
	selp.b32	%r4744, %r4743, 0, %p87;
	add.s32 	%r7117, %r4744, %r7117;
	selp.b32	%r4745, -1, 0, %p87;
	add.s32 	%r4746, %r4745, %r7116;
	shr.u32 	%r7116, %r4746, 1;
	setp.eq.s32	%p88, %r7118, 0;
	@%p88 bra 	BB1_135;

	setp.ne.s32	%p89, %r7116, 0;
	@%p89 bra 	BB1_122;

BB1_134:
	mov.u32 	%r7119, %r4734;

BB1_135:
	setp.eq.s32	%p90, %r7119, -1;
	@%p90 bra 	BB1_140;

	ld.param.u64 	%rd67, [m01000_mxx_param_16];
	ld.param.u32 	%r6961, [m01000_mxx_param_32];
	add.s32 	%r686, %r7119, %r6961;
	mul.wide.u32 	%rd54, %r686, 4;
	add.s64 	%rd55, %rd67, %rd54;
	atom.global.add.u32 	%r4748, [%rd55], 1;
	setp.ne.s32	%p91, %r4748, 0;
	@%p91 bra 	BB1_140;

	atom.global.add.u32 	%r687, [%rd17], 1;
	setp.lt.u32	%p92, %r687, %r1028;
	@%p92 bra 	BB1_139;
	bra.uni 	BB1_138;

BB1_139:
	ld.param.u32 	%r6964, [m01000_mxx_param_27];
	ld.param.u64 	%rd68, [m01000_mxx_param_14];
	mul.wide.u32 	%rd56, %r687, 20;
	add.s64 	%rd57, %rd68, %rd56;
	st.global.u32 	[%rd57], %r6964;
	st.global.u32 	[%rd57+4], %r7119;
	st.global.u32 	[%rd57+8], %r686;
	st.global.u32 	[%rd57+12], %r1;
	st.global.u32 	[%rd57+16], %r7006;
	bra.uni 	BB1_140;

BB1_138:
	atom.global.add.u32 	%r4749, [%rd17], -1;

BB1_140:
	ld.param.u32 	%r6962, [m01000_mxx_param_30];
	add.s32 	%r7006, %r7006, 1;
	setp.lt.u32	%p93, %r7006, %r6962;
	@%p93 bra 	BB1_8;

BB1_141:
	ret;
}

	// .globl	m01000_sxx
.entry m01000_sxx(
	.param .u64 .ptr .global .align 4 m01000_sxx_param_0,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_1,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_2,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_3,
	.param .u64 .ptr .global .align 1 m01000_sxx_param_4,
	.param .u64 .ptr .global .align 1 m01000_sxx_param_5,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_6,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_7,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_8,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_9,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_10,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_11,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_12,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_13,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_14,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_15,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_16,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_17,
	.param .u64 .ptr .global .align 1 m01000_sxx_param_18,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_19,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_20,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_21,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_22,
	.param .u64 .ptr .global .align 4 m01000_sxx_param_23,
	.param .u32 m01000_sxx_param_24,
	.param .u32 m01000_sxx_param_25,
	.param .u32 m01000_sxx_param_26,
	.param .u32 m01000_sxx_param_27,
	.param .u32 m01000_sxx_param_28,
	.param .u32 m01000_sxx_param_29,
	.param .u32 m01000_sxx_param_30,
	.param .u32 m01000_sxx_param_31,
	.param .u32 m01000_sxx_param_32,
	.param .u32 m01000_sxx_param_33,
	.param .u64 m01000_sxx_param_34
)
{
	.reg .pred 	%p<119>;
	.reg .b32 	%r<7064>;
	.reg .b64 	%rd<35>;


	ld.param.u64 	%rd4, [m01000_sxx_param_0];
	ld.param.u64 	%rd5, [m01000_sxx_param_2];
	ld.param.u64 	%rd7, [m01000_sxx_param_15];
	ld.param.u64 	%rd9, [m01000_sxx_param_19];
	ld.param.u32 	%r1006, [m01000_sxx_param_32];
	ld.param.u64 	%rd10, [m01000_sxx_param_34];
	mov.b32	%r1007, %envreg3;
	mov.u32 	%r1008, %ctaid.x;
	mov.u32 	%r1009, %ntid.x;
	mad.lo.s32 	%r1010, %r1008, %r1009, %r1007;
	mov.u32 	%r1011, %tid.x;
	add.s32 	%r1, %r1010, %r1011;
	cvt.s64.s32	%rd11, %r1;
	setp.ge.u64	%p1, %rd11, %rd10;
	@%p1 bra 	BB2_119;

	cvt.u64.u32	%rd1, %r1006;
	mul.wide.u32 	%rd12, %r1006, 16;
	add.s64 	%rd13, %rd7, %rd12;
	ld.global.u32 	%r2, [%rd13];
	ld.global.u32 	%r3, [%rd13+12];
	ld.global.u32 	%r4, [%rd13+8];
	ld.global.u32 	%r5, [%rd13+4];
	mul.wide.s32 	%rd14, %r1, 260;
	add.s64 	%rd15, %rd4, %rd14;
	ld.global.u32 	%r6, [%rd15+256];
	mov.u32 	%r6895, 0;
	mov.u32 	%r11, 1732584193;
	mov.u32 	%r10, -271733879;
	mov.u32 	%r9, -1732584194;
	mov.u32 	%r8, 271733878;
	mov.u32 	%r6900, %r6895;
	mov.u32 	%r6901, %r6895;
	bra.uni 	BB2_2;

BB2_167:
	mov.u32 	%r6569, 0;
	mov.u32 	%r6497, 29554;
	// inline asm
	prmt.b32 %r6438, %r21, %r6569, %r6497;
	// inline asm
	mov.u32 	%r6501, 29040;
	// inline asm
	prmt.b32 %r6442, %r21, %r6569, %r6501;
	// inline asm
	// inline asm
	prmt.b32 %r6446, %r20, %r6569, %r6497;
	// inline asm
	// inline asm
	prmt.b32 %r6450, %r20, %r6569, %r6501;
	// inline asm
	// inline asm
	prmt.b32 %r6454, %r19, %r6569, %r6497;
	// inline asm
	// inline asm
	prmt.b32 %r6458, %r19, %r6569, %r6501;
	// inline asm
	// inline asm
	prmt.b32 %r6462, %r18, %r6569, %r6497;
	// inline asm
	// inline asm
	prmt.b32 %r6466, %r18, %r6569, %r6501;
	// inline asm
	// inline asm
	prmt.b32 %r6470, %r17, %r6569, %r6497;
	// inline asm
	// inline asm
	prmt.b32 %r6474, %r17, %r6569, %r6501;
	// inline asm
	// inline asm
	prmt.b32 %r6478, %r16, %r6569, %r6497;
	// inline asm
	// inline asm
	prmt.b32 %r6482, %r16, %r6569, %r6501;
	// inline asm
	// inline asm
	prmt.b32 %r6486, %r15, %r6569, %r6497;
	// inline asm
	// inline asm
	prmt.b32 %r6490, %r15, %r6569, %r6501;
	// inline asm
	// inline asm
	prmt.b32 %r6494, %r14, %r6569, %r6497;
	// inline asm
	// inline asm
	prmt.b32 %r6498, %r14, %r6569, %r6501;
	// inline asm
	add.s32 	%r6895, %r6895, 64;
	// inline asm
	shf.r.wrap.b32 %r6502, %r6438, %r6569, %r6569;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6506, %r6442, %r6438, %r6569;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6510, %r6446, %r6442, %r6569;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6514, %r6450, %r6446, %r6569;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6518, %r6454, %r6450, %r6569;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6522, %r6458, %r6454, %r6569;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6526, %r6462, %r6458, %r6569;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6530, %r6466, %r6462, %r6569;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6534, %r6470, %r6466, %r6569;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6538, %r6474, %r6470, %r6569;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6542, %r6478, %r6474, %r6569;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6546, %r6482, %r6478, %r6569;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6550, %r6486, %r6482, %r6569;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6554, %r6490, %r6486, %r6569;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6558, %r6494, %r6490, %r6569;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6562, %r6498, %r6494, %r6569;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6566, %r6569, %r6498, %r6569;
	// inline asm
	xor.b32  	%r6570, %r9, %r8;
	and.b32  	%r6571, %r6570, %r10;
	xor.b32  	%r6572, %r6571, %r8;
	add.s32 	%r6573, %r6572, %r11;
	add.s32 	%r6574, %r6573, %r6562;
	shf.l.wrap.b32 	%r6575, %r6574, %r6574, 3;
	xor.b32  	%r6576, %r10, %r9;
	and.b32  	%r6577, %r6575, %r6576;
	xor.b32  	%r6578, %r6577, %r9;
	add.s32 	%r6579, %r6558, %r8;
	add.s32 	%r6580, %r6579, %r6578;
	shf.l.wrap.b32 	%r6581, %r6580, %r6580, 7;
	xor.b32  	%r6582, %r6575, %r10;
	and.b32  	%r6583, %r6582, %r6581;
	xor.b32  	%r6584, %r6583, %r10;
	add.s32 	%r6585, %r6554, %r9;
	add.s32 	%r6586, %r6585, %r6584;
	shf.l.wrap.b32 	%r6587, %r6586, %r6586, 11;
	xor.b32  	%r6588, %r6581, %r6575;
	and.b32  	%r6589, %r6588, %r6587;
	xor.b32  	%r6590, %r6589, %r6575;
	add.s32 	%r6591, %r6550, %r10;
	add.s32 	%r6592, %r6591, %r6590;
	shf.l.wrap.b32 	%r6593, %r6592, %r6592, 19;
	xor.b32  	%r6594, %r6587, %r6581;
	and.b32  	%r6595, %r6594, %r6593;
	xor.b32  	%r6596, %r6595, %r6581;
	add.s32 	%r6597, %r6575, %r6546;
	add.s32 	%r6598, %r6597, %r6596;
	shf.l.wrap.b32 	%r6599, %r6598, %r6598, 3;
	xor.b32  	%r6600, %r6593, %r6587;
	and.b32  	%r6601, %r6600, %r6599;
	xor.b32  	%r6602, %r6601, %r6587;
	add.s32 	%r6603, %r6581, %r6542;
	add.s32 	%r6604, %r6603, %r6602;
	shf.l.wrap.b32 	%r6605, %r6604, %r6604, 7;
	xor.b32  	%r6606, %r6599, %r6593;
	and.b32  	%r6607, %r6606, %r6605;
	xor.b32  	%r6608, %r6607, %r6593;
	add.s32 	%r6609, %r6587, %r6538;
	add.s32 	%r6610, %r6609, %r6608;
	shf.l.wrap.b32 	%r6611, %r6610, %r6610, 11;
	xor.b32  	%r6612, %r6605, %r6599;
	and.b32  	%r6613, %r6612, %r6611;
	xor.b32  	%r6614, %r6613, %r6599;
	add.s32 	%r6615, %r6593, %r6534;
	add.s32 	%r6616, %r6615, %r6614;
	shf.l.wrap.b32 	%r6617, %r6616, %r6616, 19;
	xor.b32  	%r6618, %r6611, %r6605;
	and.b32  	%r6619, %r6618, %r6617;
	xor.b32  	%r6620, %r6619, %r6605;
	add.s32 	%r6621, %r6599, %r6530;
	add.s32 	%r6622, %r6621, %r6620;
	shf.l.wrap.b32 	%r6623, %r6622, %r6622, 3;
	xor.b32  	%r6624, %r6617, %r6611;
	and.b32  	%r6625, %r6624, %r6623;
	xor.b32  	%r6626, %r6625, %r6611;
	add.s32 	%r6627, %r6605, %r6526;
	add.s32 	%r6628, %r6627, %r6626;
	shf.l.wrap.b32 	%r6629, %r6628, %r6628, 7;
	xor.b32  	%r6630, %r6623, %r6617;
	and.b32  	%r6631, %r6630, %r6629;
	xor.b32  	%r6632, %r6631, %r6617;
	add.s32 	%r6633, %r6611, %r6522;
	add.s32 	%r6634, %r6633, %r6632;
	shf.l.wrap.b32 	%r6635, %r6634, %r6634, 11;
	xor.b32  	%r6636, %r6629, %r6623;
	and.b32  	%r6637, %r6636, %r6635;
	xor.b32  	%r6638, %r6637, %r6623;
	add.s32 	%r6639, %r6617, %r6518;
	add.s32 	%r6640, %r6639, %r6638;
	shf.l.wrap.b32 	%r6641, %r6640, %r6640, 19;
	xor.b32  	%r6642, %r6635, %r6629;
	and.b32  	%r6643, %r6642, %r6641;
	xor.b32  	%r6644, %r6643, %r6629;
	add.s32 	%r6645, %r6623, %r6514;
	add.s32 	%r6646, %r6645, %r6644;
	shf.l.wrap.b32 	%r6647, %r6646, %r6646, 3;
	xor.b32  	%r6648, %r6641, %r6635;
	and.b32  	%r6649, %r6648, %r6647;
	xor.b32  	%r6650, %r6649, %r6635;
	add.s32 	%r6651, %r6629, %r6510;
	add.s32 	%r6652, %r6651, %r6650;
	shf.l.wrap.b32 	%r6653, %r6652, %r6652, 7;
	xor.b32  	%r6654, %r6647, %r6641;
	and.b32  	%r6655, %r6654, %r6653;
	xor.b32  	%r6656, %r6655, %r6641;
	add.s32 	%r6657, %r6635, %r6506;
	add.s32 	%r6658, %r6657, %r6656;
	shf.l.wrap.b32 	%r6659, %r6658, %r6658, 11;
	xor.b32  	%r6660, %r6653, %r6647;
	and.b32  	%r6661, %r6660, %r6659;
	xor.b32  	%r6662, %r6661, %r6647;
	add.s32 	%r6663, %r6641, %r6502;
	add.s32 	%r6664, %r6663, %r6662;
	shf.l.wrap.b32 	%r6665, %r6664, %r6664, 19;
	xor.b32  	%r6666, %r6665, %r6653;
	xor.b32  	%r6667, %r6665, %r6659;
	and.b32  	%r6668, %r6667, %r6666;
	xor.b32  	%r6669, %r6668, %r6665;
	add.s32 	%r6670, %r6562, %r6647;
	add.s32 	%r6671, %r6670, %r6669;
	add.s32 	%r6672, %r6671, 1518500249;
	shf.l.wrap.b32 	%r6673, %r6672, %r6672, 3;
	xor.b32  	%r6674, %r6673, %r6659;
	xor.b32  	%r6675, %r6673, %r6665;
	and.b32  	%r6676, %r6675, %r6674;
	xor.b32  	%r6677, %r6676, %r6673;
	add.s32 	%r6678, %r6546, %r6653;
	add.s32 	%r6679, %r6678, %r6677;
	add.s32 	%r6680, %r6679, 1518500249;
	shf.l.wrap.b32 	%r6681, %r6680, %r6680, 5;
	xor.b32  	%r6682, %r6681, %r6665;
	xor.b32  	%r6683, %r6681, %r6673;
	and.b32  	%r6684, %r6683, %r6682;
	xor.b32  	%r6685, %r6684, %r6681;
	add.s32 	%r6686, %r6530, %r6659;
	add.s32 	%r6687, %r6686, %r6685;
	add.s32 	%r6688, %r6687, 1518500249;
	shf.l.wrap.b32 	%r6689, %r6688, %r6688, 9;
	xor.b32  	%r6690, %r6689, %r6673;
	xor.b32  	%r6691, %r6689, %r6681;
	and.b32  	%r6692, %r6691, %r6690;
	xor.b32  	%r6693, %r6692, %r6689;
	add.s32 	%r6694, %r6514, %r6665;
	add.s32 	%r6695, %r6694, %r6693;
	add.s32 	%r6696, %r6695, 1518500249;
	shf.l.wrap.b32 	%r6697, %r6696, %r6696, 13;
	xor.b32  	%r6698, %r6697, %r6681;
	xor.b32  	%r6699, %r6697, %r6689;
	and.b32  	%r6700, %r6699, %r6698;
	xor.b32  	%r6701, %r6700, %r6697;
	add.s32 	%r6702, %r6558, %r6673;
	add.s32 	%r6703, %r6702, %r6701;
	add.s32 	%r6704, %r6703, 1518500249;
	shf.l.wrap.b32 	%r6705, %r6704, %r6704, 3;
	xor.b32  	%r6706, %r6705, %r6689;
	xor.b32  	%r6707, %r6705, %r6697;
	and.b32  	%r6708, %r6707, %r6706;
	xor.b32  	%r6709, %r6708, %r6705;
	add.s32 	%r6710, %r6542, %r6681;
	add.s32 	%r6711, %r6710, %r6709;
	add.s32 	%r6712, %r6711, 1518500249;
	shf.l.wrap.b32 	%r6713, %r6712, %r6712, 5;
	xor.b32  	%r6714, %r6713, %r6697;
	xor.b32  	%r6715, %r6713, %r6705;
	and.b32  	%r6716, %r6715, %r6714;
	xor.b32  	%r6717, %r6716, %r6713;
	add.s32 	%r6718, %r6526, %r6689;
	add.s32 	%r6719, %r6718, %r6717;
	add.s32 	%r6720, %r6719, 1518500249;
	shf.l.wrap.b32 	%r6721, %r6720, %r6720, 9;
	xor.b32  	%r6722, %r6721, %r6705;
	xor.b32  	%r6723, %r6721, %r6713;
	and.b32  	%r6724, %r6723, %r6722;
	xor.b32  	%r6725, %r6724, %r6721;
	add.s32 	%r6726, %r6510, %r6697;
	add.s32 	%r6727, %r6726, %r6725;
	add.s32 	%r6728, %r6727, 1518500249;
	shf.l.wrap.b32 	%r6729, %r6728, %r6728, 13;
	xor.b32  	%r6730, %r6729, %r6713;
	xor.b32  	%r6731, %r6729, %r6721;
	and.b32  	%r6732, %r6731, %r6730;
	xor.b32  	%r6733, %r6732, %r6729;
	add.s32 	%r6734, %r6554, %r6705;
	add.s32 	%r6735, %r6734, %r6733;
	add.s32 	%r6736, %r6735, 1518500249;
	shf.l.wrap.b32 	%r6737, %r6736, %r6736, 3;
	xor.b32  	%r6738, %r6737, %r6721;
	xor.b32  	%r6739, %r6737, %r6729;
	and.b32  	%r6740, %r6739, %r6738;
	xor.b32  	%r6741, %r6740, %r6737;
	add.s32 	%r6742, %r6538, %r6713;
	add.s32 	%r6743, %r6742, %r6741;
	add.s32 	%r6744, %r6743, 1518500249;
	shf.l.wrap.b32 	%r6745, %r6744, %r6744, 5;
	xor.b32  	%r6746, %r6745, %r6729;
	xor.b32  	%r6747, %r6745, %r6737;
	and.b32  	%r6748, %r6747, %r6746;
	xor.b32  	%r6749, %r6748, %r6745;
	add.s32 	%r6750, %r6522, %r6721;
	add.s32 	%r6751, %r6750, %r6749;
	add.s32 	%r6752, %r6751, 1518500249;
	shf.l.wrap.b32 	%r6753, %r6752, %r6752, 9;
	xor.b32  	%r6754, %r6753, %r6737;
	xor.b32  	%r6755, %r6753, %r6745;
	and.b32  	%r6756, %r6755, %r6754;
	xor.b32  	%r6757, %r6756, %r6753;
	add.s32 	%r6758, %r6506, %r6729;
	add.s32 	%r6759, %r6758, %r6757;
	add.s32 	%r6760, %r6759, 1518500249;
	shf.l.wrap.b32 	%r6761, %r6760, %r6760, 13;
	xor.b32  	%r6762, %r6761, %r6745;
	xor.b32  	%r6763, %r6761, %r6753;
	and.b32  	%r6764, %r6763, %r6762;
	xor.b32  	%r6765, %r6764, %r6761;
	add.s32 	%r6766, %r6550, %r6737;
	add.s32 	%r6767, %r6766, %r6765;
	add.s32 	%r6768, %r6767, 1518500249;
	shf.l.wrap.b32 	%r6769, %r6768, %r6768, 3;
	xor.b32  	%r6770, %r6769, %r6753;
	xor.b32  	%r6771, %r6769, %r6761;
	and.b32  	%r6772, %r6771, %r6770;
	xor.b32  	%r6773, %r6772, %r6769;
	add.s32 	%r6774, %r6534, %r6745;
	add.s32 	%r6775, %r6774, %r6773;
	add.s32 	%r6776, %r6775, 1518500249;
	shf.l.wrap.b32 	%r6777, %r6776, %r6776, 5;
	xor.b32  	%r6778, %r6777, %r6761;
	xor.b32  	%r6779, %r6777, %r6769;
	and.b32  	%r6780, %r6779, %r6778;
	xor.b32  	%r6781, %r6780, %r6777;
	add.s32 	%r6782, %r6518, %r6753;
	add.s32 	%r6783, %r6782, %r6781;
	add.s32 	%r6784, %r6783, 1518500249;
	shf.l.wrap.b32 	%r6785, %r6784, %r6784, 9;
	xor.b32  	%r6786, %r6785, %r6769;
	xor.b32  	%r6787, %r6785, %r6777;
	and.b32  	%r6788, %r6787, %r6786;
	xor.b32  	%r6789, %r6788, %r6785;
	add.s32 	%r6790, %r6502, %r6761;
	add.s32 	%r6791, %r6790, %r6789;
	add.s32 	%r6792, %r6791, 1518500249;
	shf.l.wrap.b32 	%r6793, %r6792, %r6792, 13;
	xor.b32  	%r6794, %r6787, %r6793;
	add.s32 	%r6795, %r6562, %r6769;
	add.s32 	%r6796, %r6795, %r6794;
	add.s32 	%r6797, %r6796, 1859775393;
	shf.l.wrap.b32 	%r6798, %r6797, %r6797, 3;
	xor.b32  	%r6799, %r6793, %r6785;
	xor.b32  	%r6800, %r6799, %r6798;
	add.s32 	%r6801, %r6530, %r6777;
	add.s32 	%r6802, %r6801, %r6800;
	add.s32 	%r6803, %r6802, 1859775393;
	shf.l.wrap.b32 	%r6804, %r6803, %r6803, 9;
	xor.b32  	%r6805, %r6798, %r6793;
	xor.b32  	%r6806, %r6805, %r6804;
	add.s32 	%r6807, %r6546, %r6785;
	add.s32 	%r6808, %r6807, %r6806;
	add.s32 	%r6809, %r6808, 1859775393;
	shf.l.wrap.b32 	%r6810, %r6809, %r6809, 11;
	xor.b32  	%r6811, %r6804, %r6798;
	xor.b32  	%r6812, %r6811, %r6810;
	add.s32 	%r6813, %r6514, %r6793;
	add.s32 	%r6814, %r6813, %r6812;
	add.s32 	%r6815, %r6814, 1859775393;
	shf.l.wrap.b32 	%r6816, %r6815, %r6815, 15;
	xor.b32  	%r6817, %r6810, %r6804;
	xor.b32  	%r6818, %r6817, %r6816;
	add.s32 	%r6819, %r6554, %r6798;
	add.s32 	%r6820, %r6819, %r6818;
	add.s32 	%r6821, %r6820, 1859775393;
	shf.l.wrap.b32 	%r6822, %r6821, %r6821, 3;
	xor.b32  	%r6823, %r6816, %r6810;
	xor.b32  	%r6824, %r6823, %r6822;
	add.s32 	%r6825, %r6522, %r6804;
	add.s32 	%r6826, %r6825, %r6824;
	add.s32 	%r6827, %r6826, 1859775393;
	shf.l.wrap.b32 	%r6828, %r6827, %r6827, 9;
	xor.b32  	%r6829, %r6822, %r6816;
	xor.b32  	%r6830, %r6829, %r6828;
	add.s32 	%r6831, %r6538, %r6810;
	add.s32 	%r6832, %r6831, %r6830;
	add.s32 	%r6833, %r6832, 1859775393;
	shf.l.wrap.b32 	%r6834, %r6833, %r6833, 11;
	xor.b32  	%r6835, %r6828, %r6822;
	xor.b32  	%r6836, %r6835, %r6834;
	add.s32 	%r6837, %r6506, %r6816;
	add.s32 	%r6838, %r6837, %r6836;
	add.s32 	%r6839, %r6838, 1859775393;
	shf.l.wrap.b32 	%r6840, %r6839, %r6839, 15;
	xor.b32  	%r6841, %r6834, %r6828;
	xor.b32  	%r6842, %r6841, %r6840;
	add.s32 	%r6843, %r6558, %r6822;
	add.s32 	%r6844, %r6843, %r6842;
	add.s32 	%r6845, %r6844, 1859775393;
	shf.l.wrap.b32 	%r6846, %r6845, %r6845, 3;
	xor.b32  	%r6847, %r6840, %r6834;
	xor.b32  	%r6848, %r6847, %r6846;
	add.s32 	%r6849, %r6526, %r6828;
	add.s32 	%r6850, %r6849, %r6848;
	add.s32 	%r6851, %r6850, 1859775393;
	shf.l.wrap.b32 	%r6852, %r6851, %r6851, 9;
	xor.b32  	%r6853, %r6846, %r6840;
	xor.b32  	%r6854, %r6853, %r6852;
	add.s32 	%r6855, %r6542, %r6834;
	add.s32 	%r6856, %r6855, %r6854;
	add.s32 	%r6857, %r6856, 1859775393;
	shf.l.wrap.b32 	%r6858, %r6857, %r6857, 11;
	xor.b32  	%r6859, %r6852, %r6846;
	xor.b32  	%r6860, %r6859, %r6858;
	add.s32 	%r6861, %r6510, %r6840;
	add.s32 	%r6862, %r6861, %r6860;
	add.s32 	%r6863, %r6862, 1859775393;
	shf.l.wrap.b32 	%r6864, %r6863, %r6863, 15;
	xor.b32  	%r6865, %r6858, %r6852;
	xor.b32  	%r6866, %r6865, %r6864;
	add.s32 	%r6867, %r6550, %r6846;
	add.s32 	%r6868, %r6867, %r6866;
	add.s32 	%r6869, %r6868, 1859775393;
	shf.l.wrap.b32 	%r6870, %r6869, %r6869, 3;
	xor.b32  	%r6871, %r6864, %r6858;
	xor.b32  	%r6872, %r6871, %r6870;
	add.s32 	%r6873, %r6518, %r6852;
	add.s32 	%r6874, %r6873, %r6872;
	add.s32 	%r6875, %r6874, 1859775393;
	shf.l.wrap.b32 	%r6876, %r6875, %r6875, 9;
	xor.b32  	%r6877, %r6870, %r6864;
	xor.b32  	%r6878, %r6877, %r6876;
	add.s32 	%r6879, %r6534, %r6858;
	add.s32 	%r6880, %r6879, %r6878;
	add.s32 	%r6881, %r6880, 1859775393;
	shf.l.wrap.b32 	%r6882, %r6881, %r6881, 11;
	xor.b32  	%r6883, %r6876, %r6870;
	xor.b32  	%r6884, %r6883, %r6882;
	add.s32 	%r6885, %r6502, %r6864;
	add.s32 	%r6886, %r6885, %r6884;
	add.s32 	%r6887, %r6886, 1859775393;
	shf.l.wrap.b32 	%r6888, %r6887, %r6887, 15;
	add.s32 	%r11, %r6870, %r11;
	add.s32 	%r10, %r6888, %r10;
	add.s32 	%r9, %r6882, %r9;
	add.s32 	%r8, %r6876, %r8;
	add.s32 	%r6900, %r6900, 32;
	add.s32 	%r6901, %r6901, 8;

BB2_2:
	add.s32 	%r1019, %r6, -32;
	setp.lt.s32	%p2, %r6900, %r1019;
	mul.wide.s32 	%rd18, %r6901, 4;
	add.s64 	%rd19, %rd15, %rd18;
	ld.global.u32 	%r14, [%rd19];
	ld.global.u32 	%r15, [%rd19+4];
	ld.global.u32 	%r16, [%rd19+8];
	ld.global.u32 	%r17, [%rd19+12];
	ld.global.u32 	%r18, [%rd19+16];
	ld.global.u32 	%r19, [%rd19+20];
	ld.global.u32 	%r20, [%rd19+24];
	ld.global.u32 	%r21, [%rd19+28];
	@%p2 bra 	BB2_167;

	mov.u32 	%r6902, 0;
	mov.u32 	%r1079, 29554;
	// inline asm
	prmt.b32 %r1020, %r21, %r6902, %r1079;
	// inline asm
	mov.u32 	%r1083, 29040;
	// inline asm
	prmt.b32 %r1024, %r21, %r6902, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r1028, %r20, %r6902, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r1032, %r20, %r6902, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r1036, %r19, %r6902, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r1040, %r19, %r6902, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r1044, %r18, %r6902, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r1048, %r18, %r6902, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r1052, %r17, %r6902, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r1056, %r17, %r6902, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r1060, %r16, %r6902, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r1064, %r16, %r6902, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r1068, %r15, %r6902, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r1072, %r15, %r6902, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r1076, %r14, %r6902, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r1080, %r14, %r6902, %r1083;
	// inline asm
	sub.s32 	%r1084, %r6, %r6900;
	shl.b32 	%r1085, %r1084, 1;
	add.s32 	%r38, %r1085, %r6895;
	setp.lt.s32	%p3, %r1085, 64;
	@%p3 bra 	BB2_5;
	bra.uni 	BB2_4;

BB2_5:
	mov.u32 	%r1552, 30292;
	// inline asm
	prmt.b32 %r6902, %r1024, %r1020, %r1552;
	// inline asm
	// inline asm
	prmt.b32 %r6903, %r1028, %r1024, %r1552;
	// inline asm
	// inline asm
	prmt.b32 %r6904, %r1032, %r1028, %r1552;
	// inline asm
	// inline asm
	prmt.b32 %r6905, %r1036, %r1032, %r1552;
	// inline asm
	// inline asm
	prmt.b32 %r6906, %r1040, %r1036, %r1552;
	// inline asm
	// inline asm
	prmt.b32 %r6907, %r1044, %r1040, %r1552;
	// inline asm
	// inline asm
	prmt.b32 %r6908, %r1048, %r1044, %r1552;
	// inline asm
	// inline asm
	prmt.b32 %r6909, %r1052, %r1048, %r1552;
	// inline asm
	// inline asm
	prmt.b32 %r6910, %r1056, %r1052, %r1552;
	// inline asm
	// inline asm
	prmt.b32 %r6911, %r1060, %r1056, %r1552;
	// inline asm
	// inline asm
	prmt.b32 %r6912, %r1064, %r1060, %r1552;
	// inline asm
	// inline asm
	prmt.b32 %r6913, %r1068, %r1064, %r1552;
	// inline asm
	// inline asm
	prmt.b32 %r6914, %r1072, %r1068, %r1552;
	// inline asm
	// inline asm
	prmt.b32 %r6915, %r1076, %r1072, %r1552;
	// inline asm
	// inline asm
	prmt.b32 %r6916, %r1080, %r1076, %r1552;
	// inline asm
	mov.u32 	%r1550, 0;
	// inline asm
	prmt.b32 %r6917, %r1550, %r1080, %r1552;
	// inline asm
	bra.uni 	BB2_6;

BB2_4:
	// inline asm
	shf.r.wrap.b32 %r1086, %r1020, %r6902, %r6902;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1090, %r1024, %r1020, %r6902;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1094, %r1028, %r1024, %r6902;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1098, %r1032, %r1028, %r6902;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1102, %r1036, %r1032, %r6902;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1106, %r1040, %r1036, %r6902;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1110, %r1044, %r1040, %r6902;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1114, %r1048, %r1044, %r6902;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1118, %r1052, %r1048, %r6902;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1122, %r1056, %r1052, %r6902;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1126, %r1060, %r1056, %r6902;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1130, %r1064, %r1060, %r6902;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1134, %r1068, %r1064, %r6902;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1138, %r1072, %r1068, %r6902;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1142, %r1076, %r1072, %r6902;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1146, %r1080, %r1076, %r6902;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1150, %r6902, %r1080, %r6902;
	// inline asm
	xor.b32  	%r1170, %r9, %r8;
	and.b32  	%r1171, %r1170, %r10;
	xor.b32  	%r1172, %r1171, %r8;
	add.s32 	%r1173, %r1172, %r11;
	add.s32 	%r1174, %r1173, %r1146;
	shf.l.wrap.b32 	%r1175, %r1174, %r1174, 3;
	xor.b32  	%r1176, %r10, %r9;
	and.b32  	%r1177, %r1175, %r1176;
	xor.b32  	%r1178, %r1177, %r9;
	add.s32 	%r1179, %r1142, %r8;
	add.s32 	%r1180, %r1179, %r1178;
	shf.l.wrap.b32 	%r1181, %r1180, %r1180, 7;
	xor.b32  	%r1182, %r1175, %r10;
	and.b32  	%r1183, %r1182, %r1181;
	xor.b32  	%r1184, %r1183, %r10;
	add.s32 	%r1185, %r1138, %r9;
	add.s32 	%r1186, %r1185, %r1184;
	shf.l.wrap.b32 	%r1187, %r1186, %r1186, 11;
	xor.b32  	%r1188, %r1181, %r1175;
	and.b32  	%r1189, %r1188, %r1187;
	xor.b32  	%r1190, %r1189, %r1175;
	add.s32 	%r1191, %r1134, %r10;
	add.s32 	%r1192, %r1191, %r1190;
	shf.l.wrap.b32 	%r1193, %r1192, %r1192, 19;
	xor.b32  	%r1194, %r1187, %r1181;
	and.b32  	%r1195, %r1194, %r1193;
	xor.b32  	%r1196, %r1195, %r1181;
	add.s32 	%r1197, %r1175, %r1130;
	add.s32 	%r1198, %r1197, %r1196;
	shf.l.wrap.b32 	%r1199, %r1198, %r1198, 3;
	xor.b32  	%r1200, %r1193, %r1187;
	and.b32  	%r1201, %r1200, %r1199;
	xor.b32  	%r1202, %r1201, %r1187;
	add.s32 	%r1203, %r1181, %r1126;
	add.s32 	%r1204, %r1203, %r1202;
	shf.l.wrap.b32 	%r1205, %r1204, %r1204, 7;
	xor.b32  	%r1206, %r1199, %r1193;
	and.b32  	%r1207, %r1206, %r1205;
	xor.b32  	%r1208, %r1207, %r1193;
	add.s32 	%r1209, %r1187, %r1122;
	add.s32 	%r1210, %r1209, %r1208;
	shf.l.wrap.b32 	%r1211, %r1210, %r1210, 11;
	xor.b32  	%r1212, %r1205, %r1199;
	and.b32  	%r1213, %r1212, %r1211;
	xor.b32  	%r1214, %r1213, %r1199;
	add.s32 	%r1215, %r1193, %r1118;
	add.s32 	%r1216, %r1215, %r1214;
	shf.l.wrap.b32 	%r1217, %r1216, %r1216, 19;
	xor.b32  	%r1218, %r1211, %r1205;
	and.b32  	%r1219, %r1218, %r1217;
	xor.b32  	%r1220, %r1219, %r1205;
	add.s32 	%r1221, %r1199, %r1114;
	add.s32 	%r1222, %r1221, %r1220;
	shf.l.wrap.b32 	%r1223, %r1222, %r1222, 3;
	xor.b32  	%r1224, %r1217, %r1211;
	and.b32  	%r1225, %r1224, %r1223;
	xor.b32  	%r1226, %r1225, %r1211;
	add.s32 	%r1227, %r1205, %r1110;
	add.s32 	%r1228, %r1227, %r1226;
	shf.l.wrap.b32 	%r1229, %r1228, %r1228, 7;
	xor.b32  	%r1230, %r1223, %r1217;
	and.b32  	%r1231, %r1230, %r1229;
	xor.b32  	%r1232, %r1231, %r1217;
	add.s32 	%r1233, %r1211, %r1106;
	add.s32 	%r1234, %r1233, %r1232;
	shf.l.wrap.b32 	%r1235, %r1234, %r1234, 11;
	xor.b32  	%r1236, %r1229, %r1223;
	and.b32  	%r1237, %r1236, %r1235;
	xor.b32  	%r1238, %r1237, %r1223;
	add.s32 	%r1239, %r1217, %r1102;
	add.s32 	%r1240, %r1239, %r1238;
	shf.l.wrap.b32 	%r1241, %r1240, %r1240, 19;
	xor.b32  	%r1242, %r1235, %r1229;
	and.b32  	%r1243, %r1242, %r1241;
	xor.b32  	%r1244, %r1243, %r1229;
	add.s32 	%r1245, %r1223, %r1098;
	add.s32 	%r1246, %r1245, %r1244;
	shf.l.wrap.b32 	%r1247, %r1246, %r1246, 3;
	xor.b32  	%r1248, %r1241, %r1235;
	and.b32  	%r1249, %r1248, %r1247;
	xor.b32  	%r1250, %r1249, %r1235;
	add.s32 	%r1251, %r1229, %r1094;
	add.s32 	%r1252, %r1251, %r1250;
	shf.l.wrap.b32 	%r1253, %r1252, %r1252, 7;
	xor.b32  	%r1254, %r1247, %r1241;
	and.b32  	%r1255, %r1254, %r1253;
	xor.b32  	%r1256, %r1255, %r1241;
	add.s32 	%r1257, %r1235, %r1090;
	add.s32 	%r1258, %r1257, %r1256;
	shf.l.wrap.b32 	%r1259, %r1258, %r1258, 11;
	xor.b32  	%r1260, %r1253, %r1247;
	and.b32  	%r1261, %r1260, %r1259;
	xor.b32  	%r1262, %r1261, %r1247;
	add.s32 	%r1263, %r1241, %r1086;
	add.s32 	%r1264, %r1263, %r1262;
	shf.l.wrap.b32 	%r1265, %r1264, %r1264, 19;
	xor.b32  	%r1266, %r1265, %r1253;
	xor.b32  	%r1267, %r1265, %r1259;
	and.b32  	%r1268, %r1267, %r1266;
	xor.b32  	%r1269, %r1268, %r1265;
	add.s32 	%r1270, %r1146, %r1247;
	add.s32 	%r1271, %r1270, %r1269;
	add.s32 	%r1272, %r1271, 1518500249;
	shf.l.wrap.b32 	%r1273, %r1272, %r1272, 3;
	xor.b32  	%r1274, %r1273, %r1259;
	xor.b32  	%r1275, %r1273, %r1265;
	and.b32  	%r1276, %r1275, %r1274;
	xor.b32  	%r1277, %r1276, %r1273;
	add.s32 	%r1278, %r1130, %r1253;
	add.s32 	%r1279, %r1278, %r1277;
	add.s32 	%r1280, %r1279, 1518500249;
	shf.l.wrap.b32 	%r1281, %r1280, %r1280, 5;
	xor.b32  	%r1282, %r1281, %r1265;
	xor.b32  	%r1283, %r1281, %r1273;
	and.b32  	%r1284, %r1283, %r1282;
	xor.b32  	%r1285, %r1284, %r1281;
	add.s32 	%r1286, %r1114, %r1259;
	add.s32 	%r1287, %r1286, %r1285;
	add.s32 	%r1288, %r1287, 1518500249;
	shf.l.wrap.b32 	%r1289, %r1288, %r1288, 9;
	xor.b32  	%r1290, %r1289, %r1273;
	xor.b32  	%r1291, %r1289, %r1281;
	and.b32  	%r1292, %r1291, %r1290;
	xor.b32  	%r1293, %r1292, %r1289;
	add.s32 	%r1294, %r1098, %r1265;
	add.s32 	%r1295, %r1294, %r1293;
	add.s32 	%r1296, %r1295, 1518500249;
	shf.l.wrap.b32 	%r1297, %r1296, %r1296, 13;
	xor.b32  	%r1298, %r1297, %r1281;
	xor.b32  	%r1299, %r1297, %r1289;
	and.b32  	%r1300, %r1299, %r1298;
	xor.b32  	%r1301, %r1300, %r1297;
	add.s32 	%r1302, %r1142, %r1273;
	add.s32 	%r1303, %r1302, %r1301;
	add.s32 	%r1304, %r1303, 1518500249;
	shf.l.wrap.b32 	%r1305, %r1304, %r1304, 3;
	xor.b32  	%r1306, %r1305, %r1289;
	xor.b32  	%r1307, %r1305, %r1297;
	and.b32  	%r1308, %r1307, %r1306;
	xor.b32  	%r1309, %r1308, %r1305;
	add.s32 	%r1310, %r1126, %r1281;
	add.s32 	%r1311, %r1310, %r1309;
	add.s32 	%r1312, %r1311, 1518500249;
	shf.l.wrap.b32 	%r1313, %r1312, %r1312, 5;
	xor.b32  	%r1314, %r1313, %r1297;
	xor.b32  	%r1315, %r1313, %r1305;
	and.b32  	%r1316, %r1315, %r1314;
	xor.b32  	%r1317, %r1316, %r1313;
	add.s32 	%r1318, %r1110, %r1289;
	add.s32 	%r1319, %r1318, %r1317;
	add.s32 	%r1320, %r1319, 1518500249;
	shf.l.wrap.b32 	%r1321, %r1320, %r1320, 9;
	xor.b32  	%r1322, %r1321, %r1305;
	xor.b32  	%r1323, %r1321, %r1313;
	and.b32  	%r1324, %r1323, %r1322;
	xor.b32  	%r1325, %r1324, %r1321;
	add.s32 	%r1326, %r1094, %r1297;
	add.s32 	%r1327, %r1326, %r1325;
	add.s32 	%r1328, %r1327, 1518500249;
	shf.l.wrap.b32 	%r1329, %r1328, %r1328, 13;
	xor.b32  	%r1330, %r1329, %r1313;
	xor.b32  	%r1331, %r1329, %r1321;
	and.b32  	%r1332, %r1331, %r1330;
	xor.b32  	%r1333, %r1332, %r1329;
	add.s32 	%r1334, %r1138, %r1305;
	add.s32 	%r1335, %r1334, %r1333;
	add.s32 	%r1336, %r1335, 1518500249;
	shf.l.wrap.b32 	%r1337, %r1336, %r1336, 3;
	xor.b32  	%r1338, %r1337, %r1321;
	xor.b32  	%r1339, %r1337, %r1329;
	and.b32  	%r1340, %r1339, %r1338;
	xor.b32  	%r1341, %r1340, %r1337;
	add.s32 	%r1342, %r1122, %r1313;
	add.s32 	%r1343, %r1342, %r1341;
	add.s32 	%r1344, %r1343, 1518500249;
	shf.l.wrap.b32 	%r1345, %r1344, %r1344, 5;
	xor.b32  	%r1346, %r1345, %r1329;
	xor.b32  	%r1347, %r1345, %r1337;
	and.b32  	%r1348, %r1347, %r1346;
	xor.b32  	%r1349, %r1348, %r1345;
	add.s32 	%r1350, %r1106, %r1321;
	add.s32 	%r1351, %r1350, %r1349;
	add.s32 	%r1352, %r1351, 1518500249;
	shf.l.wrap.b32 	%r1353, %r1352, %r1352, 9;
	xor.b32  	%r1354, %r1353, %r1337;
	xor.b32  	%r1355, %r1353, %r1345;
	and.b32  	%r1356, %r1355, %r1354;
	xor.b32  	%r1357, %r1356, %r1353;
	add.s32 	%r1358, %r1090, %r1329;
	add.s32 	%r1359, %r1358, %r1357;
	add.s32 	%r1360, %r1359, 1518500249;
	shf.l.wrap.b32 	%r1361, %r1360, %r1360, 13;
	xor.b32  	%r1362, %r1361, %r1345;
	xor.b32  	%r1363, %r1361, %r1353;
	and.b32  	%r1364, %r1363, %r1362;
	xor.b32  	%r1365, %r1364, %r1361;
	add.s32 	%r1366, %r1134, %r1337;
	add.s32 	%r1367, %r1366, %r1365;
	add.s32 	%r1368, %r1367, 1518500249;
	shf.l.wrap.b32 	%r1369, %r1368, %r1368, 3;
	xor.b32  	%r1370, %r1369, %r1353;
	xor.b32  	%r1371, %r1369, %r1361;
	and.b32  	%r1372, %r1371, %r1370;
	xor.b32  	%r1373, %r1372, %r1369;
	add.s32 	%r1374, %r1118, %r1345;
	add.s32 	%r1375, %r1374, %r1373;
	add.s32 	%r1376, %r1375, 1518500249;
	shf.l.wrap.b32 	%r1377, %r1376, %r1376, 5;
	xor.b32  	%r1378, %r1377, %r1361;
	xor.b32  	%r1379, %r1377, %r1369;
	and.b32  	%r1380, %r1379, %r1378;
	xor.b32  	%r1381, %r1380, %r1377;
	add.s32 	%r1382, %r1102, %r1353;
	add.s32 	%r1383, %r1382, %r1381;
	add.s32 	%r1384, %r1383, 1518500249;
	shf.l.wrap.b32 	%r1385, %r1384, %r1384, 9;
	xor.b32  	%r1386, %r1385, %r1369;
	xor.b32  	%r1387, %r1385, %r1377;
	and.b32  	%r1388, %r1387, %r1386;
	xor.b32  	%r1389, %r1388, %r1385;
	add.s32 	%r1390, %r1086, %r1361;
	add.s32 	%r1391, %r1390, %r1389;
	add.s32 	%r1392, %r1391, 1518500249;
	shf.l.wrap.b32 	%r1393, %r1392, %r1392, 13;
	xor.b32  	%r1394, %r1387, %r1393;
	add.s32 	%r1395, %r1146, %r1369;
	add.s32 	%r1396, %r1395, %r1394;
	add.s32 	%r1397, %r1396, 1859775393;
	shf.l.wrap.b32 	%r1398, %r1397, %r1397, 3;
	xor.b32  	%r1399, %r1393, %r1385;
	xor.b32  	%r1400, %r1399, %r1398;
	add.s32 	%r1401, %r1114, %r1377;
	add.s32 	%r1402, %r1401, %r1400;
	add.s32 	%r1403, %r1402, 1859775393;
	shf.l.wrap.b32 	%r1404, %r1403, %r1403, 9;
	xor.b32  	%r1405, %r1398, %r1393;
	xor.b32  	%r1406, %r1405, %r1404;
	add.s32 	%r1407, %r1130, %r1385;
	add.s32 	%r1408, %r1407, %r1406;
	add.s32 	%r1409, %r1408, 1859775393;
	shf.l.wrap.b32 	%r1410, %r1409, %r1409, 11;
	xor.b32  	%r1411, %r1404, %r1398;
	xor.b32  	%r1412, %r1411, %r1410;
	add.s32 	%r1413, %r1098, %r1393;
	add.s32 	%r1414, %r1413, %r1412;
	add.s32 	%r1415, %r1414, 1859775393;
	shf.l.wrap.b32 	%r1416, %r1415, %r1415, 15;
	xor.b32  	%r1417, %r1410, %r1404;
	xor.b32  	%r1418, %r1417, %r1416;
	add.s32 	%r1419, %r1138, %r1398;
	add.s32 	%r1420, %r1419, %r1418;
	add.s32 	%r1421, %r1420, 1859775393;
	shf.l.wrap.b32 	%r1422, %r1421, %r1421, 3;
	xor.b32  	%r1423, %r1416, %r1410;
	xor.b32  	%r1424, %r1423, %r1422;
	add.s32 	%r1425, %r1106, %r1404;
	add.s32 	%r1426, %r1425, %r1424;
	add.s32 	%r1427, %r1426, 1859775393;
	shf.l.wrap.b32 	%r1428, %r1427, %r1427, 9;
	xor.b32  	%r1429, %r1422, %r1416;
	xor.b32  	%r1430, %r1429, %r1428;
	add.s32 	%r1431, %r1122, %r1410;
	add.s32 	%r1432, %r1431, %r1430;
	add.s32 	%r1433, %r1432, 1859775393;
	shf.l.wrap.b32 	%r1434, %r1433, %r1433, 11;
	xor.b32  	%r1435, %r1428, %r1422;
	xor.b32  	%r1436, %r1435, %r1434;
	add.s32 	%r1437, %r1090, %r1416;
	add.s32 	%r1438, %r1437, %r1436;
	add.s32 	%r1439, %r1438, 1859775393;
	shf.l.wrap.b32 	%r1440, %r1439, %r1439, 15;
	xor.b32  	%r1441, %r1434, %r1428;
	xor.b32  	%r1442, %r1441, %r1440;
	add.s32 	%r1443, %r1142, %r1422;
	add.s32 	%r1444, %r1443, %r1442;
	add.s32 	%r1445, %r1444, 1859775393;
	shf.l.wrap.b32 	%r1446, %r1445, %r1445, 3;
	xor.b32  	%r1447, %r1440, %r1434;
	xor.b32  	%r1448, %r1447, %r1446;
	add.s32 	%r1449, %r1110, %r1428;
	add.s32 	%r1450, %r1449, %r1448;
	add.s32 	%r1451, %r1450, 1859775393;
	shf.l.wrap.b32 	%r1452, %r1451, %r1451, 9;
	xor.b32  	%r1453, %r1446, %r1440;
	xor.b32  	%r1454, %r1453, %r1452;
	add.s32 	%r1455, %r1126, %r1434;
	add.s32 	%r1456, %r1455, %r1454;
	add.s32 	%r1457, %r1456, 1859775393;
	shf.l.wrap.b32 	%r1458, %r1457, %r1457, 11;
	xor.b32  	%r1459, %r1452, %r1446;
	xor.b32  	%r1460, %r1459, %r1458;
	add.s32 	%r1461, %r1094, %r1440;
	add.s32 	%r1462, %r1461, %r1460;
	add.s32 	%r1463, %r1462, 1859775393;
	shf.l.wrap.b32 	%r1464, %r1463, %r1463, 15;
	xor.b32  	%r1465, %r1458, %r1452;
	xor.b32  	%r1466, %r1465, %r1464;
	add.s32 	%r1467, %r1134, %r1446;
	add.s32 	%r1468, %r1467, %r1466;
	add.s32 	%r1469, %r1468, 1859775393;
	shf.l.wrap.b32 	%r1470, %r1469, %r1469, 3;
	xor.b32  	%r1471, %r1464, %r1458;
	xor.b32  	%r1472, %r1471, %r1470;
	add.s32 	%r1473, %r1102, %r1452;
	add.s32 	%r1474, %r1473, %r1472;
	add.s32 	%r1475, %r1474, 1859775393;
	shf.l.wrap.b32 	%r1476, %r1475, %r1475, 9;
	xor.b32  	%r1477, %r1470, %r1464;
	xor.b32  	%r1478, %r1477, %r1476;
	add.s32 	%r1479, %r1118, %r1458;
	add.s32 	%r1480, %r1479, %r1478;
	add.s32 	%r1481, %r1480, 1859775393;
	shf.l.wrap.b32 	%r1482, %r1481, %r1481, 11;
	xor.b32  	%r1483, %r1476, %r1470;
	xor.b32  	%r1484, %r1483, %r1482;
	add.s32 	%r1485, %r1086, %r1464;
	add.s32 	%r1486, %r1485, %r1484;
	add.s32 	%r1487, %r1486, 1859775393;
	shf.l.wrap.b32 	%r1488, %r1487, %r1487, 15;
	add.s32 	%r11, %r1470, %r11;
	add.s32 	%r10, %r1488, %r10;
	add.s32 	%r9, %r1482, %r9;
	add.s32 	%r8, %r1476, %r8;
	mov.u32 	%r6903, %r6902;
	mov.u32 	%r6904, %r6902;
	mov.u32 	%r6905, %r6902;
	mov.u32 	%r6906, %r6902;
	mov.u32 	%r6907, %r6902;
	mov.u32 	%r6908, %r6902;
	mov.u32 	%r6909, %r6902;
	mov.u32 	%r6910, %r6902;
	mov.u32 	%r6911, %r6902;
	mov.u32 	%r6912, %r6902;
	mov.u32 	%r6913, %r6902;
	mov.u32 	%r6914, %r6902;
	mov.u32 	%r6915, %r6902;
	mov.u32 	%r6916, %r6902;
	mov.u32 	%r6917, %r6902;

BB2_6:
	ld.param.u32 	%r6894, [m01000_sxx_param_30];
	setp.eq.s32	%p4, %r6894, 0;
	@%p4 bra 	BB2_119;

	ld.param.u64 	%rd34, [m01000_sxx_param_16];
	shl.b64 	%rd20, %rd1, 2;
	add.s64 	%rd2, %rd34, %rd20;
	mov.u32 	%r6922, 0;

BB2_8:
	mov.u32 	%r6944, 0;
	mul.wide.u32 	%rd21, %r6922, 260;
	add.s64 	%rd22, %rd5, %rd21;
	ld.global.u32 	%r80, [%rd22+256];
	mov.u32 	%r6923, %r38;
	mov.u32 	%r7040, %r6902;
	mov.u32 	%r7041, %r6903;
	mov.u32 	%r7042, %r6904;
	mov.u32 	%r7043, %r6905;
	mov.u32 	%r7036, %r6906;
	mov.u32 	%r7037, %r6907;
	mov.u32 	%r7038, %r6908;
	mov.u32 	%r7039, %r6909;
	mov.u32 	%r7032, %r6910;
	mov.u32 	%r7033, %r6911;
	mov.u32 	%r7034, %r6912;
	mov.u32 	%r7035, %r6913;
	mov.u32 	%r7063, %r6914;
	mov.u32 	%r7062, %r6915;
	mov.u32 	%r7061, %r6916;
	mov.u32 	%r7060, %r6917;
	mov.u32 	%r98, %r8;
	mov.u32 	%r99, %r9;
	mov.u32 	%r100, %r10;
	mov.u32 	%r101, %r11;
	mov.u32 	%r6945, %r6944;
	bra.uni 	BB2_9;

BB2_166:
	xor.b32  	%r6103, %r99, %r98;
	and.b32  	%r6104, %r6103, %r100;
	xor.b32  	%r6105, %r6104, %r98;
	add.s32 	%r6106, %r6105, %r101;
	or.b32  	%r6107, %r7047, %r97;
	add.s32 	%r6108, %r6106, %r6107;
	shf.l.wrap.b32 	%r6109, %r6108, %r6108, 3;
	xor.b32  	%r6110, %r100, %r99;
	and.b32  	%r6111, %r6109, %r6110;
	xor.b32  	%r6112, %r6111, %r99;
	or.b32  	%r6113, %r7046, %r96;
	add.s32 	%r6114, %r6113, %r98;
	add.s32 	%r6115, %r6114, %r6112;
	shf.l.wrap.b32 	%r6116, %r6115, %r6115, 7;
	xor.b32  	%r6117, %r6109, %r100;
	and.b32  	%r6118, %r6117, %r6116;
	xor.b32  	%r6119, %r6118, %r100;
	or.b32  	%r6120, %r7045, %r95;
	add.s32 	%r6121, %r6120, %r99;
	add.s32 	%r6122, %r6121, %r6119;
	shf.l.wrap.b32 	%r6123, %r6122, %r6122, 11;
	xor.b32  	%r6124, %r6116, %r6109;
	and.b32  	%r6125, %r6124, %r6123;
	xor.b32  	%r6126, %r6125, %r6109;
	or.b32  	%r6127, %r7044, %r94;
	add.s32 	%r6128, %r6127, %r100;
	add.s32 	%r6129, %r6128, %r6126;
	shf.l.wrap.b32 	%r6130, %r6129, %r6129, 19;
	xor.b32  	%r6131, %r6123, %r6116;
	and.b32  	%r6132, %r6131, %r6130;
	xor.b32  	%r6133, %r6132, %r6116;
	or.b32  	%r6134, %r7051, %r93;
	add.s32 	%r6135, %r6109, %r6134;
	add.s32 	%r6136, %r6135, %r6133;
	shf.l.wrap.b32 	%r6137, %r6136, %r6136, 3;
	xor.b32  	%r6138, %r6130, %r6123;
	and.b32  	%r6139, %r6138, %r6137;
	xor.b32  	%r6140, %r6139, %r6123;
	or.b32  	%r6141, %r7050, %r92;
	add.s32 	%r6142, %r6116, %r6141;
	add.s32 	%r6143, %r6142, %r6140;
	shf.l.wrap.b32 	%r6144, %r6143, %r6143, 7;
	xor.b32  	%r6145, %r6137, %r6130;
	and.b32  	%r6146, %r6145, %r6144;
	xor.b32  	%r6147, %r6146, %r6130;
	or.b32  	%r6148, %r7049, %r91;
	add.s32 	%r6149, %r6123, %r6148;
	add.s32 	%r6150, %r6149, %r6147;
	shf.l.wrap.b32 	%r6151, %r6150, %r6150, 11;
	xor.b32  	%r6152, %r6144, %r6137;
	and.b32  	%r6153, %r6152, %r6151;
	xor.b32  	%r6154, %r6153, %r6137;
	or.b32  	%r6155, %r7048, %r90;
	add.s32 	%r6156, %r6130, %r6155;
	add.s32 	%r6157, %r6156, %r6154;
	shf.l.wrap.b32 	%r6158, %r6157, %r6157, 19;
	xor.b32  	%r6159, %r6151, %r6144;
	and.b32  	%r6160, %r6159, %r6158;
	xor.b32  	%r6161, %r6160, %r6144;
	or.b32  	%r6162, %r7055, %r89;
	add.s32 	%r6163, %r6137, %r6162;
	add.s32 	%r6164, %r6163, %r6161;
	shf.l.wrap.b32 	%r6165, %r6164, %r6164, 3;
	xor.b32  	%r6166, %r6158, %r6151;
	and.b32  	%r6167, %r6166, %r6165;
	xor.b32  	%r6168, %r6167, %r6151;
	or.b32  	%r6169, %r7054, %r88;
	add.s32 	%r6170, %r6144, %r6169;
	add.s32 	%r6171, %r6170, %r6168;
	shf.l.wrap.b32 	%r6172, %r6171, %r6171, 7;
	xor.b32  	%r6173, %r6165, %r6158;
	and.b32  	%r6174, %r6173, %r6172;
	xor.b32  	%r6175, %r6174, %r6158;
	or.b32  	%r6176, %r7053, %r87;
	add.s32 	%r6177, %r6151, %r6176;
	add.s32 	%r6178, %r6177, %r6175;
	shf.l.wrap.b32 	%r6179, %r6178, %r6178, 11;
	xor.b32  	%r6180, %r6172, %r6165;
	and.b32  	%r6181, %r6180, %r6179;
	xor.b32  	%r6182, %r6181, %r6165;
	or.b32  	%r6183, %r7052, %r86;
	add.s32 	%r6184, %r6158, %r6183;
	add.s32 	%r6185, %r6184, %r6182;
	shf.l.wrap.b32 	%r6186, %r6185, %r6185, 19;
	xor.b32  	%r6187, %r6179, %r6172;
	and.b32  	%r6188, %r6187, %r6186;
	xor.b32  	%r6189, %r6188, %r6172;
	or.b32  	%r6190, %r7059, %r85;
	add.s32 	%r6191, %r6165, %r6190;
	add.s32 	%r6192, %r6191, %r6189;
	shf.l.wrap.b32 	%r6193, %r6192, %r6192, 3;
	xor.b32  	%r6194, %r6186, %r6179;
	and.b32  	%r6195, %r6194, %r6193;
	xor.b32  	%r6196, %r6195, %r6179;
	or.b32  	%r6197, %r7058, %r84;
	add.s32 	%r6198, %r6172, %r6197;
	add.s32 	%r6199, %r6198, %r6196;
	shf.l.wrap.b32 	%r6200, %r6199, %r6199, 7;
	xor.b32  	%r6201, %r6193, %r6186;
	and.b32  	%r6202, %r6201, %r6200;
	xor.b32  	%r6203, %r6202, %r6186;
	or.b32  	%r6204, %r7057, %r83;
	add.s32 	%r6205, %r6179, %r6204;
	add.s32 	%r6206, %r6205, %r6203;
	shf.l.wrap.b32 	%r6207, %r6206, %r6206, 11;
	xor.b32  	%r6208, %r6200, %r6193;
	and.b32  	%r6209, %r6208, %r6207;
	xor.b32  	%r6210, %r6209, %r6193;
	or.b32  	%r6211, %r7056, %r82;
	add.s32 	%r6212, %r6186, %r6211;
	add.s32 	%r6213, %r6212, %r6210;
	shf.l.wrap.b32 	%r6214, %r6213, %r6213, 19;
	xor.b32  	%r6215, %r6214, %r6200;
	xor.b32  	%r6216, %r6214, %r6207;
	and.b32  	%r6217, %r6216, %r6215;
	xor.b32  	%r6218, %r6217, %r6214;
	add.s32 	%r6219, %r6107, %r6193;
	add.s32 	%r6220, %r6219, %r6218;
	add.s32 	%r6221, %r6220, 1518500249;
	shf.l.wrap.b32 	%r6222, %r6221, %r6221, 3;
	xor.b32  	%r6223, %r6222, %r6207;
	xor.b32  	%r6224, %r6222, %r6214;
	and.b32  	%r6225, %r6224, %r6223;
	xor.b32  	%r6226, %r6225, %r6222;
	add.s32 	%r6227, %r6134, %r6200;
	add.s32 	%r6228, %r6227, %r6226;
	add.s32 	%r6229, %r6228, 1518500249;
	shf.l.wrap.b32 	%r6230, %r6229, %r6229, 5;
	xor.b32  	%r6231, %r6230, %r6214;
	xor.b32  	%r6232, %r6230, %r6222;
	and.b32  	%r6233, %r6232, %r6231;
	xor.b32  	%r6234, %r6233, %r6230;
	add.s32 	%r6235, %r6162, %r6207;
	add.s32 	%r6236, %r6235, %r6234;
	add.s32 	%r6237, %r6236, 1518500249;
	shf.l.wrap.b32 	%r6238, %r6237, %r6237, 9;
	xor.b32  	%r6239, %r6238, %r6222;
	xor.b32  	%r6240, %r6238, %r6230;
	and.b32  	%r6241, %r6240, %r6239;
	xor.b32  	%r6242, %r6241, %r6238;
	add.s32 	%r6243, %r6190, %r6214;
	add.s32 	%r6244, %r6243, %r6242;
	add.s32 	%r6245, %r6244, 1518500249;
	shf.l.wrap.b32 	%r6246, %r6245, %r6245, 13;
	xor.b32  	%r6247, %r6246, %r6230;
	xor.b32  	%r6248, %r6246, %r6238;
	and.b32  	%r6249, %r6248, %r6247;
	xor.b32  	%r6250, %r6249, %r6246;
	add.s32 	%r6251, %r6113, %r6222;
	add.s32 	%r6252, %r6251, %r6250;
	add.s32 	%r6253, %r6252, 1518500249;
	shf.l.wrap.b32 	%r6254, %r6253, %r6253, 3;
	xor.b32  	%r6255, %r6254, %r6238;
	xor.b32  	%r6256, %r6254, %r6246;
	and.b32  	%r6257, %r6256, %r6255;
	xor.b32  	%r6258, %r6257, %r6254;
	add.s32 	%r6259, %r6141, %r6230;
	add.s32 	%r6260, %r6259, %r6258;
	add.s32 	%r6261, %r6260, 1518500249;
	shf.l.wrap.b32 	%r6262, %r6261, %r6261, 5;
	xor.b32  	%r6263, %r6262, %r6246;
	xor.b32  	%r6264, %r6262, %r6254;
	and.b32  	%r6265, %r6264, %r6263;
	xor.b32  	%r6266, %r6265, %r6262;
	add.s32 	%r6267, %r6169, %r6238;
	add.s32 	%r6268, %r6267, %r6266;
	add.s32 	%r6269, %r6268, 1518500249;
	shf.l.wrap.b32 	%r6270, %r6269, %r6269, 9;
	xor.b32  	%r6271, %r6270, %r6254;
	xor.b32  	%r6272, %r6270, %r6262;
	and.b32  	%r6273, %r6272, %r6271;
	xor.b32  	%r6274, %r6273, %r6270;
	add.s32 	%r6275, %r6197, %r6246;
	add.s32 	%r6276, %r6275, %r6274;
	add.s32 	%r6277, %r6276, 1518500249;
	shf.l.wrap.b32 	%r6278, %r6277, %r6277, 13;
	xor.b32  	%r6279, %r6278, %r6262;
	xor.b32  	%r6280, %r6278, %r6270;
	and.b32  	%r6281, %r6280, %r6279;
	xor.b32  	%r6282, %r6281, %r6278;
	add.s32 	%r6283, %r6120, %r6254;
	add.s32 	%r6284, %r6283, %r6282;
	add.s32 	%r6285, %r6284, 1518500249;
	shf.l.wrap.b32 	%r6286, %r6285, %r6285, 3;
	xor.b32  	%r6287, %r6286, %r6270;
	xor.b32  	%r6288, %r6286, %r6278;
	and.b32  	%r6289, %r6288, %r6287;
	xor.b32  	%r6290, %r6289, %r6286;
	add.s32 	%r6291, %r6148, %r6262;
	add.s32 	%r6292, %r6291, %r6290;
	add.s32 	%r6293, %r6292, 1518500249;
	shf.l.wrap.b32 	%r6294, %r6293, %r6293, 5;
	xor.b32  	%r6295, %r6294, %r6278;
	xor.b32  	%r6296, %r6294, %r6286;
	and.b32  	%r6297, %r6296, %r6295;
	xor.b32  	%r6298, %r6297, %r6294;
	add.s32 	%r6299, %r6176, %r6270;
	add.s32 	%r6300, %r6299, %r6298;
	add.s32 	%r6301, %r6300, 1518500249;
	shf.l.wrap.b32 	%r6302, %r6301, %r6301, 9;
	xor.b32  	%r6303, %r6302, %r6286;
	xor.b32  	%r6304, %r6302, %r6294;
	and.b32  	%r6305, %r6304, %r6303;
	xor.b32  	%r6306, %r6305, %r6302;
	add.s32 	%r6307, %r6204, %r6278;
	add.s32 	%r6308, %r6307, %r6306;
	add.s32 	%r6309, %r6308, 1518500249;
	shf.l.wrap.b32 	%r6310, %r6309, %r6309, 13;
	xor.b32  	%r6311, %r6310, %r6294;
	xor.b32  	%r6312, %r6310, %r6302;
	and.b32  	%r6313, %r6312, %r6311;
	xor.b32  	%r6314, %r6313, %r6310;
	add.s32 	%r6315, %r6127, %r6286;
	add.s32 	%r6316, %r6315, %r6314;
	add.s32 	%r6317, %r6316, 1518500249;
	shf.l.wrap.b32 	%r6318, %r6317, %r6317, 3;
	xor.b32  	%r6319, %r6318, %r6302;
	xor.b32  	%r6320, %r6318, %r6310;
	and.b32  	%r6321, %r6320, %r6319;
	xor.b32  	%r6322, %r6321, %r6318;
	add.s32 	%r6323, %r6155, %r6294;
	add.s32 	%r6324, %r6323, %r6322;
	add.s32 	%r6325, %r6324, 1518500249;
	shf.l.wrap.b32 	%r6326, %r6325, %r6325, 5;
	xor.b32  	%r6327, %r6326, %r6310;
	xor.b32  	%r6328, %r6326, %r6318;
	and.b32  	%r6329, %r6328, %r6327;
	xor.b32  	%r6330, %r6329, %r6326;
	add.s32 	%r6331, %r6183, %r6302;
	add.s32 	%r6332, %r6331, %r6330;
	add.s32 	%r6333, %r6332, 1518500249;
	shf.l.wrap.b32 	%r6334, %r6333, %r6333, 9;
	xor.b32  	%r6335, %r6334, %r6318;
	xor.b32  	%r6336, %r6334, %r6326;
	and.b32  	%r6337, %r6336, %r6335;
	xor.b32  	%r6338, %r6337, %r6334;
	add.s32 	%r6339, %r6211, %r6310;
	add.s32 	%r6340, %r6339, %r6338;
	add.s32 	%r6341, %r6340, 1518500249;
	shf.l.wrap.b32 	%r6342, %r6341, %r6341, 13;
	xor.b32  	%r6343, %r6336, %r6342;
	add.s32 	%r6344, %r6107, %r6318;
	add.s32 	%r6345, %r6344, %r6343;
	add.s32 	%r6346, %r6345, 1859775393;
	shf.l.wrap.b32 	%r6347, %r6346, %r6346, 3;
	xor.b32  	%r6348, %r6342, %r6334;
	xor.b32  	%r6349, %r6348, %r6347;
	add.s32 	%r6350, %r6162, %r6326;
	add.s32 	%r6351, %r6350, %r6349;
	add.s32 	%r6352, %r6351, 1859775393;
	shf.l.wrap.b32 	%r6353, %r6352, %r6352, 9;
	xor.b32  	%r6354, %r6347, %r6342;
	xor.b32  	%r6355, %r6354, %r6353;
	add.s32 	%r6356, %r6134, %r6334;
	add.s32 	%r6357, %r6356, %r6355;
	add.s32 	%r6358, %r6357, 1859775393;
	shf.l.wrap.b32 	%r6359, %r6358, %r6358, 11;
	xor.b32  	%r6360, %r6353, %r6347;
	xor.b32  	%r6361, %r6360, %r6359;
	add.s32 	%r6362, %r6190, %r6342;
	add.s32 	%r6363, %r6362, %r6361;
	add.s32 	%r6364, %r6363, 1859775393;
	shf.l.wrap.b32 	%r6365, %r6364, %r6364, 15;
	xor.b32  	%r6366, %r6359, %r6353;
	xor.b32  	%r6367, %r6366, %r6365;
	add.s32 	%r6368, %r6120, %r6347;
	add.s32 	%r6369, %r6368, %r6367;
	add.s32 	%r6370, %r6369, 1859775393;
	shf.l.wrap.b32 	%r6371, %r6370, %r6370, 3;
	xor.b32  	%r6372, %r6365, %r6359;
	xor.b32  	%r6373, %r6372, %r6371;
	add.s32 	%r6374, %r6176, %r6353;
	add.s32 	%r6375, %r6374, %r6373;
	add.s32 	%r6376, %r6375, 1859775393;
	shf.l.wrap.b32 	%r6377, %r6376, %r6376, 9;
	xor.b32  	%r6378, %r6371, %r6365;
	xor.b32  	%r6379, %r6378, %r6377;
	add.s32 	%r6380, %r6148, %r6359;
	add.s32 	%r6381, %r6380, %r6379;
	add.s32 	%r6382, %r6381, 1859775393;
	shf.l.wrap.b32 	%r6383, %r6382, %r6382, 11;
	xor.b32  	%r6384, %r6377, %r6371;
	xor.b32  	%r6385, %r6384, %r6383;
	add.s32 	%r6386, %r6204, %r6365;
	add.s32 	%r6387, %r6386, %r6385;
	add.s32 	%r6388, %r6387, 1859775393;
	shf.l.wrap.b32 	%r6389, %r6388, %r6388, 15;
	xor.b32  	%r6390, %r6383, %r6377;
	xor.b32  	%r6391, %r6390, %r6389;
	add.s32 	%r6392, %r6113, %r6371;
	add.s32 	%r6393, %r6392, %r6391;
	add.s32 	%r6394, %r6393, 1859775393;
	shf.l.wrap.b32 	%r6395, %r6394, %r6394, 3;
	xor.b32  	%r6396, %r6389, %r6383;
	xor.b32  	%r6397, %r6396, %r6395;
	add.s32 	%r6398, %r6169, %r6377;
	add.s32 	%r6399, %r6398, %r6397;
	add.s32 	%r6400, %r6399, 1859775393;
	shf.l.wrap.b32 	%r6401, %r6400, %r6400, 9;
	xor.b32  	%r6402, %r6395, %r6389;
	xor.b32  	%r6403, %r6402, %r6401;
	add.s32 	%r6404, %r6141, %r6383;
	add.s32 	%r6405, %r6404, %r6403;
	add.s32 	%r6406, %r6405, 1859775393;
	shf.l.wrap.b32 	%r6407, %r6406, %r6406, 11;
	xor.b32  	%r6408, %r6401, %r6395;
	xor.b32  	%r6409, %r6408, %r6407;
	add.s32 	%r6410, %r6197, %r6389;
	add.s32 	%r6411, %r6410, %r6409;
	add.s32 	%r6412, %r6411, 1859775393;
	shf.l.wrap.b32 	%r6413, %r6412, %r6412, 15;
	xor.b32  	%r6414, %r6407, %r6401;
	xor.b32  	%r6415, %r6414, %r6413;
	add.s32 	%r6416, %r6127, %r6395;
	add.s32 	%r6417, %r6416, %r6415;
	add.s32 	%r6418, %r6417, 1859775393;
	shf.l.wrap.b32 	%r6419, %r6418, %r6418, 3;
	xor.b32  	%r6420, %r6413, %r6407;
	xor.b32  	%r6421, %r6420, %r6419;
	add.s32 	%r6422, %r6183, %r6401;
	add.s32 	%r6423, %r6422, %r6421;
	add.s32 	%r6424, %r6423, 1859775393;
	shf.l.wrap.b32 	%r6425, %r6424, %r6424, 9;
	xor.b32  	%r6426, %r6419, %r6413;
	xor.b32  	%r6427, %r6426, %r6425;
	add.s32 	%r6428, %r6155, %r6407;
	add.s32 	%r6429, %r6428, %r6427;
	add.s32 	%r6430, %r6429, 1859775393;
	shf.l.wrap.b32 	%r6431, %r6430, %r6430, 11;
	xor.b32  	%r6432, %r6425, %r6419;
	xor.b32  	%r6433, %r6432, %r6431;
	add.s32 	%r6434, %r6211, %r6413;
	add.s32 	%r6435, %r6434, %r6433;
	add.s32 	%r6436, %r6435, 1859775393;
	shf.l.wrap.b32 	%r6437, %r6436, %r6436, 15;
	add.s32 	%r101, %r6419, %r101;
	add.s32 	%r100, %r6437, %r100;
	add.s32 	%r99, %r6431, %r99;
	add.s32 	%r98, %r6425, %r98;
	add.s32 	%r6944, %r6944, 32;
	add.s32 	%r6945, %r6945, 8;
	add.s32 	%r6923, %r6923, 64;

BB2_9:
	mov.u32 	%r97, %r7060;
	mov.u32 	%r96, %r7061;
	mov.u32 	%r95, %r7062;
	mov.u32 	%r94, %r7063;
	mov.u32 	%r93, %r7035;
	mov.u32 	%r92, %r7034;
	mov.u32 	%r91, %r7033;
	mov.u32 	%r90, %r7032;
	mov.u32 	%r89, %r7039;
	mov.u32 	%r88, %r7038;
	mov.u32 	%r87, %r7037;
	mov.u32 	%r86, %r7036;
	mov.u32 	%r85, %r7043;
	mov.u32 	%r84, %r7042;
	mov.u32 	%r83, %r7041;
	mov.u32 	%r82, %r7040;
	cvt.u64.u32	%rd32, %r6922;
	add.s32 	%r1556, %r80, -32;
	setp.lt.s32	%p5, %r6944, %r1556;
	mul.lo.s64 	%rd23, %rd32, 260;
	add.s64 	%rd24, %rd5, %rd23;
	mul.wide.s32 	%rd25, %r6945, 4;
	add.s64 	%rd26, %rd24, %rd25;
	ld.global.u32 	%r104, [%rd26];
	ld.global.u32 	%r105, [%rd26+4];
	ld.global.u32 	%r106, [%rd26+8];
	ld.global.u32 	%r107, [%rd26+12];
	ld.global.u32 	%r108, [%rd26+16];
	ld.global.u32 	%r109, [%rd26+20];
	ld.global.u32 	%r110, [%rd26+24];
	ld.global.u32 	%r111, [%rd26+28];
	and.b32  	%r112, %r6923, 3;
	mov.u32 	%r1557, 4;
	sub.s32 	%r113, %r1557, %r112;
	@%p5 bra 	BB2_120;
	bra.uni 	BB2_10;

BB2_120:
	mov.u32 	%r7032, 0;
	// inline asm
	prmt.b32 %r7056, %r111, %r7032, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r7057, %r111, %r7032, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r7058, %r110, %r7032, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r7059, %r110, %r7032, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r7052, %r109, %r7032, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r7053, %r109, %r7032, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r7054, %r108, %r7032, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r7055, %r108, %r7032, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r7048, %r107, %r7032, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r7049, %r107, %r7032, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r7050, %r106, %r7032, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r7051, %r106, %r7032, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r4726, %r105, %r7032, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r7045, %r105, %r7032, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r7046, %r104, %r7032, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r7047, %r104, %r7032, %r1083;
	// inline asm
	bfe.u32 	%r4758, %r6923, 2, 4;
	setp.gt.s32	%p80, %r4758, 7;
	@%p80 bra 	BB2_136;

	setp.gt.s32	%p92, %r4758, 3;
	@%p92 bra 	BB2_129;

	setp.gt.s32	%p98, %r4758, 1;
	@%p98 bra 	BB2_126;

	setp.eq.s32	%p101, %r4758, 0;
	@%p101 bra 	BB2_162;
	bra.uni 	BB2_124;

BB2_162:
	and.b32  	%r6102, %r113, 3;
	shl.b32 	%r6086, %r6102, 3;
	mov.u32 	%r7032, 0;
	// inline asm
	shf.r.wrap.b32 %r6019, %r7056, %r7032, %r6086;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6023, %r7057, %r7056, %r6086;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6027, %r7058, %r7057, %r6086;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6031, %r7059, %r7058, %r6086;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6035, %r7052, %r7059, %r6086;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6039, %r7053, %r7052, %r6086;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6043, %r7054, %r7053, %r6086;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6047, %r7055, %r7054, %r6086;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6051, %r7048, %r7055, %r6086;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6055, %r7049, %r7048, %r6086;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6059, %r7050, %r7049, %r6086;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6063, %r7051, %r7050, %r6086;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6067, %r4726, %r7051, %r6086;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6071, %r7045, %r4726, %r6086;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6075, %r7046, %r7045, %r6086;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6079, %r7047, %r7046, %r6086;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6083, %r7032, %r7047, %r6086;
	// inline asm
	setp.eq.s32	%p118, %r112, 0;
	selp.b32	%r7044, %r6067, %r6071, %p118;
	selp.b32	%r7045, %r6071, %r6075, %p118;
	selp.b32	%r7046, %r6075, %r6079, %p118;
	selp.b32	%r7047, %r6079, %r6083, %p118;
	selp.b32	%r7048, %r6051, %r6055, %p118;
	selp.b32	%r7049, %r6055, %r6059, %p118;
	selp.b32	%r7050, %r6059, %r6063, %p118;
	selp.b32	%r7051, %r6063, %r6067, %p118;
	selp.b32	%r7052, %r6035, %r6039, %p118;
	selp.b32	%r7053, %r6039, %r6043, %p118;
	selp.b32	%r7054, %r6043, %r6047, %p118;
	selp.b32	%r7055, %r6047, %r6051, %p118;
	selp.b32	%r7056, %r6019, %r6023, %p118;
	selp.b32	%r7057, %r6023, %r6027, %p118;
	selp.b32	%r7058, %r6027, %r6031, %p118;
	selp.b32	%r7059, %r6031, %r6035, %p118;
	selp.b32	%r7060, 0, %r6019, %p118;
	mov.u32 	%r7033, %r7032;
	mov.u32 	%r7034, %r7032;
	mov.u32 	%r7035, %r7032;
	mov.u32 	%r7036, %r7032;
	mov.u32 	%r7037, %r7032;
	mov.u32 	%r7038, %r7032;
	mov.u32 	%r7039, %r7032;
	mov.u32 	%r7040, %r7032;
	mov.u32 	%r7041, %r7032;
	mov.u32 	%r7042, %r7032;
	mov.u32 	%r7043, %r7032;
	bra.uni 	BB2_163;

BB2_136:
	setp.gt.s32	%p81, %r4758, 11;
	@%p81 bra 	BB2_144;

	setp.gt.s32	%p87, %r4758, 9;
	@%p87 bra 	BB2_141;

	setp.eq.s32	%p90, %r4758, 8;
	@%p90 bra 	BB2_156;
	bra.uni 	BB2_139;

BB2_156:
	and.b32  	%r5430, %r113, 3;
	shl.b32 	%r5414, %r5430, 3;
	mov.u32 	%r7036, 0;
	// inline asm
	shf.r.wrap.b32 %r5347, %r7056, %r7036, %r5414;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5351, %r7057, %r7056, %r5414;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5355, %r7058, %r7057, %r5414;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5359, %r7059, %r7058, %r5414;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5363, %r7052, %r7059, %r5414;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5367, %r7053, %r7052, %r5414;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5371, %r7054, %r7053, %r5414;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5375, %r7055, %r7054, %r5414;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5379, %r7048, %r7055, %r5414;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5383, %r7049, %r7048, %r5414;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5387, %r7050, %r7049, %r5414;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5391, %r7051, %r7050, %r5414;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5395, %r4726, %r7051, %r5414;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5399, %r7045, %r4726, %r5414;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5403, %r7046, %r7045, %r5414;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5407, %r7047, %r7046, %r5414;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5411, %r7036, %r7047, %r5414;
	// inline asm
	setp.eq.s32	%p110, %r112, 0;
	selp.b32	%r7032, %r5347, %r5351, %p110;
	selp.b32	%r7033, %r5351, %r5355, %p110;
	selp.b32	%r7034, %r5355, %r5359, %p110;
	selp.b32	%r7035, %r5359, %r5363, %p110;
	selp.b32	%r7039, 0, %r5347, %p110;
	selp.b32	%r7052, %r5395, %r5399, %p110;
	selp.b32	%r7053, %r5399, %r5403, %p110;
	selp.b32	%r7054, %r5403, %r5407, %p110;
	selp.b32	%r7055, %r5407, %r5411, %p110;
	selp.b32	%r7056, %r5379, %r5383, %p110;
	selp.b32	%r7057, %r5383, %r5387, %p110;
	selp.b32	%r7058, %r5387, %r5391, %p110;
	selp.b32	%r7059, %r5391, %r5395, %p110;
	selp.b32	%r7060, %r5375, %r5379, %p110;
	selp.b32	%r7061, %r5371, %r5375, %p110;
	selp.b32	%r7062, %r5367, %r5371, %p110;
	selp.b32	%r7063, %r5363, %r5367, %p110;
	mov.u32 	%r7037, %r7036;
	mov.u32 	%r7038, %r7036;
	mov.u32 	%r7040, %r7036;
	mov.u32 	%r7041, %r7036;
	mov.u32 	%r7042, %r7036;
	mov.u32 	%r7043, %r7036;
	mov.u32 	%r7044, %r7036;
	mov.u32 	%r7045, %r7036;
	mov.u32 	%r7046, %r7036;
	mov.u32 	%r7047, %r7036;
	mov.u32 	%r7048, %r7036;
	bra.uni 	BB2_157;

BB2_129:
	setp.gt.s32	%p93, %r4758, 5;
	@%p93 bra 	BB2_133;

	setp.eq.s32	%p96, %r4758, 4;
	@%p96 bra 	BB2_159;
	bra.uni 	BB2_131;

BB2_159:
	and.b32  	%r5766, %r113, 3;
	shl.b32 	%r5750, %r5766, 3;
	mov.u32 	%r7032, 0;
	// inline asm
	shf.r.wrap.b32 %r5683, %r7056, %r7032, %r5750;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5687, %r7057, %r7056, %r5750;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5691, %r7058, %r7057, %r5750;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5695, %r7059, %r7058, %r5750;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5699, %r7052, %r7059, %r5750;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5703, %r7053, %r7052, %r5750;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5707, %r7054, %r7053, %r5750;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5711, %r7055, %r7054, %r5750;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5715, %r7048, %r7055, %r5750;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5719, %r7049, %r7048, %r5750;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5723, %r7050, %r7049, %r5750;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5727, %r7051, %r7050, %r5750;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5731, %r4726, %r7051, %r5750;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5735, %r7045, %r4726, %r5750;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5739, %r7046, %r7045, %r5750;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5743, %r7047, %r7046, %r5750;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5747, %r7032, %r7047, %r5750;
	// inline asm
	setp.eq.s32	%p114, %r112, 0;
	selp.b32	%r7035, 0, %r5683, %p114;
	selp.b32	%r7048, %r5731, %r5735, %p114;
	selp.b32	%r7049, %r5735, %r5739, %p114;
	selp.b32	%r7050, %r5739, %r5743, %p114;
	selp.b32	%r7051, %r5743, %r5747, %p114;
	selp.b32	%r7052, %r5715, %r5719, %p114;
	selp.b32	%r7053, %r5719, %r5723, %p114;
	selp.b32	%r7054, %r5723, %r5727, %p114;
	selp.b32	%r7055, %r5727, %r5731, %p114;
	selp.b32	%r7056, %r5699, %r5703, %p114;
	selp.b32	%r7057, %r5703, %r5707, %p114;
	selp.b32	%r7058, %r5707, %r5711, %p114;
	selp.b32	%r7059, %r5711, %r5715, %p114;
	selp.b32	%r7060, %r5695, %r5699, %p114;
	selp.b32	%r7061, %r5691, %r5695, %p114;
	selp.b32	%r7062, %r5687, %r5691, %p114;
	selp.b32	%r7063, %r5683, %r5687, %p114;
	mov.u32 	%r7033, %r7032;
	mov.u32 	%r7034, %r7032;
	mov.u32 	%r7036, %r7032;
	mov.u32 	%r7037, %r7032;
	mov.u32 	%r7038, %r7032;
	mov.u32 	%r7039, %r7032;
	mov.u32 	%r7040, %r7032;
	mov.u32 	%r7041, %r7032;
	mov.u32 	%r7042, %r7032;
	mov.u32 	%r7043, %r7032;
	mov.u32 	%r7044, %r7032;
	bra.uni 	BB2_160;

BB2_144:
	setp.gt.s32	%p82, %r4758, 13;
	@%p82 bra 	BB2_148;

	setp.eq.s32	%p85, %r4758, 12;
	@%p85 bra 	BB2_153;
	bra.uni 	BB2_146;

BB2_153:
	and.b32  	%r5094, %r113, 3;
	shl.b32 	%r5078, %r5094, 3;
	mov.u32 	%r7040, 0;
	// inline asm
	shf.r.wrap.b32 %r5011, %r7056, %r7040, %r5078;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5015, %r7057, %r7056, %r5078;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5019, %r7058, %r7057, %r5078;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5023, %r7059, %r7058, %r5078;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5027, %r7052, %r7059, %r5078;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5031, %r7053, %r7052, %r5078;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5035, %r7054, %r7053, %r5078;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5039, %r7055, %r7054, %r5078;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5043, %r7048, %r7055, %r5078;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5047, %r7049, %r7048, %r5078;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5051, %r7050, %r7049, %r5078;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5055, %r7051, %r7050, %r5078;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5059, %r4726, %r7051, %r5078;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5063, %r7045, %r4726, %r5078;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5067, %r7046, %r7045, %r5078;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5071, %r7047, %r7046, %r5078;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5075, %r7040, %r7047, %r5078;
	// inline asm
	setp.eq.s32	%p106, %r112, 0;
	selp.b32	%r7032, %r5027, %r5031, %p106;
	selp.b32	%r7033, %r5031, %r5035, %p106;
	selp.b32	%r7034, %r5035, %r5039, %p106;
	selp.b32	%r7035, %r5039, %r5043, %p106;
	selp.b32	%r7036, %r5011, %r5015, %p106;
	selp.b32	%r7037, %r5015, %r5019, %p106;
	selp.b32	%r7038, %r5019, %r5023, %p106;
	selp.b32	%r7039, %r5023, %r5027, %p106;
	selp.b32	%r7043, 0, %r5011, %p106;
	selp.b32	%r7056, %r5059, %r5063, %p106;
	selp.b32	%r7057, %r5063, %r5067, %p106;
	selp.b32	%r7058, %r5067, %r5071, %p106;
	selp.b32	%r7059, %r5071, %r5075, %p106;
	selp.b32	%r7060, %r5055, %r5059, %p106;
	selp.b32	%r7061, %r5051, %r5055, %p106;
	selp.b32	%r7062, %r5047, %r5051, %p106;
	selp.b32	%r7063, %r5043, %r5047, %p106;
	mov.u32 	%r7041, %r7040;
	mov.u32 	%r7042, %r7040;
	mov.u32 	%r7044, %r7040;
	mov.u32 	%r7045, %r7040;
	mov.u32 	%r7046, %r7040;
	mov.u32 	%r7047, %r7040;
	mov.u32 	%r7048, %r7040;
	mov.u32 	%r7049, %r7040;
	mov.u32 	%r7050, %r7040;
	mov.u32 	%r7051, %r7040;
	mov.u32 	%r7052, %r7040;
	bra.uni 	BB2_154;

BB2_126:
	setp.eq.s32	%p99, %r4758, 2;
	@%p99 bra 	BB2_161;
	bra.uni 	BB2_127;

BB2_161:
	and.b32  	%r5934, %r113, 3;
	shl.b32 	%r5918, %r5934, 3;
	mov.u32 	%r7032, 0;
	// inline asm
	shf.r.wrap.b32 %r5851, %r7056, %r7032, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5855, %r7057, %r7056, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5859, %r7058, %r7057, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5863, %r7059, %r7058, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5867, %r7052, %r7059, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5871, %r7053, %r7052, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5875, %r7054, %r7053, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5879, %r7055, %r7054, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5883, %r7048, %r7055, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5887, %r7049, %r7048, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5891, %r7050, %r7049, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5895, %r7051, %r7050, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5899, %r4726, %r7051, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5903, %r7045, %r4726, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5907, %r7046, %r7045, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5911, %r7047, %r7046, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5915, %r7032, %r7047, %r5918;
	// inline asm
	setp.eq.s32	%p116, %r112, 0;
	selp.b32	%r7044, %r5907, %r5911, %p116;
	selp.b32	%r7045, %r5911, %r5915, %p116;
	selp.b32	%r7048, %r5891, %r5895, %p116;
	selp.b32	%r7049, %r5895, %r5899, %p116;
	selp.b32	%r7050, %r5899, %r5903, %p116;
	selp.b32	%r7051, %r5903, %r5907, %p116;
	selp.b32	%r7052, %r5875, %r5879, %p116;
	selp.b32	%r7053, %r5879, %r5883, %p116;
	selp.b32	%r7054, %r5883, %r5887, %p116;
	selp.b32	%r7055, %r5887, %r5891, %p116;
	selp.b32	%r7056, %r5859, %r5863, %p116;
	selp.b32	%r7057, %r5863, %r5867, %p116;
	selp.b32	%r7058, %r5867, %r5871, %p116;
	selp.b32	%r7059, %r5871, %r5875, %p116;
	selp.b32	%r7060, %r5855, %r5859, %p116;
	selp.b32	%r7061, %r5851, %r5855, %p116;
	selp.b32	%r7062, 0, %r5851, %p116;
	mov.u32 	%r7033, %r7032;
	mov.u32 	%r7034, %r7032;
	mov.u32 	%r7035, %r7032;
	mov.u32 	%r7036, %r7032;
	mov.u32 	%r7037, %r7032;
	mov.u32 	%r7038, %r7032;
	mov.u32 	%r7039, %r7032;
	mov.u32 	%r7040, %r7032;
	mov.u32 	%r7041, %r7032;
	mov.u32 	%r7042, %r7032;
	mov.u32 	%r7043, %r7032;
	mov.u32 	%r7046, %r7032;
	mov.u32 	%r7047, %r7032;
	bra.uni 	BB2_165;

BB2_141:
	setp.eq.s32	%p88, %r4758, 10;
	@%p88 bra 	BB2_155;
	bra.uni 	BB2_142;

BB2_155:
	and.b32  	%r5262, %r113, 3;
	shl.b32 	%r5246, %r5262, 3;
	mov.u32 	%r7036, 0;
	// inline asm
	shf.r.wrap.b32 %r5179, %r7056, %r7036, %r5246;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5183, %r7057, %r7056, %r5246;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5187, %r7058, %r7057, %r5246;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5191, %r7059, %r7058, %r5246;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5195, %r7052, %r7059, %r5246;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5199, %r7053, %r7052, %r5246;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5203, %r7054, %r7053, %r5246;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5207, %r7055, %r7054, %r5246;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5211, %r7048, %r7055, %r5246;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5215, %r7049, %r7048, %r5246;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5219, %r7050, %r7049, %r5246;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5223, %r7051, %r7050, %r5246;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5227, %r4726, %r7051, %r5246;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5231, %r7045, %r4726, %r5246;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5235, %r7046, %r7045, %r5246;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5239, %r7047, %r7046, %r5246;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5243, %r7036, %r7047, %r5246;
	// inline asm
	setp.eq.s32	%p108, %r112, 0;
	selp.b32	%r7032, %r5187, %r5191, %p108;
	selp.b32	%r7033, %r5191, %r5195, %p108;
	selp.b32	%r7034, %r5195, %r5199, %p108;
	selp.b32	%r7035, %r5199, %r5203, %p108;
	selp.b32	%r7037, 0, %r5179, %p108;
	selp.b32	%r7038, %r5179, %r5183, %p108;
	selp.b32	%r7039, %r5183, %r5187, %p108;
	selp.b32	%r7052, %r5235, %r5239, %p108;
	selp.b32	%r7053, %r5239, %r5243, %p108;
	selp.b32	%r7056, %r5219, %r5223, %p108;
	selp.b32	%r7057, %r5223, %r5227, %p108;
	selp.b32	%r7058, %r5227, %r5231, %p108;
	selp.b32	%r7059, %r5231, %r5235, %p108;
	selp.b32	%r7060, %r5215, %r5219, %p108;
	selp.b32	%r7061, %r5211, %r5215, %p108;
	selp.b32	%r7062, %r5207, %r5211, %p108;
	selp.b32	%r7063, %r5203, %r5207, %p108;
	mov.u32 	%r7040, %r7036;
	mov.u32 	%r7041, %r7036;
	mov.u32 	%r7042, %r7036;
	mov.u32 	%r7043, %r7036;
	mov.u32 	%r7044, %r7036;
	mov.u32 	%r7045, %r7036;
	mov.u32 	%r7046, %r7036;
	mov.u32 	%r7047, %r7036;
	mov.u32 	%r7048, %r7036;
	mov.u32 	%r7049, %r7036;
	mov.u32 	%r7050, %r7036;
	mov.u32 	%r7051, %r7036;
	mov.u32 	%r7054, %r7036;
	mov.u32 	%r7055, %r7036;
	bra.uni 	BB2_166;

BB2_133:
	setp.eq.s32	%p94, %r4758, 6;
	@%p94 bra 	BB2_158;
	bra.uni 	BB2_134;

BB2_158:
	and.b32  	%r5598, %r113, 3;
	shl.b32 	%r5582, %r5598, 3;
	mov.u32 	%r7032, 0;
	// inline asm
	shf.r.wrap.b32 %r5515, %r7056, %r7032, %r5582;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5519, %r7057, %r7056, %r5582;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5523, %r7058, %r7057, %r5582;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5527, %r7059, %r7058, %r5582;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5531, %r7052, %r7059, %r5582;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5535, %r7053, %r7052, %r5582;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5539, %r7054, %r7053, %r5582;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5543, %r7055, %r7054, %r5582;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5547, %r7048, %r7055, %r5582;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5551, %r7049, %r7048, %r5582;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5555, %r7050, %r7049, %r5582;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5559, %r7051, %r7050, %r5582;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5563, %r4726, %r7051, %r5582;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5567, %r7045, %r4726, %r5582;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5571, %r7046, %r7045, %r5582;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5575, %r7047, %r7046, %r5582;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5579, %r7032, %r7047, %r5582;
	// inline asm
	setp.eq.s32	%p112, %r112, 0;
	selp.b32	%r7033, 0, %r5515, %p112;
	selp.b32	%r7034, %r5515, %r5519, %p112;
	selp.b32	%r7035, %r5519, %r5523, %p112;
	selp.b32	%r7048, %r5571, %r5575, %p112;
	selp.b32	%r7049, %r5575, %r5579, %p112;
	selp.b32	%r7052, %r5555, %r5559, %p112;
	selp.b32	%r7053, %r5559, %r5563, %p112;
	selp.b32	%r7054, %r5563, %r5567, %p112;
	selp.b32	%r7055, %r5567, %r5571, %p112;
	selp.b32	%r7056, %r5539, %r5543, %p112;
	selp.b32	%r7057, %r5543, %r5547, %p112;
	selp.b32	%r7058, %r5547, %r5551, %p112;
	selp.b32	%r7059, %r5551, %r5555, %p112;
	selp.b32	%r7060, %r5535, %r5539, %p112;
	selp.b32	%r7061, %r5531, %r5535, %p112;
	selp.b32	%r7062, %r5527, %r5531, %p112;
	selp.b32	%r7063, %r5523, %r5527, %p112;
	mov.u32 	%r7036, %r7032;
	mov.u32 	%r7037, %r7032;
	mov.u32 	%r7038, %r7032;
	mov.u32 	%r7039, %r7032;
	mov.u32 	%r7040, %r7032;
	mov.u32 	%r7041, %r7032;
	mov.u32 	%r7042, %r7032;
	mov.u32 	%r7043, %r7032;
	mov.u32 	%r7044, %r7032;
	mov.u32 	%r7045, %r7032;
	mov.u32 	%r7046, %r7032;
	mov.u32 	%r7047, %r7032;
	mov.u32 	%r7050, %r7032;
	mov.u32 	%r7051, %r7032;
	bra.uni 	BB2_166;

BB2_148:
	setp.eq.s32	%p83, %r4758, 14;
	@%p83 bra 	BB2_152;
	bra.uni 	BB2_149;

BB2_152:
	and.b32  	%r4926, %r113, 3;
	shl.b32 	%r4910, %r4926, 3;
	mov.u32 	%r7040, 0;
	// inline asm
	shf.r.wrap.b32 %r4843, %r7056, %r7040, %r4910;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4847, %r7057, %r7056, %r4910;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4851, %r7058, %r7057, %r4910;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4855, %r7059, %r7058, %r4910;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4859, %r7052, %r7059, %r4910;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4863, %r7053, %r7052, %r4910;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4867, %r7054, %r7053, %r4910;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4871, %r7055, %r7054, %r4910;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4875, %r7048, %r7055, %r4910;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4879, %r7049, %r7048, %r4910;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4883, %r7050, %r7049, %r4910;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4887, %r7051, %r7050, %r4910;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4891, %r4726, %r7051, %r4910;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4895, %r7045, %r4726, %r4910;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4899, %r7046, %r7045, %r4910;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4903, %r7047, %r7046, %r4910;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4907, %r7040, %r7047, %r4910;
	// inline asm
	setp.eq.s32	%p104, %r112, 0;
	selp.b32	%r7032, %r4867, %r4871, %p104;
	selp.b32	%r7033, %r4871, %r4875, %p104;
	selp.b32	%r7034, %r4875, %r4879, %p104;
	selp.b32	%r7035, %r4879, %r4883, %p104;
	selp.b32	%r7036, %r4851, %r4855, %p104;
	selp.b32	%r7037, %r4855, %r4859, %p104;
	selp.b32	%r7038, %r4859, %r4863, %p104;
	selp.b32	%r7039, %r4863, %r4867, %p104;
	selp.b32	%r7041, 0, %r4843, %p104;
	selp.b32	%r7042, %r4843, %r4847, %p104;
	selp.b32	%r7043, %r4847, %r4851, %p104;
	selp.b32	%r7056, %r4899, %r4903, %p104;
	selp.b32	%r7057, %r4903, %r4907, %p104;
	selp.b32	%r7060, %r4895, %r4899, %p104;
	selp.b32	%r7061, %r4891, %r4895, %p104;
	selp.b32	%r7062, %r4887, %r4891, %p104;
	selp.b32	%r7063, %r4883, %r4887, %p104;
	mov.u32 	%r7044, %r7040;
	mov.u32 	%r7045, %r7040;
	mov.u32 	%r7046, %r7040;
	mov.u32 	%r7047, %r7040;
	mov.u32 	%r7048, %r7040;
	mov.u32 	%r7049, %r7040;
	mov.u32 	%r7050, %r7040;
	mov.u32 	%r7051, %r7040;
	mov.u32 	%r7052, %r7040;
	mov.u32 	%r7053, %r7040;
	mov.u32 	%r7054, %r7040;
	mov.u32 	%r7055, %r7040;
	mov.u32 	%r7058, %r7040;
	mov.u32 	%r7059, %r7040;
	bra.uni 	BB2_166;

BB2_124:
	setp.eq.s32	%p102, %r4758, 1;
	@%p102 bra 	BB2_125;
	bra.uni 	BB2_150;

BB2_125:
	and.b32  	%r6018, %r113, 3;
	shl.b32 	%r6002, %r6018, 3;
	mov.u32 	%r7032, 0;
	// inline asm
	shf.r.wrap.b32 %r5935, %r7056, %r7032, %r6002;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5939, %r7057, %r7056, %r6002;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5943, %r7058, %r7057, %r6002;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5947, %r7059, %r7058, %r6002;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5951, %r7052, %r7059, %r6002;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5955, %r7053, %r7052, %r6002;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5959, %r7054, %r7053, %r6002;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5963, %r7055, %r7054, %r6002;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5967, %r7048, %r7055, %r6002;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5971, %r7049, %r7048, %r6002;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5975, %r7050, %r7049, %r6002;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5979, %r7051, %r7050, %r6002;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5983, %r4726, %r7051, %r6002;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5987, %r7045, %r4726, %r6002;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5991, %r7046, %r7045, %r6002;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5995, %r7047, %r7046, %r6002;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5999, %r7032, %r7047, %r6002;
	// inline asm
	setp.eq.s32	%p117, %r112, 0;
	selp.b32	%r7044, %r5987, %r5991, %p117;
	selp.b32	%r7045, %r5991, %r5995, %p117;
	selp.b32	%r7046, %r5995, %r5999, %p117;
	selp.b32	%r7048, %r5971, %r5975, %p117;
	selp.b32	%r7049, %r5975, %r5979, %p117;
	selp.b32	%r7050, %r5979, %r5983, %p117;
	selp.b32	%r7051, %r5983, %r5987, %p117;
	selp.b32	%r7052, %r5955, %r5959, %p117;
	selp.b32	%r7053, %r5959, %r5963, %p117;
	selp.b32	%r7054, %r5963, %r5967, %p117;
	selp.b32	%r7055, %r5967, %r5971, %p117;
	selp.b32	%r7056, %r5939, %r5943, %p117;
	selp.b32	%r7057, %r5943, %r5947, %p117;
	selp.b32	%r7058, %r5947, %r5951, %p117;
	selp.b32	%r7059, %r5951, %r5955, %p117;
	selp.b32	%r7060, %r5935, %r5939, %p117;
	selp.b32	%r7061, 0, %r5935, %p117;
	mov.u32 	%r7033, %r7032;
	mov.u32 	%r7034, %r7032;
	mov.u32 	%r7035, %r7032;
	mov.u32 	%r7036, %r7032;
	mov.u32 	%r7037, %r7032;
	mov.u32 	%r7038, %r7032;
	mov.u32 	%r7039, %r7032;
	mov.u32 	%r7040, %r7032;
	mov.u32 	%r7041, %r7032;
	mov.u32 	%r7042, %r7032;
	mov.u32 	%r7043, %r7032;
	mov.u32 	%r7047, %r7032;
	bra.uni 	BB2_164;

BB2_139:
	setp.eq.s32	%p91, %r4758, 9;
	@%p91 bra 	BB2_140;
	bra.uni 	BB2_150;

BB2_140:
	and.b32  	%r5346, %r113, 3;
	shl.b32 	%r5330, %r5346, 3;
	mov.u32 	%r7036, 0;
	// inline asm
	shf.r.wrap.b32 %r5263, %r7056, %r7036, %r5330;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5267, %r7057, %r7056, %r5330;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5271, %r7058, %r7057, %r5330;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5275, %r7059, %r7058, %r5330;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5279, %r7052, %r7059, %r5330;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5283, %r7053, %r7052, %r5330;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5287, %r7054, %r7053, %r5330;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5291, %r7055, %r7054, %r5330;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5295, %r7048, %r7055, %r5330;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5299, %r7049, %r7048, %r5330;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5303, %r7050, %r7049, %r5330;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5307, %r7051, %r7050, %r5330;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5311, %r4726, %r7051, %r5330;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5315, %r7045, %r4726, %r5330;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5319, %r7046, %r7045, %r5330;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5323, %r7047, %r7046, %r5330;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5327, %r7036, %r7047, %r5330;
	// inline asm
	setp.eq.s32	%p109, %r112, 0;
	selp.b32	%r7032, %r5267, %r5271, %p109;
	selp.b32	%r7033, %r5271, %r5275, %p109;
	selp.b32	%r7034, %r5275, %r5279, %p109;
	selp.b32	%r7035, %r5279, %r5283, %p109;
	selp.b32	%r7038, 0, %r5263, %p109;
	selp.b32	%r7039, %r5263, %r5267, %p109;
	selp.b32	%r7052, %r5315, %r5319, %p109;
	selp.b32	%r7053, %r5319, %r5323, %p109;
	selp.b32	%r7054, %r5323, %r5327, %p109;
	selp.b32	%r7056, %r5299, %r5303, %p109;
	selp.b32	%r7057, %r5303, %r5307, %p109;
	selp.b32	%r7058, %r5307, %r5311, %p109;
	selp.b32	%r7059, %r5311, %r5315, %p109;
	selp.b32	%r7060, %r5295, %r5299, %p109;
	selp.b32	%r7061, %r5291, %r5295, %p109;
	selp.b32	%r7062, %r5287, %r5291, %p109;
	selp.b32	%r7063, %r5283, %r5287, %p109;
	mov.u32 	%r7037, %r7036;
	mov.u32 	%r7040, %r7036;
	mov.u32 	%r7041, %r7036;
	mov.u32 	%r7042, %r7036;
	mov.u32 	%r7043, %r7036;
	mov.u32 	%r7044, %r7036;
	mov.u32 	%r7045, %r7036;
	mov.u32 	%r7046, %r7036;
	mov.u32 	%r7047, %r7036;
	mov.u32 	%r7048, %r7036;
	mov.u32 	%r7049, %r7036;
	mov.u32 	%r7050, %r7036;
	mov.u32 	%r7051, %r7036;
	mov.u32 	%r7055, %r7036;
	bra.uni 	BB2_166;

BB2_131:
	setp.eq.s32	%p97, %r4758, 5;
	@%p97 bra 	BB2_132;
	bra.uni 	BB2_150;

BB2_132:
	and.b32  	%r5682, %r113, 3;
	shl.b32 	%r5666, %r5682, 3;
	mov.u32 	%r7032, 0;
	// inline asm
	shf.r.wrap.b32 %r5599, %r7056, %r7032, %r5666;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5603, %r7057, %r7056, %r5666;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5607, %r7058, %r7057, %r5666;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5611, %r7059, %r7058, %r5666;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5615, %r7052, %r7059, %r5666;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5619, %r7053, %r7052, %r5666;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5623, %r7054, %r7053, %r5666;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5627, %r7055, %r7054, %r5666;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5631, %r7048, %r7055, %r5666;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5635, %r7049, %r7048, %r5666;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5639, %r7050, %r7049, %r5666;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5643, %r7051, %r7050, %r5666;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5647, %r4726, %r7051, %r5666;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5651, %r7045, %r4726, %r5666;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5655, %r7046, %r7045, %r5666;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5659, %r7047, %r7046, %r5666;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5663, %r7032, %r7047, %r5666;
	// inline asm
	setp.eq.s32	%p113, %r112, 0;
	selp.b32	%r7034, 0, %r5599, %p113;
	selp.b32	%r7035, %r5599, %r5603, %p113;
	selp.b32	%r7048, %r5651, %r5655, %p113;
	selp.b32	%r7049, %r5655, %r5659, %p113;
	selp.b32	%r7050, %r5659, %r5663, %p113;
	selp.b32	%r7052, %r5635, %r5639, %p113;
	selp.b32	%r7053, %r5639, %r5643, %p113;
	selp.b32	%r7054, %r5643, %r5647, %p113;
	selp.b32	%r7055, %r5647, %r5651, %p113;
	selp.b32	%r7056, %r5619, %r5623, %p113;
	selp.b32	%r7057, %r5623, %r5627, %p113;
	selp.b32	%r7058, %r5627, %r5631, %p113;
	selp.b32	%r7059, %r5631, %r5635, %p113;
	selp.b32	%r7060, %r5615, %r5619, %p113;
	selp.b32	%r7061, %r5611, %r5615, %p113;
	selp.b32	%r7062, %r5607, %r5611, %p113;
	selp.b32	%r7063, %r5603, %r5607, %p113;
	mov.u32 	%r7033, %r7032;
	mov.u32 	%r7036, %r7032;
	mov.u32 	%r7037, %r7032;
	mov.u32 	%r7038, %r7032;
	mov.u32 	%r7039, %r7032;
	mov.u32 	%r7040, %r7032;
	mov.u32 	%r7041, %r7032;
	mov.u32 	%r7042, %r7032;
	mov.u32 	%r7043, %r7032;
	mov.u32 	%r7044, %r7032;
	mov.u32 	%r7045, %r7032;
	mov.u32 	%r7046, %r7032;
	mov.u32 	%r7047, %r7032;
	mov.u32 	%r7051, %r7032;
	bra.uni 	BB2_166;

BB2_146:
	setp.eq.s32	%p86, %r4758, 13;
	@%p86 bra 	BB2_147;
	bra.uni 	BB2_150;

BB2_147:
	and.b32  	%r5010, %r113, 3;
	shl.b32 	%r4994, %r5010, 3;
	mov.u32 	%r7040, 0;
	// inline asm
	shf.r.wrap.b32 %r4927, %r7056, %r7040, %r4994;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4931, %r7057, %r7056, %r4994;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4935, %r7058, %r7057, %r4994;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4939, %r7059, %r7058, %r4994;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4943, %r7052, %r7059, %r4994;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4947, %r7053, %r7052, %r4994;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4951, %r7054, %r7053, %r4994;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4955, %r7055, %r7054, %r4994;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4959, %r7048, %r7055, %r4994;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4963, %r7049, %r7048, %r4994;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4967, %r7050, %r7049, %r4994;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4971, %r7051, %r7050, %r4994;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4975, %r4726, %r7051, %r4994;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4979, %r7045, %r4726, %r4994;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4983, %r7046, %r7045, %r4994;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4987, %r7047, %r7046, %r4994;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4991, %r7040, %r7047, %r4994;
	// inline asm
	setp.eq.s32	%p105, %r112, 0;
	selp.b32	%r7032, %r4947, %r4951, %p105;
	selp.b32	%r7033, %r4951, %r4955, %p105;
	selp.b32	%r7034, %r4955, %r4959, %p105;
	selp.b32	%r7035, %r4959, %r4963, %p105;
	selp.b32	%r7036, %r4931, %r4935, %p105;
	selp.b32	%r7037, %r4935, %r4939, %p105;
	selp.b32	%r7038, %r4939, %r4943, %p105;
	selp.b32	%r7039, %r4943, %r4947, %p105;
	selp.b32	%r7042, 0, %r4927, %p105;
	selp.b32	%r7043, %r4927, %r4931, %p105;
	selp.b32	%r7056, %r4979, %r4983, %p105;
	selp.b32	%r7057, %r4983, %r4987, %p105;
	selp.b32	%r7058, %r4987, %r4991, %p105;
	selp.b32	%r7060, %r4975, %r4979, %p105;
	selp.b32	%r7061, %r4971, %r4975, %p105;
	selp.b32	%r7062, %r4967, %r4971, %p105;
	selp.b32	%r7063, %r4963, %r4967, %p105;
	mov.u32 	%r7041, %r7040;
	mov.u32 	%r7044, %r7040;
	mov.u32 	%r7045, %r7040;
	mov.u32 	%r7046, %r7040;
	mov.u32 	%r7047, %r7040;
	mov.u32 	%r7048, %r7040;
	mov.u32 	%r7049, %r7040;
	mov.u32 	%r7050, %r7040;
	mov.u32 	%r7051, %r7040;
	mov.u32 	%r7052, %r7040;
	mov.u32 	%r7053, %r7040;
	mov.u32 	%r7054, %r7040;
	mov.u32 	%r7055, %r7040;
	mov.u32 	%r7059, %r7040;
	bra.uni 	BB2_166;

BB2_127:
	setp.eq.s32	%p100, %r4758, 3;
	@%p100 bra 	BB2_128;
	bra.uni 	BB2_150;

BB2_128:
	and.b32  	%r5850, %r113, 3;
	shl.b32 	%r5834, %r5850, 3;
	mov.u32 	%r7032, 0;
	// inline asm
	shf.r.wrap.b32 %r5767, %r7056, %r7032, %r5834;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5771, %r7057, %r7056, %r5834;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5775, %r7058, %r7057, %r5834;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5779, %r7059, %r7058, %r5834;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5783, %r7052, %r7059, %r5834;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5787, %r7053, %r7052, %r5834;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5791, %r7054, %r7053, %r5834;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5795, %r7055, %r7054, %r5834;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5799, %r7048, %r7055, %r5834;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5803, %r7049, %r7048, %r5834;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5807, %r7050, %r7049, %r5834;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5811, %r7051, %r7050, %r5834;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5815, %r4726, %r7051, %r5834;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5819, %r7045, %r4726, %r5834;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5823, %r7046, %r7045, %r5834;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5827, %r7047, %r7046, %r5834;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5831, %r7032, %r7047, %r5834;
	// inline asm
	setp.eq.s32	%p115, %r112, 0;
	selp.b32	%r7044, %r5827, %r5831, %p115;
	selp.b32	%r7048, %r5811, %r5815, %p115;
	selp.b32	%r7049, %r5815, %r5819, %p115;
	selp.b32	%r7050, %r5819, %r5823, %p115;
	selp.b32	%r7051, %r5823, %r5827, %p115;
	selp.b32	%r7052, %r5795, %r5799, %p115;
	selp.b32	%r7053, %r5799, %r5803, %p115;
	selp.b32	%r7054, %r5803, %r5807, %p115;
	selp.b32	%r7055, %r5807, %r5811, %p115;
	selp.b32	%r7056, %r5779, %r5783, %p115;
	selp.b32	%r7057, %r5783, %r5787, %p115;
	selp.b32	%r7058, %r5787, %r5791, %p115;
	selp.b32	%r7059, %r5791, %r5795, %p115;
	selp.b32	%r7060, %r5775, %r5779, %p115;
	selp.b32	%r7061, %r5771, %r5775, %p115;
	selp.b32	%r7062, %r5767, %r5771, %p115;
	selp.b32	%r7063, 0, %r5767, %p115;
	mov.u32 	%r7033, %r7032;
	mov.u32 	%r7034, %r7032;
	mov.u32 	%r7035, %r7032;
	mov.u32 	%r7036, %r7032;
	mov.u32 	%r7037, %r7032;
	mov.u32 	%r7038, %r7032;
	mov.u32 	%r7039, %r7032;
	mov.u32 	%r7040, %r7032;
	mov.u32 	%r7041, %r7032;
	mov.u32 	%r7042, %r7032;
	mov.u32 	%r7043, %r7032;

BB2_160:
	mov.u32 	%r7045, %r7032;
	mov.u32 	%r7046, %r7032;
	mov.u32 	%r7047, %r7032;
	bra.uni 	BB2_166;

BB2_142:
	setp.eq.s32	%p89, %r4758, 11;
	@%p89 bra 	BB2_143;
	bra.uni 	BB2_150;

BB2_143:
	and.b32  	%r5178, %r113, 3;
	shl.b32 	%r5162, %r5178, 3;
	mov.u32 	%r7040, 0;
	// inline asm
	shf.r.wrap.b32 %r5095, %r7056, %r7040, %r5162;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5099, %r7057, %r7056, %r5162;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5103, %r7058, %r7057, %r5162;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5107, %r7059, %r7058, %r5162;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5111, %r7052, %r7059, %r5162;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5115, %r7053, %r7052, %r5162;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5119, %r7054, %r7053, %r5162;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5123, %r7055, %r7054, %r5162;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5127, %r7048, %r7055, %r5162;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5131, %r7049, %r7048, %r5162;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5135, %r7050, %r7049, %r5162;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5139, %r7051, %r7050, %r5162;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5143, %r4726, %r7051, %r5162;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5147, %r7045, %r4726, %r5162;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5151, %r7046, %r7045, %r5162;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5155, %r7047, %r7046, %r5162;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5159, %r7040, %r7047, %r5162;
	// inline asm
	setp.eq.s32	%p107, %r112, 0;
	selp.b32	%r7032, %r5107, %r5111, %p107;
	selp.b32	%r7033, %r5111, %r5115, %p107;
	selp.b32	%r7034, %r5115, %r5119, %p107;
	selp.b32	%r7035, %r5119, %r5123, %p107;
	selp.b32	%r7036, 0, %r5095, %p107;
	selp.b32	%r7037, %r5095, %r5099, %p107;
	selp.b32	%r7038, %r5099, %r5103, %p107;
	selp.b32	%r7039, %r5103, %r5107, %p107;
	selp.b32	%r7052, %r5155, %r5159, %p107;
	selp.b32	%r7056, %r5139, %r5143, %p107;
	selp.b32	%r7057, %r5143, %r5147, %p107;
	selp.b32	%r7058, %r5147, %r5151, %p107;
	selp.b32	%r7059, %r5151, %r5155, %p107;
	selp.b32	%r7060, %r5135, %r5139, %p107;
	selp.b32	%r7061, %r5131, %r5135, %p107;
	selp.b32	%r7062, %r5127, %r5131, %p107;
	selp.b32	%r7063, %r5123, %r5127, %p107;
	mov.u32 	%r7041, %r7040;
	mov.u32 	%r7042, %r7040;
	mov.u32 	%r7043, %r7040;
	mov.u32 	%r7044, %r7040;
	mov.u32 	%r7045, %r7040;
	mov.u32 	%r7046, %r7040;
	mov.u32 	%r7047, %r7040;
	mov.u32 	%r7048, %r7040;
	mov.u32 	%r7049, %r7040;
	mov.u32 	%r7050, %r7040;
	mov.u32 	%r7051, %r7040;

BB2_154:
	mov.u32 	%r7053, %r7040;
	mov.u32 	%r7054, %r7040;
	mov.u32 	%r7055, %r7040;
	bra.uni 	BB2_166;

BB2_134:
	setp.eq.s32	%p95, %r4758, 7;
	@%p95 bra 	BB2_135;
	bra.uni 	BB2_150;

BB2_135:
	and.b32  	%r5514, %r113, 3;
	shl.b32 	%r5498, %r5514, 3;
	mov.u32 	%r7036, 0;
	// inline asm
	shf.r.wrap.b32 %r5431, %r7056, %r7036, %r5498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5435, %r7057, %r7056, %r5498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5439, %r7058, %r7057, %r5498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5443, %r7059, %r7058, %r5498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5447, %r7052, %r7059, %r5498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5451, %r7053, %r7052, %r5498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5455, %r7054, %r7053, %r5498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5459, %r7055, %r7054, %r5498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5463, %r7048, %r7055, %r5498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5467, %r7049, %r7048, %r5498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5471, %r7050, %r7049, %r5498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5475, %r7051, %r7050, %r5498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5479, %r4726, %r7051, %r5498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5483, %r7045, %r4726, %r5498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5487, %r7046, %r7045, %r5498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5491, %r7047, %r7046, %r5498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r5495, %r7036, %r7047, %r5498;
	// inline asm
	setp.eq.s32	%p111, %r112, 0;
	selp.b32	%r7032, 0, %r5431, %p111;
	selp.b32	%r7033, %r5431, %r5435, %p111;
	selp.b32	%r7034, %r5435, %r5439, %p111;
	selp.b32	%r7035, %r5439, %r5443, %p111;
	selp.b32	%r7048, %r5491, %r5495, %p111;
	selp.b32	%r7052, %r5475, %r5479, %p111;
	selp.b32	%r7053, %r5479, %r5483, %p111;
	selp.b32	%r7054, %r5483, %r5487, %p111;
	selp.b32	%r7055, %r5487, %r5491, %p111;
	selp.b32	%r7056, %r5459, %r5463, %p111;
	selp.b32	%r7057, %r5463, %r5467, %p111;
	selp.b32	%r7058, %r5467, %r5471, %p111;
	selp.b32	%r7059, %r5471, %r5475, %p111;
	selp.b32	%r7060, %r5455, %r5459, %p111;
	selp.b32	%r7061, %r5451, %r5455, %p111;
	selp.b32	%r7062, %r5447, %r5451, %p111;
	selp.b32	%r7063, %r5443, %r5447, %p111;
	mov.u32 	%r7037, %r7036;
	mov.u32 	%r7038, %r7036;
	mov.u32 	%r7039, %r7036;
	mov.u32 	%r7040, %r7036;
	mov.u32 	%r7041, %r7036;
	mov.u32 	%r7042, %r7036;
	mov.u32 	%r7043, %r7036;
	mov.u32 	%r7044, %r7036;
	mov.u32 	%r7045, %r7036;
	mov.u32 	%r7046, %r7036;
	mov.u32 	%r7047, %r7036;

BB2_157:
	mov.u32 	%r7049, %r7036;
	mov.u32 	%r7050, %r7036;
	mov.u32 	%r7051, %r7036;
	bra.uni 	BB2_166;

BB2_149:
	setp.ne.s32	%p84, %r4758, 15;
	@%p84 bra 	BB2_150;

	and.b32  	%r4842, %r113, 3;
	shl.b32 	%r4826, %r4842, 3;
	mov.u32 	%r7044, 0;
	// inline asm
	shf.r.wrap.b32 %r4759, %r7056, %r7044, %r4826;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4763, %r7057, %r7056, %r4826;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4767, %r7058, %r7057, %r4826;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4771, %r7059, %r7058, %r4826;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4775, %r7052, %r7059, %r4826;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4779, %r7053, %r7052, %r4826;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4783, %r7054, %r7053, %r4826;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4787, %r7055, %r7054, %r4826;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4791, %r7048, %r7055, %r4826;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4795, %r7049, %r7048, %r4826;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4799, %r7050, %r7049, %r4826;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4803, %r7051, %r7050, %r4826;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4807, %r4726, %r7051, %r4826;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4811, %r7045, %r4726, %r4826;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4815, %r7046, %r7045, %r4826;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4819, %r7047, %r7046, %r4826;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4823, %r7044, %r7047, %r4826;
	// inline asm
	setp.eq.s32	%p103, %r112, 0;
	selp.b32	%r7032, %r4787, %r4791, %p103;
	selp.b32	%r7033, %r4791, %r4795, %p103;
	selp.b32	%r7034, %r4795, %r4799, %p103;
	selp.b32	%r7035, %r4799, %r4803, %p103;
	selp.b32	%r7036, %r4771, %r4775, %p103;
	selp.b32	%r7037, %r4775, %r4779, %p103;
	selp.b32	%r7038, %r4779, %r4783, %p103;
	selp.b32	%r7039, %r4783, %r4787, %p103;
	selp.b32	%r7040, 0, %r4759, %p103;
	selp.b32	%r7041, %r4759, %r4763, %p103;
	selp.b32	%r7042, %r4763, %r4767, %p103;
	selp.b32	%r7043, %r4767, %r4771, %p103;
	selp.b32	%r7056, %r4819, %r4823, %p103;
	selp.b32	%r7060, %r4815, %r4819, %p103;
	selp.b32	%r7061, %r4811, %r4815, %p103;
	selp.b32	%r7062, %r4807, %r4811, %p103;
	selp.b32	%r7063, %r4803, %r4807, %p103;
	mov.u32 	%r7045, %r7044;
	mov.u32 	%r7046, %r7044;
	mov.u32 	%r7047, %r7044;
	mov.u32 	%r7048, %r7044;
	mov.u32 	%r7049, %r7044;
	mov.u32 	%r7050, %r7044;
	mov.u32 	%r7051, %r7044;
	mov.u32 	%r7052, %r7044;
	mov.u32 	%r7053, %r7044;
	mov.u32 	%r7054, %r7044;
	mov.u32 	%r7055, %r7044;
	mov.u32 	%r7057, %r7044;
	mov.u32 	%r7058, %r7044;
	mov.u32 	%r7059, %r7044;
	bra.uni 	BB2_166;

BB2_150:
	mov.u32 	%r7033, %r7032;
	mov.u32 	%r7034, %r7032;
	mov.u32 	%r7035, %r7032;
	mov.u32 	%r7036, %r7032;
	mov.u32 	%r7037, %r7032;
	mov.u32 	%r7038, %r7032;
	mov.u32 	%r7039, %r7032;
	mov.u32 	%r7040, %r7032;
	mov.u32 	%r7041, %r7032;
	mov.u32 	%r7042, %r7032;
	mov.u32 	%r7043, %r7032;
	mov.u32 	%r7044, %r4726;
	mov.u32 	%r7060, %r7032;

BB2_163:
	mov.u32 	%r7061, %r7032;

BB2_164:
	mov.u32 	%r7062, %r7032;

BB2_165:
	mov.u32 	%r7063, %r7032;
	bra.uni 	BB2_166;

BB2_10:
	mov.u32 	%r6946, 0;
	// inline asm
	prmt.b32 %r1558, %r111, %r6946, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r1562, %r111, %r6946, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r1566, %r110, %r6946, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r1570, %r110, %r6946, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r1574, %r109, %r6946, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r1578, %r109, %r6946, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r1582, %r108, %r6946, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r1586, %r108, %r6946, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r1590, %r107, %r6946, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r1594, %r107, %r6946, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r1598, %r106, %r6946, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r1602, %r106, %r6946, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r1606, %r105, %r6946, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r1610, %r105, %r6946, %r1083;
	// inline asm
	// inline asm
	prmt.b32 %r1614, %r104, %r6946, %r1079;
	// inline asm
	// inline asm
	prmt.b32 %r1618, %r104, %r6946, %r1083;
	// inline asm
	sub.s32 	%r1622, %r80, %r6944;
	shl.b32 	%r1623, %r1622, 1;
	add.s32 	%r130, %r1623, %r6923;
	and.b32  	%r1624, %r6923, 63;
	add.s32 	%r1625, %r1623, %r1624;
	setp.lt.s32	%p6, %r1625, 64;
	bfe.u32 	%r131, %r6923, 2, 4;
	@%p6 bra 	BB2_58;
	bra.uni 	BB2_11;

BB2_58:
	shl.b32 	%r3321, %r113, 2;
	mov.u32 	%r3322, 1985229328;
	shr.u32 	%r3323, %r3322, %r3321;
	and.b32  	%r440, %r3323, 65535;
	setp.gt.s32	%p46, %r131, 7;
	@%p46 bra 	BB2_74;

	setp.gt.s32	%p58, %r131, 3;
	@%p58 bra 	BB2_67;

	setp.gt.s32	%p64, %r131, 1;
	@%p64 bra 	BB2_64;

	setp.eq.s32	%p67, %r131, 0;
	@%p67 bra 	BB2_109;
	bra.uni 	BB2_62;

BB2_109:
	// inline asm
	prmt.b32 %r1558, %r1562, %r1558, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1562, %r1566, %r1562, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1566, %r1570, %r1566, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1570, %r1574, %r1570, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1574, %r1578, %r1574, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1578, %r1582, %r1578, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1582, %r1586, %r1582, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1586, %r1590, %r1586, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1590, %r1594, %r1590, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1594, %r1598, %r1594, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1598, %r1602, %r1598, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1602, %r1606, %r1602, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1606, %r1610, %r1606, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1610, %r1614, %r1610, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1614, %r1618, %r1614, %r440;
	// inline asm
	mov.u32 	%r3985, 0;
	// inline asm
	prmt.b32 %r6981, %r3985, %r1618, %r440;
	// inline asm
	bra.uni 	BB2_110;

BB2_11:
	setp.gt.s32	%p7, %r131, 7;
	@%p7 bra 	BB2_27;

	setp.gt.s32	%p19, %r131, 3;
	@%p19 bra 	BB2_20;

	setp.gt.s32	%p25, %r131, 1;
	@%p25 bra 	BB2_17;

	setp.eq.s32	%p28, %r131, 0;
	@%p28 bra 	BB2_53;
	bra.uni 	BB2_15;

BB2_53:
	and.b32  	%r2985, %r113, 3;
	shl.b32 	%r2969, %r2985, 3;
	mov.u32 	%r6946, 0;
	// inline asm
	shf.r.wrap.b32 %r2902, %r1558, %r6946, %r2969;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2906, %r1562, %r1558, %r2969;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2910, %r1566, %r1562, %r2969;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2914, %r1570, %r1566, %r2969;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2918, %r1574, %r1570, %r2969;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2922, %r1578, %r1574, %r2969;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2926, %r1582, %r1578, %r2969;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2930, %r1586, %r1582, %r2969;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2934, %r1590, %r1586, %r2969;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2938, %r1594, %r1590, %r2969;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2942, %r1598, %r1594, %r2969;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2946, %r1602, %r1598, %r2969;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2950, %r1606, %r1602, %r2969;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2954, %r1610, %r1606, %r2969;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2958, %r1614, %r1610, %r2969;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2962, %r1618, %r1614, %r2969;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2966, %r6946, %r1618, %r2969;
	// inline asm
	setp.eq.s32	%p45, %r112, 0;
	selp.b32	%r6958, %r2950, %r2954, %p45;
	selp.b32	%r1610, %r2954, %r2958, %p45;
	selp.b32	%r1614, %r2958, %r2962, %p45;
	selp.b32	%r1618, %r2962, %r2966, %p45;
	selp.b32	%r1590, %r2934, %r2938, %p45;
	selp.b32	%r1594, %r2938, %r2942, %p45;
	selp.b32	%r1598, %r2942, %r2946, %p45;
	selp.b32	%r1602, %r2946, %r2950, %p45;
	selp.b32	%r1574, %r2918, %r2922, %p45;
	selp.b32	%r1578, %r2922, %r2926, %p45;
	selp.b32	%r1582, %r2926, %r2930, %p45;
	selp.b32	%r1586, %r2930, %r2934, %p45;
	selp.b32	%r1558, %r2902, %r2906, %p45;
	selp.b32	%r1562, %r2906, %r2910, %p45;
	selp.b32	%r1566, %r2910, %r2914, %p45;
	selp.b32	%r1570, %r2914, %r2918, %p45;
	selp.b32	%r6974, 0, %r2902, %p45;
	mov.u32 	%r6947, %r6946;
	mov.u32 	%r6948, %r6946;
	mov.u32 	%r6949, %r6946;
	mov.u32 	%r6950, %r6946;
	mov.u32 	%r6951, %r6946;
	mov.u32 	%r6952, %r6946;
	mov.u32 	%r6953, %r6946;
	mov.u32 	%r6954, %r6946;
	mov.u32 	%r6955, %r6946;
	mov.u32 	%r6956, %r6946;
	mov.u32 	%r6957, %r6946;
	bra.uni 	BB2_54;

BB2_74:
	setp.gt.s32	%p47, %r131, 11;
	@%p47 bra 	BB2_82;

	setp.gt.s32	%p53, %r131, 9;
	@%p53 bra 	BB2_79;

	setp.eq.s32	%p56, %r131, 8;
	@%p56 bra 	BB2_99;
	bra.uni 	BB2_77;

BB2_99:
	// inline asm
	prmt.b32 %r1558, %r1594, %r1590, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1562, %r1598, %r1594, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1566, %r1602, %r1598, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1570, %r1606, %r1602, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1574, %r1610, %r1606, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1578, %r1614, %r1610, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1582, %r1618, %r1614, %r440;
	// inline asm
	mov.u32 	%r1606, 0;
	// inline asm
	prmt.b32 %r1586, %r1606, %r1618, %r440;
	// inline asm
	mov.u32 	%r1610, %r1606;
	mov.u32 	%r1614, %r1606;
	mov.u32 	%r6981, %r1606;
	mov.u32 	%r1590, %r1606;
	bra.uni 	BB2_100;

BB2_27:
	setp.gt.s32	%p8, %r131, 11;
	@%p8 bra 	BB2_35;

	setp.gt.s32	%p14, %r131, 9;
	@%p14 bra 	BB2_32;

	setp.eq.s32	%p17, %r131, 8;
	@%p17 bra 	BB2_47;
	bra.uni 	BB2_30;

BB2_47:
	and.b32  	%r2313, %r113, 3;
	shl.b32 	%r2297, %r2313, 3;
	mov.u32 	%r6950, 0;
	// inline asm
	shf.r.wrap.b32 %r2230, %r1558, %r6950, %r2297;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2234, %r1562, %r1558, %r2297;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2238, %r1566, %r1562, %r2297;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2242, %r1570, %r1566, %r2297;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2246, %r1574, %r1570, %r2297;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2250, %r1578, %r1574, %r2297;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2254, %r1582, %r1578, %r2297;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2258, %r1586, %r1582, %r2297;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2262, %r1590, %r1586, %r2297;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2266, %r1594, %r1590, %r2297;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2270, %r1598, %r1594, %r2297;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2274, %r1602, %r1598, %r2297;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2278, %r1606, %r1602, %r2297;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2282, %r1610, %r1606, %r2297;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2286, %r1614, %r1610, %r2297;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2290, %r1618, %r1614, %r2297;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2294, %r6950, %r1618, %r2297;
	// inline asm
	setp.eq.s32	%p37, %r112, 0;
	selp.b32	%r6946, %r2230, %r2234, %p37;
	selp.b32	%r6947, %r2234, %r2238, %p37;
	selp.b32	%r6948, %r2238, %r2242, %p37;
	selp.b32	%r6949, %r2242, %r2246, %p37;
	selp.b32	%r6953, 0, %r2230, %p37;
	selp.b32	%r1574, %r2278, %r2282, %p37;
	selp.b32	%r1578, %r2282, %r2286, %p37;
	selp.b32	%r1582, %r2286, %r2290, %p37;
	selp.b32	%r1586, %r2290, %r2294, %p37;
	selp.b32	%r1558, %r2262, %r2266, %p37;
	selp.b32	%r1562, %r2266, %r2270, %p37;
	selp.b32	%r1566, %r2270, %r2274, %p37;
	selp.b32	%r1570, %r2274, %r2278, %p37;
	selp.b32	%r6974, %r2258, %r2262, %p37;
	selp.b32	%r6975, %r2254, %r2258, %p37;
	selp.b32	%r6976, %r2250, %r2254, %p37;
	selp.b32	%r6977, %r2246, %r2250, %p37;
	mov.u32 	%r6951, %r6950;
	mov.u32 	%r6952, %r6950;
	mov.u32 	%r6954, %r6950;
	mov.u32 	%r6955, %r6950;
	mov.u32 	%r6956, %r6950;
	mov.u32 	%r6957, %r6950;
	mov.u32 	%r6958, %r6950;
	mov.u32 	%r1610, %r6950;
	mov.u32 	%r1614, %r6950;
	mov.u32 	%r1618, %r6950;
	mov.u32 	%r1590, %r6950;
	bra.uni 	BB2_48;

BB2_67:
	setp.gt.s32	%p59, %r131, 5;
	@%p59 bra 	BB2_71;

	setp.eq.s32	%p62, %r131, 4;
	@%p62 bra 	BB2_105;
	bra.uni 	BB2_69;

BB2_105:
	// inline asm
	prmt.b32 %r1558, %r1578, %r1574, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1562, %r1582, %r1578, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1566, %r1586, %r1582, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1570, %r1590, %r1586, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1574, %r1594, %r1590, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1578, %r1598, %r1594, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1582, %r1602, %r1598, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1586, %r1606, %r1602, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1590, %r1610, %r1606, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1594, %r1614, %r1610, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1598, %r1618, %r1614, %r440;
	// inline asm
	mov.u32 	%r1606, 0;
	// inline asm
	prmt.b32 %r1602, %r1606, %r1618, %r440;
	// inline asm
	mov.u32 	%r1610, %r1606;
	mov.u32 	%r1614, %r1606;
	mov.u32 	%r6981, %r1606;
	bra.uni 	BB2_110;

BB2_20:
	setp.gt.s32	%p20, %r131, 5;
	@%p20 bra 	BB2_24;

	setp.eq.s32	%p23, %r131, 4;
	@%p23 bra 	BB2_50;
	bra.uni 	BB2_22;

BB2_50:
	and.b32  	%r2649, %r113, 3;
	shl.b32 	%r2633, %r2649, 3;
	mov.u32 	%r6946, 0;
	// inline asm
	shf.r.wrap.b32 %r2566, %r1558, %r6946, %r2633;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2570, %r1562, %r1558, %r2633;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2574, %r1566, %r1562, %r2633;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2578, %r1570, %r1566, %r2633;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2582, %r1574, %r1570, %r2633;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2586, %r1578, %r1574, %r2633;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2590, %r1582, %r1578, %r2633;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2594, %r1586, %r1582, %r2633;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2598, %r1590, %r1586, %r2633;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2602, %r1594, %r1590, %r2633;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2606, %r1598, %r1594, %r2633;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2610, %r1602, %r1598, %r2633;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2614, %r1606, %r1602, %r2633;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2618, %r1610, %r1606, %r2633;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2622, %r1614, %r1610, %r2633;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2626, %r1618, %r1614, %r2633;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2630, %r6946, %r1618, %r2633;
	// inline asm
	setp.eq.s32	%p41, %r112, 0;
	selp.b32	%r6949, 0, %r2566, %p41;
	selp.b32	%r1590, %r2614, %r2618, %p41;
	selp.b32	%r1594, %r2618, %r2622, %p41;
	selp.b32	%r1598, %r2622, %r2626, %p41;
	selp.b32	%r1602, %r2626, %r2630, %p41;
	selp.b32	%r1574, %r2598, %r2602, %p41;
	selp.b32	%r1578, %r2602, %r2606, %p41;
	selp.b32	%r1582, %r2606, %r2610, %p41;
	selp.b32	%r1586, %r2610, %r2614, %p41;
	selp.b32	%r1558, %r2582, %r2586, %p41;
	selp.b32	%r1562, %r2586, %r2590, %p41;
	selp.b32	%r1566, %r2590, %r2594, %p41;
	selp.b32	%r1570, %r2594, %r2598, %p41;
	selp.b32	%r6974, %r2578, %r2582, %p41;
	selp.b32	%r6975, %r2574, %r2578, %p41;
	selp.b32	%r6976, %r2570, %r2574, %p41;
	selp.b32	%r6977, %r2566, %r2570, %p41;
	mov.u32 	%r6947, %r6946;
	mov.u32 	%r6948, %r6946;
	mov.u32 	%r6950, %r6946;
	mov.u32 	%r6951, %r6946;
	mov.u32 	%r6952, %r6946;
	mov.u32 	%r6953, %r6946;
	mov.u32 	%r6954, %r6946;
	mov.u32 	%r6955, %r6946;
	mov.u32 	%r6956, %r6946;
	mov.u32 	%r6957, %r6946;
	mov.u32 	%r6958, %r6946;
	bra.uni 	BB2_51;

BB2_82:
	setp.gt.s32	%p48, %r131, 13;
	@%p48 bra 	BB2_86;

	setp.eq.s32	%p51, %r131, 12;
	@%p51 bra 	BB2_93;
	bra.uni 	BB2_84;

BB2_93:
	// inline asm
	prmt.b32 %r1558, %r1610, %r1606, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1562, %r1614, %r1610, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1566, %r1618, %r1614, %r440;
	// inline asm
	mov.u32 	%r1606, 0;
	// inline asm
	prmt.b32 %r1570, %r1606, %r1618, %r440;
	// inline asm
	mov.u32 	%r1610, %r1606;
	mov.u32 	%r1614, %r1606;
	mov.u32 	%r6981, %r1606;
	mov.u32 	%r1590, %r1606;
	mov.u32 	%r1594, %r1606;
	mov.u32 	%r1598, %r1606;
	mov.u32 	%r1602, %r1606;
	mov.u32 	%r1574, %r1606;
	bra.uni 	BB2_94;

BB2_35:
	setp.gt.s32	%p9, %r131, 13;
	@%p9 bra 	BB2_39;

	setp.eq.s32	%p12, %r131, 12;
	@%p12 bra 	BB2_44;
	bra.uni 	BB2_37;

BB2_44:
	and.b32  	%r1977, %r113, 3;
	shl.b32 	%r1961, %r1977, 3;
	mov.u32 	%r6954, 0;
	// inline asm
	shf.r.wrap.b32 %r1894, %r1558, %r6954, %r1961;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1898, %r1562, %r1558, %r1961;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1902, %r1566, %r1562, %r1961;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1906, %r1570, %r1566, %r1961;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1910, %r1574, %r1570, %r1961;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1914, %r1578, %r1574, %r1961;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1918, %r1582, %r1578, %r1961;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1922, %r1586, %r1582, %r1961;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1926, %r1590, %r1586, %r1961;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1930, %r1594, %r1590, %r1961;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1934, %r1598, %r1594, %r1961;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1938, %r1602, %r1598, %r1961;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1942, %r1606, %r1602, %r1961;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1946, %r1610, %r1606, %r1961;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1950, %r1614, %r1610, %r1961;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1954, %r1618, %r1614, %r1961;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1958, %r6954, %r1618, %r1961;
	// inline asm
	setp.eq.s32	%p33, %r112, 0;
	selp.b32	%r6946, %r1910, %r1914, %p33;
	selp.b32	%r6947, %r1914, %r1918, %p33;
	selp.b32	%r6948, %r1918, %r1922, %p33;
	selp.b32	%r6949, %r1922, %r1926, %p33;
	selp.b32	%r6950, %r1894, %r1898, %p33;
	selp.b32	%r6951, %r1898, %r1902, %p33;
	selp.b32	%r6952, %r1902, %r1906, %p33;
	selp.b32	%r6953, %r1906, %r1910, %p33;
	selp.b32	%r6957, 0, %r1894, %p33;
	selp.b32	%r1558, %r1942, %r1946, %p33;
	selp.b32	%r1562, %r1946, %r1950, %p33;
	selp.b32	%r1566, %r1950, %r1954, %p33;
	selp.b32	%r1570, %r1954, %r1958, %p33;
	selp.b32	%r6974, %r1938, %r1942, %p33;
	selp.b32	%r6975, %r1934, %r1938, %p33;
	selp.b32	%r6976, %r1930, %r1934, %p33;
	selp.b32	%r6977, %r1926, %r1930, %p33;
	mov.u32 	%r6955, %r6954;
	mov.u32 	%r6956, %r6954;
	mov.u32 	%r6958, %r6954;
	mov.u32 	%r1610, %r6954;
	mov.u32 	%r1614, %r6954;
	mov.u32 	%r1618, %r6954;
	mov.u32 	%r1590, %r6954;
	mov.u32 	%r1594, %r6954;
	mov.u32 	%r1598, %r6954;
	mov.u32 	%r1602, %r6954;
	mov.u32 	%r1574, %r6954;
	bra.uni 	BB2_45;

BB2_64:
	setp.eq.s32	%p65, %r131, 2;
	@%p65 bra 	BB2_107;
	bra.uni 	BB2_65;

BB2_107:
	// inline asm
	prmt.b32 %r1558, %r1570, %r1566, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1562, %r1574, %r1570, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1566, %r1578, %r1574, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1570, %r1582, %r1578, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1574, %r1586, %r1582, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1578, %r1590, %r1586, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1582, %r1594, %r1590, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1586, %r1598, %r1594, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1590, %r1602, %r1598, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1594, %r1606, %r1602, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1598, %r1610, %r1606, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1602, %r1614, %r1610, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1606, %r1618, %r1614, %r440;
	// inline asm
	mov.u32 	%r1614, 0;
	// inline asm
	prmt.b32 %r1610, %r1614, %r1618, %r440;
	// inline asm
	mov.u32 	%r6981, %r1614;
	bra.uni 	BB2_110;

BB2_17:
	setp.eq.s32	%p26, %r131, 2;
	@%p26 bra 	BB2_52;
	bra.uni 	BB2_18;

BB2_52:
	and.b32  	%r2817, %r113, 3;
	shl.b32 	%r2801, %r2817, 3;
	mov.u32 	%r6946, 0;
	// inline asm
	shf.r.wrap.b32 %r2734, %r1558, %r6946, %r2801;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2738, %r1562, %r1558, %r2801;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2742, %r1566, %r1562, %r2801;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2746, %r1570, %r1566, %r2801;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2750, %r1574, %r1570, %r2801;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2754, %r1578, %r1574, %r2801;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2758, %r1582, %r1578, %r2801;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2762, %r1586, %r1582, %r2801;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2766, %r1590, %r1586, %r2801;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2770, %r1594, %r1590, %r2801;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2774, %r1598, %r1594, %r2801;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2778, %r1602, %r1598, %r2801;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2782, %r1606, %r1602, %r2801;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2786, %r1610, %r1606, %r2801;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2790, %r1614, %r1610, %r2801;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2794, %r1618, %r1614, %r2801;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2798, %r6946, %r1618, %r2801;
	// inline asm
	setp.eq.s32	%p43, %r112, 0;
	selp.b32	%r6958, %r2790, %r2794, %p43;
	selp.b32	%r1610, %r2794, %r2798, %p43;
	selp.b32	%r1590, %r2774, %r2778, %p43;
	selp.b32	%r1594, %r2778, %r2782, %p43;
	selp.b32	%r1598, %r2782, %r2786, %p43;
	selp.b32	%r1602, %r2786, %r2790, %p43;
	selp.b32	%r1574, %r2758, %r2762, %p43;
	selp.b32	%r1578, %r2762, %r2766, %p43;
	selp.b32	%r1582, %r2766, %r2770, %p43;
	selp.b32	%r1586, %r2770, %r2774, %p43;
	selp.b32	%r1558, %r2742, %r2746, %p43;
	selp.b32	%r1562, %r2746, %r2750, %p43;
	selp.b32	%r1566, %r2750, %r2754, %p43;
	selp.b32	%r1570, %r2754, %r2758, %p43;
	selp.b32	%r6974, %r2738, %r2742, %p43;
	selp.b32	%r6975, %r2734, %r2738, %p43;
	selp.b32	%r6976, 0, %r2734, %p43;
	mov.u32 	%r6947, %r6946;
	mov.u32 	%r6948, %r6946;
	mov.u32 	%r6949, %r6946;
	mov.u32 	%r6950, %r6946;
	mov.u32 	%r6951, %r6946;
	mov.u32 	%r6952, %r6946;
	mov.u32 	%r6953, %r6946;
	mov.u32 	%r6954, %r6946;
	mov.u32 	%r6955, %r6946;
	mov.u32 	%r6956, %r6946;
	mov.u32 	%r6957, %r6946;
	mov.u32 	%r1614, %r6946;
	mov.u32 	%r1618, %r6946;
	bra.uni 	BB2_56;

BB2_79:
	setp.eq.s32	%p54, %r131, 10;
	@%p54 bra 	BB2_97;
	bra.uni 	BB2_80;

BB2_97:
	// inline asm
	prmt.b32 %r1558, %r1602, %r1598, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1562, %r1606, %r1602, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1566, %r1610, %r1606, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1570, %r1614, %r1610, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1574, %r1618, %r1614, %r440;
	// inline asm
	mov.u32 	%r1606, 0;
	// inline asm
	prmt.b32 %r1578, %r1606, %r1618, %r440;
	// inline asm
	mov.u32 	%r1610, %r1606;
	mov.u32 	%r1614, %r1606;
	mov.u32 	%r6981, %r1606;
	mov.u32 	%r1590, %r1606;
	mov.u32 	%r1594, %r1606;
	mov.u32 	%r1598, %r1606;
	mov.u32 	%r1602, %r1606;
	bra.uni 	BB2_95;

BB2_32:
	setp.eq.s32	%p15, %r131, 10;
	@%p15 bra 	BB2_46;
	bra.uni 	BB2_33;

BB2_46:
	and.b32  	%r2145, %r113, 3;
	shl.b32 	%r2129, %r2145, 3;
	mov.u32 	%r6950, 0;
	// inline asm
	shf.r.wrap.b32 %r2062, %r1558, %r6950, %r2129;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2066, %r1562, %r1558, %r2129;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2070, %r1566, %r1562, %r2129;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2074, %r1570, %r1566, %r2129;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2078, %r1574, %r1570, %r2129;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2082, %r1578, %r1574, %r2129;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2086, %r1582, %r1578, %r2129;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2090, %r1586, %r1582, %r2129;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2094, %r1590, %r1586, %r2129;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2098, %r1594, %r1590, %r2129;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2102, %r1598, %r1594, %r2129;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2106, %r1602, %r1598, %r2129;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2110, %r1606, %r1602, %r2129;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2114, %r1610, %r1606, %r2129;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2118, %r1614, %r1610, %r2129;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2122, %r1618, %r1614, %r2129;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2126, %r6950, %r1618, %r2129;
	// inline asm
	setp.eq.s32	%p35, %r112, 0;
	selp.b32	%r6946, %r2070, %r2074, %p35;
	selp.b32	%r6947, %r2074, %r2078, %p35;
	selp.b32	%r6948, %r2078, %r2082, %p35;
	selp.b32	%r6949, %r2082, %r2086, %p35;
	selp.b32	%r6951, 0, %r2062, %p35;
	selp.b32	%r6952, %r2062, %r2066, %p35;
	selp.b32	%r6953, %r2066, %r2070, %p35;
	selp.b32	%r1574, %r2118, %r2122, %p35;
	selp.b32	%r1578, %r2122, %r2126, %p35;
	selp.b32	%r1558, %r2102, %r2106, %p35;
	selp.b32	%r1562, %r2106, %r2110, %p35;
	selp.b32	%r1566, %r2110, %r2114, %p35;
	selp.b32	%r1570, %r2114, %r2118, %p35;
	selp.b32	%r6974, %r2098, %r2102, %p35;
	selp.b32	%r6975, %r2094, %r2098, %p35;
	selp.b32	%r6976, %r2090, %r2094, %p35;
	selp.b32	%r6977, %r2086, %r2090, %p35;
	mov.u32 	%r6954, %r6950;
	mov.u32 	%r6955, %r6950;
	mov.u32 	%r6956, %r6950;
	mov.u32 	%r6957, %r6950;
	mov.u32 	%r6958, %r6950;
	mov.u32 	%r1610, %r6950;
	mov.u32 	%r1614, %r6950;
	mov.u32 	%r1618, %r6950;
	mov.u32 	%r1590, %r6950;
	mov.u32 	%r1594, %r6950;
	mov.u32 	%r1598, %r6950;
	mov.u32 	%r1602, %r6950;
	mov.u32 	%r1582, %r6950;
	mov.u32 	%r1586, %r6950;
	bra.uni 	BB2_57;

BB2_71:
	setp.eq.s32	%p60, %r131, 6;
	@%p60 bra 	BB2_103;
	bra.uni 	BB2_72;

BB2_103:
	// inline asm
	prmt.b32 %r1558, %r1586, %r1582, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1562, %r1590, %r1586, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1566, %r1594, %r1590, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1570, %r1598, %r1594, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1574, %r1602, %r1598, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1578, %r1606, %r1602, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1582, %r1610, %r1606, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1586, %r1614, %r1610, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1590, %r1618, %r1614, %r440;
	// inline asm
	mov.u32 	%r1606, 0;
	// inline asm
	prmt.b32 %r1594, %r1606, %r1618, %r440;
	// inline asm
	mov.u32 	%r1610, %r1606;
	mov.u32 	%r1614, %r1606;
	mov.u32 	%r6981, %r1606;
	bra.uni 	BB2_101;

BB2_24:
	setp.eq.s32	%p21, %r131, 6;
	@%p21 bra 	BB2_49;
	bra.uni 	BB2_25;

BB2_49:
	and.b32  	%r2481, %r113, 3;
	shl.b32 	%r2465, %r2481, 3;
	mov.u32 	%r6946, 0;
	// inline asm
	shf.r.wrap.b32 %r2398, %r1558, %r6946, %r2465;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2402, %r1562, %r1558, %r2465;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2406, %r1566, %r1562, %r2465;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2410, %r1570, %r1566, %r2465;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2414, %r1574, %r1570, %r2465;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2418, %r1578, %r1574, %r2465;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2422, %r1582, %r1578, %r2465;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2426, %r1586, %r1582, %r2465;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2430, %r1590, %r1586, %r2465;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2434, %r1594, %r1590, %r2465;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2438, %r1598, %r1594, %r2465;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2442, %r1602, %r1598, %r2465;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2446, %r1606, %r1602, %r2465;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2450, %r1610, %r1606, %r2465;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2454, %r1614, %r1610, %r2465;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2458, %r1618, %r1614, %r2465;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2462, %r6946, %r1618, %r2465;
	// inline asm
	setp.eq.s32	%p39, %r112, 0;
	selp.b32	%r6947, 0, %r2398, %p39;
	selp.b32	%r6948, %r2398, %r2402, %p39;
	selp.b32	%r6949, %r2402, %r2406, %p39;
	selp.b32	%r1590, %r2454, %r2458, %p39;
	selp.b32	%r1594, %r2458, %r2462, %p39;
	selp.b32	%r1574, %r2438, %r2442, %p39;
	selp.b32	%r1578, %r2442, %r2446, %p39;
	selp.b32	%r1582, %r2446, %r2450, %p39;
	selp.b32	%r1586, %r2450, %r2454, %p39;
	selp.b32	%r1558, %r2422, %r2426, %p39;
	selp.b32	%r1562, %r2426, %r2430, %p39;
	selp.b32	%r1566, %r2430, %r2434, %p39;
	selp.b32	%r1570, %r2434, %r2438, %p39;
	selp.b32	%r6974, %r2418, %r2422, %p39;
	selp.b32	%r6975, %r2414, %r2418, %p39;
	selp.b32	%r6976, %r2410, %r2414, %p39;
	selp.b32	%r6977, %r2406, %r2410, %p39;
	mov.u32 	%r6950, %r6946;
	mov.u32 	%r6951, %r6946;
	mov.u32 	%r6952, %r6946;
	mov.u32 	%r6953, %r6946;
	mov.u32 	%r6954, %r6946;
	mov.u32 	%r6955, %r6946;
	mov.u32 	%r6956, %r6946;
	mov.u32 	%r6957, %r6946;
	mov.u32 	%r6958, %r6946;
	mov.u32 	%r1610, %r6946;
	mov.u32 	%r1614, %r6946;
	mov.u32 	%r1618, %r6946;
	mov.u32 	%r1598, %r6946;
	mov.u32 	%r1602, %r6946;
	bra.uni 	BB2_57;

BB2_86:
	setp.eq.s32	%p49, %r131, 14;
	@%p49 bra 	BB2_91;
	bra.uni 	BB2_87;

BB2_91:
	// inline asm
	prmt.b32 %r1558, %r1618, %r1614, %r440;
	// inline asm
	mov.u32 	%r1606, 0;
	// inline asm
	prmt.b32 %r1562, %r1606, %r1618, %r440;
	// inline asm
	mov.u32 	%r1610, %r1606;
	mov.u32 	%r1614, %r1606;
	mov.u32 	%r6981, %r1606;
	mov.u32 	%r1590, %r1606;
	mov.u32 	%r1594, %r1606;
	mov.u32 	%r1598, %r1606;
	mov.u32 	%r1602, %r1606;
	mov.u32 	%r1574, %r1606;
	mov.u32 	%r1578, %r1606;
	mov.u32 	%r1582, %r1606;
	mov.u32 	%r1586, %r1606;
	bra.uni 	BB2_90;

BB2_39:
	setp.eq.s32	%p10, %r131, 14;
	@%p10 bra 	BB2_43;
	bra.uni 	BB2_40;

BB2_43:
	and.b32  	%r1809, %r113, 3;
	shl.b32 	%r1793, %r1809, 3;
	mov.u32 	%r6954, 0;
	// inline asm
	shf.r.wrap.b32 %r1726, %r1558, %r6954, %r1793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1730, %r1562, %r1558, %r1793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1734, %r1566, %r1562, %r1793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1738, %r1570, %r1566, %r1793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1742, %r1574, %r1570, %r1793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1746, %r1578, %r1574, %r1793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1750, %r1582, %r1578, %r1793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1754, %r1586, %r1582, %r1793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1758, %r1590, %r1586, %r1793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1762, %r1594, %r1590, %r1793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1766, %r1598, %r1594, %r1793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1770, %r1602, %r1598, %r1793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1774, %r1606, %r1602, %r1793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1778, %r1610, %r1606, %r1793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1782, %r1614, %r1610, %r1793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1786, %r1618, %r1614, %r1793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1790, %r6954, %r1618, %r1793;
	// inline asm
	setp.eq.s32	%p31, %r112, 0;
	selp.b32	%r6946, %r1750, %r1754, %p31;
	selp.b32	%r6947, %r1754, %r1758, %p31;
	selp.b32	%r6948, %r1758, %r1762, %p31;
	selp.b32	%r6949, %r1762, %r1766, %p31;
	selp.b32	%r6950, %r1734, %r1738, %p31;
	selp.b32	%r6951, %r1738, %r1742, %p31;
	selp.b32	%r6952, %r1742, %r1746, %p31;
	selp.b32	%r6953, %r1746, %r1750, %p31;
	selp.b32	%r6955, 0, %r1726, %p31;
	selp.b32	%r6956, %r1726, %r1730, %p31;
	selp.b32	%r6957, %r1730, %r1734, %p31;
	selp.b32	%r1558, %r1782, %r1786, %p31;
	selp.b32	%r1562, %r1786, %r1790, %p31;
	selp.b32	%r6974, %r1778, %r1782, %p31;
	selp.b32	%r6975, %r1774, %r1778, %p31;
	selp.b32	%r6976, %r1770, %r1774, %p31;
	selp.b32	%r6977, %r1766, %r1770, %p31;
	mov.u32 	%r6958, %r6954;
	mov.u32 	%r1610, %r6954;
	mov.u32 	%r1614, %r6954;
	mov.u32 	%r1618, %r6954;
	mov.u32 	%r1590, %r6954;
	mov.u32 	%r1594, %r6954;
	mov.u32 	%r1598, %r6954;
	mov.u32 	%r1602, %r6954;
	mov.u32 	%r1574, %r6954;
	mov.u32 	%r1578, %r6954;
	mov.u32 	%r1582, %r6954;
	mov.u32 	%r1586, %r6954;
	mov.u32 	%r1566, %r6954;
	mov.u32 	%r1570, %r6954;
	bra.uni 	BB2_57;

BB2_62:
	setp.eq.s32	%p68, %r131, 1;
	@%p68 bra 	BB2_108;
	bra.uni 	BB2_63;

BB2_108:
	// inline asm
	prmt.b32 %r1558, %r1566, %r1562, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1562, %r1570, %r1566, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1566, %r1574, %r1570, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1570, %r1578, %r1574, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1574, %r1582, %r1578, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1578, %r1586, %r1582, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1582, %r1590, %r1586, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1586, %r1594, %r1590, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1590, %r1598, %r1594, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1594, %r1602, %r1598, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1598, %r1606, %r1602, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1602, %r1610, %r1606, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1606, %r1614, %r1610, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1610, %r1618, %r1614, %r440;
	// inline asm
	mov.u32 	%r6981, 0;
	// inline asm
	prmt.b32 %r1614, %r6981, %r1618, %r440;
	// inline asm
	bra.uni 	BB2_110;

BB2_15:
	setp.eq.s32	%p29, %r131, 1;
	@%p29 bra 	BB2_16;
	bra.uni 	BB2_41;

BB2_16:
	and.b32  	%r2901, %r113, 3;
	shl.b32 	%r2885, %r2901, 3;
	mov.u32 	%r6946, 0;
	// inline asm
	shf.r.wrap.b32 %r2818, %r1558, %r6946, %r2885;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2822, %r1562, %r1558, %r2885;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2826, %r1566, %r1562, %r2885;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2830, %r1570, %r1566, %r2885;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2834, %r1574, %r1570, %r2885;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2838, %r1578, %r1574, %r2885;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2842, %r1582, %r1578, %r2885;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2846, %r1586, %r1582, %r2885;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2850, %r1590, %r1586, %r2885;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2854, %r1594, %r1590, %r2885;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2858, %r1598, %r1594, %r2885;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2862, %r1602, %r1598, %r2885;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2866, %r1606, %r1602, %r2885;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2870, %r1610, %r1606, %r2885;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2874, %r1614, %r1610, %r2885;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2878, %r1618, %r1614, %r2885;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2882, %r6946, %r1618, %r2885;
	// inline asm
	setp.eq.s32	%p44, %r112, 0;
	selp.b32	%r6958, %r2870, %r2874, %p44;
	selp.b32	%r1610, %r2874, %r2878, %p44;
	selp.b32	%r1614, %r2878, %r2882, %p44;
	selp.b32	%r1590, %r2854, %r2858, %p44;
	selp.b32	%r1594, %r2858, %r2862, %p44;
	selp.b32	%r1598, %r2862, %r2866, %p44;
	selp.b32	%r1602, %r2866, %r2870, %p44;
	selp.b32	%r1574, %r2838, %r2842, %p44;
	selp.b32	%r1578, %r2842, %r2846, %p44;
	selp.b32	%r1582, %r2846, %r2850, %p44;
	selp.b32	%r1586, %r2850, %r2854, %p44;
	selp.b32	%r1558, %r2822, %r2826, %p44;
	selp.b32	%r1562, %r2826, %r2830, %p44;
	selp.b32	%r1566, %r2830, %r2834, %p44;
	selp.b32	%r1570, %r2834, %r2838, %p44;
	selp.b32	%r6974, %r2818, %r2822, %p44;
	selp.b32	%r6975, 0, %r2818, %p44;
	mov.u32 	%r6947, %r6946;
	mov.u32 	%r6948, %r6946;
	mov.u32 	%r6949, %r6946;
	mov.u32 	%r6950, %r6946;
	mov.u32 	%r6951, %r6946;
	mov.u32 	%r6952, %r6946;
	mov.u32 	%r6953, %r6946;
	mov.u32 	%r6954, %r6946;
	mov.u32 	%r6955, %r6946;
	mov.u32 	%r6956, %r6946;
	mov.u32 	%r6957, %r6946;
	mov.u32 	%r1618, %r6946;
	bra.uni 	BB2_55;

BB2_77:
	setp.eq.s32	%p57, %r131, 9;
	@%p57 bra 	BB2_98;
	bra.uni 	BB2_78;

BB2_98:
	// inline asm
	prmt.b32 %r1558, %r1598, %r1594, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1562, %r1602, %r1598, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1566, %r1606, %r1602, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1570, %r1610, %r1606, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1574, %r1614, %r1610, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1578, %r1618, %r1614, %r440;
	// inline asm
	mov.u32 	%r1606, 0;
	// inline asm
	prmt.b32 %r1582, %r1606, %r1618, %r440;
	// inline asm
	mov.u32 	%r1610, %r1606;
	mov.u32 	%r1614, %r1606;
	mov.u32 	%r6981, %r1606;
	mov.u32 	%r1590, %r1606;
	mov.u32 	%r1594, %r1606;
	mov.u32 	%r1598, %r1606;
	mov.u32 	%r1602, %r1606;
	mov.u32 	%r1586, %r1606;
	bra.uni 	BB2_110;

BB2_30:
	setp.eq.s32	%p18, %r131, 9;
	@%p18 bra 	BB2_31;
	bra.uni 	BB2_41;

BB2_31:
	and.b32  	%r2229, %r113, 3;
	shl.b32 	%r2213, %r2229, 3;
	mov.u32 	%r6950, 0;
	// inline asm
	shf.r.wrap.b32 %r2146, %r1558, %r6950, %r2213;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2150, %r1562, %r1558, %r2213;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2154, %r1566, %r1562, %r2213;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2158, %r1570, %r1566, %r2213;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2162, %r1574, %r1570, %r2213;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2166, %r1578, %r1574, %r2213;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2170, %r1582, %r1578, %r2213;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2174, %r1586, %r1582, %r2213;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2178, %r1590, %r1586, %r2213;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2182, %r1594, %r1590, %r2213;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2186, %r1598, %r1594, %r2213;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2190, %r1602, %r1598, %r2213;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2194, %r1606, %r1602, %r2213;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2198, %r1610, %r1606, %r2213;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2202, %r1614, %r1610, %r2213;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2206, %r1618, %r1614, %r2213;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2210, %r6950, %r1618, %r2213;
	// inline asm
	setp.eq.s32	%p36, %r112, 0;
	selp.b32	%r6946, %r2150, %r2154, %p36;
	selp.b32	%r6947, %r2154, %r2158, %p36;
	selp.b32	%r6948, %r2158, %r2162, %p36;
	selp.b32	%r6949, %r2162, %r2166, %p36;
	selp.b32	%r6952, 0, %r2146, %p36;
	selp.b32	%r6953, %r2146, %r2150, %p36;
	selp.b32	%r1574, %r2198, %r2202, %p36;
	selp.b32	%r1578, %r2202, %r2206, %p36;
	selp.b32	%r1582, %r2206, %r2210, %p36;
	selp.b32	%r1558, %r2182, %r2186, %p36;
	selp.b32	%r1562, %r2186, %r2190, %p36;
	selp.b32	%r1566, %r2190, %r2194, %p36;
	selp.b32	%r1570, %r2194, %r2198, %p36;
	selp.b32	%r6974, %r2178, %r2182, %p36;
	selp.b32	%r6975, %r2174, %r2178, %p36;
	selp.b32	%r6976, %r2170, %r2174, %p36;
	selp.b32	%r6977, %r2166, %r2170, %p36;
	mov.u32 	%r6951, %r6950;
	mov.u32 	%r6954, %r6950;
	mov.u32 	%r6955, %r6950;
	mov.u32 	%r6956, %r6950;
	mov.u32 	%r6957, %r6950;
	mov.u32 	%r6958, %r6950;
	mov.u32 	%r1610, %r6950;
	mov.u32 	%r1614, %r6950;
	mov.u32 	%r1618, %r6950;
	mov.u32 	%r1590, %r6950;
	mov.u32 	%r1594, %r6950;
	mov.u32 	%r1598, %r6950;
	mov.u32 	%r1602, %r6950;
	mov.u32 	%r1586, %r6950;
	bra.uni 	BB2_57;

BB2_69:
	setp.eq.s32	%p63, %r131, 5;
	@%p63 bra 	BB2_104;
	bra.uni 	BB2_70;

BB2_104:
	// inline asm
	prmt.b32 %r1558, %r1582, %r1578, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1562, %r1586, %r1582, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1566, %r1590, %r1586, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1570, %r1594, %r1590, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1574, %r1598, %r1594, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1578, %r1602, %r1598, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1582, %r1606, %r1602, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1586, %r1610, %r1606, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1590, %r1614, %r1610, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1594, %r1618, %r1614, %r440;
	// inline asm
	mov.u32 	%r1606, 0;
	// inline asm
	prmt.b32 %r1598, %r1606, %r1618, %r440;
	// inline asm
	mov.u32 	%r1610, %r1606;
	mov.u32 	%r1614, %r1606;
	mov.u32 	%r6981, %r1606;
	mov.u32 	%r1602, %r1606;
	bra.uni 	BB2_110;

BB2_22:
	setp.eq.s32	%p24, %r131, 5;
	@%p24 bra 	BB2_23;
	bra.uni 	BB2_41;

BB2_23:
	and.b32  	%r2565, %r113, 3;
	shl.b32 	%r2549, %r2565, 3;
	mov.u32 	%r6946, 0;
	// inline asm
	shf.r.wrap.b32 %r2482, %r1558, %r6946, %r2549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2486, %r1562, %r1558, %r2549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2490, %r1566, %r1562, %r2549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2494, %r1570, %r1566, %r2549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2498, %r1574, %r1570, %r2549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2502, %r1578, %r1574, %r2549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2506, %r1582, %r1578, %r2549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2510, %r1586, %r1582, %r2549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2514, %r1590, %r1586, %r2549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2518, %r1594, %r1590, %r2549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2522, %r1598, %r1594, %r2549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2526, %r1602, %r1598, %r2549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2530, %r1606, %r1602, %r2549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2534, %r1610, %r1606, %r2549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2538, %r1614, %r1610, %r2549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2542, %r1618, %r1614, %r2549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2546, %r6946, %r1618, %r2549;
	// inline asm
	setp.eq.s32	%p40, %r112, 0;
	selp.b32	%r6948, 0, %r2482, %p40;
	selp.b32	%r6949, %r2482, %r2486, %p40;
	selp.b32	%r1590, %r2534, %r2538, %p40;
	selp.b32	%r1594, %r2538, %r2542, %p40;
	selp.b32	%r1598, %r2542, %r2546, %p40;
	selp.b32	%r1574, %r2518, %r2522, %p40;
	selp.b32	%r1578, %r2522, %r2526, %p40;
	selp.b32	%r1582, %r2526, %r2530, %p40;
	selp.b32	%r1586, %r2530, %r2534, %p40;
	selp.b32	%r1558, %r2502, %r2506, %p40;
	selp.b32	%r1562, %r2506, %r2510, %p40;
	selp.b32	%r1566, %r2510, %r2514, %p40;
	selp.b32	%r1570, %r2514, %r2518, %p40;
	selp.b32	%r6974, %r2498, %r2502, %p40;
	selp.b32	%r6975, %r2494, %r2498, %p40;
	selp.b32	%r6976, %r2490, %r2494, %p40;
	selp.b32	%r6977, %r2486, %r2490, %p40;
	mov.u32 	%r6947, %r6946;
	mov.u32 	%r6950, %r6946;
	mov.u32 	%r6951, %r6946;
	mov.u32 	%r6952, %r6946;
	mov.u32 	%r6953, %r6946;
	mov.u32 	%r6954, %r6946;
	mov.u32 	%r6955, %r6946;
	mov.u32 	%r6956, %r6946;
	mov.u32 	%r6957, %r6946;
	mov.u32 	%r6958, %r6946;
	mov.u32 	%r1610, %r6946;
	mov.u32 	%r1614, %r6946;
	mov.u32 	%r1618, %r6946;
	mov.u32 	%r1602, %r6946;
	bra.uni 	BB2_57;

BB2_84:
	setp.eq.s32	%p52, %r131, 13;
	@%p52 bra 	BB2_92;
	bra.uni 	BB2_85;

BB2_92:
	// inline asm
	prmt.b32 %r1558, %r1614, %r1610, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1562, %r1618, %r1614, %r440;
	// inline asm
	mov.u32 	%r1606, 0;
	// inline asm
	prmt.b32 %r1566, %r1606, %r1618, %r440;
	// inline asm
	mov.u32 	%r1610, %r1606;
	mov.u32 	%r1614, %r1606;
	mov.u32 	%r6981, %r1606;
	mov.u32 	%r1590, %r1606;
	mov.u32 	%r1594, %r1606;
	mov.u32 	%r1598, %r1606;
	mov.u32 	%r1602, %r1606;
	mov.u32 	%r1574, %r1606;
	mov.u32 	%r1578, %r1606;
	mov.u32 	%r1582, %r1606;
	mov.u32 	%r1586, %r1606;
	mov.u32 	%r1570, %r1606;
	bra.uni 	BB2_110;

BB2_37:
	setp.eq.s32	%p13, %r131, 13;
	@%p13 bra 	BB2_38;
	bra.uni 	BB2_41;

BB2_38:
	and.b32  	%r1893, %r113, 3;
	shl.b32 	%r1877, %r1893, 3;
	mov.u32 	%r6954, 0;
	// inline asm
	shf.r.wrap.b32 %r1810, %r1558, %r6954, %r1877;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1814, %r1562, %r1558, %r1877;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1818, %r1566, %r1562, %r1877;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1822, %r1570, %r1566, %r1877;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1826, %r1574, %r1570, %r1877;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1830, %r1578, %r1574, %r1877;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1834, %r1582, %r1578, %r1877;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1838, %r1586, %r1582, %r1877;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1842, %r1590, %r1586, %r1877;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1846, %r1594, %r1590, %r1877;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1850, %r1598, %r1594, %r1877;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1854, %r1602, %r1598, %r1877;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1858, %r1606, %r1602, %r1877;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1862, %r1610, %r1606, %r1877;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1866, %r1614, %r1610, %r1877;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1870, %r1618, %r1614, %r1877;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1874, %r6954, %r1618, %r1877;
	// inline asm
	setp.eq.s32	%p32, %r112, 0;
	selp.b32	%r6946, %r1830, %r1834, %p32;
	selp.b32	%r6947, %r1834, %r1838, %p32;
	selp.b32	%r6948, %r1838, %r1842, %p32;
	selp.b32	%r6949, %r1842, %r1846, %p32;
	selp.b32	%r6950, %r1814, %r1818, %p32;
	selp.b32	%r6951, %r1818, %r1822, %p32;
	selp.b32	%r6952, %r1822, %r1826, %p32;
	selp.b32	%r6953, %r1826, %r1830, %p32;
	selp.b32	%r6956, 0, %r1810, %p32;
	selp.b32	%r6957, %r1810, %r1814, %p32;
	selp.b32	%r1558, %r1862, %r1866, %p32;
	selp.b32	%r1562, %r1866, %r1870, %p32;
	selp.b32	%r1566, %r1870, %r1874, %p32;
	selp.b32	%r6974, %r1858, %r1862, %p32;
	selp.b32	%r6975, %r1854, %r1858, %p32;
	selp.b32	%r6976, %r1850, %r1854, %p32;
	selp.b32	%r6977, %r1846, %r1850, %p32;
	mov.u32 	%r6955, %r6954;
	mov.u32 	%r6958, %r6954;
	mov.u32 	%r1610, %r6954;
	mov.u32 	%r1614, %r6954;
	mov.u32 	%r1618, %r6954;
	mov.u32 	%r1590, %r6954;
	mov.u32 	%r1594, %r6954;
	mov.u32 	%r1598, %r6954;
	mov.u32 	%r1602, %r6954;
	mov.u32 	%r1574, %r6954;
	mov.u32 	%r1578, %r6954;
	mov.u32 	%r1582, %r6954;
	mov.u32 	%r1586, %r6954;
	mov.u32 	%r1570, %r6954;
	bra.uni 	BB2_57;

BB2_65:
	setp.eq.s32	%p66, %r131, 3;
	@%p66 bra 	BB2_106;
	bra.uni 	BB2_66;

BB2_106:
	// inline asm
	prmt.b32 %r1558, %r1574, %r1570, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1562, %r1578, %r1574, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1566, %r1582, %r1578, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1570, %r1586, %r1582, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1574, %r1590, %r1586, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1578, %r1594, %r1590, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1582, %r1598, %r1594, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1586, %r1602, %r1598, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1590, %r1606, %r1602, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1594, %r1610, %r1606, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1598, %r1614, %r1610, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1602, %r1618, %r1614, %r440;
	// inline asm
	mov.u32 	%r1610, 0;
	// inline asm
	prmt.b32 %r1606, %r1610, %r1618, %r440;
	// inline asm
	mov.u32 	%r1614, %r1610;
	mov.u32 	%r6981, %r1610;
	bra.uni 	BB2_110;

BB2_18:
	setp.eq.s32	%p27, %r131, 3;
	@%p27 bra 	BB2_19;
	bra.uni 	BB2_41;

BB2_19:
	and.b32  	%r2733, %r113, 3;
	shl.b32 	%r2717, %r2733, 3;
	mov.u32 	%r6946, 0;
	// inline asm
	shf.r.wrap.b32 %r2650, %r1558, %r6946, %r2717;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2654, %r1562, %r1558, %r2717;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2658, %r1566, %r1562, %r2717;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2662, %r1570, %r1566, %r2717;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2666, %r1574, %r1570, %r2717;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2670, %r1578, %r1574, %r2717;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2674, %r1582, %r1578, %r2717;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2678, %r1586, %r1582, %r2717;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2682, %r1590, %r1586, %r2717;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2686, %r1594, %r1590, %r2717;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2690, %r1598, %r1594, %r2717;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2694, %r1602, %r1598, %r2717;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2698, %r1606, %r1602, %r2717;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2702, %r1610, %r1606, %r2717;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2706, %r1614, %r1610, %r2717;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2710, %r1618, %r1614, %r2717;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2714, %r6946, %r1618, %r2717;
	// inline asm
	setp.eq.s32	%p42, %r112, 0;
	selp.b32	%r6958, %r2710, %r2714, %p42;
	selp.b32	%r1590, %r2694, %r2698, %p42;
	selp.b32	%r1594, %r2698, %r2702, %p42;
	selp.b32	%r1598, %r2702, %r2706, %p42;
	selp.b32	%r1602, %r2706, %r2710, %p42;
	selp.b32	%r1574, %r2678, %r2682, %p42;
	selp.b32	%r1578, %r2682, %r2686, %p42;
	selp.b32	%r1582, %r2686, %r2690, %p42;
	selp.b32	%r1586, %r2690, %r2694, %p42;
	selp.b32	%r1558, %r2662, %r2666, %p42;
	selp.b32	%r1562, %r2666, %r2670, %p42;
	selp.b32	%r1566, %r2670, %r2674, %p42;
	selp.b32	%r1570, %r2674, %r2678, %p42;
	selp.b32	%r6974, %r2658, %r2662, %p42;
	selp.b32	%r6975, %r2654, %r2658, %p42;
	selp.b32	%r6976, %r2650, %r2654, %p42;
	selp.b32	%r6977, 0, %r2650, %p42;
	mov.u32 	%r6947, %r6946;
	mov.u32 	%r6948, %r6946;
	mov.u32 	%r6949, %r6946;
	mov.u32 	%r6950, %r6946;
	mov.u32 	%r6951, %r6946;
	mov.u32 	%r6952, %r6946;
	mov.u32 	%r6953, %r6946;
	mov.u32 	%r6954, %r6946;
	mov.u32 	%r6955, %r6946;
	mov.u32 	%r6956, %r6946;
	mov.u32 	%r6957, %r6946;

BB2_51:
	mov.u32 	%r1610, %r6946;
	mov.u32 	%r1614, %r6946;
	mov.u32 	%r1618, %r6946;
	bra.uni 	BB2_57;

BB2_80:
	setp.eq.s32	%p55, %r131, 11;
	@%p55 bra 	BB2_96;
	bra.uni 	BB2_81;

BB2_96:
	// inline asm
	prmt.b32 %r1558, %r1606, %r1602, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1562, %r1610, %r1606, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1566, %r1614, %r1610, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1570, %r1618, %r1614, %r440;
	// inline asm
	mov.u32 	%r1606, 0;
	// inline asm
	prmt.b32 %r1574, %r1606, %r1618, %r440;
	// inline asm
	mov.u32 	%r1610, %r1606;
	mov.u32 	%r1614, %r1606;
	mov.u32 	%r6981, %r1606;
	mov.u32 	%r1590, %r1606;
	mov.u32 	%r1594, %r1606;
	mov.u32 	%r1598, %r1606;
	mov.u32 	%r1602, %r1606;

BB2_94:
	mov.u32 	%r1578, %r1606;

BB2_95:
	mov.u32 	%r1582, %r1606;
	mov.u32 	%r1586, %r1606;
	bra.uni 	BB2_110;

BB2_33:
	setp.eq.s32	%p16, %r131, 11;
	@%p16 bra 	BB2_34;
	bra.uni 	BB2_41;

BB2_34:
	and.b32  	%r2061, %r113, 3;
	shl.b32 	%r2045, %r2061, 3;
	mov.u32 	%r6954, 0;
	// inline asm
	shf.r.wrap.b32 %r1978, %r1558, %r6954, %r2045;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1982, %r1562, %r1558, %r2045;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1986, %r1566, %r1562, %r2045;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1990, %r1570, %r1566, %r2045;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1994, %r1574, %r1570, %r2045;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1998, %r1578, %r1574, %r2045;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2002, %r1582, %r1578, %r2045;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2006, %r1586, %r1582, %r2045;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2010, %r1590, %r1586, %r2045;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2014, %r1594, %r1590, %r2045;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2018, %r1598, %r1594, %r2045;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2022, %r1602, %r1598, %r2045;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2026, %r1606, %r1602, %r2045;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2030, %r1610, %r1606, %r2045;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2034, %r1614, %r1610, %r2045;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2038, %r1618, %r1614, %r2045;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2042, %r6954, %r1618, %r2045;
	// inline asm
	setp.eq.s32	%p34, %r112, 0;
	selp.b32	%r6946, %r1990, %r1994, %p34;
	selp.b32	%r6947, %r1994, %r1998, %p34;
	selp.b32	%r6948, %r1998, %r2002, %p34;
	selp.b32	%r6949, %r2002, %r2006, %p34;
	selp.b32	%r6950, 0, %r1978, %p34;
	selp.b32	%r6951, %r1978, %r1982, %p34;
	selp.b32	%r6952, %r1982, %r1986, %p34;
	selp.b32	%r6953, %r1986, %r1990, %p34;
	selp.b32	%r1574, %r2038, %r2042, %p34;
	selp.b32	%r1558, %r2022, %r2026, %p34;
	selp.b32	%r1562, %r2026, %r2030, %p34;
	selp.b32	%r1566, %r2030, %r2034, %p34;
	selp.b32	%r1570, %r2034, %r2038, %p34;
	selp.b32	%r6974, %r2018, %r2022, %p34;
	selp.b32	%r6975, %r2014, %r2018, %p34;
	selp.b32	%r6976, %r2010, %r2014, %p34;
	selp.b32	%r6977, %r2006, %r2010, %p34;
	mov.u32 	%r6955, %r6954;
	mov.u32 	%r6956, %r6954;
	mov.u32 	%r6957, %r6954;
	mov.u32 	%r6958, %r6954;
	mov.u32 	%r1610, %r6954;
	mov.u32 	%r1614, %r6954;
	mov.u32 	%r1618, %r6954;
	mov.u32 	%r1590, %r6954;
	mov.u32 	%r1594, %r6954;
	mov.u32 	%r1598, %r6954;
	mov.u32 	%r1602, %r6954;

BB2_45:
	mov.u32 	%r1578, %r6954;
	mov.u32 	%r1582, %r6954;
	mov.u32 	%r1586, %r6954;
	bra.uni 	BB2_57;

BB2_72:
	setp.eq.s32	%p61, %r131, 7;
	@%p61 bra 	BB2_102;
	bra.uni 	BB2_73;

BB2_102:
	// inline asm
	prmt.b32 %r1558, %r1590, %r1586, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1562, %r1594, %r1590, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1566, %r1598, %r1594, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1570, %r1602, %r1598, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1574, %r1606, %r1602, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1578, %r1610, %r1606, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1582, %r1614, %r1610, %r440;
	// inline asm
	// inline asm
	prmt.b32 %r1586, %r1618, %r1614, %r440;
	// inline asm
	mov.u32 	%r1606, 0;
	// inline asm
	prmt.b32 %r1590, %r1606, %r1618, %r440;
	// inline asm
	mov.u32 	%r1610, %r1606;
	mov.u32 	%r1614, %r1606;
	mov.u32 	%r6981, %r1606;

BB2_100:
	mov.u32 	%r1594, %r1606;

BB2_101:
	mov.u32 	%r1598, %r1606;
	mov.u32 	%r1602, %r1606;
	bra.uni 	BB2_110;

BB2_25:
	setp.eq.s32	%p22, %r131, 7;
	@%p22 bra 	BB2_26;
	bra.uni 	BB2_41;

BB2_26:
	and.b32  	%r2397, %r113, 3;
	shl.b32 	%r2381, %r2397, 3;
	mov.u32 	%r6950, 0;
	// inline asm
	shf.r.wrap.b32 %r2314, %r1558, %r6950, %r2381;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2318, %r1562, %r1558, %r2381;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2322, %r1566, %r1562, %r2381;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2326, %r1570, %r1566, %r2381;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2330, %r1574, %r1570, %r2381;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2334, %r1578, %r1574, %r2381;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2338, %r1582, %r1578, %r2381;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2342, %r1586, %r1582, %r2381;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2346, %r1590, %r1586, %r2381;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2350, %r1594, %r1590, %r2381;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2354, %r1598, %r1594, %r2381;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2358, %r1602, %r1598, %r2381;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2362, %r1606, %r1602, %r2381;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2366, %r1610, %r1606, %r2381;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2370, %r1614, %r1610, %r2381;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2374, %r1618, %r1614, %r2381;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r2378, %r6950, %r1618, %r2381;
	// inline asm
	setp.eq.s32	%p38, %r112, 0;
	selp.b32	%r6946, 0, %r2314, %p38;
	selp.b32	%r6947, %r2314, %r2318, %p38;
	selp.b32	%r6948, %r2318, %r2322, %p38;
	selp.b32	%r6949, %r2322, %r2326, %p38;
	selp.b32	%r1590, %r2374, %r2378, %p38;
	selp.b32	%r1574, %r2358, %r2362, %p38;
	selp.b32	%r1578, %r2362, %r2366, %p38;
	selp.b32	%r1582, %r2366, %r2370, %p38;
	selp.b32	%r1586, %r2370, %r2374, %p38;
	selp.b32	%r1558, %r2342, %r2346, %p38;
	selp.b32	%r1562, %r2346, %r2350, %p38;
	selp.b32	%r1566, %r2350, %r2354, %p38;
	selp.b32	%r1570, %r2354, %r2358, %p38;
	selp.b32	%r6974, %r2338, %r2342, %p38;
	selp.b32	%r6975, %r2334, %r2338, %p38;
	selp.b32	%r6976, %r2330, %r2334, %p38;
	selp.b32	%r6977, %r2326, %r2330, %p38;
	mov.u32 	%r6951, %r6950;
	mov.u32 	%r6952, %r6950;
	mov.u32 	%r6953, %r6950;
	mov.u32 	%r6954, %r6950;
	mov.u32 	%r6955, %r6950;
	mov.u32 	%r6956, %r6950;
	mov.u32 	%r6957, %r6950;
	mov.u32 	%r6958, %r6950;
	mov.u32 	%r1610, %r6950;
	mov.u32 	%r1614, %r6950;
	mov.u32 	%r1618, %r6950;

BB2_48:
	mov.u32 	%r1594, %r6950;
	mov.u32 	%r1598, %r6950;
	mov.u32 	%r1602, %r6950;
	bra.uni 	BB2_57;

BB2_87:
	setp.ne.s32	%p50, %r131, 15;
	@%p50 bra 	BB2_88;

	mov.u32 	%r1606, 0;
	// inline asm
	prmt.b32 %r1558, %r1606, %r1618, %r440;
	// inline asm
	mov.u32 	%r1610, %r1606;
	mov.u32 	%r1614, %r1606;
	mov.u32 	%r6981, %r1606;
	mov.u32 	%r1590, %r1606;
	mov.u32 	%r1594, %r1606;
	mov.u32 	%r1598, %r1606;
	mov.u32 	%r1602, %r1606;
	mov.u32 	%r1574, %r1606;
	mov.u32 	%r1578, %r1606;
	mov.u32 	%r1582, %r1606;
	mov.u32 	%r1586, %r1606;
	mov.u32 	%r1562, %r1606;

BB2_90:
	mov.u32 	%r1566, %r1606;
	mov.u32 	%r1570, %r1606;
	bra.uni 	BB2_110;

BB2_40:
	setp.ne.s32	%p11, %r131, 15;
	@%p11 bra 	BB2_41;

	and.b32  	%r1725, %r113, 3;
	shl.b32 	%r1709, %r1725, 3;
	mov.u32 	%r6958, 0;
	// inline asm
	shf.r.wrap.b32 %r1642, %r1558, %r6958, %r1709;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1646, %r1562, %r1558, %r1709;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1650, %r1566, %r1562, %r1709;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1654, %r1570, %r1566, %r1709;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1658, %r1574, %r1570, %r1709;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1662, %r1578, %r1574, %r1709;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1666, %r1582, %r1578, %r1709;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1670, %r1586, %r1582, %r1709;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1674, %r1590, %r1586, %r1709;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1678, %r1594, %r1590, %r1709;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1682, %r1598, %r1594, %r1709;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1686, %r1602, %r1598, %r1709;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1690, %r1606, %r1602, %r1709;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1694, %r1610, %r1606, %r1709;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1698, %r1614, %r1610, %r1709;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1702, %r1618, %r1614, %r1709;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r1706, %r6958, %r1618, %r1709;
	// inline asm
	setp.eq.s32	%p30, %r112, 0;
	selp.b32	%r6946, %r1670, %r1674, %p30;
	selp.b32	%r6947, %r1674, %r1678, %p30;
	selp.b32	%r6948, %r1678, %r1682, %p30;
	selp.b32	%r6949, %r1682, %r1686, %p30;
	selp.b32	%r6950, %r1654, %r1658, %p30;
	selp.b32	%r6951, %r1658, %r1662, %p30;
	selp.b32	%r6952, %r1662, %r1666, %p30;
	selp.b32	%r6953, %r1666, %r1670, %p30;
	selp.b32	%r6954, 0, %r1642, %p30;
	selp.b32	%r6955, %r1642, %r1646, %p30;
	selp.b32	%r6956, %r1646, %r1650, %p30;
	selp.b32	%r6957, %r1650, %r1654, %p30;
	selp.b32	%r1558, %r1702, %r1706, %p30;
	selp.b32	%r6974, %r1698, %r1702, %p30;
	selp.b32	%r6975, %r1694, %r1698, %p30;
	selp.b32	%r6976, %r1690, %r1694, %p30;
	selp.b32	%r6977, %r1686, %r1690, %p30;
	mov.u32 	%r1610, %r6958;
	mov.u32 	%r1614, %r6958;
	mov.u32 	%r1618, %r6958;
	mov.u32 	%r1590, %r6958;
	mov.u32 	%r1594, %r6958;
	mov.u32 	%r1598, %r6958;
	mov.u32 	%r1602, %r6958;
	mov.u32 	%r1574, %r6958;
	mov.u32 	%r1578, %r6958;
	mov.u32 	%r1582, %r6958;
	mov.u32 	%r1586, %r6958;
	mov.u32 	%r1562, %r6958;
	mov.u32 	%r1566, %r6958;
	mov.u32 	%r1570, %r6958;
	bra.uni 	BB2_57;

BB2_41:
	mov.u32 	%r6947, %r6946;
	mov.u32 	%r6948, %r6946;
	mov.u32 	%r6949, %r6946;
	mov.u32 	%r6950, %r6946;
	mov.u32 	%r6951, %r6946;
	mov.u32 	%r6952, %r6946;
	mov.u32 	%r6953, %r6946;
	mov.u32 	%r6954, %r6946;
	mov.u32 	%r6955, %r6946;
	mov.u32 	%r6956, %r6946;
	mov.u32 	%r6957, %r6946;
	mov.u32 	%r6958, %r1606;
	mov.u32 	%r6974, %r6946;

BB2_54:
	mov.u32 	%r6975, %r6946;

BB2_55:
	mov.u32 	%r6976, %r6946;

BB2_56:
	mov.u32 	%r6977, %r6946;

BB2_57:
	xor.b32  	%r2986, %r99, %r98;
	and.b32  	%r2987, %r2986, %r100;
	xor.b32  	%r2988, %r2987, %r98;
	add.s32 	%r2989, %r2988, %r101;
	or.b32  	%r2990, %r1618, %r97;
	add.s32 	%r2991, %r2989, %r2990;
	shf.l.wrap.b32 	%r2992, %r2991, %r2991, 3;
	xor.b32  	%r2993, %r100, %r99;
	and.b32  	%r2994, %r2992, %r2993;
	xor.b32  	%r2995, %r2994, %r99;
	or.b32  	%r2996, %r1614, %r96;
	add.s32 	%r2997, %r2996, %r98;
	add.s32 	%r2998, %r2997, %r2995;
	shf.l.wrap.b32 	%r2999, %r2998, %r2998, 7;
	xor.b32  	%r3000, %r2992, %r100;
	and.b32  	%r3001, %r3000, %r2999;
	xor.b32  	%r3002, %r3001, %r100;
	or.b32  	%r3003, %r1610, %r95;
	add.s32 	%r3004, %r3003, %r99;
	add.s32 	%r3005, %r3004, %r3002;
	shf.l.wrap.b32 	%r3006, %r3005, %r3005, 11;
	xor.b32  	%r3007, %r2999, %r2992;
	and.b32  	%r3008, %r3007, %r3006;
	xor.b32  	%r3009, %r3008, %r2992;
	or.b32  	%r3010, %r6958, %r94;
	add.s32 	%r3011, %r3010, %r100;
	add.s32 	%r3012, %r3011, %r3009;
	shf.l.wrap.b32 	%r3013, %r3012, %r3012, 19;
	xor.b32  	%r3014, %r3006, %r2999;
	and.b32  	%r3015, %r3014, %r3013;
	xor.b32  	%r3016, %r3015, %r2999;
	or.b32  	%r3017, %r1602, %r93;
	add.s32 	%r3018, %r2992, %r3017;
	add.s32 	%r3019, %r3018, %r3016;
	shf.l.wrap.b32 	%r3020, %r3019, %r3019, 3;
	xor.b32  	%r3021, %r3013, %r3006;
	and.b32  	%r3022, %r3021, %r3020;
	xor.b32  	%r3023, %r3022, %r3006;
	or.b32  	%r3024, %r1598, %r92;
	add.s32 	%r3025, %r2999, %r3024;
	add.s32 	%r3026, %r3025, %r3023;
	shf.l.wrap.b32 	%r3027, %r3026, %r3026, 7;
	xor.b32  	%r3028, %r3020, %r3013;
	and.b32  	%r3029, %r3028, %r3027;
	xor.b32  	%r3030, %r3029, %r3013;
	or.b32  	%r3031, %r1594, %r91;
	add.s32 	%r3032, %r3006, %r3031;
	add.s32 	%r3033, %r3032, %r3030;
	shf.l.wrap.b32 	%r3034, %r3033, %r3033, 11;
	xor.b32  	%r3035, %r3027, %r3020;
	and.b32  	%r3036, %r3035, %r3034;
	xor.b32  	%r3037, %r3036, %r3020;
	or.b32  	%r3038, %r1590, %r90;
	add.s32 	%r3039, %r3013, %r3038;
	add.s32 	%r3040, %r3039, %r3037;
	shf.l.wrap.b32 	%r3041, %r3040, %r3040, 19;
	xor.b32  	%r3042, %r3034, %r3027;
	and.b32  	%r3043, %r3042, %r3041;
	xor.b32  	%r3044, %r3043, %r3027;
	or.b32  	%r3045, %r1586, %r89;
	add.s32 	%r3046, %r3020, %r3045;
	add.s32 	%r3047, %r3046, %r3044;
	shf.l.wrap.b32 	%r3048, %r3047, %r3047, 3;
	xor.b32  	%r3049, %r3041, %r3034;
	and.b32  	%r3050, %r3049, %r3048;
	xor.b32  	%r3051, %r3050, %r3034;
	or.b32  	%r3052, %r1582, %r88;
	add.s32 	%r3053, %r3027, %r3052;
	add.s32 	%r3054, %r3053, %r3051;
	shf.l.wrap.b32 	%r3055, %r3054, %r3054, 7;
	xor.b32  	%r3056, %r3048, %r3041;
	and.b32  	%r3057, %r3056, %r3055;
	xor.b32  	%r3058, %r3057, %r3041;
	or.b32  	%r3059, %r1578, %r87;
	add.s32 	%r3060, %r3034, %r3059;
	add.s32 	%r3061, %r3060, %r3058;
	shf.l.wrap.b32 	%r3062, %r3061, %r3061, 11;
	xor.b32  	%r3063, %r3055, %r3048;
	and.b32  	%r3064, %r3063, %r3062;
	xor.b32  	%r3065, %r3064, %r3048;
	or.b32  	%r3066, %r1574, %r86;
	add.s32 	%r3067, %r3041, %r3066;
	add.s32 	%r3068, %r3067, %r3065;
	shf.l.wrap.b32 	%r3069, %r3068, %r3068, 19;
	xor.b32  	%r3070, %r3062, %r3055;
	and.b32  	%r3071, %r3070, %r3069;
	xor.b32  	%r3072, %r3071, %r3055;
	or.b32  	%r3073, %r1570, %r85;
	add.s32 	%r3074, %r3048, %r3073;
	add.s32 	%r3075, %r3074, %r3072;
	shf.l.wrap.b32 	%r3076, %r3075, %r3075, 3;
	xor.b32  	%r3077, %r3069, %r3062;
	and.b32  	%r3078, %r3077, %r3076;
	xor.b32  	%r3079, %r3078, %r3062;
	or.b32  	%r3080, %r1566, %r84;
	add.s32 	%r3081, %r3055, %r3080;
	add.s32 	%r3082, %r3081, %r3079;
	shf.l.wrap.b32 	%r3083, %r3082, %r3082, 7;
	xor.b32  	%r3084, %r3076, %r3069;
	and.b32  	%r3085, %r3084, %r3083;
	xor.b32  	%r3086, %r3085, %r3069;
	or.b32  	%r3087, %r1562, %r83;
	add.s32 	%r3088, %r3062, %r3087;
	add.s32 	%r3089, %r3088, %r3086;
	shf.l.wrap.b32 	%r3090, %r3089, %r3089, 11;
	xor.b32  	%r3091, %r3083, %r3076;
	and.b32  	%r3092, %r3091, %r3090;
	xor.b32  	%r3093, %r3092, %r3076;
	or.b32  	%r3094, %r1558, %r82;
	add.s32 	%r3095, %r3069, %r3094;
	add.s32 	%r3096, %r3095, %r3093;
	shf.l.wrap.b32 	%r3097, %r3096, %r3096, 19;
	xor.b32  	%r3098, %r3097, %r3083;
	xor.b32  	%r3099, %r3097, %r3090;
	and.b32  	%r3100, %r3099, %r3098;
	xor.b32  	%r3101, %r3100, %r3097;
	add.s32 	%r3102, %r2990, %r3076;
	add.s32 	%r3103, %r3102, %r3101;
	add.s32 	%r3104, %r3103, 1518500249;
	shf.l.wrap.b32 	%r3105, %r3104, %r3104, 3;
	xor.b32  	%r3106, %r3105, %r3090;
	xor.b32  	%r3107, %r3105, %r3097;
	and.b32  	%r3108, %r3107, %r3106;
	xor.b32  	%r3109, %r3108, %r3105;
	add.s32 	%r3110, %r3017, %r3083;
	add.s32 	%r3111, %r3110, %r3109;
	add.s32 	%r3112, %r3111, 1518500249;
	shf.l.wrap.b32 	%r3113, %r3112, %r3112, 5;
	xor.b32  	%r3114, %r3113, %r3097;
	xor.b32  	%r3115, %r3113, %r3105;
	and.b32  	%r3116, %r3115, %r3114;
	xor.b32  	%r3117, %r3116, %r3113;
	add.s32 	%r3118, %r3045, %r3090;
	add.s32 	%r3119, %r3118, %r3117;
	add.s32 	%r3120, %r3119, 1518500249;
	shf.l.wrap.b32 	%r3121, %r3120, %r3120, 9;
	xor.b32  	%r3122, %r3121, %r3105;
	xor.b32  	%r3123, %r3121, %r3113;
	and.b32  	%r3124, %r3123, %r3122;
	xor.b32  	%r3125, %r3124, %r3121;
	add.s32 	%r3126, %r3073, %r3097;
	add.s32 	%r3127, %r3126, %r3125;
	add.s32 	%r3128, %r3127, 1518500249;
	shf.l.wrap.b32 	%r3129, %r3128, %r3128, 13;
	xor.b32  	%r3130, %r3129, %r3113;
	xor.b32  	%r3131, %r3129, %r3121;
	and.b32  	%r3132, %r3131, %r3130;
	xor.b32  	%r3133, %r3132, %r3129;
	add.s32 	%r3134, %r2996, %r3105;
	add.s32 	%r3135, %r3134, %r3133;
	add.s32 	%r3136, %r3135, 1518500249;
	shf.l.wrap.b32 	%r3137, %r3136, %r3136, 3;
	xor.b32  	%r3138, %r3137, %r3121;
	xor.b32  	%r3139, %r3137, %r3129;
	and.b32  	%r3140, %r3139, %r3138;
	xor.b32  	%r3141, %r3140, %r3137;
	add.s32 	%r3142, %r3024, %r3113;
	add.s32 	%r3143, %r3142, %r3141;
	add.s32 	%r3144, %r3143, 1518500249;
	shf.l.wrap.b32 	%r3145, %r3144, %r3144, 5;
	xor.b32  	%r3146, %r3145, %r3129;
	xor.b32  	%r3147, %r3145, %r3137;
	and.b32  	%r3148, %r3147, %r3146;
	xor.b32  	%r3149, %r3148, %r3145;
	add.s32 	%r3150, %r3052, %r3121;
	add.s32 	%r3151, %r3150, %r3149;
	add.s32 	%r3152, %r3151, 1518500249;
	shf.l.wrap.b32 	%r3153, %r3152, %r3152, 9;
	xor.b32  	%r3154, %r3153, %r3137;
	xor.b32  	%r3155, %r3153, %r3145;
	and.b32  	%r3156, %r3155, %r3154;
	xor.b32  	%r3157, %r3156, %r3153;
	add.s32 	%r3158, %r3080, %r3129;
	add.s32 	%r3159, %r3158, %r3157;
	add.s32 	%r3160, %r3159, 1518500249;
	shf.l.wrap.b32 	%r3161, %r3160, %r3160, 13;
	xor.b32  	%r3162, %r3161, %r3145;
	xor.b32  	%r3163, %r3161, %r3153;
	and.b32  	%r3164, %r3163, %r3162;
	xor.b32  	%r3165, %r3164, %r3161;
	add.s32 	%r3166, %r3003, %r3137;
	add.s32 	%r3167, %r3166, %r3165;
	add.s32 	%r3168, %r3167, 1518500249;
	shf.l.wrap.b32 	%r3169, %r3168, %r3168, 3;
	xor.b32  	%r3170, %r3169, %r3153;
	xor.b32  	%r3171, %r3169, %r3161;
	and.b32  	%r3172, %r3171, %r3170;
	xor.b32  	%r3173, %r3172, %r3169;
	add.s32 	%r3174, %r3031, %r3145;
	add.s32 	%r3175, %r3174, %r3173;
	add.s32 	%r3176, %r3175, 1518500249;
	shf.l.wrap.b32 	%r3177, %r3176, %r3176, 5;
	xor.b32  	%r3178, %r3177, %r3161;
	xor.b32  	%r3179, %r3177, %r3169;
	and.b32  	%r3180, %r3179, %r3178;
	xor.b32  	%r3181, %r3180, %r3177;
	add.s32 	%r3182, %r3059, %r3153;
	add.s32 	%r3183, %r3182, %r3181;
	add.s32 	%r3184, %r3183, 1518500249;
	shf.l.wrap.b32 	%r3185, %r3184, %r3184, 9;
	xor.b32  	%r3186, %r3185, %r3169;
	xor.b32  	%r3187, %r3185, %r3177;
	and.b32  	%r3188, %r3187, %r3186;
	xor.b32  	%r3189, %r3188, %r3185;
	add.s32 	%r3190, %r3087, %r3161;
	add.s32 	%r3191, %r3190, %r3189;
	add.s32 	%r3192, %r3191, 1518500249;
	shf.l.wrap.b32 	%r3193, %r3192, %r3192, 13;
	xor.b32  	%r3194, %r3193, %r3177;
	xor.b32  	%r3195, %r3193, %r3185;
	and.b32  	%r3196, %r3195, %r3194;
	xor.b32  	%r3197, %r3196, %r3193;
	add.s32 	%r3198, %r3010, %r3169;
	add.s32 	%r3199, %r3198, %r3197;
	add.s32 	%r3200, %r3199, 1518500249;
	shf.l.wrap.b32 	%r3201, %r3200, %r3200, 3;
	xor.b32  	%r3202, %r3201, %r3185;
	xor.b32  	%r3203, %r3201, %r3193;
	and.b32  	%r3204, %r3203, %r3202;
	xor.b32  	%r3205, %r3204, %r3201;
	add.s32 	%r3206, %r3038, %r3177;
	add.s32 	%r3207, %r3206, %r3205;
	add.s32 	%r3208, %r3207, 1518500249;
	shf.l.wrap.b32 	%r3209, %r3208, %r3208, 5;
	xor.b32  	%r3210, %r3209, %r3193;
	xor.b32  	%r3211, %r3209, %r3201;
	and.b32  	%r3212, %r3211, %r3210;
	xor.b32  	%r3213, %r3212, %r3209;
	add.s32 	%r3214, %r3066, %r3185;
	add.s32 	%r3215, %r3214, %r3213;
	add.s32 	%r3216, %r3215, 1518500249;
	shf.l.wrap.b32 	%r3217, %r3216, %r3216, 9;
	xor.b32  	%r3218, %r3217, %r3201;
	xor.b32  	%r3219, %r3217, %r3209;
	and.b32  	%r3220, %r3219, %r3218;
	xor.b32  	%r3221, %r3220, %r3217;
	add.s32 	%r3222, %r3094, %r3193;
	add.s32 	%r3223, %r3222, %r3221;
	add.s32 	%r3224, %r3223, 1518500249;
	shf.l.wrap.b32 	%r3225, %r3224, %r3224, 13;
	xor.b32  	%r3226, %r3219, %r3225;
	add.s32 	%r3227, %r2990, %r3201;
	add.s32 	%r3228, %r3227, %r3226;
	add.s32 	%r3229, %r3228, 1859775393;
	shf.l.wrap.b32 	%r3230, %r3229, %r3229, 3;
	xor.b32  	%r3231, %r3225, %r3217;
	xor.b32  	%r3232, %r3231, %r3230;
	add.s32 	%r3233, %r3045, %r3209;
	add.s32 	%r3234, %r3233, %r3232;
	add.s32 	%r3235, %r3234, 1859775393;
	shf.l.wrap.b32 	%r3236, %r3235, %r3235, 9;
	xor.b32  	%r3237, %r3230, %r3225;
	xor.b32  	%r3238, %r3237, %r3236;
	add.s32 	%r3239, %r3017, %r3217;
	add.s32 	%r3240, %r3239, %r3238;
	add.s32 	%r3241, %r3240, 1859775393;
	shf.l.wrap.b32 	%r3242, %r3241, %r3241, 11;
	xor.b32  	%r3243, %r3236, %r3230;
	xor.b32  	%r3244, %r3243, %r3242;
	add.s32 	%r3245, %r3073, %r3225;
	add.s32 	%r3246, %r3245, %r3244;
	add.s32 	%r3247, %r3246, 1859775393;
	shf.l.wrap.b32 	%r3248, %r3247, %r3247, 15;
	xor.b32  	%r3249, %r3242, %r3236;
	xor.b32  	%r3250, %r3249, %r3248;
	add.s32 	%r3251, %r3003, %r3230;
	add.s32 	%r3252, %r3251, %r3250;
	add.s32 	%r3253, %r3252, 1859775393;
	shf.l.wrap.b32 	%r3254, %r3253, %r3253, 3;
	xor.b32  	%r3255, %r3248, %r3242;
	xor.b32  	%r3256, %r3255, %r3254;
	add.s32 	%r3257, %r3059, %r3236;
	add.s32 	%r3258, %r3257, %r3256;
	add.s32 	%r3259, %r3258, 1859775393;
	shf.l.wrap.b32 	%r3260, %r3259, %r3259, 9;
	xor.b32  	%r3261, %r3254, %r3248;
	xor.b32  	%r3262, %r3261, %r3260;
	add.s32 	%r3263, %r3031, %r3242;
	add.s32 	%r3264, %r3263, %r3262;
	add.s32 	%r3265, %r3264, 1859775393;
	shf.l.wrap.b32 	%r3266, %r3265, %r3265, 11;
	xor.b32  	%r3267, %r3260, %r3254;
	xor.b32  	%r3268, %r3267, %r3266;
	add.s32 	%r3269, %r3087, %r3248;
	add.s32 	%r3270, %r3269, %r3268;
	add.s32 	%r3271, %r3270, 1859775393;
	shf.l.wrap.b32 	%r3272, %r3271, %r3271, 15;
	xor.b32  	%r3273, %r3266, %r3260;
	xor.b32  	%r3274, %r3273, %r3272;
	add.s32 	%r3275, %r2996, %r3254;
	add.s32 	%r3276, %r3275, %r3274;
	add.s32 	%r3277, %r3276, 1859775393;
	shf.l.wrap.b32 	%r3278, %r3277, %r3277, 3;
	xor.b32  	%r3279, %r3272, %r3266;
	xor.b32  	%r3280, %r3279, %r3278;
	add.s32 	%r3281, %r3052, %r3260;
	add.s32 	%r3282, %r3281, %r3280;
	add.s32 	%r3283, %r3282, 1859775393;
	shf.l.wrap.b32 	%r3284, %r3283, %r3283, 9;
	xor.b32  	%r3285, %r3278, %r3272;
	xor.b32  	%r3286, %r3285, %r3284;
	add.s32 	%r3287, %r3024, %r3266;
	add.s32 	%r3288, %r3287, %r3286;
	add.s32 	%r3289, %r3288, 1859775393;
	shf.l.wrap.b32 	%r3290, %r3289, %r3289, 11;
	xor.b32  	%r3291, %r3284, %r3278;
	xor.b32  	%r3292, %r3291, %r3290;
	add.s32 	%r3293, %r3080, %r3272;
	add.s32 	%r3294, %r3293, %r3292;
	add.s32 	%r3295, %r3294, 1859775393;
	shf.l.wrap.b32 	%r3296, %r3295, %r3295, 15;
	xor.b32  	%r3297, %r3290, %r3284;
	xor.b32  	%r3298, %r3297, %r3296;
	add.s32 	%r3299, %r3010, %r3278;
	add.s32 	%r3300, %r3299, %r3298;
	add.s32 	%r3301, %r3300, 1859775393;
	shf.l.wrap.b32 	%r3302, %r3301, %r3301, 3;
	xor.b32  	%r3303, %r3296, %r3290;
	xor.b32  	%r3304, %r3303, %r3302;
	add.s32 	%r3305, %r3066, %r3284;
	add.s32 	%r3306, %r3305, %r3304;
	add.s32 	%r3307, %r3306, 1859775393;
	shf.l.wrap.b32 	%r3308, %r3307, %r3307, 9;
	xor.b32  	%r3309, %r3302, %r3296;
	xor.b32  	%r3310, %r3309, %r3308;
	add.s32 	%r3311, %r3038, %r3290;
	add.s32 	%r3312, %r3311, %r3310;
	add.s32 	%r3313, %r3312, 1859775393;
	shf.l.wrap.b32 	%r3314, %r3313, %r3313, 11;
	xor.b32  	%r3315, %r3308, %r3302;
	xor.b32  	%r3316, %r3315, %r3314;
	add.s32 	%r3317, %r3094, %r3296;
	add.s32 	%r3318, %r3317, %r3316;
	add.s32 	%r3319, %r3318, 1859775393;
	shf.l.wrap.b32 	%r3320, %r3319, %r3319, 15;
	add.s32 	%r101, %r3302, %r101;
	add.s32 	%r100, %r3320, %r100;
	add.s32 	%r99, %r3314, %r99;
	add.s32 	%r98, %r3308, %r98;
	bra.uni 	BB2_111;

BB2_63:
	mov.u32 	%r6981, %r1618;
	bra.uni 	BB2_110;

BB2_78:
	mov.u32 	%r6981, %r1618;
	bra.uni 	BB2_110;

BB2_70:
	mov.u32 	%r6981, %r1618;
	bra.uni 	BB2_110;

BB2_85:
	mov.u32 	%r6981, %r1618;
	bra.uni 	BB2_110;

BB2_66:
	mov.u32 	%r6981, %r1618;
	bra.uni 	BB2_110;

BB2_81:
	mov.u32 	%r6981, %r1618;
	bra.uni 	BB2_110;

BB2_73:
	mov.u32 	%r6981, %r1618;
	bra.uni 	BB2_110;

BB2_88:
	mov.u32 	%r6981, %r1618;

BB2_110:
	or.b32  	%r6974, %r6981, %r97;
	or.b32  	%r6975, %r1614, %r96;
	or.b32  	%r6976, %r1610, %r95;
	or.b32  	%r6977, %r1606, %r94;
	or.b32  	%r6949, %r1602, %r93;
	or.b32  	%r6948, %r1598, %r92;
	or.b32  	%r6947, %r1594, %r91;
	or.b32  	%r6946, %r1590, %r90;
	or.b32  	%r6953, %r1586, %r89;
	or.b32  	%r6952, %r1582, %r88;
	or.b32  	%r6951, %r1578, %r87;
	or.b32  	%r6950, %r1574, %r86;
	or.b32  	%r6957, %r1570, %r85;
	or.b32  	%r6956, %r1566, %r84;
	or.b32  	%r6955, %r1562, %r83;
	or.b32  	%r6954, %r1558, %r82;

BB2_111:
	and.b32  	%r3988, %r130, 63;
	mul.wide.u32 	%rd27, %r3988, 64;
	mov.u64 	%rd28, c_append_helper;
	add.s64 	%rd29, %rd28, %rd27;
	ld.const.u32 	%r3989, [%rd29];
	and.b32  	%r3990, %r3989, -2139062144;
	or.b32  	%r7027, %r3990, %r6974;
	ld.const.u32 	%r3991, [%rd29+4];
	and.b32  	%r3992, %r3991, -2139062144;
	or.b32  	%r7026, %r3992, %r6975;
	ld.const.u32 	%r3993, [%rd29+8];
	and.b32  	%r3994, %r3993, -2139062144;
	or.b32  	%r7025, %r3994, %r6976;
	ld.const.u32 	%r3995, [%rd29+12];
	and.b32  	%r3996, %r3995, -2139062144;
	or.b32  	%r7024, %r3996, %r6977;
	ld.const.u32 	%r3997, [%rd29+16];
	and.b32  	%r3998, %r3997, -2139062144;
	or.b32  	%r7023, %r3998, %r6949;
	ld.const.u32 	%r3999, [%rd29+20];
	and.b32  	%r4000, %r3999, -2139062144;
	or.b32  	%r7022, %r4000, %r6948;
	ld.const.u32 	%r4001, [%rd29+24];
	and.b32  	%r4002, %r4001, -2139062144;
	or.b32  	%r7021, %r4002, %r6947;
	ld.const.u32 	%r4003, [%rd29+28];
	and.b32  	%r4004, %r4003, -2139062144;
	or.b32  	%r7020, %r4004, %r6946;
	ld.const.u32 	%r4005, [%rd29+32];
	and.b32  	%r4006, %r4005, -2139062144;
	or.b32  	%r7019, %r4006, %r6953;
	ld.const.u32 	%r4007, [%rd29+36];
	and.b32  	%r4008, %r4007, -2139062144;
	or.b32  	%r7018, %r4008, %r6952;
	ld.const.u32 	%r4009, [%rd29+40];
	and.b32  	%r4010, %r4009, -2139062144;
	or.b32  	%r7017, %r4010, %r6951;
	ld.const.u32 	%r4011, [%rd29+44];
	and.b32  	%r4012, %r4011, -2139062144;
	or.b32  	%r7016, %r4012, %r6950;
	ld.const.u32 	%r4013, [%rd29+48];
	and.b32  	%r4014, %r4013, -2139062144;
	or.b32  	%r7015, %r4014, %r6957;
	ld.const.u32 	%r4015, [%rd29+52];
	and.b32  	%r4016, %r4015, -2139062144;
	or.b32  	%r7014, %r4016, %r6956;
	ld.const.u32 	%r4017, [%rd29+56];
	and.b32  	%r4018, %r4017, -2139062144;
	or.b32  	%r643, %r4018, %r6955;
	ld.const.u32 	%r4019, [%rd29+60];
	and.b32  	%r4020, %r4019, -2139062144;
	or.b32  	%r644, %r4020, %r6954;
	setp.lt.u32	%p69, %r3988, 56;
	@%p69 bra 	BB2_113;

	xor.b32  	%r4035, %r99, %r98;
	and.b32  	%r4036, %r4035, %r100;
	xor.b32  	%r4037, %r4036, %r98;
	add.s32 	%r4038, %r4037, %r101;
	add.s32 	%r4039, %r4038, %r7027;
	shf.l.wrap.b32 	%r4040, %r4039, %r4039, 3;
	xor.b32  	%r4041, %r100, %r99;
	and.b32  	%r4042, %r4040, %r4041;
	xor.b32  	%r4043, %r4042, %r99;
	add.s32 	%r4044, %r7026, %r98;
	add.s32 	%r4045, %r4044, %r4043;
	shf.l.wrap.b32 	%r4046, %r4045, %r4045, 7;
	xor.b32  	%r4047, %r4040, %r100;
	and.b32  	%r4048, %r4047, %r4046;
	xor.b32  	%r4049, %r4048, %r100;
	add.s32 	%r4050, %r7025, %r99;
	add.s32 	%r4051, %r4050, %r4049;
	shf.l.wrap.b32 	%r4052, %r4051, %r4051, 11;
	xor.b32  	%r4053, %r4046, %r4040;
	and.b32  	%r4054, %r4053, %r4052;
	xor.b32  	%r4055, %r4054, %r4040;
	add.s32 	%r4056, %r7024, %r100;
	add.s32 	%r4057, %r4056, %r4055;
	shf.l.wrap.b32 	%r4058, %r4057, %r4057, 19;
	xor.b32  	%r4059, %r4052, %r4046;
	and.b32  	%r4060, %r4059, %r4058;
	xor.b32  	%r4061, %r4060, %r4046;
	add.s32 	%r4062, %r4040, %r7023;
	add.s32 	%r4063, %r4062, %r4061;
	shf.l.wrap.b32 	%r4064, %r4063, %r4063, 3;
	xor.b32  	%r4065, %r4058, %r4052;
	and.b32  	%r4066, %r4065, %r4064;
	xor.b32  	%r4067, %r4066, %r4052;
	add.s32 	%r4068, %r4046, %r7022;
	add.s32 	%r4069, %r4068, %r4067;
	shf.l.wrap.b32 	%r4070, %r4069, %r4069, 7;
	xor.b32  	%r4071, %r4064, %r4058;
	and.b32  	%r4072, %r4071, %r4070;
	xor.b32  	%r4073, %r4072, %r4058;
	add.s32 	%r4074, %r4052, %r7021;
	add.s32 	%r4075, %r4074, %r4073;
	shf.l.wrap.b32 	%r4076, %r4075, %r4075, 11;
	xor.b32  	%r4077, %r4070, %r4064;
	and.b32  	%r4078, %r4077, %r4076;
	xor.b32  	%r4079, %r4078, %r4064;
	add.s32 	%r4080, %r4058, %r7020;
	add.s32 	%r4081, %r4080, %r4079;
	shf.l.wrap.b32 	%r4082, %r4081, %r4081, 19;
	xor.b32  	%r4083, %r4076, %r4070;
	and.b32  	%r4084, %r4083, %r4082;
	xor.b32  	%r4085, %r4084, %r4070;
	add.s32 	%r4086, %r4064, %r7019;
	add.s32 	%r4087, %r4086, %r4085;
	shf.l.wrap.b32 	%r4088, %r4087, %r4087, 3;
	xor.b32  	%r4089, %r4082, %r4076;
	and.b32  	%r4090, %r4089, %r4088;
	xor.b32  	%r4091, %r4090, %r4076;
	add.s32 	%r4092, %r4070, %r7018;
	add.s32 	%r4093, %r4092, %r4091;
	shf.l.wrap.b32 	%r4094, %r4093, %r4093, 7;
	xor.b32  	%r4095, %r4088, %r4082;
	and.b32  	%r4096, %r4095, %r4094;
	xor.b32  	%r4097, %r4096, %r4082;
	add.s32 	%r4098, %r4076, %r7017;
	add.s32 	%r4099, %r4098, %r4097;
	shf.l.wrap.b32 	%r4100, %r4099, %r4099, 11;
	xor.b32  	%r4101, %r4094, %r4088;
	and.b32  	%r4102, %r4101, %r4100;
	xor.b32  	%r4103, %r4102, %r4088;
	add.s32 	%r4104, %r4082, %r7016;
	add.s32 	%r4105, %r4104, %r4103;
	shf.l.wrap.b32 	%r4106, %r4105, %r4105, 19;
	xor.b32  	%r4107, %r4100, %r4094;
	and.b32  	%r4108, %r4107, %r4106;
	xor.b32  	%r4109, %r4108, %r4094;
	add.s32 	%r4110, %r4088, %r7015;
	add.s32 	%r4111, %r4110, %r4109;
	shf.l.wrap.b32 	%r4112, %r4111, %r4111, 3;
	xor.b32  	%r4113, %r4106, %r4100;
	and.b32  	%r4114, %r4113, %r4112;
	xor.b32  	%r4115, %r4114, %r4100;
	add.s32 	%r4116, %r4094, %r7014;
	add.s32 	%r4117, %r4116, %r4115;
	shf.l.wrap.b32 	%r4118, %r4117, %r4117, 7;
	xor.b32  	%r4119, %r4112, %r4106;
	and.b32  	%r4120, %r4119, %r4118;
	xor.b32  	%r4121, %r4120, %r4106;
	add.s32 	%r4122, %r4100, %r643;
	add.s32 	%r4123, %r4122, %r4121;
	shf.l.wrap.b32 	%r4124, %r4123, %r4123, 11;
	xor.b32  	%r4125, %r4118, %r4112;
	and.b32  	%r4126, %r4125, %r4124;
	xor.b32  	%r4127, %r4126, %r4112;
	add.s32 	%r4128, %r4106, %r644;
	add.s32 	%r4129, %r4128, %r4127;
	shf.l.wrap.b32 	%r4130, %r4129, %r4129, 19;
	xor.b32  	%r4131, %r4130, %r4118;
	xor.b32  	%r4132, %r4130, %r4124;
	and.b32  	%r4133, %r4132, %r4131;
	xor.b32  	%r4134, %r4133, %r4130;
	add.s32 	%r4135, %r7027, %r4112;
	add.s32 	%r4136, %r4135, %r4134;
	add.s32 	%r4137, %r4136, 1518500249;
	shf.l.wrap.b32 	%r4138, %r4137, %r4137, 3;
	xor.b32  	%r4139, %r4138, %r4124;
	xor.b32  	%r4140, %r4138, %r4130;
	and.b32  	%r4141, %r4140, %r4139;
	xor.b32  	%r4142, %r4141, %r4138;
	add.s32 	%r4143, %r7023, %r4118;
	add.s32 	%r4144, %r4143, %r4142;
	add.s32 	%r4145, %r4144, 1518500249;
	shf.l.wrap.b32 	%r4146, %r4145, %r4145, 5;
	xor.b32  	%r4147, %r4146, %r4130;
	xor.b32  	%r4148, %r4146, %r4138;
	and.b32  	%r4149, %r4148, %r4147;
	xor.b32  	%r4150, %r4149, %r4146;
	add.s32 	%r4151, %r7019, %r4124;
	add.s32 	%r4152, %r4151, %r4150;
	add.s32 	%r4153, %r4152, 1518500249;
	shf.l.wrap.b32 	%r4154, %r4153, %r4153, 9;
	xor.b32  	%r4155, %r4154, %r4138;
	xor.b32  	%r4156, %r4154, %r4146;
	and.b32  	%r4157, %r4156, %r4155;
	xor.b32  	%r4158, %r4157, %r4154;
	add.s32 	%r4159, %r7015, %r4130;
	add.s32 	%r4160, %r4159, %r4158;
	add.s32 	%r4161, %r4160, 1518500249;
	shf.l.wrap.b32 	%r4162, %r4161, %r4161, 13;
	xor.b32  	%r4163, %r4162, %r4146;
	xor.b32  	%r4164, %r4162, %r4154;
	and.b32  	%r4165, %r4164, %r4163;
	xor.b32  	%r4166, %r4165, %r4162;
	add.s32 	%r4167, %r7026, %r4138;
	add.s32 	%r4168, %r4167, %r4166;
	add.s32 	%r4169, %r4168, 1518500249;
	shf.l.wrap.b32 	%r4170, %r4169, %r4169, 3;
	xor.b32  	%r4171, %r4170, %r4154;
	xor.b32  	%r4172, %r4170, %r4162;
	and.b32  	%r4173, %r4172, %r4171;
	xor.b32  	%r4174, %r4173, %r4170;
	add.s32 	%r4175, %r7022, %r4146;
	add.s32 	%r4176, %r4175, %r4174;
	add.s32 	%r4177, %r4176, 1518500249;
	shf.l.wrap.b32 	%r4178, %r4177, %r4177, 5;
	xor.b32  	%r4179, %r4178, %r4162;
	xor.b32  	%r4180, %r4178, %r4170;
	and.b32  	%r4181, %r4180, %r4179;
	xor.b32  	%r4182, %r4181, %r4178;
	add.s32 	%r4183, %r7018, %r4154;
	add.s32 	%r4184, %r4183, %r4182;
	add.s32 	%r4185, %r4184, 1518500249;
	shf.l.wrap.b32 	%r4186, %r4185, %r4185, 9;
	xor.b32  	%r4187, %r4186, %r4170;
	xor.b32  	%r4188, %r4186, %r4178;
	and.b32  	%r4189, %r4188, %r4187;
	xor.b32  	%r4190, %r4189, %r4186;
	add.s32 	%r4191, %r7014, %r4162;
	add.s32 	%r4192, %r4191, %r4190;
	add.s32 	%r4193, %r4192, 1518500249;
	shf.l.wrap.b32 	%r4194, %r4193, %r4193, 13;
	xor.b32  	%r4195, %r4194, %r4178;
	xor.b32  	%r4196, %r4194, %r4186;
	and.b32  	%r4197, %r4196, %r4195;
	xor.b32  	%r4198, %r4197, %r4194;
	add.s32 	%r4199, %r7025, %r4170;
	add.s32 	%r4200, %r4199, %r4198;
	add.s32 	%r4201, %r4200, 1518500249;
	shf.l.wrap.b32 	%r4202, %r4201, %r4201, 3;
	xor.b32  	%r4203, %r4202, %r4186;
	xor.b32  	%r4204, %r4202, %r4194;
	and.b32  	%r4205, %r4204, %r4203;
	xor.b32  	%r4206, %r4205, %r4202;
	add.s32 	%r4207, %r7021, %r4178;
	add.s32 	%r4208, %r4207, %r4206;
	add.s32 	%r4209, %r4208, 1518500249;
	shf.l.wrap.b32 	%r4210, %r4209, %r4209, 5;
	xor.b32  	%r4211, %r4210, %r4194;
	xor.b32  	%r4212, %r4210, %r4202;
	and.b32  	%r4213, %r4212, %r4211;
	xor.b32  	%r4214, %r4213, %r4210;
	add.s32 	%r4215, %r7017, %r4186;
	add.s32 	%r4216, %r4215, %r4214;
	add.s32 	%r4217, %r4216, 1518500249;
	shf.l.wrap.b32 	%r4218, %r4217, %r4217, 9;
	xor.b32  	%r4219, %r4218, %r4202;
	xor.b32  	%r4220, %r4218, %r4210;
	and.b32  	%r4221, %r4220, %r4219;
	xor.b32  	%r4222, %r4221, %r4218;
	add.s32 	%r4223, %r643, %r4194;
	add.s32 	%r4224, %r4223, %r4222;
	add.s32 	%r4225, %r4224, 1518500249;
	shf.l.wrap.b32 	%r4226, %r4225, %r4225, 13;
	xor.b32  	%r4227, %r4226, %r4210;
	xor.b32  	%r4228, %r4226, %r4218;
	and.b32  	%r4229, %r4228, %r4227;
	xor.b32  	%r4230, %r4229, %r4226;
	add.s32 	%r4231, %r7024, %r4202;
	add.s32 	%r4232, %r4231, %r4230;
	add.s32 	%r4233, %r4232, 1518500249;
	shf.l.wrap.b32 	%r4234, %r4233, %r4233, 3;
	xor.b32  	%r4235, %r4234, %r4218;
	xor.b32  	%r4236, %r4234, %r4226;
	and.b32  	%r4237, %r4236, %r4235;
	xor.b32  	%r4238, %r4237, %r4234;
	add.s32 	%r4239, %r7020, %r4210;
	add.s32 	%r4240, %r4239, %r4238;
	add.s32 	%r4241, %r4240, 1518500249;
	shf.l.wrap.b32 	%r4242, %r4241, %r4241, 5;
	xor.b32  	%r4243, %r4242, %r4226;
	xor.b32  	%r4244, %r4242, %r4234;
	and.b32  	%r4245, %r4244, %r4243;
	xor.b32  	%r4246, %r4245, %r4242;
	add.s32 	%r4247, %r7016, %r4218;
	add.s32 	%r4248, %r4247, %r4246;
	add.s32 	%r4249, %r4248, 1518500249;
	shf.l.wrap.b32 	%r4250, %r4249, %r4249, 9;
	xor.b32  	%r4251, %r4250, %r4234;
	xor.b32  	%r4252, %r4250, %r4242;
	and.b32  	%r4253, %r4252, %r4251;
	xor.b32  	%r4254, %r4253, %r4250;
	add.s32 	%r4255, %r644, %r4226;
	add.s32 	%r4256, %r4255, %r4254;
	add.s32 	%r4257, %r4256, 1518500249;
	shf.l.wrap.b32 	%r4258, %r4257, %r4257, 13;
	xor.b32  	%r4259, %r4252, %r4258;
	add.s32 	%r4260, %r7027, %r4234;
	add.s32 	%r4261, %r4260, %r4259;
	add.s32 	%r4262, %r4261, 1859775393;
	shf.l.wrap.b32 	%r4263, %r4262, %r4262, 3;
	xor.b32  	%r4264, %r4258, %r4250;
	xor.b32  	%r4265, %r4264, %r4263;
	add.s32 	%r4266, %r7019, %r4242;
	add.s32 	%r4267, %r4266, %r4265;
	add.s32 	%r4268, %r4267, 1859775393;
	shf.l.wrap.b32 	%r4269, %r4268, %r4268, 9;
	xor.b32  	%r4270, %r4263, %r4258;
	xor.b32  	%r4271, %r4270, %r4269;
	add.s32 	%r4272, %r7023, %r4250;
	add.s32 	%r4273, %r4272, %r4271;
	add.s32 	%r4274, %r4273, 1859775393;
	shf.l.wrap.b32 	%r4275, %r4274, %r4274, 11;
	xor.b32  	%r4276, %r4269, %r4263;
	xor.b32  	%r4277, %r4276, %r4275;
	add.s32 	%r4278, %r7015, %r4258;
	add.s32 	%r4279, %r4278, %r4277;
	add.s32 	%r4280, %r4279, 1859775393;
	shf.l.wrap.b32 	%r4281, %r4280, %r4280, 15;
	xor.b32  	%r4282, %r4275, %r4269;
	xor.b32  	%r4283, %r4282, %r4281;
	add.s32 	%r4284, %r7025, %r4263;
	add.s32 	%r4285, %r4284, %r4283;
	add.s32 	%r4286, %r4285, 1859775393;
	shf.l.wrap.b32 	%r4287, %r4286, %r4286, 3;
	xor.b32  	%r4288, %r4281, %r4275;
	xor.b32  	%r4289, %r4288, %r4287;
	add.s32 	%r4290, %r7017, %r4269;
	add.s32 	%r4291, %r4290, %r4289;
	add.s32 	%r4292, %r4291, 1859775393;
	shf.l.wrap.b32 	%r4293, %r4292, %r4292, 9;
	xor.b32  	%r4294, %r4287, %r4281;
	xor.b32  	%r4295, %r4294, %r4293;
	add.s32 	%r4296, %r7021, %r4275;
	add.s32 	%r4297, %r4296, %r4295;
	add.s32 	%r4298, %r4297, 1859775393;
	shf.l.wrap.b32 	%r4299, %r4298, %r4298, 11;
	xor.b32  	%r4300, %r4293, %r4287;
	xor.b32  	%r4301, %r4300, %r4299;
	add.s32 	%r4302, %r643, %r4281;
	add.s32 	%r4303, %r4302, %r4301;
	add.s32 	%r4304, %r4303, 1859775393;
	shf.l.wrap.b32 	%r4305, %r4304, %r4304, 15;
	xor.b32  	%r4306, %r4299, %r4293;
	xor.b32  	%r4307, %r4306, %r4305;
	add.s32 	%r4308, %r7026, %r4287;
	add.s32 	%r4309, %r4308, %r4307;
	add.s32 	%r4310, %r4309, 1859775393;
	shf.l.wrap.b32 	%r4311, %r4310, %r4310, 3;
	xor.b32  	%r4312, %r4305, %r4299;
	xor.b32  	%r4313, %r4312, %r4311;
	add.s32 	%r4314, %r7018, %r4293;
	add.s32 	%r4315, %r4314, %r4313;
	add.s32 	%r4316, %r4315, 1859775393;
	shf.l.wrap.b32 	%r4317, %r4316, %r4316, 9;
	xor.b32  	%r4318, %r4311, %r4305;
	xor.b32  	%r4319, %r4318, %r4317;
	add.s32 	%r4320, %r7022, %r4299;
	add.s32 	%r4321, %r4320, %r4319;
	add.s32 	%r4322, %r4321, 1859775393;
	shf.l.wrap.b32 	%r4323, %r4322, %r4322, 11;
	xor.b32  	%r4324, %r4317, %r4311;
	xor.b32  	%r4325, %r4324, %r4323;
	add.s32 	%r4326, %r7014, %r4305;
	add.s32 	%r4327, %r4326, %r4325;
	add.s32 	%r4328, %r4327, 1859775393;
	shf.l.wrap.b32 	%r4329, %r4328, %r4328, 15;
	xor.b32  	%r4330, %r4323, %r4317;
	xor.b32  	%r4331, %r4330, %r4329;
	add.s32 	%r4332, %r7024, %r4311;
	add.s32 	%r4333, %r4332, %r4331;
	add.s32 	%r4334, %r4333, 1859775393;
	shf.l.wrap.b32 	%r4335, %r4334, %r4334, 3;
	xor.b32  	%r4336, %r4329, %r4323;
	xor.b32  	%r4337, %r4336, %r4335;
	add.s32 	%r4338, %r7016, %r4317;
	add.s32 	%r4339, %r4338, %r4337;
	add.s32 	%r4340, %r4339, 1859775393;
	shf.l.wrap.b32 	%r4341, %r4340, %r4340, 9;
	xor.b32  	%r4342, %r4335, %r4329;
	xor.b32  	%r4343, %r4342, %r4341;
	add.s32 	%r4344, %r7020, %r4323;
	add.s32 	%r4345, %r4344, %r4343;
	add.s32 	%r4346, %r4345, 1859775393;
	shf.l.wrap.b32 	%r4347, %r4346, %r4346, 11;
	xor.b32  	%r4348, %r4341, %r4335;
	xor.b32  	%r4349, %r4348, %r4347;
	add.s32 	%r4350, %r644, %r4329;
	add.s32 	%r4351, %r4350, %r4349;
	add.s32 	%r4352, %r4351, 1859775393;
	shf.l.wrap.b32 	%r4353, %r4352, %r4352, 15;
	add.s32 	%r101, %r4335, %r101;
	add.s32 	%r100, %r4353, %r100;
	add.s32 	%r99, %r4347, %r99;
	add.s32 	%r98, %r4341, %r98;
	mov.u32 	%r7014, 0;
	mov.u32 	%r7015, %r7014;
	mov.u32 	%r7016, %r7014;
	mov.u32 	%r7017, %r7014;
	mov.u32 	%r7018, %r7014;
	mov.u32 	%r7019, %r7014;
	mov.u32 	%r7020, %r7014;
	mov.u32 	%r7021, %r7014;
	mov.u32 	%r7022, %r7014;
	mov.u32 	%r7023, %r7014;
	mov.u32 	%r7024, %r7014;
	mov.u32 	%r7025, %r7014;
	mov.u32 	%r7026, %r7014;
	mov.u32 	%r7027, %r7014;

BB2_113:
	xor.b32  	%r4354, %r99, %r98;
	and.b32  	%r4355, %r4354, %r100;
	xor.b32  	%r4356, %r4355, %r98;
	add.s32 	%r4357, %r101, %r7027;
	add.s32 	%r4358, %r4357, %r4356;
	shf.l.wrap.b32 	%r4359, %r4358, %r4358, 3;
	xor.b32  	%r4360, %r100, %r99;
	and.b32  	%r4361, %r4359, %r4360;
	xor.b32  	%r4362, %r4361, %r99;
	add.s32 	%r4363, %r98, %r7026;
	add.s32 	%r4364, %r4363, %r4362;
	shf.l.wrap.b32 	%r4365, %r4364, %r4364, 7;
	xor.b32  	%r4366, %r4359, %r100;
	and.b32  	%r4367, %r4366, %r4365;
	xor.b32  	%r4368, %r4367, %r100;
	add.s32 	%r4369, %r99, %r7025;
	add.s32 	%r4370, %r4369, %r4368;
	shf.l.wrap.b32 	%r4371, %r4370, %r4370, 11;
	xor.b32  	%r4372, %r4365, %r4359;
	and.b32  	%r4373, %r4372, %r4371;
	xor.b32  	%r4374, %r4373, %r4359;
	add.s32 	%r4375, %r100, %r7024;
	add.s32 	%r4376, %r4375, %r4374;
	shf.l.wrap.b32 	%r4377, %r4376, %r4376, 19;
	xor.b32  	%r4378, %r4371, %r4365;
	and.b32  	%r4379, %r4378, %r4377;
	xor.b32  	%r4380, %r4379, %r4365;
	add.s32 	%r4381, %r4359, %r7023;
	add.s32 	%r4382, %r4381, %r4380;
	shf.l.wrap.b32 	%r4383, %r4382, %r4382, 3;
	xor.b32  	%r4384, %r4377, %r4371;
	and.b32  	%r4385, %r4384, %r4383;
	xor.b32  	%r4386, %r4385, %r4371;
	add.s32 	%r4387, %r4365, %r7022;
	add.s32 	%r4388, %r4387, %r4386;
	shf.l.wrap.b32 	%r4389, %r4388, %r4388, 7;
	xor.b32  	%r4390, %r4383, %r4377;
	and.b32  	%r4391, %r4390, %r4389;
	xor.b32  	%r4392, %r4391, %r4377;
	add.s32 	%r4393, %r4371, %r7021;
	add.s32 	%r4394, %r4393, %r4392;
	shf.l.wrap.b32 	%r4395, %r4394, %r4394, 11;
	xor.b32  	%r4396, %r4389, %r4383;
	and.b32  	%r4397, %r4396, %r4395;
	xor.b32  	%r4398, %r4397, %r4383;
	add.s32 	%r4399, %r4377, %r7020;
	add.s32 	%r4400, %r4399, %r4398;
	shf.l.wrap.b32 	%r4401, %r4400, %r4400, 19;
	xor.b32  	%r4402, %r4395, %r4389;
	and.b32  	%r4403, %r4402, %r4401;
	xor.b32  	%r4404, %r4403, %r4389;
	add.s32 	%r4405, %r4383, %r7019;
	add.s32 	%r4406, %r4405, %r4404;
	shf.l.wrap.b32 	%r4407, %r4406, %r4406, 3;
	xor.b32  	%r4408, %r4401, %r4395;
	and.b32  	%r4409, %r4408, %r4407;
	xor.b32  	%r4410, %r4409, %r4395;
	add.s32 	%r4411, %r4389, %r7018;
	add.s32 	%r4412, %r4411, %r4410;
	shf.l.wrap.b32 	%r4413, %r4412, %r4412, 7;
	xor.b32  	%r4414, %r4407, %r4401;
	and.b32  	%r4415, %r4414, %r4413;
	xor.b32  	%r4416, %r4415, %r4401;
	add.s32 	%r4417, %r4395, %r7017;
	add.s32 	%r4418, %r4417, %r4416;
	shf.l.wrap.b32 	%r4419, %r4418, %r4418, 11;
	xor.b32  	%r4420, %r4413, %r4407;
	and.b32  	%r4421, %r4420, %r4419;
	xor.b32  	%r4422, %r4421, %r4407;
	add.s32 	%r4423, %r4401, %r7016;
	add.s32 	%r4424, %r4423, %r4422;
	shf.l.wrap.b32 	%r4425, %r4424, %r4424, 19;
	xor.b32  	%r4426, %r4419, %r4413;
	and.b32  	%r4427, %r4426, %r4425;
	xor.b32  	%r4428, %r4427, %r4413;
	add.s32 	%r4429, %r4407, %r7015;
	add.s32 	%r4430, %r4429, %r4428;
	shf.l.wrap.b32 	%r4431, %r4430, %r4430, 3;
	xor.b32  	%r4432, %r4425, %r4419;
	and.b32  	%r4433, %r4432, %r4431;
	xor.b32  	%r4434, %r4433, %r4419;
	add.s32 	%r4435, %r4413, %r7014;
	add.s32 	%r4436, %r4435, %r4434;
	shf.l.wrap.b32 	%r4437, %r4436, %r4436, 7;
	xor.b32  	%r4438, %r4431, %r4425;
	and.b32  	%r4439, %r4438, %r4437;
	xor.b32  	%r4440, %r4439, %r4425;
	shl.b32 	%r4441, %r130, 3;
	add.s32 	%r4442, %r4419, %r4441;
	add.s32 	%r4443, %r4442, %r4440;
	shf.l.wrap.b32 	%r4444, %r4443, %r4443, 11;
	xor.b32  	%r4445, %r4437, %r4431;
	and.b32  	%r4446, %r4445, %r4444;
	xor.b32  	%r4447, %r4446, %r4431;
	add.s32 	%r4448, %r4447, %r4425;
	shf.l.wrap.b32 	%r4449, %r4448, %r4448, 19;
	xor.b32  	%r4450, %r4449, %r4437;
	xor.b32  	%r4451, %r4449, %r4444;
	and.b32  	%r4452, %r4451, %r4450;
	xor.b32  	%r4453, %r4452, %r4449;
	add.s32 	%r4454, %r7027, %r4431;
	add.s32 	%r4455, %r4454, %r4453;
	add.s32 	%r4456, %r4455, 1518500249;
	shf.l.wrap.b32 	%r4457, %r4456, %r4456, 3;
	xor.b32  	%r4458, %r4457, %r4444;
	xor.b32  	%r4459, %r4457, %r4449;
	and.b32  	%r4460, %r4459, %r4458;
	xor.b32  	%r4461, %r4460, %r4457;
	add.s32 	%r4462, %r7023, %r4437;
	add.s32 	%r4463, %r4462, %r4461;
	add.s32 	%r4464, %r4463, 1518500249;
	shf.l.wrap.b32 	%r4465, %r4464, %r4464, 5;
	xor.b32  	%r4466, %r4465, %r4449;
	xor.b32  	%r4467, %r4465, %r4457;
	and.b32  	%r4468, %r4467, %r4466;
	xor.b32  	%r4469, %r4468, %r4465;
	add.s32 	%r4470, %r7019, %r4444;
	add.s32 	%r4471, %r4470, %r4469;
	add.s32 	%r4472, %r4471, 1518500249;
	shf.l.wrap.b32 	%r4473, %r4472, %r4472, 9;
	xor.b32  	%r4474, %r4473, %r4457;
	xor.b32  	%r4475, %r4473, %r4465;
	and.b32  	%r4476, %r4475, %r4474;
	xor.b32  	%r4477, %r4476, %r4473;
	add.s32 	%r4478, %r7015, %r4449;
	add.s32 	%r4479, %r4478, %r4477;
	add.s32 	%r4480, %r4479, 1518500249;
	shf.l.wrap.b32 	%r4481, %r4480, %r4480, 13;
	xor.b32  	%r4482, %r4481, %r4465;
	xor.b32  	%r4483, %r4481, %r4473;
	and.b32  	%r4484, %r4483, %r4482;
	xor.b32  	%r4485, %r4484, %r4481;
	add.s32 	%r4486, %r7026, %r4457;
	add.s32 	%r4487, %r4486, %r4485;
	add.s32 	%r4488, %r4487, 1518500249;
	shf.l.wrap.b32 	%r4489, %r4488, %r4488, 3;
	xor.b32  	%r4490, %r4489, %r4473;
	xor.b32  	%r4491, %r4489, %r4481;
	and.b32  	%r4492, %r4491, %r4490;
	xor.b32  	%r4493, %r4492, %r4489;
	add.s32 	%r4494, %r7022, %r4465;
	add.s32 	%r4495, %r4494, %r4493;
	add.s32 	%r4496, %r4495, 1518500249;
	shf.l.wrap.b32 	%r4497, %r4496, %r4496, 5;
	xor.b32  	%r4498, %r4497, %r4481;
	xor.b32  	%r4499, %r4497, %r4489;
	and.b32  	%r4500, %r4499, %r4498;
	xor.b32  	%r4501, %r4500, %r4497;
	add.s32 	%r4502, %r7018, %r4473;
	add.s32 	%r4503, %r4502, %r4501;
	add.s32 	%r4504, %r4503, 1518500249;
	shf.l.wrap.b32 	%r4505, %r4504, %r4504, 9;
	xor.b32  	%r4506, %r4505, %r4489;
	xor.b32  	%r4507, %r4505, %r4497;
	and.b32  	%r4508, %r4507, %r4506;
	xor.b32  	%r4509, %r4508, %r4505;
	add.s32 	%r4510, %r7014, %r4481;
	add.s32 	%r4511, %r4510, %r4509;
	add.s32 	%r4512, %r4511, 1518500249;
	shf.l.wrap.b32 	%r4513, %r4512, %r4512, 13;
	xor.b32  	%r4514, %r4513, %r4497;
	xor.b32  	%r4515, %r4513, %r4505;
	and.b32  	%r4516, %r4515, %r4514;
	xor.b32  	%r4517, %r4516, %r4513;
	add.s32 	%r4518, %r7025, %r4489;
	add.s32 	%r4519, %r4518, %r4517;
	add.s32 	%r4520, %r4519, 1518500249;
	shf.l.wrap.b32 	%r4521, %r4520, %r4520, 3;
	xor.b32  	%r4522, %r4521, %r4505;
	xor.b32  	%r4523, %r4521, %r4513;
	and.b32  	%r4524, %r4523, %r4522;
	xor.b32  	%r4525, %r4524, %r4521;
	add.s32 	%r4526, %r7021, %r4497;
	add.s32 	%r4527, %r4526, %r4525;
	add.s32 	%r4528, %r4527, 1518500249;
	shf.l.wrap.b32 	%r4529, %r4528, %r4528, 5;
	xor.b32  	%r4530, %r4529, %r4513;
	xor.b32  	%r4531, %r4529, %r4521;
	and.b32  	%r4532, %r4531, %r4530;
	xor.b32  	%r4533, %r4532, %r4529;
	add.s32 	%r4534, %r7017, %r4505;
	add.s32 	%r4535, %r4534, %r4533;
	add.s32 	%r4536, %r4535, 1518500249;
	shf.l.wrap.b32 	%r4537, %r4536, %r4536, 9;
	xor.b32  	%r4538, %r4537, %r4521;
	xor.b32  	%r4539, %r4537, %r4529;
	and.b32  	%r4540, %r4539, %r4538;
	xor.b32  	%r4541, %r4540, %r4537;
	add.s32 	%r4542, %r4441, %r4513;
	add.s32 	%r4543, %r4542, %r4541;
	add.s32 	%r4544, %r4543, 1518500249;
	shf.l.wrap.b32 	%r4545, %r4544, %r4544, 13;
	xor.b32  	%r4546, %r4545, %r4529;
	xor.b32  	%r4547, %r4545, %r4537;
	and.b32  	%r4548, %r4547, %r4546;
	xor.b32  	%r4549, %r4548, %r4545;
	add.s32 	%r4550, %r7024, %r4521;
	add.s32 	%r4551, %r4550, %r4549;
	add.s32 	%r4552, %r4551, 1518500249;
	shf.l.wrap.b32 	%r4553, %r4552, %r4552, 3;
	xor.b32  	%r4554, %r4553, %r4537;
	xor.b32  	%r4555, %r4553, %r4545;
	and.b32  	%r4556, %r4555, %r4554;
	xor.b32  	%r4557, %r4556, %r4553;
	add.s32 	%r4558, %r7020, %r4529;
	add.s32 	%r4559, %r4558, %r4557;
	add.s32 	%r4560, %r4559, 1518500249;
	shf.l.wrap.b32 	%r4561, %r4560, %r4560, 5;
	xor.b32  	%r4562, %r4561, %r4545;
	xor.b32  	%r4563, %r4561, %r4553;
	and.b32  	%r4564, %r4563, %r4562;
	xor.b32  	%r4565, %r4564, %r4561;
	add.s32 	%r4566, %r7016, %r4537;
	add.s32 	%r4567, %r4566, %r4565;
	add.s32 	%r4568, %r4567, 1518500249;
	shf.l.wrap.b32 	%r4569, %r4568, %r4568, 9;
	xor.b32  	%r4570, %r4569, %r4553;
	xor.b32  	%r4571, %r4569, %r4561;
	and.b32  	%r4572, %r4571, %r4570;
	xor.b32  	%r4573, %r4572, %r4569;
	add.s32 	%r4574, %r4545, %r4573;
	add.s32 	%r4575, %r4574, 1518500249;
	shf.l.wrap.b32 	%r4576, %r4575, %r4575, 13;
	xor.b32  	%r4577, %r4571, %r4576;
	add.s32 	%r4578, %r7027, %r4553;
	add.s32 	%r4579, %r4578, %r4577;
	add.s32 	%r4580, %r4579, 1859775393;
	shf.l.wrap.b32 	%r4581, %r4580, %r4580, 3;
	xor.b32  	%r4582, %r4576, %r4569;
	xor.b32  	%r4583, %r4582, %r4581;
	add.s32 	%r4584, %r7019, %r4561;
	add.s32 	%r4585, %r4584, %r4583;
	add.s32 	%r4586, %r4585, 1859775393;
	shf.l.wrap.b32 	%r4587, %r4586, %r4586, 9;
	xor.b32  	%r4588, %r4581, %r4576;
	xor.b32  	%r4589, %r4588, %r4587;
	add.s32 	%r4590, %r7023, %r4569;
	add.s32 	%r4591, %r4590, %r4589;
	add.s32 	%r4592, %r4591, 1859775393;
	shf.l.wrap.b32 	%r4593, %r4592, %r4592, 11;
	xor.b32  	%r4594, %r4587, %r4581;
	xor.b32  	%r4595, %r4594, %r4593;
	add.s32 	%r4596, %r7015, %r4576;
	add.s32 	%r4597, %r4596, %r4595;
	add.s32 	%r4598, %r4597, 1859775393;
	shf.l.wrap.b32 	%r4599, %r4598, %r4598, 15;
	xor.b32  	%r4600, %r4593, %r4587;
	xor.b32  	%r4601, %r4600, %r4599;
	add.s32 	%r4602, %r7025, %r4581;
	add.s32 	%r4603, %r4602, %r4601;
	add.s32 	%r4604, %r4603, 1859775393;
	shf.l.wrap.b32 	%r4605, %r4604, %r4604, 3;
	xor.b32  	%r4606, %r4599, %r4593;
	xor.b32  	%r4607, %r4606, %r4605;
	add.s32 	%r4608, %r7017, %r4587;
	add.s32 	%r4609, %r4608, %r4607;
	add.s32 	%r4610, %r4609, 1859775393;
	shf.l.wrap.b32 	%r4611, %r4610, %r4610, 9;
	xor.b32  	%r4612, %r4605, %r4599;
	xor.b32  	%r4613, %r4612, %r4611;
	add.s32 	%r4614, %r7021, %r4593;
	add.s32 	%r4615, %r4614, %r4613;
	add.s32 	%r4616, %r4615, 1859775393;
	shf.l.wrap.b32 	%r4617, %r4616, %r4616, 11;
	xor.b32  	%r4618, %r4611, %r4605;
	xor.b32  	%r4619, %r4618, %r4617;
	add.s32 	%r4620, %r4441, %r4599;
	add.s32 	%r4621, %r4620, %r4619;
	add.s32 	%r4622, %r4621, 1859775393;
	shf.l.wrap.b32 	%r4623, %r4622, %r4622, 15;
	xor.b32  	%r4624, %r4617, %r4611;
	xor.b32  	%r4625, %r4624, %r4623;
	add.s32 	%r4626, %r7026, %r4605;
	add.s32 	%r4627, %r4626, %r4625;
	add.s32 	%r4628, %r4627, 1859775393;
	shf.l.wrap.b32 	%r4629, %r4628, %r4628, 3;
	xor.b32  	%r4630, %r4623, %r4617;
	xor.b32  	%r4631, %r4630, %r4629;
	add.s32 	%r4632, %r7018, %r4611;
	add.s32 	%r4633, %r4632, %r4631;
	add.s32 	%r4634, %r4633, 1859775393;
	shf.l.wrap.b32 	%r4635, %r4634, %r4634, 9;
	xor.b32  	%r4636, %r4629, %r4623;
	xor.b32  	%r4637, %r4636, %r4635;
	add.s32 	%r4638, %r7022, %r4617;
	add.s32 	%r4639, %r4638, %r4637;
	add.s32 	%r4640, %r4639, 1859775393;
	shf.l.wrap.b32 	%r4641, %r4640, %r4640, 11;
	xor.b32  	%r4642, %r4635, %r4629;
	xor.b32  	%r4643, %r4642, %r4641;
	add.s32 	%r4644, %r7014, %r4623;
	add.s32 	%r4645, %r4644, %r4643;
	add.s32 	%r4646, %r4645, 1859775393;
	shf.l.wrap.b32 	%r4647, %r4646, %r4646, 15;
	xor.b32  	%r4648, %r4641, %r4635;
	xor.b32  	%r4649, %r4648, %r4647;
	add.s32 	%r4650, %r7024, %r4629;
	add.s32 	%r4651, %r4650, %r4649;
	add.s32 	%r4652, %r4651, 1859775393;
	shf.l.wrap.b32 	%r4653, %r4652, %r4652, 3;
	xor.b32  	%r4654, %r4647, %r4641;
	xor.b32  	%r4655, %r4654, %r4653;
	add.s32 	%r4656, %r7016, %r4635;
	add.s32 	%r4657, %r4656, %r4655;
	add.s32 	%r4658, %r4657, 1859775393;
	shf.l.wrap.b32 	%r4659, %r4658, %r4658, 9;
	xor.b32  	%r4660, %r4653, %r4647;
	xor.b32  	%r4661, %r4660, %r4659;
	add.s32 	%r4662, %r7020, %r4641;
	add.s32 	%r4663, %r4662, %r4661;
	add.s32 	%r4664, %r4663, 1859775393;
	shf.l.wrap.b32 	%r4665, %r4664, %r4664, 11;
	xor.b32  	%r4666, %r4659, %r4653;
	xor.b32  	%r4667, %r4666, %r4665;
	add.s32 	%r4668, %r4647, %r4667;
	add.s32 	%r4669, %r4668, 1859775393;
	shf.l.wrap.b32 	%r4670, %r4669, %r4669, 15;
	add.s32 	%r4671, %r4653, %r101;
	add.s32 	%r4672, %r4670, %r100;
	add.s32 	%r4673, %r4665, %r99;
	add.s32 	%r4674, %r4659, %r98;
	setp.eq.s32	%p70, %r4671, %r2;
	setp.eq.s32	%p71, %r4674, %r3;
	and.pred  	%p72, %p70, %p71;
	setp.eq.s32	%p73, %r4673, %r4;
	and.pred  	%p74, %p72, %p73;
	setp.eq.s32	%p75, %r4672, %r5;
	and.pred  	%p76, %p74, %p75;
	@!%p76 bra 	BB2_118;
	bra.uni 	BB2_114;

BB2_114:
	atom.global.add.u32 	%r4675, [%rd2], 1;
	setp.ne.s32	%p77, %r4675, 0;
	@%p77 bra 	BB2_118;

	ld.param.u32 	%r6891, [m01000_sxx_param_31];
	atom.global.add.u32 	%r667, [%rd9], 1;
	setp.lt.u32	%p78, %r667, %r6891;
	@%p78 bra 	BB2_117;
	bra.uni 	BB2_116;

BB2_117:
	ld.param.u32 	%r6893, [m01000_sxx_param_27];
	ld.param.u64 	%rd33, [m01000_sxx_param_14];
	ld.param.u32 	%r6892, [m01000_sxx_param_32];
	mul.wide.u32 	%rd30, %r667, 20;
	add.s64 	%rd31, %rd33, %rd30;
	st.global.u32 	[%rd31], %r6893;
	mov.u32 	%r4677, 0;
	st.global.u32 	[%rd31+4], %r4677;
	st.global.u32 	[%rd31+8], %r6892;
	st.global.u32 	[%rd31+12], %r1;
	st.global.u32 	[%rd31+16], %r6922;
	bra.uni 	BB2_118;

BB2_116:
	atom.global.add.u32 	%r4676, [%rd9], -1;

BB2_118:
	ld.param.u32 	%r6889, [m01000_sxx_param_30];
	add.s32 	%r6922, %r6922, 1;
	setp.lt.u32	%p79, %r6922, %r6889;
	@%p79 bra 	BB2_8;

BB2_119:
	ret;
}


  