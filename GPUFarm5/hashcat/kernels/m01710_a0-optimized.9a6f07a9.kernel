//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: UNKNOWN
// Driver 
// Based on LLVM 3.4svn
//

.version 6.2
.target sm_61, texmode_independent
.address_size 64

	// .globl	gpu_decompress
.const .align 8 .b8 k_sha512[640] = {34, 174, 40, 215, 152, 47, 138, 66, 205, 101, 239, 35, 145, 68, 55, 113, 47, 59, 77, 236, 207, 251, 192, 181, 188, 219, 137, 129, 165, 219, 181, 233, 56, 181, 72, 243, 91, 194, 86, 57, 25, 208, 5, 182, 241, 17, 241, 89, 155, 79, 25, 175, 164, 130, 63, 146, 24, 129, 109, 218, 213, 94, 28, 171, 66, 2, 3, 163, 152, 170, 7, 216, 190, 111, 112, 69, 1, 91, 131, 18, 140, 178, 228, 78, 190, 133, 49, 36, 226, 180, 255, 213, 195, 125, 12, 85, 111, 137, 123, 242, 116, 93, 190, 114, 177, 150, 22, 59, 254, 177, 222, 128, 53, 18, 199, 37, 167, 6, 220, 155, 148, 38, 105, 207, 116, 241, 155, 193, 210, 74, 241, 158, 193, 105, 155, 228, 227, 37, 79, 56, 134, 71, 190, 239, 181, 213, 140, 139, 198, 157, 193, 15, 101, 156, 172, 119, 204, 161, 12, 36, 117, 2, 43, 89, 111, 44, 233, 45, 131, 228, 166, 110, 170, 132, 116, 74, 212, 251, 65, 189, 220, 169, 176, 92, 181, 83, 17, 131, 218, 136, 249, 118, 171, 223, 102, 238, 82, 81, 62, 152, 16, 50, 180, 45, 109, 198, 49, 168, 63, 33, 251, 152, 200, 39, 3, 176, 228, 14, 239, 190, 199, 127, 89, 191, 194, 143, 168, 61, 243, 11, 224, 198, 37, 167, 10, 147, 71, 145, 167, 213, 111, 130, 3, 224, 81, 99, 202, 6, 112, 110, 14, 10, 103, 41, 41, 20, 252, 47, 210, 70, 133, 10, 183, 39, 38, 201, 38, 92, 56, 33, 27, 46, 237, 42, 196, 90, 252, 109, 44, 77, 223, 179, 149, 157, 19, 13, 56, 83, 222, 99, 175, 139, 84, 115, 10, 101, 168, 178, 119, 60, 187, 10, 106, 118, 230, 174, 237, 71, 46, 201, 194, 129, 59, 53, 130, 20, 133, 44, 114, 146, 100, 3, 241, 76, 161, 232, 191, 162, 1, 48, 66, 188, 75, 102, 26, 168, 145, 151, 248, 208, 112, 139, 75, 194, 48, 190, 84, 6, 163, 81, 108, 199, 24, 82, 239, 214, 25, 232, 146, 209, 16, 169, 101, 85, 36, 6, 153, 214, 42, 32, 113, 87, 133, 53, 14, 244, 184, 209, 187, 50, 112, 160, 106, 16, 200, 208, 210, 184, 22, 193, 164, 25, 83, 171, 65, 81, 8, 108, 55, 30, 153, 235, 142, 223, 76, 119, 72, 39, 168, 72, 155, 225, 181, 188, 176, 52, 99, 90, 201, 197, 179, 12, 28, 57, 203, 138, 65, 227, 74, 170, 216, 78, 115, 227, 99, 119, 79, 202, 156, 91, 163, 184, 178, 214, 243, 111, 46, 104, 252, 178, 239, 93, 238, 130, 143, 116, 96, 47, 23, 67, 111, 99, 165, 120, 114, 171, 240, 161, 20, 120, 200, 132, 236, 57, 100, 26, 8, 2, 199, 140, 40, 30, 99, 35, 250, 255, 190, 144, 233, 189, 130, 222, 235, 108, 80, 164, 21, 121, 198, 178, 247, 163, 249, 190, 43, 83, 114, 227, 242, 120, 113, 198, 156, 97, 38, 234, 206, 62, 39, 202, 7, 194, 192, 33, 199, 184, 134, 209, 30, 235, 224, 205, 214, 125, 218, 234, 120, 209, 110, 238, 127, 79, 125, 245, 186, 111, 23, 114, 170, 103, 240, 6, 166, 152, 200, 162, 197, 125, 99, 10, 174, 13, 249, 190, 4, 152, 63, 17, 27, 71, 28, 19, 53, 11, 113, 27, 132, 125, 4, 35, 245, 119, 219, 40, 147, 36, 199, 64, 123, 171, 202, 50, 188, 190, 201, 21, 10, 190, 158, 60, 76, 13, 16, 156, 196, 103, 29, 67, 182, 66, 62, 203, 190, 212, 197, 76, 42, 126, 101, 252, 156, 41, 127, 89, 236, 250, 214, 58, 171, 111, 203, 95, 23, 88, 71, 74, 140, 25, 68, 108};

.entry gpu_decompress(
	.param .u64 .ptr .global .align 4 gpu_decompress_param_0,
	.param .u64 .ptr .global .align 4 gpu_decompress_param_1,
	.param .u64 .ptr .global .align 4 gpu_decompress_param_2,
	.param .u64 gpu_decompress_param_3
)
{
	.local .align 4 .b8 	__local_depot0[260];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<10>;
	.reg .b32 	%r<56>;
	.reg .b64 	%rd<44>;


	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd5, [gpu_decompress_param_0];
	ld.param.u64 	%rd6, [gpu_decompress_param_1];
	ld.param.u64 	%rd7, [gpu_decompress_param_2];
	ld.param.u64 	%rd8, [gpu_decompress_param_3];
	add.u64 	%rd9, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd9;
	mov.u32 	%r23, %ctaid.x;
	mov.u32 	%r24, %ntid.x;
	mov.b32	%r25, %envreg3;
	mad.lo.s32 	%r26, %r23, %r24, %r25;
	mov.u32 	%r27, %tid.x;
	add.s32 	%r1, %r26, %r27;
	cvt.s64.s32	%rd10, %r1;
	setp.ge.u64	%p1, %rd10, %rd8;
	@%p1 bra 	BB0_12;

	mul.wide.s32 	%rd11, %r1, 12;
	add.s64 	%rd12, %rd5, %rd11;
	ld.global.u32 	%r2, [%rd12];
	ld.global.u32 	%r3, [%rd12+4];
	ld.global.u32 	%r4, [%rd12+8];
	mov.u64 	%rd13, 0;
	st.local.u32 	[%rd1+4], %rd13;
	st.local.u32 	[%rd1], %rd13;
	st.local.u32 	[%rd1+12], %rd13;
	st.local.u32 	[%rd1+8], %rd13;
	st.local.u32 	[%rd1+20], %rd13;
	st.local.u32 	[%rd1+16], %rd13;
	st.local.u32 	[%rd1+28], %rd13;
	st.local.u32 	[%rd1+24], %rd13;
	st.local.u32 	[%rd1+36], %rd13;
	st.local.u32 	[%rd1+32], %rd13;
	st.local.u32 	[%rd1+44], %rd13;
	st.local.u32 	[%rd1+40], %rd13;
	st.local.u32 	[%rd1+52], %rd13;
	st.local.u32 	[%rd1+48], %rd13;
	st.local.u32 	[%rd1+60], %rd13;
	st.local.u32 	[%rd1+56], %rd13;
	st.local.u32 	[%rd1+68], %rd13;
	st.local.u32 	[%rd1+64], %rd13;
	st.local.u32 	[%rd1+76], %rd13;
	st.local.u32 	[%rd1+72], %rd13;
	st.local.u32 	[%rd1+84], %rd13;
	st.local.u32 	[%rd1+80], %rd13;
	st.local.u32 	[%rd1+92], %rd13;
	st.local.u32 	[%rd1+88], %rd13;
	st.local.u32 	[%rd1+100], %rd13;
	st.local.u32 	[%rd1+96], %rd13;
	st.local.u32 	[%rd1+108], %rd13;
	st.local.u32 	[%rd1+104], %rd13;
	st.local.u32 	[%rd1+116], %rd13;
	st.local.u32 	[%rd1+112], %rd13;
	st.local.u32 	[%rd1+124], %rd13;
	st.local.u32 	[%rd1+120], %rd13;
	st.local.u32 	[%rd1+132], %rd13;
	st.local.u32 	[%rd1+128], %rd13;
	st.local.u32 	[%rd1+140], %rd13;
	st.local.u32 	[%rd1+136], %rd13;
	st.local.u32 	[%rd1+148], %rd13;
	st.local.u32 	[%rd1+144], %rd13;
	st.local.u32 	[%rd1+156], %rd13;
	st.local.u32 	[%rd1+152], %rd13;
	st.local.u32 	[%rd1+164], %rd13;
	st.local.u32 	[%rd1+160], %rd13;
	st.local.u32 	[%rd1+172], %rd13;
	st.local.u32 	[%rd1+168], %rd13;
	st.local.u32 	[%rd1+180], %rd13;
	st.local.u32 	[%rd1+176], %rd13;
	st.local.u32 	[%rd1+188], %rd13;
	st.local.u32 	[%rd1+184], %rd13;
	st.local.u32 	[%rd1+196], %rd13;
	st.local.u32 	[%rd1+192], %rd13;
	st.local.u32 	[%rd1+204], %rd13;
	st.local.u32 	[%rd1+200], %rd13;
	st.local.u32 	[%rd1+212], %rd13;
	st.local.u32 	[%rd1+208], %rd13;
	st.local.u32 	[%rd1+220], %rd13;
	st.local.u32 	[%rd1+216], %rd13;
	st.local.u32 	[%rd1+228], %rd13;
	st.local.u32 	[%rd1+224], %rd13;
	st.local.u32 	[%rd1+236], %rd13;
	st.local.u32 	[%rd1+232], %rd13;
	st.local.u32 	[%rd1+244], %rd13;
	st.local.u32 	[%rd1+240], %rd13;
	st.local.u32 	[%rd1+252], %rd13;
	st.local.u32 	[%rd1+248], %rd13;
	setp.eq.s32	%p2, %r3, 0;
	@%p2 bra 	BB0_10;

	and.b32  	%r5, %r3, 3;
	setp.eq.s32	%p3, %r5, 0;
	mov.u32 	%r54, 0;
	@%p3 bra 	BB0_8;

	setp.eq.s32	%p4, %r5, 1;
	mov.u32 	%r50, 0;
	@%p4 bra 	BB0_7;

	setp.eq.s32	%p5, %r5, 2;
	mov.u32 	%r48, 0;
	@%p5 bra 	BB0_6;

	mul.wide.u32 	%rd14, %r2, 4;
	add.s64 	%rd15, %rd6, %rd14;
	ld.global.u32 	%r32, [%rd15];
	st.local.u32 	[%rd1], %r32;
	add.s32 	%r2, %r2, 1;
	mov.u32 	%r48, 1;

BB0_6:
	mul.wide.u32 	%rd16, %r2, 4;
	add.s64 	%rd17, %rd6, %rd16;
	ld.global.u32 	%r33, [%rd17];
	mul.wide.u32 	%rd18, %r48, 4;
	add.s64 	%rd19, %rd1, %rd18;
	st.local.u32 	[%rd19], %r33;
	add.s32 	%r50, %r48, 1;
	add.s32 	%r2, %r2, 1;

BB0_7:
	mul.wide.u32 	%rd20, %r2, 4;
	add.s64 	%rd21, %rd6, %rd20;
	ld.global.u32 	%r34, [%rd21];
	mul.wide.u32 	%rd22, %r50, 4;
	add.s64 	%rd23, %rd1, %rd22;
	st.local.u32 	[%rd23], %r34;
	add.s32 	%r54, %r50, 1;
	add.s32 	%r2, %r2, 1;

BB0_8:
	setp.lt.u32	%p6, %r3, 4;
	@%p6 bra 	BB0_10;

BB0_9:
	mul.wide.u32 	%rd24, %r2, 4;
	add.s64 	%rd25, %rd6, %rd24;
	ld.global.u32 	%r35, [%rd25];
	mul.wide.u32 	%rd26, %r54, 4;
	add.s64 	%rd27, %rd1, %rd26;
	st.local.u32 	[%rd27], %r35;
	add.s32 	%r36, %r2, 1;
	mul.wide.u32 	%rd28, %r36, 4;
	add.s64 	%rd29, %rd6, %rd28;
	ld.global.u32 	%r37, [%rd29];
	add.s32 	%r38, %r54, 1;
	mul.wide.u32 	%rd30, %r38, 4;
	add.s64 	%rd31, %rd1, %rd30;
	st.local.u32 	[%rd31], %r37;
	add.s32 	%r39, %r2, 2;
	mul.wide.u32 	%rd32, %r39, 4;
	add.s64 	%rd33, %rd6, %rd32;
	ld.global.u32 	%r40, [%rd33];
	add.s32 	%r41, %r54, 2;
	mul.wide.u32 	%rd34, %r41, 4;
	add.s64 	%rd35, %rd1, %rd34;
	st.local.u32 	[%rd35], %r40;
	add.s32 	%r42, %r2, 3;
	mul.wide.u32 	%rd36, %r42, 4;
	add.s64 	%rd37, %rd6, %rd36;
	ld.global.u32 	%r43, [%rd37];
	add.s32 	%r44, %r54, 3;
	mul.wide.u32 	%rd38, %r44, 4;
	add.s64 	%rd39, %rd1, %rd38;
	st.local.u32 	[%rd39], %r43;
	add.s32 	%r2, %r2, 4;
	add.s32 	%r54, %r54, 4;
	setp.lt.u32	%p7, %r54, %r3;
	@%p7 bra 	BB0_9;

BB0_10:
	st.local.u32 	[%rd1+256], %r4;
	mul.wide.s32 	%rd40, %r1, 260;
	add.s64 	%rd4, %rd7, %rd40;
	mov.u32 	%r55, 0;
	mov.pred 	%p8, 0;
	@%p8 bra 	BB0_12;

BB0_11:
	mul.wide.s32 	%rd41, %r55, 4;
	add.s64 	%rd42, %rd1, %rd41;
	ld.local.u32 	%r46, [%rd42];
	add.s64 	%rd43, %rd4, %rd41;
	st.global.u32 	[%rd43], %r46;
	add.s32 	%r55, %r55, 1;
	setp.lt.u32	%p9, %r55, 65;
	@%p9 bra 	BB0_11;

BB0_12:
	ret;
}

	// .globl	gpu_memset
.entry gpu_memset(
	.param .u64 .ptr .global .align 16 gpu_memset_param_0,
	.param .u32 gpu_memset_param_1,
	.param .u64 gpu_memset_param_2
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd1, [gpu_memset_param_0];
	ld.param.u32 	%r2, [gpu_memset_param_1];
	ld.param.u64 	%rd2, [gpu_memset_param_2];
	mov.b32	%r3, %envreg3;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mad.lo.s32 	%r6, %r4, %r5, %r3;
	mov.u32 	%r7, %tid.x;
	add.s32 	%r1, %r6, %r7;
	cvt.s64.s32	%rd3, %r1;
	setp.ge.u64	%p1, %rd3, %rd2;
	@%p1 bra 	BB1_2;

	mul.wide.s32 	%rd4, %r1, 16;
	add.s64 	%rd5, %rd1, %rd4;
	st.global.v4.u32 	[%rd5], {%r2, %r2, %r2, %r2};

BB1_2:
	ret;
}

	// .globl	gpu_atinit
.entry gpu_atinit(
	.param .u64 .ptr .global .align 4 gpu_atinit_param_0,
	.param .u64 gpu_atinit_param_1
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<7>;


	ld.param.u64 	%rd2, [gpu_atinit_param_0];
	ld.param.u64 	%rd3, [gpu_atinit_param_1];
	mov.b32	%r2, %envreg3;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mad.lo.s32 	%r5, %r3, %r4, %r2;
	mov.u32 	%r6, %tid.x;
	add.s32 	%r1, %r5, %r6;
	cvt.s64.s32	%rd1, %r1;
	setp.ge.u64	%p1, %rd1, %rd3;
	@%p1 bra 	BB2_2;

	cvt.u32.u64	%r7, %rd1;
	shr.u64 	%rd4, %rd1, 32;
	cvt.u32.u64	%r8, %rd4;
	xor.b32  	%r9, %r7, 1549556828;
	xor.b32  	%r10, %r8, 909522486;
	mul.wide.s32 	%rd5, %r1, 260;
	add.s64 	%rd6, %rd2, %rd5;
	st.global.u32 	[%rd6], %r9;
	st.global.u32 	[%rd6+4], %r10;
	mov.u32 	%r11, 0;
	st.global.u32 	[%rd6+8], %r11;
	st.global.u32 	[%rd6+12], %r11;
	st.global.u32 	[%rd6+16], %r11;
	st.global.u32 	[%rd6+20], %r11;
	st.global.u32 	[%rd6+24], %r11;
	st.global.u32 	[%rd6+28], %r11;
	st.global.u32 	[%rd6+32], %r11;
	st.global.u32 	[%rd6+36], %r11;
	st.global.u32 	[%rd6+40], %r11;
	st.global.u32 	[%rd6+44], %r11;
	st.global.u32 	[%rd6+48], %r11;
	st.global.u32 	[%rd6+52], %r11;
	st.global.u32 	[%rd6+56], %r11;
	st.global.u32 	[%rd6+60], %r11;
	st.global.u32 	[%rd6+64], %r11;
	st.global.u32 	[%rd6+68], %r11;
	st.global.u32 	[%rd6+72], %r11;
	st.global.u32 	[%rd6+76], %r11;
	st.global.u32 	[%rd6+80], %r11;
	st.global.u32 	[%rd6+84], %r11;
	st.global.u32 	[%rd6+88], %r11;
	st.global.u32 	[%rd6+92], %r11;
	st.global.u32 	[%rd6+96], %r11;
	st.global.u32 	[%rd6+100], %r11;
	st.global.u32 	[%rd6+104], %r11;
	st.global.u32 	[%rd6+108], %r11;
	st.global.u32 	[%rd6+112], %r11;
	st.global.u32 	[%rd6+116], %r11;
	st.global.u32 	[%rd6+120], %r11;
	st.global.u32 	[%rd6+124], %r11;
	st.global.u32 	[%rd6+128], %r11;
	st.global.u32 	[%rd6+132], %r11;
	st.global.u32 	[%rd6+136], %r11;
	st.global.u32 	[%rd6+140], %r11;
	st.global.u32 	[%rd6+144], %r11;
	st.global.u32 	[%rd6+148], %r11;
	st.global.u32 	[%rd6+152], %r11;
	st.global.u32 	[%rd6+156], %r11;
	st.global.u32 	[%rd6+160], %r11;
	st.global.u32 	[%rd6+164], %r11;
	st.global.u32 	[%rd6+168], %r11;
	st.global.u32 	[%rd6+172], %r11;
	st.global.u32 	[%rd6+176], %r11;
	st.global.u32 	[%rd6+180], %r11;
	st.global.u32 	[%rd6+184], %r11;
	st.global.u32 	[%rd6+188], %r11;
	st.global.u32 	[%rd6+192], %r11;
	st.global.u32 	[%rd6+196], %r11;
	st.global.u32 	[%rd6+200], %r11;
	st.global.u32 	[%rd6+204], %r11;
	st.global.u32 	[%rd6+208], %r11;
	st.global.u32 	[%rd6+212], %r11;
	st.global.u32 	[%rd6+216], %r11;
	st.global.u32 	[%rd6+220], %r11;
	st.global.u32 	[%rd6+224], %r11;
	st.global.u32 	[%rd6+228], %r11;
	st.global.u32 	[%rd6+232], %r11;
	st.global.u32 	[%rd6+236], %r11;
	st.global.u32 	[%rd6+240], %r11;
	st.global.u32 	[%rd6+244], %r11;
	st.global.u32 	[%rd6+248], %r11;
	st.global.u32 	[%rd6+252], %r11;
	mov.u32 	%r12, 7;
	st.global.u32 	[%rd6+256], %r12;

BB2_2:
	ret;
}

	// .globl	m01710_m04
.entry m01710_m04(
	.param .u64 .ptr .global .align 4 m01710_m04_param_0,
	.param .u64 .ptr .const .align 4 m01710_m04_param_1,
	.param .u64 .ptr .global .align 4 m01710_m04_param_2,
	.param .u64 .ptr .global .align 4 m01710_m04_param_3,
	.param .u64 .ptr .global .align 1 m01710_m04_param_4,
	.param .u64 .ptr .global .align 1 m01710_m04_param_5,
	.param .u64 .ptr .global .align 4 m01710_m04_param_6,
	.param .u64 .ptr .global .align 4 m01710_m04_param_7,
	.param .u64 .ptr .global .align 4 m01710_m04_param_8,
	.param .u64 .ptr .global .align 4 m01710_m04_param_9,
	.param .u64 .ptr .global .align 4 m01710_m04_param_10,
	.param .u64 .ptr .global .align 4 m01710_m04_param_11,
	.param .u64 .ptr .global .align 4 m01710_m04_param_12,
	.param .u64 .ptr .global .align 4 m01710_m04_param_13,
	.param .u64 .ptr .global .align 8 m01710_m04_param_14,
	.param .u64 .ptr .global .align 4 m01710_m04_param_15,
	.param .u64 .ptr .global .align 4 m01710_m04_param_16,
	.param .u64 .ptr .global .align 4 m01710_m04_param_17,
	.param .u64 .ptr .global .align 1 m01710_m04_param_18,
	.param .u64 .ptr .global .align 4 m01710_m04_param_19,
	.param .u64 .ptr .global .align 4 m01710_m04_param_20,
	.param .u64 .ptr .global .align 4 m01710_m04_param_21,
	.param .u64 .ptr .global .align 4 m01710_m04_param_22,
	.param .u64 .ptr .global .align 4 m01710_m04_param_23,
	.param .u32 m01710_m04_param_24,
	.param .u32 m01710_m04_param_25,
	.param .u32 m01710_m04_param_26,
	.param .u32 m01710_m04_param_27,
	.param .u32 m01710_m04_param_28,
	.param .u32 m01710_m04_param_29,
	.param .u32 m01710_m04_param_30,
	.param .u32 m01710_m04_param_31,
	.param .u32 m01710_m04_param_32,
	.param .u32 m01710_m04_param_33,
	.param .u64 m01710_m04_param_34
)
{
	.local .align 16 .b8 	__local_depot3[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<839>;
	.reg .b16 	%rs<24>;
	.reg .b32 	%r<11771>;
	.reg .b64 	%rd<2868>;


	mov.u64 	%SPL, __local_depot3;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd92, [m01710_m04_param_0];
	ld.param.u64 	%rd93, [m01710_m04_param_1];
	ld.param.u64 	%rd105, [m01710_m04_param_17];
	ld.param.u64 	%rd106, [m01710_m04_param_19];
	ld.param.u32 	%r1745, [m01710_m04_param_24];
	ld.param.u32 	%r1748, [m01710_m04_param_27];
	ld.param.u32 	%r1749, [m01710_m04_param_30];
	ld.param.u32 	%r1750, [m01710_m04_param_31];
	ld.param.u32 	%r1751, [m01710_m04_param_32];
	ld.param.u64 	%rd107, [m01710_m04_param_34];
	mov.u32 	%r1752, %ctaid.x;
	mov.u32 	%r1753, %ntid.x;
	mov.b32	%r1754, %envreg3;
	mad.lo.s32 	%r1755, %r1752, %r1753, %r1754;
	mov.u32 	%r1756, %tid.x;
	add.s32 	%r1, %r1755, %r1756;
	cvt.s64.s32	%rd1, %r1;
	setp.ge.u64	%p1, %rd1, %rd107;
	@%p1 bra 	BB3_1098;

	mul.wide.s32 	%rd108, %r1, 260;
	add.s64 	%rd109, %rd92, %rd108;
	ld.global.u32 	%r2, [%rd109];
	ld.global.u32 	%r3, [%rd109+4];
	ld.global.u32 	%r4, [%rd109+8];
	ld.global.u32 	%r5, [%rd109+12];
	ld.global.u32 	%r6, [%rd109+16];
	ld.global.u32 	%r7, [%rd109+20];
	ld.global.u32 	%r8, [%rd109+24];
	ld.global.u32 	%r9, [%rd109+28];
	ld.global.u32 	%r10, [%rd109+256];
	mul.wide.u32 	%rd110, %r1748, 564;
	add.s64 	%rd2, %rd105, %rd110;
	ld.global.u32 	%r11, [%rd2];
	ld.global.u32 	%r12, [%rd2+4];
	ld.global.u32 	%r13, [%rd2+8];
	ld.global.u32 	%r14, [%rd2+12];
	ld.global.u32 	%r15, [%rd2+16];
	ld.global.u32 	%r16, [%rd2+20];
	ld.global.u32 	%r17, [%rd2+24];
	ld.global.u32 	%r18, [%rd2+28];
	ld.global.u32 	%r19, [%rd2+32];
	ld.global.u32 	%r20, [%rd2+36];
	ld.global.u32 	%r21, [%rd2+40];
	ld.global.u32 	%r22, [%rd2+44];
	ld.global.u32 	%r23, [%rd2+48];
	ld.global.u32 	%r24, [%rd2+52];
	setp.eq.s32	%p2, %r1749, 0;
	@%p2 bra 	BB3_1098;

	ld.global.u32 	%r25, [%rd2+512];
	mov.u64 	%rd111, 5840696475078001361;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1759,%dummy}, %rd111;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1760}, %rd111;
	}
	shf.r.wrap.b32 	%r1761, %r1760, %r1759, 18;
	shf.r.wrap.b32 	%r1762, %r1759, %r1760, 18;
	mov.b64 	%rd112, {%r1762, %r1761};
	shf.r.wrap.b32 	%r1763, %r1760, %r1759, 14;
	shf.r.wrap.b32 	%r1764, %r1759, %r1760, 14;
	mov.b64 	%rd113, {%r1764, %r1763};
	xor.b64  	%rd114, %rd112, %rd113;
	shf.l.wrap.b32 	%r1765, %r1759, %r1760, 23;
	shf.l.wrap.b32 	%r1766, %r1760, %r1759, 23;
	mov.b64 	%rd115, {%r1766, %r1765};
	xor.b64  	%rd4, %rd114, %rd115;
	mov.u64 	%rd116, 7640891576956012808;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1767}, %rd116;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1768,%dummy}, %rd116;
	}
	shf.l.wrap.b32 	%r1769, %r1768, %r1767, 30;
	shf.l.wrap.b32 	%r1770, %r1767, %r1768, 30;
	mov.b64 	%rd117, {%r1770, %r1769};
	shf.r.wrap.b32 	%r1771, %r1767, %r1768, 28;
	shf.r.wrap.b32 	%r1772, %r1768, %r1767, 28;
	mov.b64 	%rd118, {%r1772, %r1771};
	xor.b64  	%rd119, %rd117, %rd118;
	shf.l.wrap.b32 	%r1773, %r1768, %r1767, 25;
	shf.l.wrap.b32 	%r1774, %r1767, %r1768, 25;
	mov.b64 	%rd120, {%r1774, %r1773};
	xor.b64  	%rd5, %rd119, %rd120;
	and.b64  	%rd21, %rd1, 4294967295;
	mov.u64 	%rd121, 0;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1775}, %rd121;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1776,%dummy}, %rd121;
	}
	shf.l.wrap.b32 	%r1777, %r1776, %r1775, 3;
	shf.l.wrap.b32 	%r1778, %r1775, %r1776, 3;
	mov.b64 	%rd122, {%r1778, %r1777};
	shf.r.wrap.b32 	%r1779, %r1775, %r1776, 19;
	shf.r.wrap.b32 	%r1780, %r1776, %r1775, 19;
	mov.b64 	%rd123, {%r1780, %r1779};
	xor.b64  	%rd22, %rd123, %rd122;
	shf.r.wrap.b32 	%r1781, %r1775, %r1776, 8;
	shf.r.wrap.b32 	%r1782, %r1776, %r1775, 8;
	mov.b64 	%rd124, {%r1782, %r1781};
	shf.r.wrap.b32 	%r1783, %r1775, %r1776, 1;
	shf.r.wrap.b32 	%r1784, %r1776, %r1775, 1;
	mov.b64 	%rd125, {%r1784, %r1783};
	xor.b64  	%rd23, %rd125, %rd124;
	mov.u32 	%r11551, 0;

BB3_3:
	mov.u32 	%r11561, 0;
	cvt.u64.u32	%rd85, %r11551;
	mul.wide.u32 	%rd126, %r11551, 128;
	add.s64 	%rd127, %rd93, %rd126;
	ld.const.u32 	%r11560, [%rd127];
	setp.eq.s32	%p3, %r11560, 0;
	mov.u32 	%r11752, %r10;
	mov.u32 	%r11743, %r5;
	mov.u32 	%r11742, %r4;
	mov.u32 	%r11741, %r3;
	mov.u32 	%r11740, %r2;
	mov.u32 	%r11739, %r6;
	mov.u32 	%r11738, %r7;
	mov.u32 	%r11737, %r8;
	mov.u32 	%r11736, %r9;
	@%p3 bra 	BB3_1028;
	bra.uni 	BB3_4;

BB3_375:
	setp.gt.s32	%p284, %r56, 23;
	@%p284 bra 	BB3_391;

	setp.gt.s32	%p296, %r56, 19;
	@%p296 bra 	BB3_384;

	setp.gt.s32	%p302, %r56, 17;
	@%p302 bra 	BB3_381;

	setp.eq.s32	%p305, %r56, 16;
	@%p305 bra 	BB3_414;
	bra.uni 	BB3_379;

BB3_414:
	mov.u32 	%r11626, 0;
	mov.u32 	%r54, %r47;
	mov.u32 	%r53, %r48;
	mov.u32 	%r52, %r49;
	mov.u32 	%r51, %r50;
	bra.uni 	BB3_415;

BB3_511:
	setp.eq.s32	%p481, %r5357, 2;
	@%p481 bra 	BB3_523;
	bra.uni 	BB3_512;

BB3_523:
	and.b32  	%r5378, %r724, %r48;
	or.b32  	%r5379, %r5378, %r723;
	and.b32  	%r5380, %r11750, %r725;
	or.b32  	%r11742, %r5379, %r5380;
	mov.u32 	%r11736, %r11747;
	mov.u32 	%r11737, %r11746;
	mov.u32 	%r11738, %r11745;
	mov.u32 	%r11739, %r11744;
	mov.u32 	%r11740, %r50;
	mov.u32 	%r11741, %r49;
	mov.u32 	%r11743, %r11751;
	bra.uni 	BB3_1027;

BB3_654:
	setp.gt.s32	%p574, %r56, 11;
	@%p574 bra 	BB3_662;

	setp.gt.s32	%p580, %r56, 9;
	@%p580 bra 	BB3_659;

	setp.eq.s32	%p583, %r56, 8;
	@%p583 bra 	BB3_714;
	bra.uni 	BB3_657;

BB3_714:
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r54;
	mov.u32 	%r11739, %r53;
	mov.u32 	%r11740, %r48;
	mov.u32 	%r11741, %r47;
	mov.u32 	%r11742, %r51;
	mov.u32 	%r54, %r52;
	bra.uni 	BB3_724;

BB3_486:
	setp.gt.s32	%p460, %r5275, 5;
	@%p460 bra 	BB3_490;

	setp.eq.s32	%p463, %r5275, 4;
	@%p463 bra 	BB3_499;
	bra.uni 	BB3_488;

BB3_499:
	and.b32  	%r11739, %r697, %r51;
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	bra.uni 	BB3_497;

BB3_952:
	setp.gt.s32	%p770, %r8013, 11;
	@%p770 bra 	BB3_960;

	setp.gt.s32	%p776, %r8013, 9;
	@%p776 bra 	BB3_957;

	setp.eq.s32	%p779, %r8013, 8;
	@%p779 bra 	BB3_1012;
	bra.uni 	BB3_955;

BB3_1012:
	mov.u32 	%r11747, 0;
	mov.u32 	%r11748, %r52;
	mov.u32 	%r11749, %r51;
	mov.u32 	%r11750, %r47;
	mov.u32 	%r11751, %r48;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11745, %r50;
	mov.u32 	%r11744, %r49;
	bra.uni 	BB3_1020;

BB3_152:
	setp.gt.s32	%p131, %r56, 23;
	@%p131 bra 	BB3_168;

	setp.gt.s32	%p143, %r56, 19;
	@%p143 bra 	BB3_161;

	setp.gt.s32	%p149, %r56, 17;
	@%p149 bra 	BB3_158;

	setp.eq.s32	%p152, %r56, 16;
	@%p152 bra 	BB3_193;
	bra.uni 	BB3_156;

BB3_193:
	mov.u32 	%r11748, 0;
	mov.u32 	%r11744, %r50;
	mov.u32 	%r11745, %r49;
	mov.u32 	%r11746, %r48;
	mov.u32 	%r11747, %r47;
	bra.uni 	BB3_194;

BB3_229:
	setp.gt.s32	%p192, %r3326, 5;
	@%p192 bra 	BB3_233;

	setp.eq.s32	%p195, %r3326, 4;
	@%p195 bra 	BB3_238;
	bra.uni 	BB3_231;

BB3_238:
	and.b32  	%r11744, %r248, %r51;
	mov.u32 	%r11745, 0;
	mov.u32 	%r11746, %r11745;
	mov.u32 	%r11747, %r11745;
	bra.uni 	BB3_236;

BB3_518:
	setp.eq.s32	%p476, %r5357, 6;
	@%p476 bra 	BB3_521;
	bra.uni 	BB3_519;

BB3_521:
	and.b32  	%r5366, %r724, %r53;
	or.b32  	%r5367, %r5366, %r723;
	and.b32  	%r5368, %r11746, %r725;
	or.b32  	%r11737, %r5367, %r5368;
	mov.u32 	%r11736, %r11747;
	bra.uni 	BB3_116;

BB3_687:
	setp.gt.s32	%p551, %r56, 27;
	@%p551 bra 	BB3_696;

	setp.gt.s32	%p557, %r56, 25;
	@%p557 bra 	BB3_692;

	setp.eq.s32	%p560, %r56, 24;
	@%p560 bra 	BB3_706;
	bra.uni 	BB3_690;

BB3_706:
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;
	mov.u32 	%r11740, %r53;
	mov.u32 	%r11741, %r54;
	bra.uni 	BB3_722;

BB3_984:
	setp.gt.s32	%p747, %r8013, 27;
	@%p747 bra 	BB3_992;

	setp.gt.s32	%p753, %r8013, 25;
	@%p753 bra 	BB3_989;

	setp.eq.s32	%p756, %r8013, 24;
	@%p756 bra 	BB3_1001;
	bra.uni 	BB3_987;

BB3_1001:
	mov.u32 	%r11750, 0;
	mov.u32 	%r11748, %r49;
	mov.u32 	%r11749, %r50;
	bra.uni 	BB3_1002;

BB3_543:
	setp.gt.s32	%p513, %r58, 11;
	@%p513 bra 	BB3_551;

	setp.gt.s32	%p519, %r58, 9;
	@%p519 bra 	BB3_548;

	setp.eq.s32	%p522, %r58, 8;
	@%p522 bra 	BB3_607;
	bra.uni 	BB3_546;

BB3_607:
	mov.u32 	%r11744, %r53;
	mov.u32 	%r11745, %r54;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r48;
	mov.u32 	%r11749, %r47;
	mov.u32 	%r11750, %r51;
	mov.u32 	%r11751, %r52;
	bra.uni 	BB3_612;

BB3_930:
	setp.eq.s32	%p741, %r7834, 2;
	@%p741 bra 	BB3_942;
	bra.uni 	BB3_931;

BB3_942:
	// inline asm
	prmt.b32 %r11727, %r51, %r52, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11726, %r47, %r51, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11725, %r48, %r47, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11724, %r49, %r48, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11723, %r50, %r49, %r1340;
	// inline asm
	mov.u32 	%r11720, 0;
	// inline asm
	prmt.b32 %r11722, %r11720, %r50, %r1340;
	// inline asm
	mov.u32 	%r11721, %r11720;
	bra.uni 	BB3_944;

BB3_830:
	setp.gt.s32	%p662, %r6846, 23;
	@%p662 bra 	BB3_847;

	setp.gt.s32	%p674, %r6846, 19;
	@%p674 bra 	BB3_840;

	setp.gt.s32	%p680, %r6846, 17;
	@%p680 bra 	BB3_836;

	setp.eq.s32	%p683, %r6846, 16;
	@%p683 bra 	BB3_873;
	bra.uni 	BB3_834;

BB3_873:
	mov.u32 	%r11751, %r50;
	mov.u32 	%r11750, %r49;
	mov.u32 	%r11749, %r48;
	mov.u32 	%r11748, %r47;
	bra.uni 	BB3_867;

BB3_751:
	setp.gt.s32	%p607, %r6571, 5;
	@%p607 bra 	BB3_755;

	setp.eq.s32	%p610, %r6571, 4;
	@%p610 bra 	BB3_764;
	bra.uni 	BB3_753;

BB3_764:
	and.b32  	%r6582, %r1002, %r51;
	and.b32  	%r6583, %r11744, %r1003;
	or.b32  	%r11739, %r6583, %r6582;
	mov.u32 	%r11736, %r11747;
	mov.u32 	%r11737, %r11746;
	mov.u32 	%r11738, %r11745;
	bra.uni 	BB3_761;

BB3_360:
	setp.gt.s32	%p308, %r56, 11;
	@%p308 bra 	BB3_368;

	setp.gt.s32	%p314, %r56, 9;
	@%p314 bra 	BB3_365;

	setp.eq.s32	%p317, %r56, 8;
	@%p317 bra 	BB3_420;
	bra.uni 	BB3_363;

BB3_420:
	mov.u32 	%r11626, 0;
	mov.u32 	%r54, %r52;
	mov.u32 	%r53, %r51;
	mov.u32 	%r52, %r47;
	mov.u32 	%r51, %r48;
	mov.u32 	%r11627, %r11626;
	mov.u32 	%r11628, %r50;
	mov.u32 	%r50, %r49;
	bra.uni 	BB3_428;

BB3_772:
	setp.ne.s32	%p633, %r6642, 1;
	mov.u32 	%r11685, %r6641;
	@%p633 bra 	BB3_775;

	and.b32  	%r6648, %r11748, %r51;
	and.b32  	%r6649, %r11749, %r52;
	or.b32  	%r6650, %r6648, %r6649;
	and.b32  	%r6651, %r11750, %r53;
	or.b32  	%r6652, %r6650, %r6651;
	and.b32  	%r6653, %r11751, %r54;
	or.b32  	%r11685, %r6652, %r6653;

BB3_775:
	shr.u32 	%r6693, %r11685, %r1039;
	and.b32  	%r6694, %r6693, 255;
	mov.u32 	%r6691, 24;
	// inline asm
	shf.r.wrap.b32 %r11736, %r53, %r54, %r6691;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11737, %r52, %r53, %r6691;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11738, %r51, %r52, %r6691;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11739, %r47, %r51, %r6691;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11743, %r48, %r47, %r6691;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11742, %r49, %r48, %r6691;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r50, %r49, %r6691;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6688, %r6641, %r50, %r6691;
	// inline asm
	or.b32  	%r11740, %r6688, %r6694;
	and.b32  	%r6695, %r38, 3;
	shl.b32 	%r6696, %r6695, 3;
	mov.u32 	%r6697, 1;
	shl.b32 	%r6698, %r6697, %r6696;
	add.s32 	%r1055, %r6698, -1;
	shr.u32 	%r6692, %r38, 2;
	setp.gt.s32	%p634, %r6692, 3;
	@%p634 bra 	BB3_784;

	setp.gt.s32	%p640, %r6692, 1;
	@%p640 bra 	BB3_781;

	setp.eq.s32	%p643, %r6692, 0;
	@%p643 bra 	BB3_795;
	bra.uni 	BB3_778;

BB3_795:
	and.b32  	%r11740, %r11740, %r1055;
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;
	mov.u32 	%r11741, %r11736;
	bra.uni 	BB3_780;

BB3_577:
	setp.gt.s32	%p490, %r58, 27;
	@%p490 bra 	BB3_587;

	setp.gt.s32	%p496, %r58, 25;
	@%p496 bra 	BB3_582;

	setp.eq.s32	%p499, %r58, 24;
	@%p499 bra 	BB3_599;
	bra.uni 	BB3_580;

BB3_599:
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r53;
	mov.u32 	%r11749, %r54;
	bra.uni 	BB3_597;

BB3_937:
	setp.eq.s32	%p736, %r7834, 6;
	@%p736 bra 	BB3_940;
	bra.uni 	BB3_938;

BB3_940:
	// inline asm
	prmt.b32 %r11727, %r50, %r49, %r1340;
	// inline asm
	mov.u32 	%r11720, 0;
	// inline asm
	prmt.b32 %r11726, %r11720, %r50, %r1340;
	// inline asm
	mov.u32 	%r11721, %r11720;
	mov.u32 	%r11722, %r11720;
	mov.u32 	%r11723, %r11720;
	mov.u32 	%r11724, %r11720;
	mov.u32 	%r11725, %r11720;
	bra.uni 	BB3_944;

BB3_391:
	setp.gt.s32	%p285, %r56, 27;
	@%p285 bra 	BB3_399;

	setp.gt.s32	%p291, %r56, 25;
	@%p291 bra 	BB3_396;

	setp.eq.s32	%p294, %r56, 24;
	@%p294 bra 	BB3_408;
	bra.uni 	BB3_394;

BB3_408:
	mov.u32 	%r52, 0;
	mov.u32 	%r54, %r49;
	mov.u32 	%r53, %r50;
	bra.uni 	BB3_409;

BB3_784:
	setp.gt.s32	%p635, %r6692, 5;
	@%p635 bra 	BB3_788;

	setp.eq.s32	%p638, %r6692, 4;
	@%p638 bra 	BB3_792;
	bra.uni 	BB3_786;

BB3_792:
	and.b32  	%r11739, %r11739, %r1055;
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	bra.uni 	BB3_1026;

BB3_508:
	setp.eq.s32	%p484, %r5357, 1;
	@%p484 bra 	BB3_509;
	bra.uni 	BB3_114;

BB3_509:
	and.b32  	%r5381, %r724, %r49;
	or.b32  	%r5382, %r5381, %r723;
	and.b32  	%r5383, %r11749, %r725;
	or.b32  	%r11741, %r5382, %r5383;
	mov.u32 	%r11736, %r11747;
	mov.u32 	%r11737, %r11746;
	mov.u32 	%r11738, %r11745;
	mov.u32 	%r11739, %r11744;
	mov.u32 	%r11740, %r50;

BB3_510:
	mov.u32 	%r11742, %r11750;
	mov.u32 	%r11743, %r11751;
	bra.uni 	BB3_1027;

BB3_647:
	setp.gt.s32	%p586, %r56, 5;
	@%p586 bra 	BB3_651;

	setp.eq.s32	%p589, %r56, 4;
	@%p589 bra 	BB3_716;
	bra.uni 	BB3_649;

BB3_716:
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r54;
	mov.u32 	%r11738, %r53;
	mov.u32 	%r11739, %r52;
	mov.u32 	%r11740, %r49;
	mov.u32 	%r11741, %r48;
	mov.u32 	%r11742, %r47;
	mov.u32 	%r54, %r51;
	bra.uni 	BB3_724;

BB3_483:
	setp.eq.s32	%p466, %r5275, 2;
	@%p466 bra 	BB3_500;
	bra.uni 	BB3_484;

BB3_500:
	and.b32  	%r11742, %r697, %r48;
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;
	mov.u32 	%r11740, %r50;
	mov.u32 	%r11741, %r49;
	bra.uni 	BB3_501;

BB3_945:
	setp.gt.s32	%p782, %r8013, 5;
	@%p782 bra 	BB3_949;

	setp.eq.s32	%p785, %r8013, 4;
	@%p785 bra 	BB3_1014;
	bra.uni 	BB3_947;

BB3_1014:
	mov.u32 	%r11747, 0;
	mov.u32 	%r11748, %r53;
	mov.u32 	%r11749, %r52;
	mov.u32 	%r11750, %r51;
	mov.u32 	%r11751, %r47;
	mov.u32 	%r11746, %r50;
	mov.u32 	%r11745, %r49;
	mov.u32 	%r11744, %r48;
	bra.uni 	BB3_1020;

BB3_136:
	setp.gt.s32	%p155, %r56, 11;
	@%p155 bra 	BB3_144;

	setp.gt.s32	%p161, %r56, 9;
	@%p161 bra 	BB3_141;

	setp.eq.s32	%p164, %r56, 8;
	@%p164 bra 	BB3_198;
	bra.uni 	BB3_139;

BB3_198:
	mov.u32 	%r11748, 0;
	mov.u32 	%r11744, %r48;
	mov.u32 	%r11745, %r47;
	mov.u32 	%r11746, %r51;
	mov.u32 	%r11747, %r52;
	mov.u32 	%r11749, %r11748;
	mov.u32 	%r11750, %r50;
	mov.u32 	%r11751, %r49;
	bra.uni 	BB3_202;

BB3_226:
	setp.eq.s32	%p198, %r3326, 2;
	@%p198 bra 	BB3_239;
	bra.uni 	BB3_227;

BB3_239:
	and.b32  	%r11750, %r248, %r48;
	mov.u32 	%r11744, 0;
	mov.u32 	%r11745, %r11744;
	mov.u32 	%r11746, %r11744;
	mov.u32 	%r11747, %r11744;
	mov.u32 	%r11748, %r50;
	mov.u32 	%r11749, %r49;
	bra.uni 	BB3_240;

BB3_516:
	setp.eq.s32	%p479, %r5357, 5;
	@%p479 bra 	BB3_517;
	bra.uni 	BB3_114;

BB3_517:
	and.b32  	%r5369, %r724, %r52;
	or.b32  	%r5370, %r5369, %r723;
	and.b32  	%r5371, %r11745, %r725;
	or.b32  	%r11738, %r5370, %r5371;
	mov.u32 	%r11736, %r11747;
	mov.u32 	%r11737, %r11746;
	bra.uni 	BB3_117;

BB3_679:
	setp.gt.s32	%p563, %r56, 21;
	@%p563 bra 	BB3_683;

	setp.eq.s32	%p566, %r56, 20;
	@%p566 bra 	BB3_708;
	bra.uni 	BB3_681;

BB3_708:
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;
	mov.u32 	%r11740, %r52;
	mov.u32 	%r11741, %r53;
	mov.u32 	%r11742, %r54;
	bra.uni 	BB3_723;

BB3_977:
	setp.gt.s32	%p759, %r8013, 21;
	@%p759 bra 	BB3_981;

	setp.eq.s32	%p762, %r8013, 20;
	@%p762 bra 	BB3_1004;
	bra.uni 	BB3_979;

BB3_1004:
	mov.u32 	%r11751, 0;
	mov.u32 	%r11748, %r48;
	mov.u32 	%r11749, %r49;
	mov.u32 	%r11750, %r50;
	bra.uni 	BB3_1005;

BB3_512:
	setp.eq.s32	%p482, %r5357, 3;
	@%p482 bra 	BB3_513;
	bra.uni 	BB3_114;

BB3_513:
	and.b32  	%r5375, %r724, %r47;
	or.b32  	%r5376, %r5375, %r723;
	and.b32  	%r5377, %r11751, %r725;
	or.b32  	%r11743, %r5376, %r5377;
	mov.u32 	%r11736, %r11747;
	mov.u32 	%r11737, %r11746;
	mov.u32 	%r11738, %r11745;
	mov.u32 	%r11739, %r11744;
	mov.u32 	%r11740, %r50;
	mov.u32 	%r11741, %r49;
	mov.u32 	%r11742, %r48;
	bra.uni 	BB3_1027;

BB3_662:
	setp.gt.s32	%p575, %r56, 13;
	@%p575 bra 	BB3_666;

	setp.eq.s32	%p578, %r56, 12;
	@%p578 bra 	BB3_712;
	bra.uni 	BB3_664;

BB3_712:
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r54;
	mov.u32 	%r11740, %r47;
	mov.u32 	%r11741, %r51;
	mov.u32 	%r11742, %r52;
	mov.u32 	%r54, %r53;
	bra.uni 	BB3_724;

BB3_490:
	setp.eq.s32	%p461, %r5275, 6;
	@%p461 bra 	BB3_498;
	bra.uni 	BB3_491;

BB3_498:
	and.b32  	%r11737, %r697, %r53;
	mov.u32 	%r11736, 0;
	bra.uni 	BB3_495;

BB3_960:
	setp.gt.s32	%p771, %r8013, 13;
	@%p771 bra 	BB3_964;

	setp.eq.s32	%p774, %r8013, 12;
	@%p774 bra 	BB3_1010;
	bra.uni 	BB3_962;

BB3_1010:
	mov.u32 	%r11747, 0;
	mov.u32 	%r11748, %r51;
	mov.u32 	%r11749, %r47;
	mov.u32 	%r11750, %r48;
	mov.u32 	%r11751, %r49;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11744, %r50;
	bra.uni 	BB3_1020;

BB3_168:
	setp.gt.s32	%p132, %r56, 27;
	@%p132 bra 	BB3_177;

	setp.gt.s32	%p138, %r56, 25;
	@%p138 bra 	BB3_173;

	setp.eq.s32	%p141, %r56, 24;
	@%p141 bra 	BB3_189;
	bra.uni 	BB3_171;

BB3_189:
	mov.u32 	%r11744, 0;
	mov.u32 	%r11745, %r11744;
	mov.u32 	%r11746, %r50;
	mov.u32 	%r11747, %r49;
	bra.uni 	BB3_187;

BB3_233:
	setp.eq.s32	%p193, %r3326, 6;
	@%p193 bra 	BB3_237;
	bra.uni 	BB3_234;

BB3_237:
	and.b32  	%r11746, %r248, %r53;
	mov.u32 	%r11747, 0;
	mov.u32 	%r11744, %r51;
	mov.u32 	%r11745, %r52;
	bra.uni 	BB3_236;

BB3_519:
	setp.ne.s32	%p477, %r5357, 7;
	@%p477 bra 	BB3_114;

	and.b32  	%r5363, %r724, %r54;
	or.b32  	%r5364, %r5363, %r723;
	and.b32  	%r5365, %r11747, %r725;
	or.b32  	%r11736, %r5364, %r5365;
	bra.uni 	BB3_115;

BB3_114:
	mov.u32 	%r11736, %r54;

BB3_115:
	mov.u32 	%r11737, %r53;

BB3_116:
	mov.u32 	%r11738, %r52;

BB3_117:
	mov.u32 	%r11739, %r51;

BB3_118:
	mov.u32 	%r11740, %r50;
	mov.u32 	%r11741, %r49;
	mov.u32 	%r11742, %r48;
	mov.u32 	%r11743, %r47;
	bra.uni 	BB3_1027;

BB3_696:
	setp.gt.s32	%p552, %r56, 29;
	@%p552 bra 	BB3_700;

	setp.eq.s32	%p555, %r56, 28;
	@%p555 bra 	BB3_704;
	bra.uni 	BB3_698;

BB3_704:
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;
	mov.u32 	%r11740, %r54;
	bra.uni 	BB3_721;

BB3_992:
	setp.gt.s32	%p748, %r8013, 29;
	@%p748 bra 	BB3_996;

	setp.eq.s32	%p751, %r8013, 28;
	@%p751 bra 	BB3_999;
	bra.uni 	BB3_994;

BB3_999:
	mov.u32 	%r11749, 0;
	mov.u32 	%r11748, %r50;
	bra.uni 	BB3_1019;

BB3_536:
	setp.gt.s32	%p525, %r58, 5;
	@%p525 bra 	BB3_540;

	setp.eq.s32	%p528, %r58, 4;
	@%p528 bra 	BB3_609;
	bra.uni 	BB3_538;

BB3_609:
	mov.u32 	%r11744, %r52;
	mov.u32 	%r11745, %r53;
	mov.u32 	%r11746, %r54;
	mov.u32 	%r11748, %r49;
	mov.u32 	%r11749, %r48;
	mov.u32 	%r11750, %r47;
	mov.u32 	%r11751, %r51;
	bra.uni 	BB3_612;

BB3_928:
	setp.eq.s32	%p744, %r7834, 1;
	mov.u32 	%r11721, %r11720;
	mov.u32 	%r11722, %r11720;
	mov.u32 	%r11723, %r11720;
	mov.u32 	%r11724, %r11720;
	mov.u32 	%r11725, %r11720;
	mov.u32 	%r11726, %r11720;
	mov.u32 	%r11727, %r11720;
	@%p744 bra 	BB3_929;
	bra.uni 	BB3_944;

BB3_929:
	// inline asm
	prmt.b32 %r11727, %r52, %r53, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11726, %r51, %r52, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11725, %r47, %r51, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11724, %r48, %r47, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11723, %r49, %r48, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11722, %r50, %r49, %r1340;
	// inline asm
	mov.u32 	%r11720, 0;
	// inline asm
	prmt.b32 %r11721, %r11720, %r50, %r1340;
	// inline asm
	bra.uni 	BB3_944;

BB3_814:
	setp.gt.s32	%p686, %r6846, 11;
	@%p686 bra 	BB3_822;

	setp.gt.s32	%p692, %r6846, 9;
	@%p692 bra 	BB3_819;

	setp.eq.s32	%p695, %r6846, 8;
	@%p695 bra 	BB3_877;
	bra.uni 	BB3_817;

BB3_877:
	mov.u32 	%r11751, %r48;
	mov.u32 	%r11750, %r47;
	mov.u32 	%r11749, %r51;
	mov.u32 	%r11748, %r52;
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r11694;
	mov.u32 	%r11745, %r50;
	mov.u32 	%r11744, %r49;
	bra.uni 	BB3_881;

BB3_457:
	setp.eq.s32	%p375, %r648, 2;
	mov.u32 	%r11752, 0;
	@%p375 bra 	BB3_458;
	bra.uni 	BB3_459;

BB3_458:
	mov.u32 	%r11638, %r11752;
	bra.uni 	BB3_461;

BB3_748:
	setp.eq.s32	%p613, %r6571, 2;
	@%p613 bra 	BB3_765;
	bra.uni 	BB3_749;

BB3_765:
	and.b32  	%r6586, %r1002, %r48;
	and.b32  	%r6587, %r11750, %r1003;
	or.b32  	%r11742, %r6587, %r6586;
	mov.u32 	%r11736, %r11747;
	mov.u32 	%r11737, %r11746;
	mov.u32 	%r11738, %r11745;
	mov.u32 	%r11739, %r11744;
	mov.u32 	%r11740, %r50;
	mov.u32 	%r11741, %r49;
	bra.uni 	BB3_766;

BB3_353:
	setp.gt.s32	%p320, %r56, 5;
	@%p320 bra 	BB3_357;

	setp.eq.s32	%p323, %r56, 4;
	@%p323 bra 	BB3_422;
	bra.uni 	BB3_355;

BB3_422:
	mov.u32 	%r11626, 0;
	mov.u32 	%r54, %r53;
	mov.u32 	%r53, %r52;
	mov.u32 	%r52, %r51;
	mov.u32 	%r51, %r47;
	mov.u32 	%r11627, %r50;
	mov.u32 	%r11628, %r49;
	mov.u32 	%r50, %r48;
	bra.uni 	BB3_428;

BB3_569:
	setp.gt.s32	%p502, %r58, 21;
	@%p502 bra 	BB3_573;

	setp.eq.s32	%p505, %r58, 20;
	@%p505 bra 	BB3_601;
	bra.uni 	BB3_571;

BB3_601:
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r52;
	mov.u32 	%r11749, %r53;
	mov.u32 	%r11750, %r54;
	mov.u32 	%r11751, %r11747;
	bra.uni 	BB3_612;

BB3_935:
	setp.eq.s32	%p739, %r7834, 5;
	mov.u32 	%r11721, %r11720;
	mov.u32 	%r11722, %r11720;
	mov.u32 	%r11723, %r11720;
	mov.u32 	%r11724, %r11720;
	mov.u32 	%r11725, %r11720;
	mov.u32 	%r11726, %r11720;
	mov.u32 	%r11727, %r11720;
	@%p739 bra 	BB3_936;
	bra.uni 	BB3_944;

BB3_936:
	// inline asm
	prmt.b32 %r11727, %r49, %r48, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11726, %r50, %r49, %r1340;
	// inline asm
	mov.u32 	%r11720, 0;
	// inline asm
	prmt.b32 %r11725, %r11720, %r50, %r1340;
	// inline asm
	mov.u32 	%r11721, %r11720;
	mov.u32 	%r11722, %r11720;
	mov.u32 	%r11723, %r11720;
	mov.u32 	%r11724, %r11720;
	bra.uni 	BB3_944;

BB3_384:
	setp.gt.s32	%p297, %r56, 21;
	@%p297 bra 	BB3_388;

	setp.eq.s32	%p300, %r56, 20;
	@%p300 bra 	BB3_411;
	bra.uni 	BB3_386;

BB3_411:
	mov.u32 	%r51, 0;
	mov.u32 	%r54, %r48;
	mov.u32 	%r53, %r49;
	mov.u32 	%r52, %r50;
	bra.uni 	BB3_412;

BB3_551:
	setp.gt.s32	%p514, %r58, 13;
	@%p514 bra 	BB3_555;

	setp.eq.s32	%p517, %r58, 12;
	@%p517 bra 	BB3_605;
	bra.uni 	BB3_553;

BB3_605:
	mov.u32 	%r11744, %r54;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r47;
	mov.u32 	%r11749, %r51;
	mov.u32 	%r11750, %r52;
	mov.u32 	%r11751, %r53;
	bra.uni 	BB3_612;

BB3_931:
	setp.eq.s32	%p742, %r7834, 3;
	mov.u32 	%r11721, %r11720;
	mov.u32 	%r11722, %r11720;
	mov.u32 	%r11723, %r11720;
	mov.u32 	%r11724, %r11720;
	mov.u32 	%r11725, %r11720;
	mov.u32 	%r11726, %r11720;
	mov.u32 	%r11727, %r11720;
	@%p742 bra 	BB3_932;
	bra.uni 	BB3_944;

BB3_932:
	// inline asm
	prmt.b32 %r11727, %r47, %r51, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11726, %r48, %r47, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11725, %r49, %r48, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11724, %r50, %r49, %r1340;
	// inline asm
	mov.u32 	%r11720, 0;
	// inline asm
	prmt.b32 %r11723, %r11720, %r50, %r1340;
	// inline asm
	mov.u32 	%r11721, %r11720;
	mov.u32 	%r11722, %r11720;
	bra.uni 	BB3_944;

BB3_847:
	setp.gt.s32	%p663, %r6846, 27;
	@%p663 bra 	BB3_857;

	setp.gt.s32	%p669, %r6846, 25;
	@%p669 bra 	BB3_852;

	setp.eq.s32	%p672, %r6846, 24;
	@%p672 bra 	BB3_869;
	bra.uni 	BB3_850;

BB3_869:
	mov.u32 	%r11751, %r11694;
	mov.u32 	%r11750, %r11694;
	mov.u32 	%r11749, %r50;
	mov.u32 	%r11748, %r49;
	bra.uni 	BB3_867;

BB3_755:
	setp.eq.s32	%p608, %r6571, 6;
	@%p608 bra 	BB3_763;
	bra.uni 	BB3_756;

BB3_763:
	and.b32  	%r6578, %r1002, %r53;
	and.b32  	%r6579, %r11746, %r1003;
	or.b32  	%r11737, %r6579, %r6578;
	mov.u32 	%r11736, %r11747;
	bra.uni 	BB3_759;

BB3_368:
	setp.gt.s32	%p309, %r56, 13;
	@%p309 bra 	BB3_372;

	setp.eq.s32	%p312, %r56, 12;
	@%p312 bra 	BB3_417;
	bra.uni 	BB3_370;

BB3_417:
	mov.u32 	%r11626, 0;
	mov.u32 	%r54, %r51;
	mov.u32 	%r53, %r47;
	mov.u32 	%r52, %r48;
	mov.u32 	%r51, %r49;
	bra.uni 	BB3_418;

BB3_781:
	setp.eq.s32	%p641, %r6692, 2;
	@%p641 bra 	BB3_793;
	bra.uni 	BB3_782;

BB3_793:
	and.b32  	%r11742, %r11742, %r1055;
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;
	bra.uni 	BB3_794;

BB3_587:
	setp.gt.s32	%p491, %r58, 29;
	@%p491 bra 	BB3_591;

	setp.eq.s32	%p494, %r58, 28;
	@%p494 bra 	BB3_596;
	bra.uni 	BB3_589;

BB3_596:
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r54;
	mov.u32 	%r11749, %r11747;

BB3_597:
	mov.u32 	%r11750, %r11747;
	mov.u32 	%r11751, %r11747;
	bra.uni 	BB3_612;

BB3_938:
	setp.ne.s32	%p737, %r7834, 7;
	mov.u32 	%r11721, %r11720;
	mov.u32 	%r11722, %r11720;
	mov.u32 	%r11723, %r11720;
	mov.u32 	%r11724, %r11720;
	mov.u32 	%r11725, %r11720;
	mov.u32 	%r11726, %r11720;
	mov.u32 	%r11727, %r11720;
	@%p737 bra 	BB3_944;

	mov.u32 	%r11720, 0;
	// inline asm
	prmt.b32 %r11727, %r11720, %r50, %r1340;
	// inline asm
	mov.u32 	%r11721, %r11720;
	mov.u32 	%r11722, %r11720;
	mov.u32 	%r11723, %r11720;
	mov.u32 	%r11724, %r11720;
	mov.u32 	%r11725, %r11720;
	mov.u32 	%r11726, %r11720;

BB3_944:
	or.b32  	%r11740, %r11720, %r50;
	or.b32  	%r11741, %r11721, %r49;
	or.b32  	%r11742, %r11722, %r48;
	or.b32  	%r11743, %r11723, %r47;
	or.b32  	%r11739, %r11724, %r51;
	or.b32  	%r11738, %r11725, %r52;
	or.b32  	%r11737, %r11726, %r53;
	or.b32  	%r11736, %r11727, %r54;
	bra.uni 	BB3_1027;

BB3_399:
	setp.gt.s32	%p286, %r56, 29;
	@%p286 bra 	BB3_403;

	setp.eq.s32	%p289, %r56, 28;
	@%p289 bra 	BB3_406;
	bra.uni 	BB3_401;

BB3_406:
	mov.u32 	%r53, 0;
	mov.u32 	%r54, %r50;
	bra.uni 	BB3_427;

BB3_788:
	setp.eq.s32	%p636, %r6692, 6;
	@%p636 bra 	BB3_791;
	bra.uni 	BB3_789;

BB3_791:
	and.b32  	%r11737, %r11737, %r1055;
	mov.u32 	%r11736, 0;
	bra.uni 	BB3_1026;

BB3_480:
	setp.eq.s32	%p469, %r5275, 1;
	@%p469 bra 	BB3_481;
	bra.uni 	BB3_492;

BB3_481:
	and.b32  	%r11741, %r697, %r49;
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;
	mov.u32 	%r11740, %r50;

BB3_482:
	mov.u32 	%r11742, %r11736;

BB3_501:
	mov.u32 	%r11743, %r11736;
	mov.u32 	%r11752, %r56;
	bra.uni 	BB3_1027;

BB3_129:
	setp.gt.s32	%p167, %r56, 5;
	@%p167 bra 	BB3_133;

	setp.eq.s32	%p170, %r56, 4;
	@%p170 bra 	BB3_200;
	bra.uni 	BB3_131;

BB3_200:
	mov.u32 	%r11748, 0;
	mov.u32 	%r11744, %r47;
	mov.u32 	%r11745, %r51;
	mov.u32 	%r11746, %r52;
	mov.u32 	%r11747, %r53;
	mov.u32 	%r11749, %r50;
	mov.u32 	%r11750, %r49;
	mov.u32 	%r11751, %r48;
	bra.uni 	BB3_202;

BB3_224:
	setp.eq.s32	%p201, %r3326, 1;
	mov.u32 	%r11744, %r51;
	mov.u32 	%r11745, %r52;
	mov.u32 	%r11746, %r53;
	mov.u32 	%r11747, %r54;
	mov.u32 	%r11748, %r50;
	mov.u32 	%r11749, %r49;
	mov.u32 	%r11750, %r48;
	mov.u32 	%r11751, %r47;
	@%p201 bra 	BB3_225;
	bra.uni 	BB3_241;

BB3_225:
	and.b32  	%r11749, %r248, %r49;
	mov.u32 	%r11744, 0;
	mov.u32 	%r11745, %r11744;
	mov.u32 	%r11746, %r11744;
	mov.u32 	%r11747, %r11744;
	mov.u32 	%r11748, %r50;

BB3_107:
	mov.u32 	%r11750, %r11744;

BB3_240:
	mov.u32 	%r11751, %r11744;
	bra.uni 	BB3_241;

BB3_675:
	setp.eq.s32	%p569, %r56, 18;
	@%p569 bra 	BB3_709;
	bra.uni 	BB3_676;

BB3_709:
	mov.u32 	%r6130, 16;
	// inline asm
	shf.r.wrap.b32 %r11740, %r51, %r52, %r6130;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r52, %r53, %r6130;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11742, %r53, %r54, %r6130;
	// inline asm
	mov.u32 	%r11736, 0;
	// inline asm
	shf.r.wrap.b32 %r54, %r54, %r11736, %r6130;
	// inline asm
	bra.uni 	BB3_678;

BB3_974:
	setp.eq.s32	%p765, %r8013, 18;
	@%p765 bra 	BB3_1006;
	bra.uni 	BB3_975;

BB3_1006:
	mov.u32 	%r8183, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r48, %r47, %r8183;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r49, %r48, %r8183;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r50, %r49, %r8183;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11751, %r11747, %r50, %r8183;
	// inline asm
	bra.uni 	BB3_1008;

BB3_659:
	setp.eq.s32	%p581, %r56, 10;
	@%p581 bra 	BB3_713;
	bra.uni 	BB3_660;

BB3_713:
	mov.u32 	%r6280, 16;
	// inline asm
	shf.r.wrap.b32 %r11740, %r48, %r47, %r6280;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r47, %r51, %r6280;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11742, %r51, %r52, %r6280;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6269, %r52, %r53, %r6280;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11739, %r53, %r54, %r6280;
	// inline asm
	mov.u32 	%r11736, 0;
	// inline asm
	shf.r.wrap.b32 %r11738, %r54, %r11736, %r6280;
	// inline asm
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r54, %r6269;
	bra.uni 	BB3_724;

BB3_488:
	setp.eq.s32	%p464, %r5275, 5;
	@%p464 bra 	BB3_489;
	bra.uni 	BB3_492;

BB3_489:
	and.b32  	%r11738, %r697, %r52;
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	bra.uni 	BB3_496;

BB3_957:
	setp.eq.s32	%p777, %r8013, 10;
	@%p777 bra 	BB3_1011;
	bra.uni 	BB3_958;

BB3_1011:
	mov.u32 	%r8333, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r51, %r52, %r8333;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r47, %r51, %r8333;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r48, %r47, %r8333;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r49, %r48, %r8333;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r50, %r49, %r8333;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11745, %r11747, %r50, %r8333;
	// inline asm
	mov.u32 	%r11746, %r11747;
	bra.uni 	BB3_1020;

BB3_161:
	setp.gt.s32	%p144, %r56, 21;
	@%p144 bra 	BB3_165;

	setp.eq.s32	%p147, %r56, 20;
	@%p147 bra 	BB3_191;
	bra.uni 	BB3_163;

BB3_191:
	mov.u32 	%r11744, 0;
	mov.u32 	%r11745, %r50;
	mov.u32 	%r11746, %r49;
	mov.u32 	%r11747, %r48;
	bra.uni 	BB3_187;

BB3_231:
	setp.eq.s32	%p196, %r3326, 5;
	mov.u32 	%r11744, %r51;
	mov.u32 	%r11745, %r52;
	mov.u32 	%r11746, %r53;
	mov.u32 	%r11747, %r54;
	mov.u32 	%r11748, %r50;
	mov.u32 	%r11749, %r49;
	mov.u32 	%r11750, %r48;
	mov.u32 	%r11751, %r47;
	@%p196 bra 	BB3_232;
	bra.uni 	BB3_241;

BB3_232:
	and.b32  	%r11745, %r248, %r52;
	mov.u32 	%r11746, 0;
	mov.u32 	%r11744, %r51;
	mov.u32 	%r11747, %r11746;
	bra.uni 	BB3_236;

BB3_692:
	setp.eq.s32	%p558, %r56, 26;
	@%p558 bra 	BB3_705;
	bra.uni 	BB3_693;

BB3_705:
	mov.u32 	%r6012, 16;
	// inline asm
	shf.r.wrap.b32 %r11740, %r53, %r54, %r6012;
	// inline asm
	mov.u32 	%r11736, 0;
	// inline asm
	shf.r.wrap.b32 %r11741, %r54, %r11736, %r6012;
	// inline asm
	bra.uni 	BB3_695;

BB3_989:
	setp.eq.s32	%p754, %r8013, 26;
	@%p754 bra 	BB3_1000;
	bra.uni 	BB3_990;

BB3_1000:
	mov.u32 	%r8065, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r50, %r49, %r8065;
	// inline asm
	mov.u32 	%r11750, 0;
	// inline asm
	shf.r.wrap.b32 %r11749, %r11750, %r50, %r8065;
	// inline asm
	bra.uni 	BB3_1002;

BB3_651:
	setp.eq.s32	%p587, %r56, 6;
	@%p587 bra 	BB3_715;
	bra.uni 	BB3_652;

BB3_715:
	mov.u32 	%r6367, 16;
	// inline asm
	shf.r.wrap.b32 %r11740, %r49, %r48, %r6367;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r48, %r47, %r6367;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11742, %r47, %r51, %r6367;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6352, %r51, %r52, %r6367;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11739, %r52, %r53, %r6367;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11738, %r53, %r54, %r6367;
	// inline asm
	mov.u32 	%r11736, 0;
	// inline asm
	shf.r.wrap.b32 %r11737, %r54, %r11736, %r6367;
	// inline asm
	mov.u32 	%r54, %r6352;
	bra.uni 	BB3_724;

BB3_484:
	setp.eq.s32	%p467, %r5275, 3;
	@%p467 bra 	BB3_485;
	bra.uni 	BB3_492;

BB3_485:
	and.b32  	%r11743, %r697, %r47;
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;
	mov.u32 	%r11740, %r50;
	mov.u32 	%r11741, %r49;
	mov.u32 	%r11742, %r48;
	mov.u32 	%r11752, %r56;
	bra.uni 	BB3_1027;

BB3_949:
	setp.eq.s32	%p783, %r8013, 6;
	@%p783 bra 	BB3_1013;
	bra.uni 	BB3_950;

BB3_1013:
	mov.u32 	%r8420, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r52, %r53, %r8420;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r51, %r52, %r8420;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r47, %r51, %r8420;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r48, %r47, %r8420;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r49, %r48, %r8420;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r50, %r49, %r8420;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11746, %r11747, %r50, %r8420;
	// inline asm
	bra.uni 	BB3_1020;

BB3_144:
	setp.gt.s32	%p156, %r56, 13;
	@%p156 bra 	BB3_148;

	setp.eq.s32	%p159, %r56, 12;
	@%p159 bra 	BB3_196;
	bra.uni 	BB3_146;

BB3_196:
	mov.u32 	%r11748, 0;
	mov.u32 	%r11744, %r49;
	mov.u32 	%r11745, %r48;
	mov.u32 	%r11746, %r47;
	mov.u32 	%r11747, %r51;
	mov.u32 	%r11749, %r11748;
	mov.u32 	%r11750, %r11748;
	mov.u32 	%r11751, %r50;
	bra.uni 	BB3_202;

BB3_227:
	setp.eq.s32	%p199, %r3326, 3;
	mov.u32 	%r11744, %r51;
	mov.u32 	%r11745, %r52;
	mov.u32 	%r11746, %r53;
	mov.u32 	%r11747, %r54;
	mov.u32 	%r11748, %r50;
	mov.u32 	%r11749, %r49;
	mov.u32 	%r11750, %r48;
	mov.u32 	%r11751, %r47;
	@%p199 bra 	BB3_228;
	bra.uni 	BB3_241;

BB3_228:
	and.b32  	%r11751, %r248, %r47;
	mov.u32 	%r11744, 0;
	mov.u32 	%r11745, %r11744;
	mov.u32 	%r11746, %r11744;
	mov.u32 	%r11747, %r11744;
	mov.u32 	%r11748, %r50;
	mov.u32 	%r11749, %r49;
	mov.u32 	%r11750, %r48;
	bra.uni 	BB3_241;

BB3_683:
	setp.eq.s32	%p564, %r56, 22;
	@%p564 bra 	BB3_707;
	bra.uni 	BB3_684;

BB3_707:
	mov.u32 	%r6067, 16;
	// inline asm
	shf.r.wrap.b32 %r11740, %r52, %r53, %r6067;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r53, %r54, %r6067;
	// inline asm
	mov.u32 	%r11736, 0;
	// inline asm
	shf.r.wrap.b32 %r11742, %r54, %r11736, %r6067;
	// inline asm
	bra.uni 	BB3_686;

BB3_981:
	setp.eq.s32	%p760, %r8013, 22;
	@%p760 bra 	BB3_1003;
	bra.uni 	BB3_982;

BB3_1003:
	mov.u32 	%r8120, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r49, %r48, %r8120;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r50, %r49, %r8120;
	// inline asm
	mov.u32 	%r11751, 0;
	// inline asm
	shf.r.wrap.b32 %r11750, %r11751, %r50, %r8120;
	// inline asm
	bra.uni 	BB3_1005;

BB3_666:
	setp.eq.s32	%p576, %r56, 14;
	@%p576 bra 	BB3_711;
	bra.uni 	BB3_667;

BB3_711:
	mov.u32 	%r6201, 16;
	// inline asm
	shf.r.wrap.b32 %r11740, %r47, %r51, %r6201;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r51, %r52, %r6201;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11742, %r52, %r53, %r6201;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6194, %r53, %r54, %r6201;
	// inline asm
	mov.u32 	%r11736, 0;
	// inline asm
	shf.r.wrap.b32 %r11739, %r54, %r11736, %r6201;
	// inline asm
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r54, %r6194;
	bra.uni 	BB3_724;

BB3_491:
	setp.ne.s32	%p462, %r5275, 7;
	@%p462 bra 	BB3_492;

	and.b32  	%r11736, %r697, %r54;
	bra.uni 	BB3_494;

BB3_492:
	mov.u32 	%r11736, %r54;

BB3_494:
	mov.u32 	%r11737, %r53;

BB3_495:
	mov.u32 	%r11738, %r52;

BB3_496:
	mov.u32 	%r11739, %r51;

BB3_497:
	mov.u32 	%r11740, %r50;
	mov.u32 	%r11741, %r49;
	mov.u32 	%r11742, %r48;
	mov.u32 	%r11743, %r47;
	mov.u32 	%r11752, %r56;
	bra.uni 	BB3_1027;

BB3_964:
	setp.eq.s32	%p772, %r8013, 14;
	@%p772 bra 	BB3_1009;
	bra.uni 	BB3_965;

BB3_1009:
	mov.u32 	%r8254, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r47, %r51, %r8254;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r48, %r47, %r8254;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r49, %r48, %r8254;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r50, %r49, %r8254;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11744, %r11747, %r50, %r8254;
	// inline asm
	bra.uni 	BB3_967;

BB3_177:
	setp.gt.s32	%p133, %r56, 29;
	@%p133 bra 	BB3_181;

	setp.eq.s32	%p136, %r56, 28;
	@%p136 bra 	BB3_186;
	bra.uni 	BB3_179;

BB3_186:
	mov.u32 	%r11744, 0;
	mov.u32 	%r11745, %r11744;
	mov.u32 	%r11746, %r11744;
	mov.u32 	%r11747, %r50;
	bra.uni 	BB3_187;

BB3_234:
	setp.ne.s32	%p194, %r3326, 7;
	mov.u32 	%r11744, %r51;
	mov.u32 	%r11745, %r52;
	mov.u32 	%r11746, %r53;
	mov.u32 	%r11747, %r54;
	mov.u32 	%r11748, %r50;
	mov.u32 	%r11749, %r49;
	mov.u32 	%r11750, %r48;
	mov.u32 	%r11751, %r47;
	@%p194 bra 	BB3_241;

	and.b32  	%r11747, %r248, %r54;
	mov.u32 	%r11744, %r51;
	mov.u32 	%r11745, %r52;
	mov.u32 	%r11746, %r53;

BB3_236:
	mov.u32 	%r11748, %r50;
	mov.u32 	%r11749, %r49;
	mov.u32 	%r11750, %r48;
	mov.u32 	%r11751, %r47;

BB3_241:
	setp.gt.s32	%p202, %r56, 15;
	@%p202 bra 	BB3_270;

	setp.gt.s32	%p226, %r56, 7;
	@%p226 bra 	BB3_255;

	setp.gt.s32	%p238, %r56, 3;
	@%p238 bra 	BB3_248;

	setp.eq.s32	%p244, %r56, 1;
	@%p244 bra 	BB3_320;

	setp.eq.s32	%p245, %r56, 2;
	@%p245 bra 	BB3_319;
	bra.uni 	BB3_246;

BB3_319:
	mov.u32 	%r3859, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r53, %r54, %r3859;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r52, %r53, %r3859;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r51, %r52, %r3859;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r47, %r51, %r3859;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3844, %r48, %r47, %r3859;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11609, %r49, %r48, %r3859;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11608, %r50, %r49, %r3859;
	// inline asm
	mov.u32 	%r3857, 0;
	// inline asm
	shf.r.wrap.b32 %r11607, %r3857, %r50, %r3859;
	// inline asm
	mov.u32 	%r50, %r3844;
	bra.uni 	BB3_323;

BB3_270:
	setp.gt.s32	%p203, %r56, 23;
	@%p203 bra 	BB3_286;

	setp.gt.s32	%p215, %r56, 19;
	@%p215 bra 	BB3_279;

	setp.gt.s32	%p221, %r56, 17;
	@%p221 bra 	BB3_276;

	setp.eq.s32	%p224, %r56, 16;
	@%p224 bra 	BB3_309;
	bra.uni 	BB3_274;

BB3_309:
	mov.u32 	%r11607, 0;
	mov.u32 	%r54, %r47;
	mov.u32 	%r53, %r48;
	mov.u32 	%r52, %r49;
	mov.u32 	%r51, %r50;
	bra.uni 	BB3_310;

BB3_255:
	setp.gt.s32	%p227, %r56, 11;
	@%p227 bra 	BB3_263;

	setp.gt.s32	%p233, %r56, 9;
	@%p233 bra 	BB3_260;

	setp.eq.s32	%p236, %r56, 8;
	@%p236 bra 	BB3_315;
	bra.uni 	BB3_258;

BB3_315:
	mov.u32 	%r11607, 0;
	mov.u32 	%r54, %r52;
	mov.u32 	%r53, %r51;
	mov.u32 	%r52, %r47;
	mov.u32 	%r51, %r48;
	mov.u32 	%r11608, %r11607;
	mov.u32 	%r11609, %r50;
	mov.u32 	%r50, %r49;
	bra.uni 	BB3_323;

BB3_286:
	setp.gt.s32	%p204, %r56, 27;
	@%p204 bra 	BB3_294;

	setp.gt.s32	%p210, %r56, 25;
	@%p210 bra 	BB3_291;

	setp.eq.s32	%p213, %r56, 24;
	@%p213 bra 	BB3_303;
	bra.uni 	BB3_289;

BB3_303:
	mov.u32 	%r52, 0;
	mov.u32 	%r54, %r49;
	mov.u32 	%r53, %r50;
	bra.uni 	BB3_304;

BB3_700:
	setp.eq.s32	%p553, %r56, 31;
	@%p553 bra 	BB3_719;
	bra.uni 	BB3_701;

BB3_719:
	mov.u32 	%r11736, 0;
	mov.u32 	%r6498, 24;
	// inline asm
	shf.r.wrap.b32 %r11740, %r54, %r11736, %r6498;
	// inline asm
	bra.uni 	BB3_720;

BB3_996:
	setp.eq.s32	%p749, %r8013, 31;
	@%p749 bra 	BB3_1018;
	bra.uni 	BB3_997;

BB3_1018:
	mov.u32 	%r11749, 0;
	mov.u32 	%r8551, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r11749, %r50, %r8551;
	// inline asm
	bra.uni 	BB3_1019;

BB3_248:
	setp.gt.s32	%p239, %r56, 5;
	@%p239 bra 	BB3_252;

	setp.eq.s32	%p242, %r56, 4;
	@%p242 bra 	BB3_317;
	bra.uni 	BB3_250;

BB3_317:
	mov.u32 	%r11607, 0;
	mov.u32 	%r54, %r53;
	mov.u32 	%r53, %r52;
	mov.u32 	%r52, %r51;
	mov.u32 	%r51, %r47;
	mov.u32 	%r11608, %r50;
	mov.u32 	%r11609, %r49;
	mov.u32 	%r50, %r48;
	bra.uni 	BB3_323;

BB3_279:
	setp.gt.s32	%p216, %r56, 21;
	@%p216 bra 	BB3_283;

	setp.eq.s32	%p219, %r56, 20;
	@%p219 bra 	BB3_306;
	bra.uni 	BB3_281;

BB3_306:
	mov.u32 	%r51, 0;
	mov.u32 	%r54, %r48;
	mov.u32 	%r53, %r49;
	mov.u32 	%r52, %r50;
	bra.uni 	BB3_307;

BB3_263:
	setp.gt.s32	%p228, %r56, 13;
	@%p228 bra 	BB3_267;

	setp.eq.s32	%p231, %r56, 12;
	@%p231 bra 	BB3_312;
	bra.uni 	BB3_265;

BB3_312:
	mov.u32 	%r11607, 0;
	mov.u32 	%r54, %r51;
	mov.u32 	%r53, %r47;
	mov.u32 	%r52, %r48;
	mov.u32 	%r51, %r49;
	bra.uni 	BB3_313;

BB3_294:
	setp.gt.s32	%p205, %r56, 29;
	@%p205 bra 	BB3_298;

	setp.eq.s32	%p208, %r56, 28;
	@%p208 bra 	BB3_301;
	bra.uni 	BB3_296;

BB3_301:
	mov.u32 	%r53, 0;
	mov.u32 	%r54, %r50;
	bra.uni 	BB3_322;

BB3_718:
	mov.u32 	%r6494, 8;
	// inline asm
	shf.r.wrap.b32 %r11740, %r50, %r49, %r6494;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r49, %r48, %r6494;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11742, %r48, %r47, %r6494;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6475, %r47, %r51, %r6494;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11739, %r51, %r52, %r6494;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11738, %r52, %r53, %r6494;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11737, %r53, %r54, %r6494;
	// inline asm
	mov.u32 	%r6493, 0;
	// inline asm
	shf.r.wrap.b32 %r11736, %r54, %r6493, %r6494;
	// inline asm
	mov.u32 	%r54, %r6475;
	bra.uni 	BB3_724;

BB3_645:
	setp.eq.s32	%p593, %r56, 3;
	@%p593 bra 	BB3_646;
	bra.uni 	BB3_702;

BB3_646:
	mov.u32 	%r6430, 24;
	// inline asm
	shf.r.wrap.b32 %r11740, %r50, %r49, %r6430;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r49, %r48, %r6430;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11742, %r48, %r47, %r6430;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6411, %r47, %r51, %r6430;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11739, %r51, %r52, %r6430;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11738, %r52, %r53, %r6430;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11737, %r53, %r54, %r6430;
	// inline asm
	mov.u32 	%r6429, 0;
	// inline asm
	shf.r.wrap.b32 %r11736, %r54, %r6429, %r6430;
	// inline asm
	mov.u32 	%r54, %r6411;
	bra.uni 	BB3_724;

BB3_533:
	setp.eq.s32	%p531, %r58, 2;
	@%p531 bra 	BB3_610;
	bra.uni 	BB3_534;

BB3_610:
	mov.u32 	%r5907, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r50, %r49, %r5907;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r49, %r48, %r5907;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r48, %r47, %r5907;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r47, %r51, %r5907;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r51, %r52, %r5907;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r52, %r53, %r5907;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r53, %r54, %r5907;
	// inline asm
	mov.u32 	%r5906, 0;
	// inline asm
	shf.r.wrap.b32 %r11747, %r54, %r5906, %r5907;
	// inline asm
	bra.uni 	BB3_612;

BB3_1017:
	mov.u32 	%r8547, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r53, %r54, %r8547;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r52, %r53, %r8547;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r51, %r52, %r8547;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r47, %r51, %r8547;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r48, %r47, %r8547;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r49, %r48, %r8547;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r50, %r49, %r8547;
	// inline asm
	mov.u32 	%r8545, 0;
	// inline asm
	shf.r.wrap.b32 %r11747, %r8545, %r50, %r8547;
	// inline asm
	bra.uni 	BB3_1020;

BB3_93:
	setp.eq.s32	%p789, %r8013, 3;
	@%p789 bra 	BB3_1015;
	bra.uni 	BB3_94;

BB3_1015:
	mov.u32 	%r8483, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r53, %r54, %r8483;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r52, %r53, %r8483;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r51, %r52, %r8483;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r47, %r51, %r8483;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r48, %r47, %r8483;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r49, %r48, %r8483;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r50, %r49, %r8483;
	// inline asm
	mov.u32 	%r8481, 0;
	// inline asm
	shf.r.wrap.b32 %r11747, %r8481, %r50, %r8483;
	// inline asm
	bra.uni 	BB3_1020;

BB3_807:
	setp.gt.s32	%p698, %r6846, 5;
	@%p698 bra 	BB3_811;

	setp.eq.s32	%p701, %r6846, 4;
	@%p701 bra 	BB3_879;
	bra.uni 	BB3_809;

BB3_879:
	mov.u32 	%r11751, %r47;
	mov.u32 	%r11750, %r51;
	mov.u32 	%r11749, %r52;
	mov.u32 	%r11748, %r53;
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r50;
	mov.u32 	%r11745, %r49;
	mov.u32 	%r11744, %r48;
	bra.uni 	BB3_881;

BB3_746:
	setp.eq.s32	%p616, %r6571, 1;
	@%p616 bra 	BB3_747;
	bra.uni 	BB3_757;

BB3_747:
	and.b32  	%r6588, %r1002, %r49;
	and.b32  	%r6589, %r11749, %r1003;
	or.b32  	%r11741, %r6589, %r6588;
	mov.u32 	%r11736, %r11747;
	mov.u32 	%r11737, %r11746;
	mov.u32 	%r11738, %r11745;
	mov.u32 	%r11739, %r11744;
	mov.u32 	%r11740, %r50;

BB3_32:
	mov.u32 	%r11742, %r11750;

BB3_766:
	mov.u32 	%r11743, %r11751;
	bra.uni 	BB3_767;

BB3_565:
	setp.eq.s32	%p508, %r58, 18;
	@%p508 bra 	BB3_602;
	bra.uni 	BB3_566;

BB3_602:
	mov.u32 	%r5575, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r51, %r52, %r5575;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r52, %r53, %r5575;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r53, %r54, %r5575;
	// inline asm
	mov.u32 	%r11744, 0;
	// inline asm
	shf.r.wrap.b32 %r11751, %r54, %r11744, %r5575;
	// inline asm
	bra.uni 	BB3_568;

BB3_381:
	setp.eq.s32	%p303, %r56, 18;
	@%p303 bra 	BB3_413;
	bra.uni 	BB3_382;

BB3_413:
	mov.u32 	%r4348, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r48, %r47, %r4348;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r49, %r48, %r4348;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r50, %r49, %r4348;
	// inline asm
	mov.u32 	%r11626, 0;
	// inline asm
	shf.r.wrap.b32 %r51, %r11626, %r50, %r4348;
	// inline asm
	bra.uni 	BB3_415;

BB3_548:
	setp.eq.s32	%p520, %r58, 10;
	@%p520 bra 	BB3_606;
	bra.uni 	BB3_549;

BB3_606:
	mov.u32 	%r5725, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r48, %r47, %r5725;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r47, %r51, %r5725;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r51, %r52, %r5725;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r52, %r53, %r5725;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r53, %r54, %r5725;
	// inline asm
	mov.u32 	%r11746, 0;
	// inline asm
	shf.r.wrap.b32 %r11745, %r54, %r11746, %r5725;
	// inline asm
	mov.u32 	%r11747, %r11746;
	bra.uni 	BB3_612;

BB3_840:
	setp.gt.s32	%p675, %r6846, 21;
	@%p675 bra 	BB3_844;

	setp.eq.s32	%p678, %r6846, 20;
	@%p678 bra 	BB3_871;
	bra.uni 	BB3_842;

BB3_871:
	mov.u32 	%r11751, %r11694;
	mov.u32 	%r11750, %r50;
	mov.u32 	%r11749, %r49;
	mov.u32 	%r11748, %r48;
	bra.uni 	BB3_867;

BB3_753:
	setp.eq.s32	%p611, %r6571, 5;
	@%p611 bra 	BB3_754;
	bra.uni 	BB3_757;

BB3_754:
	and.b32  	%r6580, %r1002, %r52;
	and.b32  	%r6581, %r11745, %r1003;
	or.b32  	%r11738, %r6581, %r6580;
	mov.u32 	%r11736, %r11747;
	mov.u32 	%r11737, %r11746;
	bra.uni 	BB3_760;

BB3_365:
	setp.eq.s32	%p315, %r56, 10;
	@%p315 bra 	BB3_419;
	bra.uni 	BB3_366;

BB3_419:
	mov.u32 	%r4498, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r51, %r52, %r4498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r47, %r51, %r4498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r48, %r47, %r4498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r49, %r48, %r4498;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4491, %r50, %r49, %r4498;
	// inline asm
	mov.u32 	%r11626, 0;
	// inline asm
	shf.r.wrap.b32 %r11628, %r11626, %r50, %r4498;
	// inline asm
	mov.u32 	%r11627, %r11626;
	mov.u32 	%r50, %r4491;
	bra.uni 	BB3_428;

BB3_778:
	setp.eq.s32	%p644, %r6692, 1;
	@%p644 bra 	BB3_779;
	bra.uni 	BB3_1026;

BB3_779:
	and.b32  	%r11741, %r11741, %r1055;
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;

BB3_780:
	mov.u32 	%r11742, %r11736;

BB3_794:
	mov.u32 	%r11743, %r11736;
	bra.uni 	BB3_1026;

BB3_582:
	setp.eq.s32	%p497, %r58, 26;
	@%p497 bra 	BB3_598;
	bra.uni 	BB3_583;

BB3_598:
	mov.u32 	%r5457, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r53, %r54, %r5457;
	// inline asm
	mov.u32 	%r11744, 0;
	// inline asm
	shf.r.wrap.b32 %r11749, %r54, %r11744, %r5457;
	// inline asm
	bra.uni 	BB3_585;

BB3_396:
	setp.eq.s32	%p292, %r56, 26;
	@%p292 bra 	BB3_407;
	bra.uni 	BB3_397;

BB3_407:
	mov.u32 	%r4230, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r50, %r49, %r4230;
	// inline asm
	mov.u32 	%r52, 0;
	// inline asm
	shf.r.wrap.b32 %r53, %r52, %r50, %r4230;
	// inline asm
	bra.uni 	BB3_409;

BB3_786:
	setp.eq.s32	%p639, %r6692, 5;
	@%p639 bra 	BB3_787;
	bra.uni 	BB3_1026;

BB3_787:
	and.b32  	%r11738, %r11738, %r1055;
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	bra.uni 	BB3_1026;

BB3_540:
	setp.eq.s32	%p526, %r58, 6;
	@%p526 bra 	BB3_608;
	bra.uni 	BB3_541;

BB3_608:
	mov.u32 	%r5812, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r49, %r48, %r5812;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r48, %r47, %r5812;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r47, %r51, %r5812;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r51, %r52, %r5812;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r52, %r53, %r5812;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r53, %r54, %r5812;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11746, %r54, %r11747, %r5812;
	// inline asm
	bra.uni 	BB3_612;

BB3_822:
	setp.gt.s32	%p687, %r6846, 13;
	@%p687 bra 	BB3_826;

	setp.eq.s32	%p690, %r6846, 12;
	@%p690 bra 	BB3_875;
	bra.uni 	BB3_824;

BB3_875:
	mov.u32 	%r11751, %r49;
	mov.u32 	%r11750, %r48;
	mov.u32 	%r11749, %r47;
	mov.u32 	%r11748, %r51;
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r11694;
	mov.u32 	%r11745, %r11694;
	mov.u32 	%r11744, %r50;
	bra.uni 	BB3_881;

BB3_459:
	and.b16  	%rs14, %rs2, 255;
	and.b16  	%rs15, %rs23, 255;
	setp.eq.s16	%p376, %rs15, %rs14;
	mov.u32 	%r11638, 1;
	@%p376 bra 	BB3_461;

	st.local.u8 	[%rd87], %rs23;
	mov.u32 	%r11638, 1;
	mov.u32 	%r11752, %r11638;

BB3_461:
	cvt.u64.u32	%rd148, %r11638;
	add.s64 	%rd149, %rd86, %rd148;
	ld.local.u8 	%rs3, [%rd149];
	and.b16  	%rs16, %rs2, 255;
	setp.eq.s16	%p377, %rs3, %rs16;
	@%p377 bra 	BB3_463;

	cvt.u64.u32	%rd150, %r11752;
	add.s64 	%rd153, %rd87, %rd150;
	st.local.u8 	[%rd153], %rs3;
	add.s32 	%r11752, %r11752, 1;

BB3_463:
	add.s32 	%r11641, %r11638, 1;
	cvt.u64.u32	%rd154, %r11641;
	add.s64 	%rd157, %rd86, %rd154;
	ld.local.u8 	%rs23, [%rd157];

BB3_464:
	and.b16  	%rs17, %rs2, 255;
	and.b16  	%rs18, %rs23, 255;
	setp.eq.s16	%p378, %rs18, %rs17;
	@%p378 bra 	BB3_466;

	cvt.u64.u32	%rd158, %r11752;
	add.s64 	%rd161, %rd87, %rd158;
	st.local.u8 	[%rd161], %rs23;
	add.s32 	%r11752, %r11752, 1;

BB3_466:
	add.s32 	%r11646, %r11641, 1;

BB3_467:
	setp.lt.u32	%p379, %r38, 4;
	@%p379 bra 	BB3_477;

BB3_468:
	cvt.u64.u32	%rd163, %r11646;
	add.s64 	%rd164, %rd86, %rd163;
	ld.local.u8 	%rs6, [%rd164];
	and.b16  	%rs19, %rs2, 255;
	setp.eq.s16	%p380, %rs6, %rs19;
	@%p380 bra 	BB3_470;

	cvt.u64.u32	%rd165, %r11752;
	add.s64 	%rd168, %rd87, %rd165;
	st.local.u8 	[%rd168], %rs6;
	add.s32 	%r11752, %r11752, 1;

BB3_470:
	add.s32 	%r4959, %r11646, 1;
	cvt.u64.u32	%rd169, %r4959;
	add.s64 	%rd170, %rd86, %rd169;
	ld.local.u8 	%rs7, [%rd170];
	setp.eq.s16	%p381, %rs7, %rs19;
	@%p381 bra 	BB3_472;

	cvt.u64.u32	%rd171, %r11752;
	add.s64 	%rd174, %rd87, %rd171;
	st.local.u8 	[%rd174], %rs7;
	add.s32 	%r11752, %r11752, 1;

BB3_472:
	add.s32 	%r4960, %r11646, 2;
	cvt.u64.u32	%rd175, %r4960;
	add.s64 	%rd176, %rd86, %rd175;
	ld.local.u8 	%rs8, [%rd176];
	setp.eq.s16	%p382, %rs8, %rs19;
	@%p382 bra 	BB3_474;

	cvt.u64.u32	%rd177, %r11752;
	add.s64 	%rd180, %rd87, %rd177;
	st.local.u8 	[%rd180], %rs8;
	add.s32 	%r11752, %r11752, 1;

BB3_474:
	add.s32 	%r4961, %r11646, 3;
	cvt.u64.u32	%rd181, %r4961;
	add.s64 	%rd182, %rd86, %rd181;
	ld.local.u8 	%rs9, [%rd182];
	setp.eq.s16	%p383, %rs9, %rs19;
	@%p383 bra 	BB3_476;

	cvt.u64.u32	%rd183, %r11752;
	add.s64 	%rd186, %rd87, %rd183;
	st.local.u8 	[%rd186], %rs9;
	add.s32 	%r11752, %r11752, 1;

BB3_476:
	add.s32 	%r11646, %r11646, 4;
	setp.lt.u32	%p384, %r11646, %r38;
	@%p384 bra 	BB3_468;

BB3_477:
	ld.local.v4.u32 	{%r11740, %r11741, %r11742, %r11743}, [%rd87];
	ld.local.v4.u32 	{%r11739, %r11738, %r11737, %r11736}, [%rd87+16];
	bra.uni 	BB3_1027;

BB3_749:
	setp.eq.s32	%p614, %r6571, 3;
	@%p614 bra 	BB3_750;
	bra.uni 	BB3_757;

BB3_750:
	and.b32  	%r6584, %r1002, %r47;
	and.b32  	%r6585, %r11751, %r1003;
	or.b32  	%r11743, %r6585, %r6584;
	mov.u32 	%r11736, %r11747;
	mov.u32 	%r11737, %r11746;
	mov.u32 	%r11738, %r11745;
	mov.u32 	%r11739, %r11744;
	mov.u32 	%r11740, %r50;
	mov.u32 	%r11741, %r49;
	mov.u32 	%r11742, %r48;
	bra.uni 	BB3_767;

BB3_357:
	setp.eq.s32	%p321, %r56, 6;
	@%p321 bra 	BB3_421;
	bra.uni 	BB3_358;

BB3_421:
	mov.u32 	%r4585, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r52, %r53, %r4585;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r51, %r52, %r4585;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r47, %r51, %r4585;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r48, %r47, %r4585;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4574, %r49, %r48, %r4585;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11628, %r50, %r49, %r4585;
	// inline asm
	mov.u32 	%r11626, 0;
	// inline asm
	shf.r.wrap.b32 %r11627, %r11626, %r50, %r4585;
	// inline asm
	mov.u32 	%r50, %r4574;
	bra.uni 	BB3_428;

BB3_573:
	setp.eq.s32	%p503, %r58, 22;
	@%p503 bra 	BB3_600;
	bra.uni 	BB3_574;

BB3_600:
	mov.u32 	%r5512, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r52, %r53, %r5512;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r53, %r54, %r5512;
	// inline asm
	mov.u32 	%r11744, 0;
	// inline asm
	shf.r.wrap.b32 %r11750, %r54, %r11744, %r5512;
	// inline asm
	bra.uni 	BB3_576;

BB3_388:
	setp.eq.s32	%p298, %r56, 22;
	@%p298 bra 	BB3_410;
	bra.uni 	BB3_389;

BB3_410:
	mov.u32 	%r4285, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r49, %r48, %r4285;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r50, %r49, %r4285;
	// inline asm
	mov.u32 	%r51, 0;
	// inline asm
	shf.r.wrap.b32 %r52, %r51, %r50, %r4285;
	// inline asm
	bra.uni 	BB3_412;

BB3_555:
	setp.eq.s32	%p515, %r58, 14;
	@%p515 bra 	BB3_604;
	bra.uni 	BB3_556;

BB3_604:
	mov.u32 	%r5646, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r47, %r51, %r5646;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r51, %r52, %r5646;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r52, %r53, %r5646;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r53, %r54, %r5646;
	// inline asm
	mov.u32 	%r11745, 0;
	// inline asm
	shf.r.wrap.b32 %r11744, %r54, %r11745, %r5646;
	// inline asm
	bra.uni 	BB3_558;

BB3_857:
	setp.gt.s32	%p664, %r6846, 29;
	@%p664 bra 	BB3_861;

	setp.eq.s32	%p667, %r6846, 28;
	@%p667 bra 	BB3_866;
	bra.uni 	BB3_859;

BB3_866:
	mov.u32 	%r11751, %r11694;
	mov.u32 	%r11750, %r11694;
	mov.u32 	%r11749, %r11694;
	mov.u32 	%r11748, %r50;

BB3_867:
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r11694;
	mov.u32 	%r11745, %r11694;
	mov.u32 	%r11744, %r11694;
	bra.uni 	BB3_881;

BB3_756:
	setp.ne.s32	%p609, %r6571, 7;
	@%p609 bra 	BB3_757;

	and.b32  	%r6576, %r1002, %r54;
	and.b32  	%r6577, %r11747, %r1003;
	or.b32  	%r11736, %r6577, %r6576;
	bra.uni 	BB3_758;

BB3_757:
	mov.u32 	%r11736, %r54;

BB3_758:
	mov.u32 	%r11737, %r53;

BB3_759:
	mov.u32 	%r11738, %r52;

BB3_760:
	mov.u32 	%r11739, %r51;

BB3_761:
	mov.u32 	%r11740, %r50;
	mov.u32 	%r11741, %r49;
	mov.u32 	%r11742, %r48;
	mov.u32 	%r11743, %r47;

BB3_767:
	add.s32 	%r11752, %r38, -1;
	bra.uni 	BB3_1027;

BB3_372:
	setp.eq.s32	%p310, %r56, 14;
	@%p310 bra 	BB3_416;
	bra.uni 	BB3_373;

BB3_416:
	mov.u32 	%r4419, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r47, %r51, %r4419;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r48, %r47, %r4419;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r49, %r48, %r4419;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r50, %r49, %r4419;
	// inline asm
	mov.u32 	%r11626, 0;
	// inline asm
	shf.r.wrap.b32 %r50, %r11626, %r50, %r4419;
	// inline asm
	bra.uni 	BB3_418;

BB3_782:
	setp.eq.s32	%p642, %r6692, 3;
	@%p642 bra 	BB3_783;
	bra.uni 	BB3_1026;

BB3_783:
	and.b32  	%r11743, %r11743, %r1055;
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;
	bra.uni 	BB3_1026;

BB3_591:
	setp.eq.s32	%p492, %r58, 30;
	@%p492 bra 	BB3_595;
	bra.uni 	BB3_592;

BB3_595:
	mov.u32 	%r11744, 0;
	mov.u32 	%r5410, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r54, %r11744, %r5410;
	// inline asm
	bra.uni 	BB3_594;

BB3_403:
	setp.eq.s32	%p287, %r56, 31;
	@%p287 bra 	BB3_426;
	bra.uni 	BB3_404;

BB3_426:
	mov.u32 	%r53, 0;
	mov.u32 	%r4716, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r53, %r50, %r4716;
	// inline asm
	bra.uni 	BB3_427;

BB3_789:
	setp.ne.s32	%p637, %r6692, 7;
	@%p637 bra 	BB3_1026;

	and.b32  	%r11736, %r11736, %r1055;
	bra.uni 	BB3_1026;

BB3_126:
	setp.eq.s32	%p173, %r56, 2;
	@%p173 bra 	BB3_201;
	bra.uni 	BB3_127;

BB3_201:
	mov.u32 	%r3261, 16;
	// inline asm
	shf.r.wrap.b32 %r11747, %r53, %r54, %r3261;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r52, %r53, %r3261;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r51, %r52, %r3261;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r47, %r51, %r3261;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r48, %r47, %r3261;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r49, %r48, %r3261;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r50, %r49, %r3261;
	// inline asm
	mov.u32 	%r3259, 0;
	// inline asm
	shf.r.wrap.b32 %r11748, %r3259, %r50, %r3261;
	// inline asm
	bra.uni 	BB3_202;

BB3_673:
	setp.eq.s32	%p572, %r56, 17;
	@%p572 bra 	BB3_674;
	bra.uni 	BB3_702;

BB3_674:
	mov.u32 	%r6150, 8;
	// inline asm
	shf.r.wrap.b32 %r11740, %r51, %r52, %r6150;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r52, %r53, %r6150;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11742, %r53, %r54, %r6150;
	// inline asm
	mov.u32 	%r11736, 0;
	// inline asm
	shf.r.wrap.b32 %r54, %r54, %r11736, %r6150;
	// inline asm
	bra.uni 	BB3_678;

BB3_972:
	setp.eq.s32	%p768, %r8013, 17;
	@%p768 bra 	BB3_973;
	bra.uni 	BB3_94;

BB3_973:
	mov.u32 	%r8203, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r48, %r47, %r8203;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r49, %r48, %r8203;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r50, %r49, %r8203;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11751, %r11747, %r50, %r8203;
	// inline asm
	bra.uni 	BB3_1008;

BB3_657:
	setp.eq.s32	%p584, %r56, 9;
	@%p584 bra 	BB3_658;
	bra.uni 	BB3_702;

BB3_658:
	mov.u32 	%r6306, 8;
	// inline asm
	shf.r.wrap.b32 %r11740, %r48, %r47, %r6306;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r47, %r51, %r6306;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11742, %r51, %r52, %r6306;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6295, %r52, %r53, %r6306;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11739, %r53, %r54, %r6306;
	// inline asm
	mov.u32 	%r11736, 0;
	// inline asm
	shf.r.wrap.b32 %r11738, %r54, %r11736, %r6306;
	// inline asm
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r54, %r6295;
	bra.uni 	BB3_724;

BB3_955:
	setp.eq.s32	%p780, %r8013, 9;
	@%p780 bra 	BB3_956;
	bra.uni 	BB3_94;

BB3_956:
	mov.u32 	%r8359, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r51, %r52, %r8359;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r47, %r51, %r8359;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r48, %r47, %r8359;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r49, %r48, %r8359;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r50, %r49, %r8359;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11745, %r11747, %r50, %r8359;
	// inline asm
	mov.u32 	%r11746, %r11747;
	bra.uni 	BB3_1020;

BB3_158:
	setp.eq.s32	%p150, %r56, 18;
	@%p150 bra 	BB3_192;
	bra.uni 	BB3_159;

BB3_192:
	mov.u32 	%r2929, 16;
	// inline asm
	shf.r.wrap.b32 %r11747, %r48, %r47, %r2929;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r49, %r48, %r2929;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r50, %r49, %r2929;
	// inline asm
	mov.u32 	%r11748, 0;
	// inline asm
	shf.r.wrap.b32 %r11744, %r11748, %r50, %r2929;
	// inline asm
	bra.uni 	BB3_194;

BB3_690:
	setp.eq.s32	%p561, %r56, 25;
	@%p561 bra 	BB3_691;
	bra.uni 	BB3_702;

BB3_691:
	mov.u32 	%r6026, 8;
	// inline asm
	shf.r.wrap.b32 %r11740, %r53, %r54, %r6026;
	// inline asm
	mov.u32 	%r11736, 0;
	// inline asm
	shf.r.wrap.b32 %r11741, %r54, %r11736, %r6026;
	// inline asm
	bra.uni 	BB3_695;

BB3_987:
	setp.eq.s32	%p757, %r8013, 25;
	@%p757 bra 	BB3_988;
	bra.uni 	BB3_94;

BB3_988:
	mov.u32 	%r8079, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r50, %r49, %r8079;
	// inline asm
	mov.u32 	%r11750, 0;
	// inline asm
	shf.r.wrap.b32 %r11749, %r11750, %r50, %r8079;
	// inline asm
	bra.uni 	BB3_1002;

BB3_649:
	setp.eq.s32	%p590, %r56, 5;
	@%p590 bra 	BB3_650;
	bra.uni 	BB3_702;

BB3_650:
	mov.u32 	%r6396, 8;
	// inline asm
	shf.r.wrap.b32 %r11740, %r49, %r48, %r6396;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r48, %r47, %r6396;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11742, %r47, %r51, %r6396;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6381, %r51, %r52, %r6396;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11739, %r52, %r53, %r6396;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11738, %r53, %r54, %r6396;
	// inline asm
	mov.u32 	%r11736, 0;
	// inline asm
	shf.r.wrap.b32 %r11737, %r54, %r11736, %r6396;
	// inline asm
	mov.u32 	%r54, %r6381;
	bra.uni 	BB3_724;

BB3_947:
	setp.eq.s32	%p786, %r8013, 5;
	@%p786 bra 	BB3_948;
	bra.uni 	BB3_94;

BB3_948:
	mov.u32 	%r8449, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r52, %r53, %r8449;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r51, %r52, %r8449;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r47, %r51, %r8449;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r48, %r47, %r8449;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r49, %r48, %r8449;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r50, %r49, %r8449;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11746, %r11747, %r50, %r8449;
	// inline asm
	bra.uni 	BB3_1020;

BB3_141:
	setp.eq.s32	%p162, %r56, 10;
	@%p162 bra 	BB3_197;
	bra.uni 	BB3_142;

BB3_197:
	mov.u32 	%r3079, 16;
	// inline asm
	shf.r.wrap.b32 %r11747, %r51, %r52, %r3079;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r47, %r51, %r3079;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r48, %r47, %r3079;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r49, %r48, %r3079;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r50, %r49, %r3079;
	// inline asm
	mov.u32 	%r11748, 0;
	// inline asm
	shf.r.wrap.b32 %r11750, %r11748, %r50, %r3079;
	// inline asm
	mov.u32 	%r11749, %r11748;
	bra.uni 	BB3_202;

BB3_681:
	setp.eq.s32	%p567, %r56, 21;
	@%p567 bra 	BB3_682;
	bra.uni 	BB3_702;

BB3_682:
	mov.u32 	%r6084, 8;
	// inline asm
	shf.r.wrap.b32 %r11740, %r52, %r53, %r6084;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r53, %r54, %r6084;
	// inline asm
	mov.u32 	%r11736, 0;
	// inline asm
	shf.r.wrap.b32 %r11742, %r54, %r11736, %r6084;
	// inline asm
	bra.uni 	BB3_686;

BB3_979:
	setp.eq.s32	%p763, %r8013, 21;
	@%p763 bra 	BB3_980;
	bra.uni 	BB3_94;

BB3_980:
	mov.u32 	%r8137, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r49, %r48, %r8137;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r50, %r49, %r8137;
	// inline asm
	mov.u32 	%r11751, 0;
	// inline asm
	shf.r.wrap.b32 %r11750, %r11751, %r50, %r8137;
	// inline asm
	bra.uni 	BB3_1005;

BB3_664:
	setp.eq.s32	%p579, %r56, 13;
	@%p579 bra 	BB3_665;
	bra.uni 	BB3_702;

BB3_665:
	mov.u32 	%r6224, 8;
	// inline asm
	shf.r.wrap.b32 %r11740, %r47, %r51, %r6224;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r51, %r52, %r6224;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11742, %r52, %r53, %r6224;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6217, %r53, %r54, %r6224;
	// inline asm
	mov.u32 	%r11736, 0;
	// inline asm
	shf.r.wrap.b32 %r11739, %r54, %r11736, %r6224;
	// inline asm
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r54, %r6217;
	bra.uni 	BB3_724;

BB3_962:
	setp.eq.s32	%p775, %r8013, 13;
	@%p775 bra 	BB3_963;
	bra.uni 	BB3_94;

BB3_963:
	mov.u32 	%r8277, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r47, %r51, %r8277;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r48, %r47, %r8277;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r49, %r48, %r8277;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r50, %r49, %r8277;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11744, %r11747, %r50, %r8277;
	// inline asm
	bra.uni 	BB3_967;

BB3_173:
	setp.eq.s32	%p139, %r56, 26;
	@%p139 bra 	BB3_188;
	bra.uni 	BB3_174;

BB3_188:
	mov.u32 	%r2811, 16;
	// inline asm
	shf.r.wrap.b32 %r11747, %r50, %r49, %r2811;
	// inline asm
	mov.u32 	%r11744, 0;
	// inline asm
	shf.r.wrap.b32 %r11746, %r11744, %r50, %r2811;
	// inline asm
	bra.uni 	BB3_176;

BB3_698:
	setp.eq.s32	%p556, %r56, 29;
	@%p556 bra 	BB3_699;
	bra.uni 	BB3_702;

BB3_699:
	mov.u32 	%r11736, 0;
	mov.u32 	%r5976, 8;
	// inline asm
	shf.r.wrap.b32 %r11740, %r54, %r11736, %r5976;
	// inline asm
	bra.uni 	BB3_720;

BB3_994:
	setp.eq.s32	%p752, %r8013, 29;
	@%p752 bra 	BB3_995;
	bra.uni 	BB3_94;

BB3_995:
	mov.u32 	%r11749, 0;
	mov.u32 	%r8029, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r11749, %r50, %r8029;
	// inline asm
	bra.uni 	BB3_1019;

BB3_133:
	setp.eq.s32	%p168, %r56, 6;
	@%p168 bra 	BB3_199;
	bra.uni 	BB3_134;

BB3_199:
	mov.u32 	%r3166, 16;
	// inline asm
	shf.r.wrap.b32 %r11747, %r52, %r53, %r3166;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r51, %r52, %r3166;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r47, %r51, %r3166;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r48, %r47, %r3166;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r49, %r48, %r3166;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r50, %r49, %r3166;
	// inline asm
	mov.u32 	%r11748, 0;
	// inline asm
	shf.r.wrap.b32 %r11749, %r11748, %r50, %r3166;
	// inline asm
	bra.uni 	BB3_202;

BB3_676:
	setp.eq.s32	%p570, %r56, 19;
	@%p570 bra 	BB3_677;
	bra.uni 	BB3_702;

BB3_677:
	mov.u32 	%r6110, 24;
	// inline asm
	shf.r.wrap.b32 %r11740, %r51, %r52, %r6110;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r52, %r53, %r6110;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11742, %r53, %r54, %r6110;
	// inline asm
	mov.u32 	%r11736, 0;
	// inline asm
	shf.r.wrap.b32 %r54, %r54, %r11736, %r6110;
	// inline asm

BB3_678:
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;
	bra.uni 	BB3_724;

BB3_975:
	setp.eq.s32	%p766, %r8013, 19;
	@%p766 bra 	BB3_976;
	bra.uni 	BB3_94;

BB3_976:
	mov.u32 	%r8163, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r48, %r47, %r8163;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r49, %r48, %r8163;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r50, %r49, %r8163;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11751, %r11747, %r50, %r8163;
	// inline asm

BB3_1008:
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11744, %r11747;
	bra.uni 	BB3_1020;

BB3_660:
	setp.eq.s32	%p582, %r56, 11;
	@%p582 bra 	BB3_661;
	bra.uni 	BB3_702;

BB3_661:
	mov.u32 	%r6254, 24;
	// inline asm
	shf.r.wrap.b32 %r11740, %r48, %r47, %r6254;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r47, %r51, %r6254;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11742, %r51, %r52, %r6254;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6243, %r52, %r53, %r6254;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11739, %r53, %r54, %r6254;
	// inline asm
	mov.u32 	%r11736, 0;
	// inline asm
	shf.r.wrap.b32 %r11738, %r54, %r11736, %r6254;
	// inline asm
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r54, %r6243;
	bra.uni 	BB3_724;

BB3_958:
	setp.eq.s32	%p778, %r8013, 11;
	@%p778 bra 	BB3_959;
	bra.uni 	BB3_94;

BB3_959:
	mov.u32 	%r8307, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r51, %r52, %r8307;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r47, %r51, %r8307;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r48, %r47, %r8307;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r49, %r48, %r8307;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r50, %r49, %r8307;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11745, %r11747, %r50, %r8307;
	// inline asm
	mov.u32 	%r11746, %r11747;
	bra.uni 	BB3_1020;

BB3_165:
	setp.eq.s32	%p145, %r56, 22;
	@%p145 bra 	BB3_190;
	bra.uni 	BB3_166;

BB3_190:
	mov.u32 	%r2866, 16;
	// inline asm
	shf.r.wrap.b32 %r11747, %r49, %r48, %r2866;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r50, %r49, %r2866;
	// inline asm
	mov.u32 	%r11744, 0;
	// inline asm
	shf.r.wrap.b32 %r11745, %r11744, %r50, %r2866;
	// inline asm
	bra.uni 	BB3_187;

BB3_693:
	setp.eq.s32	%p559, %r56, 27;
	@%p559 bra 	BB3_694;
	bra.uni 	BB3_702;

BB3_694:
	mov.u32 	%r5998, 24;
	// inline asm
	shf.r.wrap.b32 %r11740, %r53, %r54, %r5998;
	// inline asm
	mov.u32 	%r11736, 0;
	// inline asm
	shf.r.wrap.b32 %r11741, %r54, %r11736, %r5998;
	// inline asm

BB3_695:
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;
	bra.uni 	BB3_722;

BB3_990:
	setp.eq.s32	%p755, %r8013, 27;
	@%p755 bra 	BB3_991;
	bra.uni 	BB3_94;

BB3_991:
	mov.u32 	%r8051, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r50, %r49, %r8051;
	// inline asm
	mov.u32 	%r11750, 0;
	// inline asm
	shf.r.wrap.b32 %r11749, %r11750, %r50, %r8051;
	// inline asm

BB3_1002:
	mov.u32 	%r11751, %r11750;
	mov.u32 	%r11747, %r11750;
	mov.u32 	%r11746, %r11750;
	mov.u32 	%r11745, %r11750;
	mov.u32 	%r11744, %r11750;
	bra.uni 	BB3_1020;

BB3_652:
	setp.eq.s32	%p588, %r56, 7;
	@%p588 bra 	BB3_653;
	bra.uni 	BB3_702;

BB3_653:
	mov.u32 	%r6338, 24;
	// inline asm
	shf.r.wrap.b32 %r11740, %r49, %r48, %r6338;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r48, %r47, %r6338;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11742, %r47, %r51, %r6338;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6323, %r51, %r52, %r6338;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11739, %r52, %r53, %r6338;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11738, %r53, %r54, %r6338;
	// inline asm
	mov.u32 	%r11736, 0;
	// inline asm
	shf.r.wrap.b32 %r11737, %r54, %r11736, %r6338;
	// inline asm
	mov.u32 	%r54, %r6323;
	bra.uni 	BB3_724;

BB3_950:
	setp.eq.s32	%p784, %r8013, 7;
	@%p784 bra 	BB3_951;
	bra.uni 	BB3_94;

BB3_951:
	mov.u32 	%r8391, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r52, %r53, %r8391;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r51, %r52, %r8391;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r47, %r51, %r8391;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r48, %r47, %r8391;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r49, %r48, %r8391;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r50, %r49, %r8391;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11746, %r11747, %r50, %r8391;
	// inline asm
	bra.uni 	BB3_1020;

BB3_148:
	setp.eq.s32	%p157, %r56, 14;
	@%p157 bra 	BB3_195;
	bra.uni 	BB3_149;

BB3_195:
	mov.u32 	%r3000, 16;
	// inline asm
	shf.r.wrap.b32 %r11747, %r47, %r51, %r3000;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r48, %r47, %r3000;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r49, %r48, %r3000;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r50, %r49, %r3000;
	// inline asm
	mov.u32 	%r11748, 0;
	// inline asm
	shf.r.wrap.b32 %r11751, %r11748, %r50, %r3000;
	// inline asm
	bra.uni 	BB3_151;

BB3_684:
	setp.eq.s32	%p565, %r56, 23;
	@%p565 bra 	BB3_685;
	bra.uni 	BB3_702;

BB3_685:
	mov.u32 	%r6050, 24;
	// inline asm
	shf.r.wrap.b32 %r11740, %r52, %r53, %r6050;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r53, %r54, %r6050;
	// inline asm
	mov.u32 	%r11736, 0;
	// inline asm
	shf.r.wrap.b32 %r11742, %r54, %r11736, %r6050;
	// inline asm

BB3_686:
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;
	bra.uni 	BB3_723;

BB3_982:
	setp.eq.s32	%p761, %r8013, 23;
	@%p761 bra 	BB3_983;
	bra.uni 	BB3_94;

BB3_983:
	mov.u32 	%r8103, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r49, %r48, %r8103;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r50, %r49, %r8103;
	// inline asm
	mov.u32 	%r11751, 0;
	// inline asm
	shf.r.wrap.b32 %r11750, %r11751, %r50, %r8103;
	// inline asm

BB3_1005:
	mov.u32 	%r11747, %r11751;
	mov.u32 	%r11746, %r11751;
	mov.u32 	%r11745, %r11751;
	mov.u32 	%r11744, %r11751;
	bra.uni 	BB3_1020;

BB3_667:
	setp.eq.s32	%p577, %r56, 15;
	@%p577 bra 	BB3_668;
	bra.uni 	BB3_702;

BB3_668:
	mov.u32 	%r6178, 24;
	// inline asm
	shf.r.wrap.b32 %r11740, %r47, %r51, %r6178;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r51, %r52, %r6178;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11742, %r52, %r53, %r6178;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6171, %r53, %r54, %r6178;
	// inline asm
	mov.u32 	%r11736, 0;
	// inline asm
	shf.r.wrap.b32 %r11739, %r54, %r11736, %r6178;
	// inline asm
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r54, %r6171;
	bra.uni 	BB3_724;

BB3_965:
	setp.eq.s32	%p773, %r8013, 15;
	@%p773 bra 	BB3_966;
	bra.uni 	BB3_94;

BB3_966:
	mov.u32 	%r8231, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r47, %r51, %r8231;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r48, %r47, %r8231;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r49, %r48, %r8231;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r50, %r49, %r8231;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11744, %r11747, %r50, %r8231;
	// inline asm

BB3_967:
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11745, %r11747;
	bra.uni 	BB3_1020;

BB3_181:
	setp.eq.s32	%p134, %r56, 30;
	@%p134 bra 	BB3_185;
	bra.uni 	BB3_182;

BB3_185:
	mov.u32 	%r11744, 0;
	mov.u32 	%r2764, 16;
	// inline asm
	shf.r.wrap.b32 %r11747, %r11744, %r50, %r2764;
	// inline asm
	bra.uni 	BB3_184;

BB3_276:
	setp.eq.s32	%p222, %r56, 18;
	@%p222 bra 	BB3_308;
	bra.uni 	BB3_277;

BB3_308:
	mov.u32 	%r3527, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r48, %r47, %r3527;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r49, %r48, %r3527;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r50, %r49, %r3527;
	// inline asm
	mov.u32 	%r11607, 0;
	// inline asm
	shf.r.wrap.b32 %r51, %r11607, %r50, %r3527;
	// inline asm
	bra.uni 	BB3_310;

BB3_260:
	setp.eq.s32	%p234, %r56, 10;
	@%p234 bra 	BB3_314;
	bra.uni 	BB3_261;

BB3_314:
	mov.u32 	%r3677, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r51, %r52, %r3677;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r47, %r51, %r3677;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r48, %r47, %r3677;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r49, %r48, %r3677;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3670, %r50, %r49, %r3677;
	// inline asm
	mov.u32 	%r11607, 0;
	// inline asm
	shf.r.wrap.b32 %r11609, %r11607, %r50, %r3677;
	// inline asm
	mov.u32 	%r11608, %r11607;
	mov.u32 	%r50, %r3670;
	bra.uni 	BB3_323;

BB3_291:
	setp.eq.s32	%p211, %r56, 26;
	@%p211 bra 	BB3_302;
	bra.uni 	BB3_292;

BB3_302:
	mov.u32 	%r3409, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r50, %r49, %r3409;
	// inline asm
	mov.u32 	%r52, 0;
	// inline asm
	shf.r.wrap.b32 %r53, %r52, %r50, %r3409;
	// inline asm
	bra.uni 	BB3_304;

BB3_701:
	setp.ne.s32	%p554, %r56, 30;
	@%p554 bra 	BB3_702;

	mov.u32 	%r11736, 0;
	mov.u32 	%r5965, 16;
	// inline asm
	shf.r.wrap.b32 %r11740, %r54, %r11736, %r5965;
	// inline asm

BB3_720:
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;

BB3_721:
	mov.u32 	%r11741, %r11736;

BB3_722:
	mov.u32 	%r11742, %r11736;

BB3_723:
	mov.u32 	%r54, %r11736;
	bra.uni 	BB3_724;

BB3_702:
	mov.u32 	%r11736, %r54;
	mov.u32 	%r11737, %r53;
	mov.u32 	%r11738, %r52;
	mov.u32 	%r11739, %r51;
	mov.u32 	%r11740, %r50;
	mov.u32 	%r11741, %r49;
	mov.u32 	%r11742, %r48;
	mov.u32 	%r54, %r47;

BB3_724:
	and.b32  	%r6507, %r57, 3;
	shl.b32 	%r6508, %r6507, 3;
	mov.u32 	%r6509, 1;
	shl.b32 	%r6510, %r6509, %r6508;
	add.s32 	%r985, %r6510, -1;
	shr.u32 	%r6506, %r58, 2;
	setp.gt.s32	%p594, %r6506, 3;
	@%p594 bra 	BB3_733;

	setp.gt.s32	%p600, %r6506, 1;
	@%p600 bra 	BB3_730;

	setp.eq.s32	%p603, %r6506, 0;
	@%p603 bra 	BB3_745;
	bra.uni 	BB3_727;

BB3_745:
	and.b32  	%r11740, %r11740, %r985;
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;
	mov.u32 	%r11741, %r11736;
	bra.uni 	BB3_729;

BB3_733:
	setp.gt.s32	%p595, %r6506, 5;
	@%p595 bra 	BB3_737;

	setp.eq.s32	%p598, %r6506, 4;
	@%p598 bra 	BB3_742;
	bra.uni 	BB3_735;

BB3_742:
	and.b32  	%r11739, %r11739, %r985;
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	bra.uni 	BB3_739;

BB3_730:
	setp.eq.s32	%p601, %r6506, 2;
	@%p601 bra 	BB3_743;
	bra.uni 	BB3_731;

BB3_743:
	and.b32  	%r11742, %r11742, %r985;
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;
	bra.uni 	BB3_744;

BB3_737:
	setp.eq.s32	%p596, %r6506, 6;
	@%p596 bra 	BB3_741;
	bra.uni 	BB3_738;

BB3_741:
	and.b32  	%r11737, %r11737, %r985;
	mov.u32 	%r11736, 0;
	bra.uni 	BB3_739;

BB3_727:
	setp.eq.s32	%p604, %r6506, 1;
	@%p604 bra 	BB3_728;
	bra.uni 	BB3_739;

BB3_728:
	and.b32  	%r11741, %r11741, %r985;
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;

BB3_729:
	mov.u32 	%r11742, %r11736;

BB3_744:
	mov.u32 	%r11743, %r11736;
	mov.u32 	%r11752, %r58;
	bra.uni 	BB3_1027;

BB3_735:
	setp.eq.s32	%p599, %r6506, 5;
	@%p599 bra 	BB3_736;
	bra.uni 	BB3_739;

BB3_736:
	and.b32  	%r11738, %r11738, %r985;
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	bra.uni 	BB3_739;

BB3_731:
	setp.eq.s32	%p602, %r6506, 3;
	@%p602 bra 	BB3_732;
	bra.uni 	BB3_739;

BB3_732:
	and.b32  	%r11743, %r54, %r985;
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;
	mov.u32 	%r11752, %r58;
	bra.uni 	BB3_1027;

BB3_738:
	setp.ne.s32	%p597, %r6506, 7;
	@%p597 bra 	BB3_739;

	and.b32  	%r11736, %r11736, %r985;

BB3_739:
	mov.u32 	%r11743, %r54;
	mov.u32 	%r11752, %r58;
	bra.uni 	BB3_1027;

BB3_997:
	setp.ne.s32	%p750, %r8013, 30;
	@%p750 bra 	BB3_94;

	mov.u32 	%r11749, 0;
	mov.u32 	%r8018, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r11749, %r50, %r8018;
	// inline asm

BB3_1019:
	mov.u32 	%r11750, %r11749;
	mov.u32 	%r11751, %r11749;
	mov.u32 	%r11747, %r11749;
	mov.u32 	%r11746, %r11749;
	mov.u32 	%r11745, %r11749;
	mov.u32 	%r11744, %r11749;
	bra.uni 	BB3_1020;

BB3_94:
	mov.u32 	%r11748, %r54;
	mov.u32 	%r11749, %r53;
	mov.u32 	%r11750, %r52;
	mov.u32 	%r11751, %r51;
	mov.u32 	%r11747, %r50;
	mov.u32 	%r11746, %r49;
	mov.u32 	%r11745, %r48;
	mov.u32 	%r11744, %r47;

BB3_1020:
	// inline asm
	prmt.b32 %r11740, %r11748, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r11741, %r11749, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r11742, %r11750, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r11743, %r11751, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r11739, %r11744, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r11738, %r11745, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r11737, %r11746, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r11736, %r11747, 0, 0x0123;
	// inline asm
	bra.uni 	BB3_1026;

BB3_252:
	setp.eq.s32	%p240, %r56, 6;
	@%p240 bra 	BB3_316;
	bra.uni 	BB3_253;

BB3_316:
	mov.u32 	%r3764, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r52, %r53, %r3764;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r51, %r52, %r3764;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r47, %r51, %r3764;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r48, %r47, %r3764;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3753, %r49, %r48, %r3764;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11609, %r50, %r49, %r3764;
	// inline asm
	mov.u32 	%r11607, 0;
	// inline asm
	shf.r.wrap.b32 %r11608, %r11607, %r50, %r3764;
	// inline asm
	mov.u32 	%r50, %r3753;
	bra.uni 	BB3_323;

BB3_283:
	setp.eq.s32	%p217, %r56, 22;
	@%p217 bra 	BB3_305;
	bra.uni 	BB3_284;

BB3_305:
	mov.u32 	%r3464, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r49, %r48, %r3464;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r50, %r49, %r3464;
	// inline asm
	mov.u32 	%r51, 0;
	// inline asm
	shf.r.wrap.b32 %r52, %r51, %r50, %r3464;
	// inline asm
	bra.uni 	BB3_307;

BB3_267:
	setp.eq.s32	%p229, %r56, 14;
	@%p229 bra 	BB3_311;
	bra.uni 	BB3_268;

BB3_311:
	mov.u32 	%r3598, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r47, %r51, %r3598;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r48, %r47, %r3598;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r49, %r48, %r3598;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r50, %r49, %r3598;
	// inline asm
	mov.u32 	%r11607, 0;
	// inline asm
	shf.r.wrap.b32 %r50, %r11607, %r50, %r3598;
	// inline asm
	bra.uni 	BB3_313;

BB3_298:
	setp.eq.s32	%p206, %r56, 31;
	@%p206 bra 	BB3_321;
	bra.uni 	BB3_299;

BB3_321:
	mov.u32 	%r53, 0;
	mov.u32 	%r3895, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r53, %r50, %r3895;
	// inline asm
	bra.uni 	BB3_322;

BB3_425:
	mov.u32 	%r4712, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r53, %r54, %r4712;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r52, %r53, %r4712;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r51, %r52, %r4712;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r47, %r51, %r4712;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4697, %r48, %r47, %r4712;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11628, %r49, %r48, %r4712;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11627, %r50, %r49, %r4712;
	// inline asm
	mov.u32 	%r4710, 0;
	// inline asm
	shf.r.wrap.b32 %r11626, %r4710, %r50, %r4712;
	// inline asm
	mov.u32 	%r50, %r4697;
	bra.uni 	BB3_428;

BB3_351:
	setp.eq.s32	%p327, %r56, 3;
	@%p327 bra 	BB3_423;
	bra.uni 	BB3_352;

BB3_423:
	mov.u32 	%r4648, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r53, %r54, %r4648;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r52, %r53, %r4648;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r51, %r52, %r4648;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r47, %r51, %r4648;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4633, %r48, %r47, %r4648;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11628, %r49, %r48, %r4648;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11627, %r50, %r49, %r4648;
	// inline asm
	mov.u32 	%r4646, 0;
	// inline asm
	shf.r.wrap.b32 %r11626, %r4646, %r50, %r4648;
	// inline asm
	mov.u32 	%r50, %r4633;
	bra.uni 	BB3_428;

BB3_531:
	setp.eq.s32	%p534, %r58, 1;
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r11747;
	mov.u32 	%r11749, %r11747;
	mov.u32 	%r11750, %r11747;
	mov.u32 	%r11751, %r11747;
	@%p534 bra 	BB3_532;
	bra.uni 	BB3_612;

BB3_532:
	mov.u32 	%r5939, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r50, %r49, %r5939;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r49, %r48, %r5939;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r48, %r47, %r5939;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r47, %r51, %r5939;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r51, %r52, %r5939;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r52, %r53, %r5939;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r53, %r54, %r5939;
	// inline asm
	mov.u32 	%r5938, 0;
	// inline asm
	shf.r.wrap.b32 %r11747, %r54, %r5938, %r5939;
	// inline asm
	bra.uni 	BB3_612;

BB3_804:
	setp.eq.s32	%p704, %r6846, 2;
	@%p704 bra 	BB3_880;
	bra.uni 	BB3_805;

BB3_880:
	mov.u32 	%r7359, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r53, %r54, %r7359;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r52, %r53, %r7359;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r51, %r52, %r7359;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r47, %r51, %r7359;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r48, %r47, %r7359;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r49, %r48, %r7359;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r50, %r49, %r7359;
	// inline asm
	mov.u32 	%r7357, 0;
	// inline asm
	shf.r.wrap.b32 %r11747, %r7357, %r50, %r7359;
	// inline asm
	bra.uni 	BB3_881;

BB3_563:
	setp.eq.s32	%p511, %r58, 17;
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r11747;
	mov.u32 	%r11749, %r11747;
	mov.u32 	%r11750, %r11747;
	mov.u32 	%r11751, %r11747;
	@%p511 bra 	BB3_564;
	bra.uni 	BB3_612;

BB3_564:
	mov.u32 	%r5595, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r51, %r52, %r5595;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r52, %r53, %r5595;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r53, %r54, %r5595;
	// inline asm
	mov.u32 	%r11744, 0;
	// inline asm
	shf.r.wrap.b32 %r11751, %r54, %r11744, %r5595;
	// inline asm
	bra.uni 	BB3_568;

BB3_379:
	setp.eq.s32	%p306, %r56, 17;
	@%p306 bra 	BB3_380;
	bra.uni 	BB3_352;

BB3_380:
	mov.u32 	%r4368, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r48, %r47, %r4368;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r49, %r48, %r4368;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r50, %r49, %r4368;
	// inline asm
	mov.u32 	%r11626, 0;
	// inline asm
	shf.r.wrap.b32 %r51, %r11626, %r50, %r4368;
	// inline asm
	bra.uni 	BB3_415;

BB3_546:
	setp.eq.s32	%p523, %r58, 9;
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r11747;
	mov.u32 	%r11749, %r11747;
	mov.u32 	%r11750, %r11747;
	mov.u32 	%r11751, %r11747;
	@%p523 bra 	BB3_547;
	bra.uni 	BB3_612;

BB3_547:
	mov.u32 	%r5751, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r48, %r47, %r5751;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r47, %r51, %r5751;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r51, %r52, %r5751;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r52, %r53, %r5751;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r53, %r54, %r5751;
	// inline asm
	mov.u32 	%r11746, 0;
	// inline asm
	shf.r.wrap.b32 %r11745, %r54, %r11746, %r5751;
	// inline asm
	mov.u32 	%r11747, %r11746;
	bra.uni 	BB3_612;

BB3_836:
	setp.eq.s32	%p681, %r6846, 18;
	@%p681 bra 	BB3_872;
	bra.uni 	BB3_837;

BB3_872:
	mov.u32 	%r7027, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r48, %r47, %r7027;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r49, %r48, %r7027;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r50, %r49, %r7027;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11751, %r11747, %r50, %r7027;
	// inline asm
	bra.uni 	BB3_839;

BB3_363:
	setp.eq.s32	%p318, %r56, 9;
	@%p318 bra 	BB3_364;
	bra.uni 	BB3_352;

BB3_364:
	mov.u32 	%r4524, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r51, %r52, %r4524;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r47, %r51, %r4524;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r48, %r47, %r4524;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r49, %r48, %r4524;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4517, %r50, %r49, %r4524;
	// inline asm
	mov.u32 	%r11626, 0;
	// inline asm
	shf.r.wrap.b32 %r11628, %r11626, %r50, %r4524;
	// inline asm
	mov.u32 	%r11627, %r11626;
	mov.u32 	%r50, %r4517;
	bra.uni 	BB3_428;

BB3_580:
	setp.eq.s32	%p500, %r58, 25;
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r11747;
	mov.u32 	%r11749, %r11747;
	mov.u32 	%r11750, %r11747;
	mov.u32 	%r11751, %r11747;
	@%p500 bra 	BB3_581;
	bra.uni 	BB3_612;

BB3_581:
	mov.u32 	%r5471, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r53, %r54, %r5471;
	// inline asm
	mov.u32 	%r11744, 0;
	// inline asm
	shf.r.wrap.b32 %r11749, %r54, %r11744, %r5471;
	// inline asm
	bra.uni 	BB3_585;

BB3_394:
	setp.eq.s32	%p295, %r56, 25;
	@%p295 bra 	BB3_395;
	bra.uni 	BB3_352;

BB3_395:
	mov.u32 	%r4244, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r50, %r49, %r4244;
	// inline asm
	mov.u32 	%r52, 0;
	// inline asm
	shf.r.wrap.b32 %r53, %r52, %r50, %r4244;
	// inline asm
	bra.uni 	BB3_409;

BB3_538:
	setp.eq.s32	%p529, %r58, 5;
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r11747;
	mov.u32 	%r11749, %r11747;
	mov.u32 	%r11750, %r11747;
	mov.u32 	%r11751, %r11747;
	@%p529 bra 	BB3_539;
	bra.uni 	BB3_612;

BB3_539:
	mov.u32 	%r5841, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r49, %r48, %r5841;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r48, %r47, %r5841;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r47, %r51, %r5841;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r51, %r52, %r5841;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r52, %r53, %r5841;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r53, %r54, %r5841;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11746, %r54, %r11747, %r5841;
	// inline asm
	bra.uni 	BB3_612;

BB3_819:
	setp.eq.s32	%p693, %r6846, 10;
	@%p693 bra 	BB3_876;
	bra.uni 	BB3_820;

BB3_876:
	mov.u32 	%r7177, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r51, %r52, %r7177;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r47, %r51, %r7177;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r48, %r47, %r7177;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r49, %r48, %r7177;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r50, %r49, %r7177;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11745, %r11747, %r50, %r7177;
	// inline asm
	mov.u32 	%r11746, %r11747;
	bra.uni 	BB3_881;

BB3_355:
	setp.eq.s32	%p324, %r56, 5;
	@%p324 bra 	BB3_356;
	bra.uni 	BB3_352;

BB3_356:
	mov.u32 	%r4614, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r52, %r53, %r4614;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r51, %r52, %r4614;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r47, %r51, %r4614;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r48, %r47, %r4614;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4603, %r49, %r48, %r4614;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11628, %r50, %r49, %r4614;
	// inline asm
	mov.u32 	%r11626, 0;
	// inline asm
	shf.r.wrap.b32 %r11627, %r11626, %r50, %r4614;
	// inline asm
	mov.u32 	%r50, %r4603;
	bra.uni 	BB3_428;

BB3_571:
	setp.eq.s32	%p506, %r58, 21;
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r11747;
	mov.u32 	%r11749, %r11747;
	mov.u32 	%r11750, %r11747;
	mov.u32 	%r11751, %r11747;
	@%p506 bra 	BB3_572;
	bra.uni 	BB3_612;

BB3_572:
	mov.u32 	%r5529, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r52, %r53, %r5529;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r53, %r54, %r5529;
	// inline asm
	mov.u32 	%r11744, 0;
	// inline asm
	shf.r.wrap.b32 %r11750, %r54, %r11744, %r5529;
	// inline asm
	bra.uni 	BB3_576;

BB3_386:
	setp.eq.s32	%p301, %r56, 21;
	@%p301 bra 	BB3_387;
	bra.uni 	BB3_352;

BB3_387:
	mov.u32 	%r4302, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r49, %r48, %r4302;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r50, %r49, %r4302;
	// inline asm
	mov.u32 	%r51, 0;
	// inline asm
	shf.r.wrap.b32 %r52, %r51, %r50, %r4302;
	// inline asm
	bra.uni 	BB3_412;

BB3_553:
	setp.eq.s32	%p518, %r58, 13;
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r11747;
	mov.u32 	%r11749, %r11747;
	mov.u32 	%r11750, %r11747;
	mov.u32 	%r11751, %r11747;
	@%p518 bra 	BB3_554;
	bra.uni 	BB3_612;

BB3_554:
	mov.u32 	%r5669, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r47, %r51, %r5669;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r51, %r52, %r5669;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r52, %r53, %r5669;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r53, %r54, %r5669;
	// inline asm
	mov.u32 	%r11745, 0;
	// inline asm
	shf.r.wrap.b32 %r11744, %r54, %r11745, %r5669;
	// inline asm
	bra.uni 	BB3_558;

BB3_852:
	setp.eq.s32	%p670, %r6846, 26;
	@%p670 bra 	BB3_868;
	bra.uni 	BB3_853;

BB3_868:
	mov.u32 	%r6909, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r50, %r49, %r6909;
	// inline asm
	mov.u32 	%r11751, 0;
	// inline asm
	shf.r.wrap.b32 %r11749, %r11751, %r50, %r6909;
	// inline asm
	bra.uni 	BB3_855;

BB3_370:
	setp.eq.s32	%p313, %r56, 13;
	@%p313 bra 	BB3_371;
	bra.uni 	BB3_352;

BB3_371:
	mov.u32 	%r4442, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r47, %r51, %r4442;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r48, %r47, %r4442;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r49, %r48, %r4442;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r50, %r49, %r4442;
	// inline asm
	mov.u32 	%r11626, 0;
	// inline asm
	shf.r.wrap.b32 %r50, %r11626, %r50, %r4442;
	// inline asm
	bra.uni 	BB3_418;

BB3_589:
	setp.eq.s32	%p495, %r58, 29;
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r11747;
	mov.u32 	%r11749, %r11747;
	mov.u32 	%r11750, %r11747;
	mov.u32 	%r11751, %r11747;
	@%p495 bra 	BB3_590;
	bra.uni 	BB3_612;

BB3_590:
	mov.u32 	%r11744, 0;
	mov.u32 	%r5421, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r54, %r11744, %r5421;
	// inline asm
	bra.uni 	BB3_594;

BB3_401:
	setp.eq.s32	%p290, %r56, 29;
	@%p290 bra 	BB3_402;
	bra.uni 	BB3_352;

BB3_402:
	mov.u32 	%r53, 0;
	mov.u32 	%r4194, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r53, %r50, %r4194;
	// inline asm
	bra.uni 	BB3_427;

BB3_320:
	mov.u32 	%r3891, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r53, %r54, %r3891;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r52, %r53, %r3891;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r51, %r52, %r3891;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r47, %r51, %r3891;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3876, %r48, %r47, %r3891;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11609, %r49, %r48, %r3891;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11608, %r50, %r49, %r3891;
	// inline asm
	mov.u32 	%r3889, 0;
	// inline asm
	shf.r.wrap.b32 %r11607, %r3889, %r50, %r3891;
	// inline asm
	mov.u32 	%r50, %r3876;
	bra.uni 	BB3_323;

BB3_246:
	setp.eq.s32	%p246, %r56, 3;
	@%p246 bra 	BB3_318;
	bra.uni 	BB3_247;

BB3_318:
	mov.u32 	%r3827, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r53, %r54, %r3827;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r52, %r53, %r3827;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r51, %r52, %r3827;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r47, %r51, %r3827;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3812, %r48, %r47, %r3827;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11609, %r49, %r48, %r3827;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11608, %r50, %r49, %r3827;
	// inline asm
	mov.u32 	%r3825, 0;
	// inline asm
	shf.r.wrap.b32 %r11607, %r3825, %r50, %r3827;
	// inline asm
	mov.u32 	%r50, %r3812;
	bra.uni 	BB3_323;

BB3_534:
	setp.eq.s32	%p532, %r58, 3;
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r11747;
	mov.u32 	%r11749, %r11747;
	mov.u32 	%r11750, %r11747;
	mov.u32 	%r11751, %r11747;
	@%p532 bra 	BB3_535;
	bra.uni 	BB3_612;

BB3_535:
	mov.u32 	%r5875, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r50, %r49, %r5875;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r49, %r48, %r5875;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r48, %r47, %r5875;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r47, %r51, %r5875;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r51, %r52, %r5875;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r52, %r53, %r5875;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r53, %r54, %r5875;
	// inline asm
	mov.u32 	%r5874, 0;
	// inline asm
	shf.r.wrap.b32 %r11747, %r54, %r5874, %r5875;
	// inline asm
	bra.uni 	BB3_612;

BB3_811:
	setp.eq.s32	%p699, %r6846, 6;
	@%p699 bra 	BB3_878;
	bra.uni 	BB3_812;

BB3_878:
	mov.u32 	%r7264, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r52, %r53, %r7264;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r51, %r52, %r7264;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r47, %r51, %r7264;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r48, %r47, %r7264;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r49, %r48, %r7264;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r50, %r49, %r7264;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11746, %r11747, %r50, %r7264;
	// inline asm
	bra.uni 	BB3_881;

BB3_566:
	setp.eq.s32	%p509, %r58, 19;
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r11747;
	mov.u32 	%r11749, %r11747;
	mov.u32 	%r11750, %r11747;
	mov.u32 	%r11751, %r11747;
	@%p509 bra 	BB3_567;
	bra.uni 	BB3_612;

BB3_567:
	mov.u32 	%r5555, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r51, %r52, %r5555;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r52, %r53, %r5555;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r53, %r54, %r5555;
	// inline asm
	mov.u32 	%r11744, 0;
	// inline asm
	shf.r.wrap.b32 %r11751, %r54, %r11744, %r5555;
	// inline asm

BB3_568:
	mov.u32 	%r11745, %r11744;
	mov.u32 	%r11746, %r11744;
	mov.u32 	%r11747, %r11744;
	bra.uni 	BB3_612;

BB3_382:
	setp.eq.s32	%p304, %r56, 19;
	@%p304 bra 	BB3_383;
	bra.uni 	BB3_352;

BB3_383:
	mov.u32 	%r4328, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r48, %r47, %r4328;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r49, %r48, %r4328;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r50, %r49, %r4328;
	// inline asm
	mov.u32 	%r11626, 0;
	// inline asm
	shf.r.wrap.b32 %r51, %r11626, %r50, %r4328;
	// inline asm

BB3_415:
	mov.u32 	%r11627, %r11626;
	mov.u32 	%r11628, %r11626;
	mov.u32 	%r50, %r11626;
	bra.uni 	BB3_428;

BB3_549:
	setp.eq.s32	%p521, %r58, 11;
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r11747;
	mov.u32 	%r11749, %r11747;
	mov.u32 	%r11750, %r11747;
	mov.u32 	%r11751, %r11747;
	@%p521 bra 	BB3_550;
	bra.uni 	BB3_612;

BB3_550:
	mov.u32 	%r5699, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r48, %r47, %r5699;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r47, %r51, %r5699;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r51, %r52, %r5699;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r52, %r53, %r5699;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r53, %r54, %r5699;
	// inline asm
	mov.u32 	%r11746, 0;
	// inline asm
	shf.r.wrap.b32 %r11745, %r54, %r11746, %r5699;
	// inline asm
	mov.u32 	%r11747, %r11746;
	bra.uni 	BB3_612;

BB3_844:
	setp.eq.s32	%p676, %r6846, 22;
	@%p676 bra 	BB3_870;
	bra.uni 	BB3_845;

BB3_870:
	mov.u32 	%r6964, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r49, %r48, %r6964;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r50, %r49, %r6964;
	// inline asm
	mov.u32 	%r11751, 0;
	// inline asm
	shf.r.wrap.b32 %r11750, %r11751, %r50, %r6964;
	// inline asm
	bra.uni 	BB3_856;

BB3_366:
	setp.eq.s32	%p316, %r56, 11;
	@%p316 bra 	BB3_367;
	bra.uni 	BB3_352;

BB3_367:
	mov.u32 	%r4472, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r51, %r52, %r4472;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r47, %r51, %r4472;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r48, %r47, %r4472;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r49, %r48, %r4472;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4465, %r50, %r49, %r4472;
	// inline asm
	mov.u32 	%r11626, 0;
	// inline asm
	shf.r.wrap.b32 %r11628, %r11626, %r50, %r4472;
	// inline asm
	mov.u32 	%r11627, %r11626;
	mov.u32 	%r50, %r4465;
	bra.uni 	BB3_428;

BB3_583:
	setp.eq.s32	%p498, %r58, 27;
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r11747;
	mov.u32 	%r11749, %r11747;
	mov.u32 	%r11750, %r11747;
	mov.u32 	%r11751, %r11747;
	@%p498 bra 	BB3_584;
	bra.uni 	BB3_612;

BB3_584:
	mov.u32 	%r5443, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r53, %r54, %r5443;
	// inline asm
	mov.u32 	%r11744, 0;
	// inline asm
	shf.r.wrap.b32 %r11749, %r54, %r11744, %r5443;
	// inline asm

BB3_585:
	mov.u32 	%r11745, %r11744;
	mov.u32 	%r11746, %r11744;
	mov.u32 	%r11747, %r11744;
	bra.uni 	BB3_586;

BB3_397:
	setp.eq.s32	%p293, %r56, 27;
	@%p293 bra 	BB3_398;
	bra.uni 	BB3_352;

BB3_398:
	mov.u32 	%r4216, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r50, %r49, %r4216;
	// inline asm
	mov.u32 	%r52, 0;
	// inline asm
	shf.r.wrap.b32 %r53, %r52, %r50, %r4216;
	// inline asm

BB3_409:
	mov.u32 	%r51, %r52;
	mov.u32 	%r11626, %r52;
	mov.u32 	%r11627, %r52;
	mov.u32 	%r11628, %r52;
	mov.u32 	%r50, %r52;
	bra.uni 	BB3_428;

BB3_541:
	setp.eq.s32	%p527, %r58, 7;
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r11747;
	mov.u32 	%r11749, %r11747;
	mov.u32 	%r11750, %r11747;
	mov.u32 	%r11751, %r11747;
	@%p527 bra 	BB3_542;
	bra.uni 	BB3_612;

BB3_542:
	mov.u32 	%r5783, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r49, %r48, %r5783;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r48, %r47, %r5783;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r47, %r51, %r5783;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r51, %r52, %r5783;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r52, %r53, %r5783;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r53, %r54, %r5783;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11746, %r54, %r11747, %r5783;
	// inline asm
	bra.uni 	BB3_612;

BB3_826:
	setp.eq.s32	%p688, %r6846, 14;
	@%p688 bra 	BB3_874;
	bra.uni 	BB3_827;

BB3_874:
	mov.u32 	%r7098, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r47, %r51, %r7098;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r48, %r47, %r7098;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r49, %r48, %r7098;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r50, %r49, %r7098;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11744, %r11747, %r50, %r7098;
	// inline asm
	bra.uni 	BB3_829;

BB3_358:
	setp.eq.s32	%p322, %r56, 7;
	@%p322 bra 	BB3_359;
	bra.uni 	BB3_352;

BB3_359:
	mov.u32 	%r4556, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r52, %r53, %r4556;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r51, %r52, %r4556;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r47, %r51, %r4556;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r48, %r47, %r4556;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4545, %r49, %r48, %r4556;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11628, %r50, %r49, %r4556;
	// inline asm
	mov.u32 	%r11626, 0;
	// inline asm
	shf.r.wrap.b32 %r11627, %r11626, %r50, %r4556;
	// inline asm
	mov.u32 	%r50, %r4545;
	bra.uni 	BB3_428;

BB3_574:
	setp.eq.s32	%p504, %r58, 23;
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r11747;
	mov.u32 	%r11749, %r11747;
	mov.u32 	%r11750, %r11747;
	mov.u32 	%r11751, %r11747;
	@%p504 bra 	BB3_575;
	bra.uni 	BB3_612;

BB3_575:
	mov.u32 	%r5495, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r52, %r53, %r5495;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r53, %r54, %r5495;
	// inline asm
	mov.u32 	%r11744, 0;
	// inline asm
	shf.r.wrap.b32 %r11750, %r54, %r11744, %r5495;
	// inline asm

BB3_576:
	mov.u32 	%r11745, %r11744;
	mov.u32 	%r11746, %r11744;
	mov.u32 	%r11747, %r11744;
	mov.u32 	%r11751, %r11744;
	bra.uni 	BB3_612;

BB3_389:
	setp.eq.s32	%p299, %r56, 23;
	@%p299 bra 	BB3_390;
	bra.uni 	BB3_352;

BB3_390:
	mov.u32 	%r4268, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r49, %r48, %r4268;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r50, %r49, %r4268;
	// inline asm
	mov.u32 	%r51, 0;
	// inline asm
	shf.r.wrap.b32 %r52, %r51, %r50, %r4268;
	// inline asm

BB3_412:
	mov.u32 	%r11626, %r51;
	mov.u32 	%r11627, %r51;
	mov.u32 	%r11628, %r51;
	mov.u32 	%r50, %r51;
	bra.uni 	BB3_428;

BB3_556:
	setp.eq.s32	%p516, %r58, 15;
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r11747;
	mov.u32 	%r11749, %r11747;
	mov.u32 	%r11750, %r11747;
	mov.u32 	%r11751, %r11747;
	@%p516 bra 	BB3_557;
	bra.uni 	BB3_612;

BB3_557:
	mov.u32 	%r5623, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r47, %r51, %r5623;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r51, %r52, %r5623;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r52, %r53, %r5623;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r53, %r54, %r5623;
	// inline asm
	mov.u32 	%r11745, 0;
	// inline asm
	shf.r.wrap.b32 %r11744, %r54, %r11745, %r5623;
	// inline asm

BB3_558:
	mov.u32 	%r11746, %r11745;
	mov.u32 	%r11747, %r11745;
	bra.uni 	BB3_612;

BB3_861:
	setp.eq.s32	%p665, %r6846, 30;
	@%p665 bra 	BB3_865;
	bra.uni 	BB3_862;

BB3_865:
	mov.u32 	%r11751, 0;
	mov.u32 	%r6862, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r11751, %r50, %r6862;
	// inline asm
	bra.uni 	BB3_864;

BB3_373:
	setp.eq.s32	%p311, %r56, 15;
	@%p311 bra 	BB3_374;
	bra.uni 	BB3_352;

BB3_374:
	mov.u32 	%r4396, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r47, %r51, %r4396;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r48, %r47, %r4396;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r49, %r48, %r4396;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r50, %r49, %r4396;
	// inline asm
	mov.u32 	%r11626, 0;
	// inline asm
	shf.r.wrap.b32 %r50, %r11626, %r50, %r4396;
	// inline asm

BB3_418:
	mov.u32 	%r11627, %r11626;
	mov.u32 	%r11628, %r11626;
	bra.uni 	BB3_428;

BB3_592:
	setp.ne.s32	%p493, %r58, 31;
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r11747;
	mov.u32 	%r11749, %r11747;
	mov.u32 	%r11750, %r11747;
	mov.u32 	%r11751, %r11747;
	@%p493 bra 	BB3_612;

	mov.u32 	%r11744, 0;
	mov.u32 	%r5399, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r54, %r11744, %r5399;
	// inline asm

BB3_594:
	mov.u32 	%r11745, %r11744;
	mov.u32 	%r11746, %r11744;
	mov.u32 	%r11747, %r11744;
	mov.u32 	%r11749, %r11744;

BB3_586:
	mov.u32 	%r11750, %r11744;
	mov.u32 	%r11751, %r11744;

BB3_612:
	and.b32  	%r5941, %r55, 3;
	shl.b32 	%r5942, %r5941, 3;
	mov.u32 	%r5943, 1;
	shl.b32 	%r5944, %r5943, %r5942;
	add.s32 	%r850, %r5944, -1;
	neg.s32 	%r851, %r5944;
	shr.u32 	%r5940, %r56, 2;
	setp.gt.s32	%p535, %r5940, 3;
	@%p535 bra 	BB3_620;

	setp.gt.s32	%p541, %r5940, 1;
	@%p541 bra 	BB3_617;

	setp.eq.s32	%p544, %r5940, 0;
	@%p544 bra 	BB3_635;
	bra.uni 	BB3_615;

BB3_635:
	and.b32  	%r5959, %r850, %r50;
	and.b32  	%r5960, %r11748, %r851;
	or.b32  	%r11740, %r5960, %r5959;
	mov.u32 	%r11736, %r11747;
	mov.u32 	%r11737, %r11746;
	mov.u32 	%r11738, %r11745;
	mov.u32 	%r11739, %r11744;
	mov.u32 	%r11741, %r11749;
	bra.uni 	BB3_636;

BB3_620:
	setp.gt.s32	%p536, %r5940, 5;
	@%p536 bra 	BB3_624;

	setp.eq.s32	%p539, %r5940, 4;
	@%p539 bra 	BB3_633;
	bra.uni 	BB3_622;

BB3_633:
	and.b32  	%r5951, %r850, %r51;
	and.b32  	%r5952, %r11744, %r851;
	or.b32  	%r11739, %r5952, %r5951;
	mov.u32 	%r11736, %r11747;
	mov.u32 	%r11737, %r11746;
	mov.u32 	%r11738, %r11745;
	bra.uni 	BB3_630;

BB3_617:
	setp.eq.s32	%p542, %r5940, 2;
	@%p542 bra 	BB3_634;
	bra.uni 	BB3_618;

BB3_634:
	and.b32  	%r5955, %r850, %r48;
	and.b32  	%r5956, %r11750, %r851;
	or.b32  	%r11742, %r5956, %r5955;
	mov.u32 	%r11736, %r11747;
	mov.u32 	%r11737, %r11746;
	mov.u32 	%r11738, %r11745;
	mov.u32 	%r11739, %r11744;
	mov.u32 	%r11740, %r50;
	mov.u32 	%r11741, %r49;
	bra.uni 	BB3_637;

BB3_624:
	setp.eq.s32	%p537, %r5940, 6;
	@%p537 bra 	BB3_632;
	bra.uni 	BB3_625;

BB3_632:
	and.b32  	%r5947, %r850, %r53;
	and.b32  	%r5948, %r11746, %r851;
	or.b32  	%r11737, %r5948, %r5947;
	mov.u32 	%r11736, %r11747;
	bra.uni 	BB3_628;

BB3_615:
	setp.eq.s32	%p545, %r5940, 1;
	@%p545 bra 	BB3_616;
	bra.uni 	BB3_626;

BB3_616:
	and.b32  	%r5957, %r850, %r49;
	and.b32  	%r5958, %r11749, %r851;
	or.b32  	%r11741, %r5958, %r5957;
	mov.u32 	%r11736, %r11747;
	mov.u32 	%r11737, %r11746;
	mov.u32 	%r11738, %r11745;
	mov.u32 	%r11739, %r11744;
	mov.u32 	%r11740, %r50;

BB3_636:
	mov.u32 	%r11742, %r11750;

BB3_637:
	mov.u32 	%r11743, %r11751;
	bra.uni 	BB3_638;

BB3_622:
	setp.eq.s32	%p540, %r5940, 5;
	@%p540 bra 	BB3_623;
	bra.uni 	BB3_626;

BB3_623:
	and.b32  	%r5949, %r850, %r52;
	and.b32  	%r5950, %r11745, %r851;
	or.b32  	%r11738, %r5950, %r5949;
	mov.u32 	%r11736, %r11747;
	mov.u32 	%r11737, %r11746;
	bra.uni 	BB3_629;

BB3_618:
	setp.eq.s32	%p543, %r5940, 3;
	@%p543 bra 	BB3_619;
	bra.uni 	BB3_626;

BB3_619:
	and.b32  	%r5953, %r850, %r47;
	and.b32  	%r5954, %r11751, %r851;
	or.b32  	%r11743, %r5954, %r5953;
	mov.u32 	%r11736, %r11747;
	mov.u32 	%r11737, %r11746;
	mov.u32 	%r11738, %r11745;
	mov.u32 	%r11739, %r11744;
	mov.u32 	%r11740, %r50;
	mov.u32 	%r11741, %r49;
	mov.u32 	%r11742, %r48;
	bra.uni 	BB3_638;

BB3_625:
	setp.ne.s32	%p538, %r5940, 7;
	@%p538 bra 	BB3_626;

	and.b32  	%r5945, %r850, %r54;
	and.b32  	%r5946, %r11747, %r851;
	or.b32  	%r11736, %r5946, %r5945;
	bra.uni 	BB3_627;

BB3_626:
	mov.u32 	%r11736, %r54;

BB3_627:
	mov.u32 	%r11737, %r53;

BB3_628:
	mov.u32 	%r11738, %r52;

BB3_629:
	mov.u32 	%r11739, %r51;

BB3_630:
	mov.u32 	%r11740, %r50;
	mov.u32 	%r11741, %r49;
	mov.u32 	%r11742, %r48;
	mov.u32 	%r11743, %r47;

BB3_638:
	sub.s32 	%r11752, %r38, %r58;
	bra.uni 	BB3_1027;

BB3_404:
	setp.ne.s32	%p288, %r56, 30;
	@%p288 bra 	BB3_352;

	mov.u32 	%r53, 0;
	mov.u32 	%r4183, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r53, %r50, %r4183;
	// inline asm

BB3_427:
	mov.u32 	%r52, %r53;
	mov.u32 	%r51, %r53;
	mov.u32 	%r11626, %r53;
	mov.u32 	%r11627, %r53;
	mov.u32 	%r11628, %r53;
	mov.u32 	%r50, %r53;
	bra.uni 	BB3_428;

BB3_352:
	mov.u32 	%r11626, %r50;
	mov.u32 	%r11627, %r49;
	mov.u32 	%r11628, %r48;
	mov.u32 	%r50, %r47;

BB3_428:
	and.b32  	%r4725, %r55, 3;
	shl.b32 	%r4726, %r4725, 3;
	mov.u32 	%r4727, 1;
	shl.b32 	%r4728, %r4727, %r4726;
	add.s32 	%r623, %r4728, -1;
	shr.u32 	%r4724, %r56, 2;
	setp.gt.s32	%p328, %r4724, 3;
	@%p328 bra 	BB3_436;

	setp.gt.s32	%p334, %r4724, 1;
	@%p334 bra 	BB3_433;

	setp.eq.s32	%p337, %r4724, 0;
	@%p337 bra 	BB3_448;
	bra.uni 	BB3_431;

BB3_448:
	and.b32  	%r11748, %r11748, %r623;
	mov.u32 	%r11744, 0;
	mov.u32 	%r11745, %r11744;
	mov.u32 	%r11746, %r11744;
	mov.u32 	%r11747, %r11744;
	mov.u32 	%r11749, %r11744;
	bra.uni 	BB3_449;

BB3_436:
	setp.gt.s32	%p329, %r4724, 5;
	@%p329 bra 	BB3_440;

	setp.eq.s32	%p332, %r4724, 4;
	@%p332 bra 	BB3_446;
	bra.uni 	BB3_438;

BB3_446:
	and.b32  	%r11744, %r11748, %r623;
	mov.u32 	%r11745, 0;
	mov.u32 	%r11746, %r11745;
	mov.u32 	%r11747, %r11745;
	bra.uni 	BB3_444;

BB3_433:
	setp.eq.s32	%p335, %r4724, 2;
	@%p335 bra 	BB3_447;
	bra.uni 	BB3_434;

BB3_447:
	and.b32  	%r11750, %r11748, %r623;
	mov.u32 	%r11744, 0;
	mov.u32 	%r11745, %r11744;
	mov.u32 	%r11746, %r11744;
	mov.u32 	%r11747, %r11744;
	mov.u32 	%r11749, %r11748;
	bra.uni 	BB3_450;

BB3_440:
	setp.eq.s32	%p330, %r4724, 6;
	@%p330 bra 	BB3_445;
	bra.uni 	BB3_441;

BB3_445:
	and.b32  	%r11746, %r11748, %r623;
	mov.u32 	%r11747, 0;
	mov.u32 	%r11744, %r11748;
	mov.u32 	%r11745, %r11748;
	bra.uni 	BB3_444;

BB3_431:
	setp.eq.s32	%p338, %r4724, 1;
	@%p338 bra 	BB3_432;
	bra.uni 	BB3_442;

BB3_432:
	and.b32  	%r11749, %r11748, %r623;
	mov.u32 	%r11744, 0;
	mov.u32 	%r11745, %r11744;
	mov.u32 	%r11746, %r11744;
	mov.u32 	%r11747, %r11744;

BB3_449:
	mov.u32 	%r11750, %r11744;

BB3_450:
	mov.u32 	%r11751, %r11744;
	bra.uni 	BB3_451;

BB3_438:
	setp.eq.s32	%p333, %r4724, 5;
	@%p333 bra 	BB3_439;
	bra.uni 	BB3_442;

BB3_439:
	and.b32  	%r11745, %r11748, %r623;
	mov.u32 	%r11746, 0;
	mov.u32 	%r11744, %r11748;
	mov.u32 	%r11747, %r11746;
	bra.uni 	BB3_444;

BB3_434:
	setp.eq.s32	%p336, %r4724, 3;
	@%p336 bra 	BB3_435;
	bra.uni 	BB3_442;

BB3_435:
	and.b32  	%r11751, %r11748, %r623;
	mov.u32 	%r11744, 0;
	mov.u32 	%r11745, %r11744;
	mov.u32 	%r11746, %r11744;
	mov.u32 	%r11747, %r11744;
	mov.u32 	%r11749, %r11748;
	mov.u32 	%r11750, %r11748;
	bra.uni 	BB3_451;

BB3_441:
	setp.ne.s32	%p331, %r4724, 7;
	@%p331 bra 	BB3_442;

	and.b32  	%r11747, %r11748, %r623;
	mov.u32 	%r11744, %r11748;
	mov.u32 	%r11745, %r11748;
	mov.u32 	%r11746, %r11748;
	bra.uni 	BB3_444;

BB3_442:
	mov.u32 	%r11744, %r11748;
	mov.u32 	%r11745, %r11748;
	mov.u32 	%r11746, %r11748;
	mov.u32 	%r11747, %r11748;

BB3_444:
	mov.u32 	%r11749, %r11748;
	mov.u32 	%r11750, %r11748;
	mov.u32 	%r11751, %r11748;

BB3_451:
	or.b32  	%r11740, %r11748, %r11626;
	or.b32  	%r11741, %r11749, %r11627;
	or.b32  	%r11742, %r11750, %r11628;
	or.b32  	%r11743, %r11751, %r50;
	bra.uni 	BB3_223;

BB3_124:
	setp.eq.s32	%p176, %r56, 1;
	@%p176 bra 	BB3_125;
	bra.uni 	BB3_202;

BB3_125:
	mov.u32 	%r3293, 24;
	// inline asm
	shf.r.wrap.b32 %r11747, %r53, %r54, %r3293;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r52, %r53, %r3293;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r51, %r52, %r3293;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r47, %r51, %r3293;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r48, %r47, %r3293;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r49, %r48, %r3293;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r50, %r49, %r3293;
	// inline asm
	mov.u32 	%r3291, 0;
	// inline asm
	shf.r.wrap.b32 %r11748, %r3291, %r50, %r3293;
	// inline asm
	bra.uni 	BB3_202;

BB3_156:
	setp.eq.s32	%p153, %r56, 17;
	@%p153 bra 	BB3_157;
	bra.uni 	BB3_202;

BB3_157:
	mov.u32 	%r2949, 24;
	// inline asm
	shf.r.wrap.b32 %r11747, %r48, %r47, %r2949;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r49, %r48, %r2949;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r50, %r49, %r2949;
	// inline asm
	mov.u32 	%r11748, 0;
	// inline asm
	shf.r.wrap.b32 %r11744, %r11748, %r50, %r2949;
	// inline asm
	bra.uni 	BB3_194;

BB3_139:
	setp.eq.s32	%p165, %r56, 9;
	@%p165 bra 	BB3_140;
	bra.uni 	BB3_202;

BB3_140:
	mov.u32 	%r3105, 24;
	// inline asm
	shf.r.wrap.b32 %r11747, %r51, %r52, %r3105;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r47, %r51, %r3105;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r48, %r47, %r3105;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r49, %r48, %r3105;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r50, %r49, %r3105;
	// inline asm
	mov.u32 	%r11748, 0;
	// inline asm
	shf.r.wrap.b32 %r11750, %r11748, %r50, %r3105;
	// inline asm
	mov.u32 	%r11749, %r11748;
	bra.uni 	BB3_202;

BB3_171:
	setp.eq.s32	%p142, %r56, 25;
	@%p142 bra 	BB3_172;
	bra.uni 	BB3_202;

BB3_172:
	mov.u32 	%r2825, 24;
	// inline asm
	shf.r.wrap.b32 %r11747, %r50, %r49, %r2825;
	// inline asm
	mov.u32 	%r11744, 0;
	// inline asm
	shf.r.wrap.b32 %r11746, %r11744, %r50, %r2825;
	// inline asm
	bra.uni 	BB3_176;

BB3_131:
	setp.eq.s32	%p171, %r56, 5;
	@%p171 bra 	BB3_132;
	bra.uni 	BB3_202;

BB3_132:
	mov.u32 	%r3195, 24;
	// inline asm
	shf.r.wrap.b32 %r11747, %r52, %r53, %r3195;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r51, %r52, %r3195;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r47, %r51, %r3195;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r48, %r47, %r3195;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r49, %r48, %r3195;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r50, %r49, %r3195;
	// inline asm
	mov.u32 	%r11748, 0;
	// inline asm
	shf.r.wrap.b32 %r11749, %r11748, %r50, %r3195;
	// inline asm
	bra.uni 	BB3_202;

BB3_163:
	setp.eq.s32	%p148, %r56, 21;
	@%p148 bra 	BB3_164;
	bra.uni 	BB3_202;

BB3_164:
	mov.u32 	%r2883, 24;
	// inline asm
	shf.r.wrap.b32 %r11747, %r49, %r48, %r2883;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r50, %r49, %r2883;
	// inline asm
	mov.u32 	%r11744, 0;
	// inline asm
	shf.r.wrap.b32 %r11745, %r11744, %r50, %r2883;
	// inline asm
	bra.uni 	BB3_187;

BB3_146:
	setp.eq.s32	%p160, %r56, 13;
	@%p160 bra 	BB3_147;
	bra.uni 	BB3_202;

BB3_147:
	mov.u32 	%r3023, 24;
	// inline asm
	shf.r.wrap.b32 %r11747, %r47, %r51, %r3023;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r48, %r47, %r3023;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r49, %r48, %r3023;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r50, %r49, %r3023;
	// inline asm
	mov.u32 	%r11748, 0;
	// inline asm
	shf.r.wrap.b32 %r11751, %r11748, %r50, %r3023;
	// inline asm
	bra.uni 	BB3_151;

BB3_179:
	setp.eq.s32	%p137, %r56, 29;
	@%p137 bra 	BB3_180;
	bra.uni 	BB3_202;

BB3_180:
	mov.u32 	%r11744, 0;
	mov.u32 	%r2775, 24;
	// inline asm
	shf.r.wrap.b32 %r11747, %r11744, %r50, %r2775;
	// inline asm
	bra.uni 	BB3_184;

BB3_274:
	setp.eq.s32	%p225, %r56, 17;
	@%p225 bra 	BB3_275;
	bra.uni 	BB3_247;

BB3_275:
	mov.u32 	%r3547, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r48, %r47, %r3547;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r49, %r48, %r3547;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r50, %r49, %r3547;
	// inline asm
	mov.u32 	%r11607, 0;
	// inline asm
	shf.r.wrap.b32 %r51, %r11607, %r50, %r3547;
	// inline asm
	bra.uni 	BB3_310;

BB3_258:
	setp.eq.s32	%p237, %r56, 9;
	@%p237 bra 	BB3_259;
	bra.uni 	BB3_247;

BB3_259:
	mov.u32 	%r3703, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r51, %r52, %r3703;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r47, %r51, %r3703;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r48, %r47, %r3703;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r49, %r48, %r3703;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3696, %r50, %r49, %r3703;
	// inline asm
	mov.u32 	%r11607, 0;
	// inline asm
	shf.r.wrap.b32 %r11609, %r11607, %r50, %r3703;
	// inline asm
	mov.u32 	%r11608, %r11607;
	mov.u32 	%r50, %r3696;
	bra.uni 	BB3_323;

BB3_289:
	setp.eq.s32	%p214, %r56, 25;
	@%p214 bra 	BB3_290;
	bra.uni 	BB3_247;

BB3_290:
	mov.u32 	%r3423, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r50, %r49, %r3423;
	// inline asm
	mov.u32 	%r52, 0;
	// inline asm
	shf.r.wrap.b32 %r53, %r52, %r50, %r3423;
	// inline asm
	bra.uni 	BB3_304;

BB3_250:
	setp.eq.s32	%p243, %r56, 5;
	@%p243 bra 	BB3_251;
	bra.uni 	BB3_247;

BB3_251:
	mov.u32 	%r3793, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r52, %r53, %r3793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r51, %r52, %r3793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r47, %r51, %r3793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r48, %r47, %r3793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3782, %r49, %r48, %r3793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11609, %r50, %r49, %r3793;
	// inline asm
	mov.u32 	%r11607, 0;
	// inline asm
	shf.r.wrap.b32 %r11608, %r11607, %r50, %r3793;
	// inline asm
	mov.u32 	%r50, %r3782;
	bra.uni 	BB3_323;

BB3_281:
	setp.eq.s32	%p220, %r56, 21;
	@%p220 bra 	BB3_282;
	bra.uni 	BB3_247;

BB3_282:
	mov.u32 	%r3481, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r49, %r48, %r3481;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r50, %r49, %r3481;
	// inline asm
	mov.u32 	%r51, 0;
	// inline asm
	shf.r.wrap.b32 %r52, %r51, %r50, %r3481;
	// inline asm
	bra.uni 	BB3_307;

BB3_265:
	setp.eq.s32	%p232, %r56, 13;
	@%p232 bra 	BB3_266;
	bra.uni 	BB3_247;

BB3_266:
	mov.u32 	%r3621, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r47, %r51, %r3621;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r48, %r47, %r3621;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r49, %r48, %r3621;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r50, %r49, %r3621;
	// inline asm
	mov.u32 	%r11607, 0;
	// inline asm
	shf.r.wrap.b32 %r50, %r11607, %r50, %r3621;
	// inline asm
	bra.uni 	BB3_313;

BB3_296:
	setp.eq.s32	%p209, %r56, 29;
	@%p209 bra 	BB3_297;
	bra.uni 	BB3_247;

BB3_297:
	mov.u32 	%r53, 0;
	mov.u32 	%r3373, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r53, %r50, %r3373;
	// inline asm
	bra.uni 	BB3_322;

BB3_127:
	setp.eq.s32	%p174, %r56, 3;
	@%p174 bra 	BB3_128;
	bra.uni 	BB3_202;

BB3_128:
	mov.u32 	%r3229, 8;
	// inline asm
	shf.r.wrap.b32 %r11747, %r53, %r54, %r3229;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r52, %r53, %r3229;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r51, %r52, %r3229;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r47, %r51, %r3229;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r48, %r47, %r3229;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r49, %r48, %r3229;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r50, %r49, %r3229;
	// inline asm
	mov.u32 	%r3227, 0;
	// inline asm
	shf.r.wrap.b32 %r11748, %r3227, %r50, %r3229;
	// inline asm
	bra.uni 	BB3_202;

BB3_159:
	setp.eq.s32	%p151, %r56, 19;
	@%p151 bra 	BB3_160;
	bra.uni 	BB3_202;

BB3_160:
	mov.u32 	%r2909, 8;
	// inline asm
	shf.r.wrap.b32 %r11747, %r48, %r47, %r2909;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r49, %r48, %r2909;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r50, %r49, %r2909;
	// inline asm
	mov.u32 	%r11748, 0;
	// inline asm
	shf.r.wrap.b32 %r11744, %r11748, %r50, %r2909;
	// inline asm

BB3_194:
	mov.u32 	%r11749, %r11748;
	mov.u32 	%r11750, %r11748;
	mov.u32 	%r11751, %r11748;
	bra.uni 	BB3_202;

BB3_142:
	setp.eq.s32	%p163, %r56, 11;
	@%p163 bra 	BB3_143;
	bra.uni 	BB3_202;

BB3_143:
	mov.u32 	%r3053, 8;
	// inline asm
	shf.r.wrap.b32 %r11747, %r51, %r52, %r3053;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r47, %r51, %r3053;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r48, %r47, %r3053;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r49, %r48, %r3053;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r50, %r49, %r3053;
	// inline asm
	mov.u32 	%r11748, 0;
	// inline asm
	shf.r.wrap.b32 %r11750, %r11748, %r50, %r3053;
	// inline asm
	mov.u32 	%r11749, %r11748;
	bra.uni 	BB3_202;

BB3_174:
	setp.eq.s32	%p140, %r56, 27;
	@%p140 bra 	BB3_175;
	bra.uni 	BB3_202;

BB3_175:
	mov.u32 	%r2797, 8;
	// inline asm
	shf.r.wrap.b32 %r11747, %r50, %r49, %r2797;
	// inline asm
	mov.u32 	%r11744, 0;
	// inline asm
	shf.r.wrap.b32 %r11746, %r11744, %r50, %r2797;
	// inline asm

BB3_176:
	mov.u32 	%r11745, %r11744;
	bra.uni 	BB3_187;

BB3_134:
	setp.eq.s32	%p169, %r56, 7;
	@%p169 bra 	BB3_135;
	bra.uni 	BB3_202;

BB3_135:
	mov.u32 	%r3137, 8;
	// inline asm
	shf.r.wrap.b32 %r11747, %r52, %r53, %r3137;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r51, %r52, %r3137;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r47, %r51, %r3137;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r48, %r47, %r3137;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r49, %r48, %r3137;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r50, %r49, %r3137;
	// inline asm
	mov.u32 	%r11748, 0;
	// inline asm
	shf.r.wrap.b32 %r11749, %r11748, %r50, %r3137;
	// inline asm
	bra.uni 	BB3_202;

BB3_166:
	setp.eq.s32	%p146, %r56, 23;
	@%p146 bra 	BB3_167;
	bra.uni 	BB3_202;

BB3_167:
	mov.u32 	%r2849, 8;
	// inline asm
	shf.r.wrap.b32 %r11747, %r49, %r48, %r2849;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r50, %r49, %r2849;
	// inline asm
	mov.u32 	%r11744, 0;
	// inline asm
	shf.r.wrap.b32 %r11745, %r11744, %r50, %r2849;
	// inline asm
	bra.uni 	BB3_187;

BB3_149:
	setp.eq.s32	%p158, %r56, 15;
	@%p158 bra 	BB3_150;
	bra.uni 	BB3_202;

BB3_150:
	mov.u32 	%r2977, 8;
	// inline asm
	shf.r.wrap.b32 %r11747, %r47, %r51, %r2977;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r48, %r47, %r2977;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r49, %r48, %r2977;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r50, %r49, %r2977;
	// inline asm
	mov.u32 	%r11748, 0;
	// inline asm
	shf.r.wrap.b32 %r11751, %r11748, %r50, %r2977;
	// inline asm

BB3_151:
	mov.u32 	%r11749, %r11748;
	mov.u32 	%r11750, %r11748;
	bra.uni 	BB3_202;

BB3_182:
	setp.ne.s32	%p135, %r56, 31;
	@%p135 bra 	BB3_202;

	mov.u32 	%r11744, 0;
	mov.u32 	%r2753, 8;
	// inline asm
	shf.r.wrap.b32 %r11747, %r11744, %r50, %r2753;
	// inline asm

BB3_184:
	mov.u32 	%r11745, %r11744;
	mov.u32 	%r11746, %r11744;

BB3_187:
	mov.u32 	%r11748, %r11744;
	mov.u32 	%r11749, %r11744;
	mov.u32 	%r11750, %r11744;
	mov.u32 	%r11751, %r11744;

BB3_202:
	and.b32  	%r3295, %r38, 3;
	shl.b32 	%r3296, %r3295, 3;
	mov.u32 	%r3297, -1;
	shl.b32 	%r222, %r3297, %r3296;
	shr.u32 	%r3294, %r38, 2;
	setp.gt.s32	%p177, %r3294, 3;
	@%p177 bra 	BB3_210;

	setp.gt.s32	%p183, %r3294, 1;
	@%p183 bra 	BB3_207;

	setp.eq.s32	%p186, %r3294, 0;
	@%p186 bra 	BB3_221;
	bra.uni 	BB3_205;

BB3_221:
	and.b32  	%r11748, %r11748, %r222;
	bra.uni 	BB3_222;

BB3_210:
	setp.gt.s32	%p178, %r3294, 5;
	@%p178 bra 	BB3_214;

	setp.eq.s32	%p181, %r3294, 4;
	@%p181 bra 	BB3_219;
	bra.uni 	BB3_212;

BB3_219:
	and.b32  	%r11744, %r11744, %r222;
	mov.u32 	%r11748, 0;
	mov.u32 	%r11749, %r11748;
	mov.u32 	%r11750, %r11748;
	mov.u32 	%r11751, %r11748;
	bra.uni 	BB3_222;

BB3_207:
	setp.eq.s32	%p184, %r3294, 2;
	@%p184 bra 	BB3_220;
	bra.uni 	BB3_208;

BB3_220:
	and.b32  	%r11750, %r11750, %r222;
	mov.u32 	%r11748, 0;
	mov.u32 	%r11749, %r11748;
	bra.uni 	BB3_222;

BB3_214:
	setp.eq.s32	%p179, %r3294, 6;
	@%p179 bra 	BB3_217;
	bra.uni 	BB3_215;

BB3_217:
	and.b32  	%r11746, %r11746, %r222;
	mov.u32 	%r11744, 0;
	mov.u32 	%r11745, %r11744;
	bra.uni 	BB3_218;

BB3_205:
	setp.eq.s32	%p187, %r3294, 1;
	@%p187 bra 	BB3_206;
	bra.uni 	BB3_222;

BB3_206:
	and.b32  	%r11749, %r11749, %r222;
	mov.u32 	%r11748, 0;
	bra.uni 	BB3_222;

BB3_212:
	setp.eq.s32	%p182, %r3294, 5;
	@%p182 bra 	BB3_213;
	bra.uni 	BB3_222;

BB3_213:
	and.b32  	%r11745, %r11745, %r222;
	mov.u32 	%r11744, 0;
	bra.uni 	BB3_218;

BB3_208:
	setp.eq.s32	%p185, %r3294, 3;
	@%p185 bra 	BB3_209;
	bra.uni 	BB3_222;

BB3_209:
	and.b32  	%r11751, %r11751, %r222;
	mov.u32 	%r11748, 0;
	mov.u32 	%r11749, %r11748;
	mov.u32 	%r11750, %r11748;
	bra.uni 	BB3_222;

BB3_215:
	setp.ne.s32	%p180, %r3294, 7;
	@%p180 bra 	BB3_222;

	and.b32  	%r11747, %r11747, %r222;
	mov.u32 	%r11744, 0;
	mov.u32 	%r11745, %r11744;
	mov.u32 	%r11746, %r11744;

BB3_218:
	mov.u32 	%r11748, %r11744;
	mov.u32 	%r11749, %r11744;
	mov.u32 	%r11750, %r11744;
	mov.u32 	%r11751, %r11744;

BB3_222:
	or.b32  	%r11740, %r11748, %r50;
	or.b32  	%r11741, %r11749, %r49;
	or.b32  	%r11742, %r11750, %r48;
	or.b32  	%r11743, %r11751, %r47;

BB3_223:
	or.b32  	%r11739, %r11744, %r51;
	or.b32  	%r11738, %r11745, %r52;
	or.b32  	%r11737, %r11746, %r53;
	or.b32  	%r11736, %r11747, %r54;
	bra.uni 	BB3_1027;

BB3_277:
	setp.eq.s32	%p223, %r56, 19;
	@%p223 bra 	BB3_278;
	bra.uni 	BB3_247;

BB3_278:
	mov.u32 	%r3507, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r48, %r47, %r3507;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r49, %r48, %r3507;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r50, %r49, %r3507;
	// inline asm
	mov.u32 	%r11607, 0;
	// inline asm
	shf.r.wrap.b32 %r51, %r11607, %r50, %r3507;
	// inline asm

BB3_310:
	mov.u32 	%r11608, %r11607;
	mov.u32 	%r11609, %r11607;
	mov.u32 	%r50, %r11607;
	bra.uni 	BB3_323;

BB3_261:
	setp.eq.s32	%p235, %r56, 11;
	@%p235 bra 	BB3_262;
	bra.uni 	BB3_247;

BB3_262:
	mov.u32 	%r3651, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r51, %r52, %r3651;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r47, %r51, %r3651;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r48, %r47, %r3651;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r49, %r48, %r3651;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3644, %r50, %r49, %r3651;
	// inline asm
	mov.u32 	%r11607, 0;
	// inline asm
	shf.r.wrap.b32 %r11609, %r11607, %r50, %r3651;
	// inline asm
	mov.u32 	%r11608, %r11607;
	mov.u32 	%r50, %r3644;
	bra.uni 	BB3_323;

BB3_292:
	setp.eq.s32	%p212, %r56, 27;
	@%p212 bra 	BB3_293;
	bra.uni 	BB3_247;

BB3_293:
	mov.u32 	%r3395, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r50, %r49, %r3395;
	// inline asm
	mov.u32 	%r52, 0;
	// inline asm
	shf.r.wrap.b32 %r53, %r52, %r50, %r3395;
	// inline asm

BB3_304:
	mov.u32 	%r51, %r52;
	mov.u32 	%r11607, %r52;
	mov.u32 	%r11608, %r52;
	mov.u32 	%r11609, %r52;
	mov.u32 	%r50, %r52;
	bra.uni 	BB3_323;

BB3_253:
	setp.eq.s32	%p241, %r56, 7;
	@%p241 bra 	BB3_254;
	bra.uni 	BB3_247;

BB3_254:
	mov.u32 	%r3735, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r52, %r53, %r3735;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r51, %r52, %r3735;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r47, %r51, %r3735;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r48, %r47, %r3735;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3724, %r49, %r48, %r3735;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11609, %r50, %r49, %r3735;
	// inline asm
	mov.u32 	%r11607, 0;
	// inline asm
	shf.r.wrap.b32 %r11608, %r11607, %r50, %r3735;
	// inline asm
	mov.u32 	%r50, %r3724;
	bra.uni 	BB3_323;

BB3_284:
	setp.eq.s32	%p218, %r56, 23;
	@%p218 bra 	BB3_285;
	bra.uni 	BB3_247;

BB3_285:
	mov.u32 	%r3447, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r49, %r48, %r3447;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r50, %r49, %r3447;
	// inline asm
	mov.u32 	%r51, 0;
	// inline asm
	shf.r.wrap.b32 %r52, %r51, %r50, %r3447;
	// inline asm

BB3_307:
	mov.u32 	%r11607, %r51;
	mov.u32 	%r11608, %r51;
	mov.u32 	%r11609, %r51;
	mov.u32 	%r50, %r51;
	bra.uni 	BB3_323;

BB3_268:
	setp.eq.s32	%p230, %r56, 15;
	@%p230 bra 	BB3_269;
	bra.uni 	BB3_247;

BB3_269:
	mov.u32 	%r3575, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r47, %r51, %r3575;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r48, %r47, %r3575;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r49, %r48, %r3575;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r50, %r49, %r3575;
	// inline asm
	mov.u32 	%r11607, 0;
	// inline asm
	shf.r.wrap.b32 %r50, %r11607, %r50, %r3575;
	// inline asm

BB3_313:
	mov.u32 	%r11608, %r11607;
	mov.u32 	%r11609, %r11607;
	bra.uni 	BB3_323;

BB3_299:
	setp.ne.s32	%p207, %r56, 30;
	@%p207 bra 	BB3_247;

	mov.u32 	%r53, 0;
	mov.u32 	%r3362, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r53, %r50, %r3362;
	// inline asm

BB3_322:
	mov.u32 	%r52, %r53;
	mov.u32 	%r51, %r53;
	mov.u32 	%r11607, %r53;
	mov.u32 	%r11608, %r53;
	mov.u32 	%r11609, %r53;
	mov.u32 	%r50, %r53;
	bra.uni 	BB3_323;

BB3_247:
	mov.u32 	%r11607, %r50;
	mov.u32 	%r11608, %r49;
	mov.u32 	%r11609, %r48;
	mov.u32 	%r50, %r47;

BB3_323:
	or.b32  	%r11740, %r11607, %r11748;
	or.b32  	%r11741, %r11608, %r11749;
	or.b32  	%r11742, %r11609, %r11750;
	or.b32  	%r11743, %r50, %r11751;
	or.b32  	%r11739, %r51, %r11744;
	or.b32  	%r11738, %r52, %r11745;
	or.b32  	%r11737, %r53, %r11746;
	or.b32  	%r11736, %r54, %r11747;
	bra.uni 	BB3_1027;

BB3_802:
	setp.eq.s32	%p707, %r6846, 1;
	mov.u32 	%r11751, %r11694;
	mov.u32 	%r11750, %r11694;
	mov.u32 	%r11749, %r11694;
	mov.u32 	%r11748, %r11694;
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r11694;
	mov.u32 	%r11745, %r11694;
	mov.u32 	%r11744, %r11694;
	@%p707 bra 	BB3_803;
	bra.uni 	BB3_881;

BB3_803:
	mov.u32 	%r7391, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r53, %r54, %r7391;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r52, %r53, %r7391;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r51, %r52, %r7391;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r47, %r51, %r7391;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r48, %r47, %r7391;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r49, %r48, %r7391;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r50, %r49, %r7391;
	// inline asm
	mov.u32 	%r7389, 0;
	// inline asm
	shf.r.wrap.b32 %r11747, %r7389, %r50, %r7391;
	// inline asm
	bra.uni 	BB3_881;

BB3_834:
	setp.eq.s32	%p684, %r6846, 17;
	mov.u32 	%r11751, %r11694;
	mov.u32 	%r11750, %r11694;
	mov.u32 	%r11749, %r11694;
	mov.u32 	%r11748, %r11694;
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r11694;
	mov.u32 	%r11745, %r11694;
	mov.u32 	%r11744, %r11694;
	@%p684 bra 	BB3_835;
	bra.uni 	BB3_881;

BB3_835:
	mov.u32 	%r7047, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r48, %r47, %r7047;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r49, %r48, %r7047;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r50, %r49, %r7047;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11751, %r11747, %r50, %r7047;
	// inline asm
	bra.uni 	BB3_839;

BB3_817:
	setp.eq.s32	%p696, %r6846, 9;
	mov.u32 	%r11751, %r11694;
	mov.u32 	%r11750, %r11694;
	mov.u32 	%r11749, %r11694;
	mov.u32 	%r11748, %r11694;
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r11694;
	mov.u32 	%r11745, %r11694;
	mov.u32 	%r11744, %r11694;
	@%p696 bra 	BB3_818;
	bra.uni 	BB3_881;

BB3_818:
	mov.u32 	%r7203, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r51, %r52, %r7203;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r47, %r51, %r7203;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r48, %r47, %r7203;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r49, %r48, %r7203;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r50, %r49, %r7203;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11745, %r11747, %r50, %r7203;
	// inline asm
	mov.u32 	%r11746, %r11747;
	bra.uni 	BB3_881;

BB3_850:
	setp.eq.s32	%p673, %r6846, 25;
	mov.u32 	%r11751, %r11694;
	mov.u32 	%r11750, %r11694;
	mov.u32 	%r11749, %r11694;
	mov.u32 	%r11748, %r11694;
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r11694;
	mov.u32 	%r11745, %r11694;
	mov.u32 	%r11744, %r11694;
	@%p673 bra 	BB3_851;
	bra.uni 	BB3_881;

BB3_851:
	mov.u32 	%r6923, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r50, %r49, %r6923;
	// inline asm
	mov.u32 	%r11751, 0;
	// inline asm
	shf.r.wrap.b32 %r11749, %r11751, %r50, %r6923;
	// inline asm
	bra.uni 	BB3_855;

BB3_809:
	setp.eq.s32	%p702, %r6846, 5;
	mov.u32 	%r11751, %r11694;
	mov.u32 	%r11750, %r11694;
	mov.u32 	%r11749, %r11694;
	mov.u32 	%r11748, %r11694;
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r11694;
	mov.u32 	%r11745, %r11694;
	mov.u32 	%r11744, %r11694;
	@%p702 bra 	BB3_810;
	bra.uni 	BB3_881;

BB3_810:
	mov.u32 	%r7293, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r52, %r53, %r7293;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r51, %r52, %r7293;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r47, %r51, %r7293;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r48, %r47, %r7293;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r49, %r48, %r7293;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r50, %r49, %r7293;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11746, %r11747, %r50, %r7293;
	// inline asm
	bra.uni 	BB3_881;

BB3_842:
	setp.eq.s32	%p679, %r6846, 21;
	mov.u32 	%r11751, %r11694;
	mov.u32 	%r11750, %r11694;
	mov.u32 	%r11749, %r11694;
	mov.u32 	%r11748, %r11694;
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r11694;
	mov.u32 	%r11745, %r11694;
	mov.u32 	%r11744, %r11694;
	@%p679 bra 	BB3_843;
	bra.uni 	BB3_881;

BB3_843:
	mov.u32 	%r6981, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r49, %r48, %r6981;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r50, %r49, %r6981;
	// inline asm
	mov.u32 	%r11751, 0;
	// inline asm
	shf.r.wrap.b32 %r11750, %r11751, %r50, %r6981;
	// inline asm
	bra.uni 	BB3_856;

BB3_824:
	setp.eq.s32	%p691, %r6846, 13;
	mov.u32 	%r11751, %r11694;
	mov.u32 	%r11750, %r11694;
	mov.u32 	%r11749, %r11694;
	mov.u32 	%r11748, %r11694;
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r11694;
	mov.u32 	%r11745, %r11694;
	mov.u32 	%r11744, %r11694;
	@%p691 bra 	BB3_825;
	bra.uni 	BB3_881;

BB3_825:
	mov.u32 	%r7121, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r47, %r51, %r7121;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r48, %r47, %r7121;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r49, %r48, %r7121;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r50, %r49, %r7121;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11744, %r11747, %r50, %r7121;
	// inline asm
	bra.uni 	BB3_829;

BB3_859:
	setp.eq.s32	%p668, %r6846, 29;
	mov.u32 	%r11751, %r11694;
	mov.u32 	%r11750, %r11694;
	mov.u32 	%r11749, %r11694;
	mov.u32 	%r11748, %r11694;
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r11694;
	mov.u32 	%r11745, %r11694;
	mov.u32 	%r11744, %r11694;
	@%p668 bra 	BB3_860;
	bra.uni 	BB3_881;

BB3_860:
	mov.u32 	%r11751, 0;
	mov.u32 	%r6873, 24;
	// inline asm
	shf.r.wrap.b32 %r11748, %r11751, %r50, %r6873;
	// inline asm
	bra.uni 	BB3_864;

BB3_805:
	setp.eq.s32	%p705, %r6846, 3;
	mov.u32 	%r11751, %r11694;
	mov.u32 	%r11750, %r11694;
	mov.u32 	%r11749, %r11694;
	mov.u32 	%r11748, %r11694;
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r11694;
	mov.u32 	%r11745, %r11694;
	mov.u32 	%r11744, %r11694;
	@%p705 bra 	BB3_806;
	bra.uni 	BB3_881;

BB3_806:
	mov.u32 	%r7327, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r53, %r54, %r7327;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r52, %r53, %r7327;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r51, %r52, %r7327;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r47, %r51, %r7327;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r48, %r47, %r7327;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r49, %r48, %r7327;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r50, %r49, %r7327;
	// inline asm
	mov.u32 	%r7325, 0;
	// inline asm
	shf.r.wrap.b32 %r11747, %r7325, %r50, %r7327;
	// inline asm
	bra.uni 	BB3_881;

BB3_837:
	setp.eq.s32	%p682, %r6846, 19;
	mov.u32 	%r11751, %r11694;
	mov.u32 	%r11750, %r11694;
	mov.u32 	%r11749, %r11694;
	mov.u32 	%r11748, %r11694;
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r11694;
	mov.u32 	%r11745, %r11694;
	mov.u32 	%r11744, %r11694;
	@%p682 bra 	BB3_838;
	bra.uni 	BB3_881;

BB3_838:
	mov.u32 	%r7007, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r48, %r47, %r7007;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r49, %r48, %r7007;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r50, %r49, %r7007;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11751, %r11747, %r50, %r7007;
	// inline asm

BB3_839:
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11744, %r11747;
	bra.uni 	BB3_881;

BB3_820:
	setp.eq.s32	%p694, %r6846, 11;
	mov.u32 	%r11751, %r11694;
	mov.u32 	%r11750, %r11694;
	mov.u32 	%r11749, %r11694;
	mov.u32 	%r11748, %r11694;
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r11694;
	mov.u32 	%r11745, %r11694;
	mov.u32 	%r11744, %r11694;
	@%p694 bra 	BB3_821;
	bra.uni 	BB3_881;

BB3_821:
	mov.u32 	%r7151, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r51, %r52, %r7151;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r47, %r51, %r7151;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r48, %r47, %r7151;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r49, %r48, %r7151;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r50, %r49, %r7151;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11745, %r11747, %r50, %r7151;
	// inline asm
	mov.u32 	%r11746, %r11747;
	bra.uni 	BB3_881;

BB3_853:
	setp.eq.s32	%p671, %r6846, 27;
	mov.u32 	%r11751, %r11694;
	mov.u32 	%r11750, %r11694;
	mov.u32 	%r11749, %r11694;
	mov.u32 	%r11748, %r11694;
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r11694;
	mov.u32 	%r11745, %r11694;
	mov.u32 	%r11744, %r11694;
	@%p671 bra 	BB3_854;
	bra.uni 	BB3_881;

BB3_854:
	mov.u32 	%r6895, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r50, %r49, %r6895;
	// inline asm
	mov.u32 	%r11751, 0;
	// inline asm
	shf.r.wrap.b32 %r11749, %r11751, %r50, %r6895;
	// inline asm

BB3_855:
	mov.u32 	%r11750, %r11751;
	bra.uni 	BB3_856;

BB3_812:
	setp.eq.s32	%p700, %r6846, 7;
	mov.u32 	%r11751, %r11694;
	mov.u32 	%r11750, %r11694;
	mov.u32 	%r11749, %r11694;
	mov.u32 	%r11748, %r11694;
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r11694;
	mov.u32 	%r11745, %r11694;
	mov.u32 	%r11744, %r11694;
	@%p700 bra 	BB3_813;
	bra.uni 	BB3_881;

BB3_813:
	mov.u32 	%r7235, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r52, %r53, %r7235;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r51, %r52, %r7235;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r47, %r51, %r7235;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r48, %r47, %r7235;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r49, %r48, %r7235;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r50, %r49, %r7235;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11746, %r11747, %r50, %r7235;
	// inline asm
	bra.uni 	BB3_881;

BB3_845:
	setp.eq.s32	%p677, %r6846, 23;
	mov.u32 	%r11751, %r11694;
	mov.u32 	%r11750, %r11694;
	mov.u32 	%r11749, %r11694;
	mov.u32 	%r11748, %r11694;
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r11694;
	mov.u32 	%r11745, %r11694;
	mov.u32 	%r11744, %r11694;
	@%p677 bra 	BB3_846;
	bra.uni 	BB3_881;

BB3_846:
	mov.u32 	%r6947, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r49, %r48, %r6947;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r50, %r49, %r6947;
	// inline asm
	mov.u32 	%r11751, 0;
	// inline asm
	shf.r.wrap.b32 %r11750, %r11751, %r50, %r6947;
	// inline asm
	bra.uni 	BB3_856;

BB3_827:
	setp.eq.s32	%p689, %r6846, 15;
	mov.u32 	%r11751, %r11694;
	mov.u32 	%r11750, %r11694;
	mov.u32 	%r11749, %r11694;
	mov.u32 	%r11748, %r11694;
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r11694;
	mov.u32 	%r11745, %r11694;
	mov.u32 	%r11744, %r11694;
	@%p689 bra 	BB3_828;
	bra.uni 	BB3_881;

BB3_828:
	mov.u32 	%r7075, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r47, %r51, %r7075;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r48, %r47, %r7075;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r49, %r48, %r7075;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r50, %r49, %r7075;
	// inline asm
	mov.u32 	%r11747, 0;
	// inline asm
	shf.r.wrap.b32 %r11744, %r11747, %r50, %r7075;
	// inline asm

BB3_829:
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11745, %r11747;
	bra.uni 	BB3_881;

BB3_862:
	setp.ne.s32	%p666, %r6846, 31;
	mov.u32 	%r11751, %r11694;
	mov.u32 	%r11750, %r11694;
	mov.u32 	%r11749, %r11694;
	mov.u32 	%r11748, %r11694;
	mov.u32 	%r11747, %r11694;
	mov.u32 	%r11746, %r11694;
	mov.u32 	%r11745, %r11694;
	mov.u32 	%r11744, %r11694;
	@%p666 bra 	BB3_881;

	mov.u32 	%r11751, 0;
	mov.u32 	%r6851, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r11751, %r50, %r6851;
	// inline asm

BB3_864:
	mov.u32 	%r11750, %r11751;
	mov.u32 	%r11749, %r11751;

BB3_856:
	mov.u32 	%r11747, %r11751;
	mov.u32 	%r11746, %r11751;
	mov.u32 	%r11745, %r11751;
	mov.u32 	%r11744, %r11751;

BB3_881:
	// inline asm
	prmt.b32 %r7392, %r11748, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r7394, %r11749, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r7396, %r11750, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r7398, %r11751, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r7400, %r11744, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r7402, %r11745, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r7404, %r11746, 0, 0x0123;
	// inline asm
	shr.u32 	%r7414, %r38, 2;
	setp.gt.s32	%p708, %r7414, 3;
	@%p708 bra 	BB3_889;

	setp.gt.s32	%p714, %r7414, 1;
	@%p714 bra 	BB3_886;

	setp.eq.s32	%p717, %r7414, 0;
	@%p717 bra 	BB3_899;
	bra.uni 	BB3_884;

BB3_899:
	// inline asm
	prmt.b32 %r7597, %r11747, 0, 0x0123;
	// inline asm
	and.b32  	%r7631, %r38, 3;
	mov.u32 	%r7632, 4;
	sub.s32 	%r7633, %r7632, %r7631;
	shl.b32 	%r7634, %r7633, 2;
	mov.u32 	%r7635, 1985229328;
	shr.u32 	%r7636, %r7635, %r7634;
	and.b32  	%r7630, %r7636, 65535;
	// inline asm
	prmt.b32 %r11701, %r7404, %r7597, %r7630;
	// inline asm
	// inline asm
	prmt.b32 %r11700, %r7402, %r7404, %r7630;
	// inline asm
	// inline asm
	prmt.b32 %r11699, %r7400, %r7402, %r7630;
	// inline asm
	// inline asm
	prmt.b32 %r11698, %r7398, %r7400, %r7630;
	// inline asm
	// inline asm
	prmt.b32 %r11697, %r7396, %r7398, %r7630;
	// inline asm
	// inline asm
	prmt.b32 %r11696, %r7394, %r7396, %r7630;
	// inline asm
	// inline asm
	prmt.b32 %r11695, %r7392, %r7394, %r7630;
	// inline asm
	mov.u32 	%r7628, 0;
	// inline asm
	prmt.b32 %r11694, %r7628, %r7392, %r7630;
	// inline asm
	bra.uni 	BB3_900;

BB3_889:
	setp.gt.s32	%p709, %r7414, 5;
	@%p709 bra 	BB3_893;

	setp.eq.s32	%p712, %r7414, 4;
	@%p712 bra 	BB3_897;
	bra.uni 	BB3_891;

BB3_897:
	and.b32  	%r7495, %r38, 3;
	mov.u32 	%r7496, 4;
	sub.s32 	%r7497, %r7496, %r7495;
	shl.b32 	%r7498, %r7497, 2;
	mov.u32 	%r7499, 1985229328;
	shr.u32 	%r7500, %r7499, %r7498;
	and.b32  	%r7490, %r7500, 65535;
	// inline asm
	prmt.b32 %r11701, %r7396, %r7398, %r7490;
	// inline asm
	// inline asm
	prmt.b32 %r11700, %r7394, %r7396, %r7490;
	// inline asm
	// inline asm
	prmt.b32 %r11699, %r7392, %r7394, %r7490;
	// inline asm
	mov.u32 	%r11694, 0;
	// inline asm
	prmt.b32 %r11698, %r11694, %r7392, %r7490;
	// inline asm
	mov.u32 	%r11695, %r11694;
	mov.u32 	%r11696, %r11694;
	mov.u32 	%r11697, %r11694;
	bra.uni 	BB3_900;

BB3_886:
	setp.eq.s32	%p715, %r7414, 2;
	@%p715 bra 	BB3_898;
	bra.uni 	BB3_887;

BB3_898:
	and.b32  	%r7556, %r38, 3;
	mov.u32 	%r7557, 4;
	sub.s32 	%r7558, %r7557, %r7556;
	shl.b32 	%r7559, %r7558, 2;
	mov.u32 	%r7560, 1985229328;
	shr.u32 	%r7561, %r7560, %r7559;
	and.b32  	%r7553, %r7561, 65535;
	// inline asm
	prmt.b32 %r11701, %r7400, %r7402, %r7553;
	// inline asm
	// inline asm
	prmt.b32 %r11700, %r7398, %r7400, %r7553;
	// inline asm
	// inline asm
	prmt.b32 %r11699, %r7396, %r7398, %r7553;
	// inline asm
	// inline asm
	prmt.b32 %r11698, %r7394, %r7396, %r7553;
	// inline asm
	// inline asm
	prmt.b32 %r11697, %r7392, %r7394, %r7553;
	// inline asm
	mov.u32 	%r11694, 0;
	// inline asm
	prmt.b32 %r11696, %r11694, %r7392, %r7553;
	// inline asm
	mov.u32 	%r11695, %r11694;
	bra.uni 	BB3_900;

BB3_893:
	setp.eq.s32	%p710, %r7414, 6;
	@%p710 bra 	BB3_896;
	bra.uni 	BB3_894;

BB3_896:
	and.b32  	%r7446, %r38, 3;
	mov.u32 	%r7447, 4;
	sub.s32 	%r7448, %r7447, %r7446;
	shl.b32 	%r7449, %r7448, 2;
	mov.u32 	%r7450, 1985229328;
	shr.u32 	%r7451, %r7450, %r7449;
	and.b32  	%r7439, %r7451, 65535;
	// inline asm
	prmt.b32 %r11701, %r7392, %r7394, %r7439;
	// inline asm
	mov.u32 	%r11694, 0;
	// inline asm
	prmt.b32 %r11700, %r11694, %r7392, %r7439;
	// inline asm
	mov.u32 	%r11695, %r11694;
	mov.u32 	%r11696, %r11694;
	mov.u32 	%r11697, %r11694;
	mov.u32 	%r11698, %r11694;
	mov.u32 	%r11699, %r11694;
	bra.uni 	BB3_900;

BB3_884:
	setp.eq.s32	%p718, %r7414, 1;
	mov.u32 	%r11695, %r11694;
	mov.u32 	%r11696, %r11694;
	mov.u32 	%r11697, %r11694;
	mov.u32 	%r11698, %r11694;
	mov.u32 	%r11699, %r11694;
	mov.u32 	%r11700, %r11694;
	mov.u32 	%r11701, %r11694;
	@%p718 bra 	BB3_885;
	bra.uni 	BB3_900;

BB3_885:
	and.b32  	%r7591, %r38, 3;
	mov.u32 	%r7592, 4;
	sub.s32 	%r7593, %r7592, %r7591;
	shl.b32 	%r7594, %r7593, 2;
	mov.u32 	%r7595, 1985229328;
	shr.u32 	%r7596, %r7595, %r7594;
	and.b32  	%r7589, %r7596, 65535;
	// inline asm
	prmt.b32 %r11701, %r7402, %r7404, %r7589;
	// inline asm
	// inline asm
	prmt.b32 %r11700, %r7400, %r7402, %r7589;
	// inline asm
	// inline asm
	prmt.b32 %r11699, %r7398, %r7400, %r7589;
	// inline asm
	// inline asm
	prmt.b32 %r11698, %r7396, %r7398, %r7589;
	// inline asm
	// inline asm
	prmt.b32 %r11697, %r7394, %r7396, %r7589;
	// inline asm
	// inline asm
	prmt.b32 %r11696, %r7392, %r7394, %r7589;
	// inline asm
	mov.u32 	%r11694, 0;
	// inline asm
	prmt.b32 %r11695, %r11694, %r7392, %r7589;
	// inline asm
	bra.uni 	BB3_900;

BB3_891:
	setp.eq.s32	%p713, %r7414, 5;
	mov.u32 	%r11695, %r11694;
	mov.u32 	%r11696, %r11694;
	mov.u32 	%r11697, %r11694;
	mov.u32 	%r11698, %r11694;
	mov.u32 	%r11699, %r11694;
	mov.u32 	%r11700, %r11694;
	mov.u32 	%r11701, %r11694;
	@%p713 bra 	BB3_892;
	bra.uni 	BB3_900;

BB3_892:
	and.b32  	%r7469, %r38, 3;
	mov.u32 	%r7470, 4;
	sub.s32 	%r7471, %r7470, %r7469;
	shl.b32 	%r7472, %r7471, 2;
	mov.u32 	%r7473, 1985229328;
	shr.u32 	%r7474, %r7473, %r7472;
	and.b32  	%r7463, %r7474, 65535;
	// inline asm
	prmt.b32 %r11701, %r7394, %r7396, %r7463;
	// inline asm
	// inline asm
	prmt.b32 %r11700, %r7392, %r7394, %r7463;
	// inline asm
	mov.u32 	%r11694, 0;
	// inline asm
	prmt.b32 %r11699, %r11694, %r7392, %r7463;
	// inline asm
	mov.u32 	%r11695, %r11694;
	mov.u32 	%r11696, %r11694;
	mov.u32 	%r11697, %r11694;
	mov.u32 	%r11698, %r11694;
	bra.uni 	BB3_900;

BB3_887:
	setp.eq.s32	%p716, %r7414, 3;
	mov.u32 	%r11695, %r11694;
	mov.u32 	%r11696, %r11694;
	mov.u32 	%r11697, %r11694;
	mov.u32 	%r11698, %r11694;
	mov.u32 	%r11699, %r11694;
	mov.u32 	%r11700, %r11694;
	mov.u32 	%r11701, %r11694;
	@%p716 bra 	BB3_888;
	bra.uni 	BB3_900;

BB3_888:
	and.b32  	%r7524, %r38, 3;
	mov.u32 	%r7525, 4;
	sub.s32 	%r7526, %r7525, %r7524;
	shl.b32 	%r7527, %r7526, 2;
	mov.u32 	%r7528, 1985229328;
	shr.u32 	%r7529, %r7528, %r7527;
	and.b32  	%r7520, %r7529, 65535;
	// inline asm
	prmt.b32 %r11701, %r7398, %r7400, %r7520;
	// inline asm
	// inline asm
	prmt.b32 %r11700, %r7396, %r7398, %r7520;
	// inline asm
	// inline asm
	prmt.b32 %r11699, %r7394, %r7396, %r7520;
	// inline asm
	// inline asm
	prmt.b32 %r11698, %r7392, %r7394, %r7520;
	// inline asm
	mov.u32 	%r11694, 0;
	// inline asm
	prmt.b32 %r11697, %r11694, %r7392, %r7520;
	// inline asm
	mov.u32 	%r11695, %r11694;
	mov.u32 	%r11696, %r11694;
	bra.uni 	BB3_900;

BB3_894:
	setp.ne.s32	%p711, %r7414, 7;
	mov.u32 	%r11695, %r11694;
	mov.u32 	%r11696, %r11694;
	mov.u32 	%r11697, %r11694;
	mov.u32 	%r11698, %r11694;
	mov.u32 	%r11699, %r11694;
	mov.u32 	%r11700, %r11694;
	mov.u32 	%r11701, %r11694;
	@%p711 bra 	BB3_900;

	and.b32  	%r7426, %r38, 3;
	mov.u32 	%r7427, 4;
	sub.s32 	%r7428, %r7427, %r7426;
	shl.b32 	%r7429, %r7428, 2;
	mov.u32 	%r7430, 1985229328;
	shr.u32 	%r7431, %r7430, %r7429;
	and.b32  	%r7418, %r7431, 65535;
	mov.u32 	%r11694, 0;
	// inline asm
	prmt.b32 %r11701, %r11694, %r7392, %r7418;
	// inline asm
	mov.u32 	%r11695, %r11694;
	mov.u32 	%r11696, %r11694;
	mov.u32 	%r11697, %r11694;
	mov.u32 	%r11698, %r11694;
	mov.u32 	%r11699, %r11694;
	mov.u32 	%r11700, %r11694;

BB3_900:
	or.b32  	%r11740, %r11694, %r50;
	or.b32  	%r11741, %r11695, %r49;
	or.b32  	%r11742, %r11696, %r48;
	or.b32  	%r11743, %r11697, %r47;
	or.b32  	%r11739, %r11698, %r51;
	or.b32  	%r11738, %r11699, %r52;
	or.b32  	%r11737, %r11700, %r53;
	or.b32  	%r11736, %r11701, %r54;
	bra.uni 	BB3_1027;

BB3_4:
	mov.u32 	%r54, %r11736;
	mov.u32 	%r53, %r11737;
	mov.u32 	%r52, %r11738;
	mov.u32 	%r51, %r11739;
	mov.u32 	%r50, %r11740;
	mov.u32 	%r49, %r11741;
	mov.u32 	%r48, %r11742;
	mov.u32 	%r47, %r11743;
	mov.u32 	%r38, %r11752;
	shr.u32 	%r55, %r11560, 8;
	bfe.u32 	%r56, %r11560, 8, 8;
	shr.u32 	%r57, %r11560, 16;
	bfe.u32 	%r58, %r11560, 16, 8;
	add.u64 	%rd128, %SP, 32;
	cvta.to.local.u64 	%rd86, %rd128;
	add.u64 	%rd129, %SP, 0;
	cvta.to.local.u64 	%rd87, %rd129;
	and.b32  	%r1787, %r55, 252;
	cvt.u64.u32	%rd130, %r1787;
	add.s64 	%rd88, %rd87, %rd130;
	add.s64 	%rd89, %rd86, %rd130;
	and.b32  	%r1786, %r11560, 255;
	setp.gt.s32	%p4, %r1786, 93;
	@%p4 bra 	BB3_59;

	setp.gt.s32	%p32, %r1786, 68;
	@%p32 bra 	BB3_33;

	setp.gt.s32	%p46, %r1786, 44;
	@%p46 bra 	BB3_19;

	setp.gt.s32	%p53, %r1786, 41;
	@%p53 bra 	BB3_15;

	setp.eq.s32	%p57, %r1786, 36;
	@%p57 bra 	BB3_800;
	bra.uni 	BB3_9;

BB3_800:
	add.s32 	%r11752, %r38, 1;
	setp.gt.u32	%p653, %r11752, 31;
	@%p653 bra 	BB3_111;

	mov.u32 	%r6815, 64;
	prmt.b32 	%r6816, %r56, %r56, %r6815;
	mov.u32 	%r6817, 1040;
	prmt.b32 	%r6818, %r6816, %r56, %r6817;
	mov.u32 	%r6819, 16912;
	prmt.b32 	%r6820, %r6818, %r56, %r6819;
	bfe.u32 	%r6821, %r38, 2, 2;
	and.b32  	%r6822, %r38, 3;
	shl.b32 	%r6823, %r6822, 3;
	mov.u32 	%r6824, 255;
	shl.b32 	%r6825, %r6824, %r6823;
	setp.eq.s32	%p654, %r6821, 0;
	selp.b32	%r11748, %r6825, 0, %p654;
	setp.eq.s32	%p655, %r6821, 1;
	selp.b32	%r11749, %r6825, 0, %p655;
	setp.eq.s32	%p656, %r6821, 2;
	selp.b32	%r11750, %r6825, 0, %p656;
	setp.eq.s32	%p657, %r6821, 3;
	selp.b32	%r11751, %r6825, 0, %p657;
	shr.u32 	%r6826, %r38, 4;
	setp.eq.s32	%p658, %r6826, 0;
	selp.b32	%r6827, %r6820, 0, %p658;
	and.b32  	%r6828, %r6827, %r11748;
	or.b32  	%r11740, %r6828, %r50;
	and.b32  	%r6829, %r6827, %r11749;
	or.b32  	%r11741, %r6829, %r49;
	and.b32  	%r6830, %r6827, %r11750;
	or.b32  	%r11742, %r6830, %r48;
	and.b32  	%r6831, %r6827, %r11751;
	or.b32  	%r11743, %r6831, %r47;
	setp.eq.s32	%p659, %r6826, 1;
	selp.b32	%r6832, %r6820, 0, %p659;
	and.b32  	%r6833, %r6832, %r11748;
	or.b32  	%r11739, %r6833, %r51;
	and.b32  	%r6834, %r6832, %r11749;
	or.b32  	%r11738, %r6834, %r52;
	and.b32  	%r6835, %r6832, %r11750;
	or.b32  	%r11737, %r6835, %r53;
	and.b32  	%r6836, %r6832, %r11751;
	or.b32  	%r11736, %r6836, %r54;
	bra.uni 	BB3_1027;

BB3_59:
	setp.gt.s32	%p5, %r1786, 112;
	@%p5 bra 	BB3_84;

	setp.gt.s32	%p19, %r1786, 104;
	@%p19 bra 	BB3_75;

	setp.gt.s32	%p26, %r1786, 99;
	@%p26 bra 	BB3_65;

	setp.eq.s32	%p30, %r1786, 94;
	@%p30 bra 	BB3_798;
	bra.uni 	BB3_63;

BB3_798:
	add.s32 	%r11752, %r38, 1;
	setp.gt.u32	%p652, %r11752, 31;
	@%p652 bra 	BB3_111;

	mov.u32 	%r6814, 24;
	// inline asm
	shf.r.wrap.b32 %r11736, %r53, %r54, %r6814;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11737, %r52, %r53, %r6814;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11738, %r51, %r52, %r6814;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11739, %r47, %r51, %r6814;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11743, %r48, %r47, %r6814;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11742, %r49, %r48, %r6814;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r50, %r49, %r6814;
	// inline asm
	mov.u32 	%r6812, 0;
	// inline asm
	shf.r.wrap.b32 %r6811, %r6812, %r50, %r6814;
	// inline asm
	or.b32  	%r11740, %r6811, %r56;
	bra.uni 	BB3_1027;

BB3_33:
	setp.gt.s32	%p33, %r1786, 83;
	@%p33 bra 	BB3_44;

	setp.gt.s32	%p40, %r1786, 75;
	@%p40 bra 	BB3_39;

	setp.eq.s32	%p44, %r1786, 69;
	@%p44 bra 	BB3_113;
	bra.uni 	BB3_36;

BB3_113:
	setp.eq.s32	%p59, %r38, 0;
	mov.u32 	%r11752, 0;
	@%p59 bra 	BB3_114;

	mov.u32 	%r1903, 0;
	mov.u32 	%r1916, 8;
	// inline asm
	bfe.u32 %r1789, %r50, %r1903, %r1916;
	// inline asm
	setp.eq.s32	%p60, %r1789, 32;
	selp.u32	%r1917, 1, 0, %p60;
	// inline asm
	bfe.u32 %r1793, %r50, %r1916, %r1916;
	// inline asm
	setp.eq.s32	%p61, %r1793, 32;
	or.b32  	%r1918, %r1917, 2;
	selp.b32	%r1919, %r1918, %r1917, %p61;
	mov.u32 	%r1911, 16;
	// inline asm
	bfe.u32 %r1797, %r50, %r1911, %r1916;
	// inline asm
	setp.eq.s32	%p62, %r1797, 32;
	or.b32  	%r1920, %r1919, 4;
	selp.b32	%r1921, %r1920, %r1919, %p62;
	mov.u32 	%r1915, 24;
	// inline asm
	bfe.u32 %r1801, %r50, %r1915, %r1916;
	// inline asm
	setp.eq.s32	%p63, %r1801, 32;
	or.b32  	%r1922, %r1921, 8;
	selp.b32	%r59, %r1922, %r1921, %p63;
	// inline asm
	bfe.u32 %r1805, %r49, %r1903, %r1916;
	// inline asm
	setp.eq.s32	%p64, %r1805, 32;
	selp.u32	%r1923, 1, 0, %p64;
	// inline asm
	bfe.u32 %r1809, %r49, %r1916, %r1916;
	// inline asm
	setp.eq.s32	%p65, %r1809, 32;
	or.b32  	%r1924, %r1923, 2;
	selp.b32	%r1925, %r1924, %r1923, %p65;
	// inline asm
	bfe.u32 %r1813, %r49, %r1911, %r1916;
	// inline asm
	setp.eq.s32	%p66, %r1813, 32;
	or.b32  	%r1926, %r1925, 4;
	selp.b32	%r1927, %r1926, %r1925, %p66;
	// inline asm
	bfe.u32 %r1817, %r49, %r1915, %r1916;
	// inline asm
	setp.eq.s32	%p67, %r1817, 32;
	or.b32  	%r1928, %r1927, 8;
	selp.b32	%r60, %r1928, %r1927, %p67;
	// inline asm
	bfe.u32 %r1821, %r48, %r1903, %r1916;
	// inline asm
	setp.eq.s32	%p68, %r1821, 32;
	selp.u32	%r1929, 1, 0, %p68;
	// inline asm
	bfe.u32 %r1825, %r48, %r1916, %r1916;
	// inline asm
	setp.eq.s32	%p69, %r1825, 32;
	or.b32  	%r1930, %r1929, 2;
	selp.b32	%r1931, %r1930, %r1929, %p69;
	// inline asm
	bfe.u32 %r1829, %r48, %r1911, %r1916;
	// inline asm
	setp.eq.s32	%p70, %r1829, 32;
	or.b32  	%r1932, %r1931, 4;
	selp.b32	%r1933, %r1932, %r1931, %p70;
	// inline asm
	bfe.u32 %r1833, %r48, %r1915, %r1916;
	// inline asm
	setp.eq.s32	%p71, %r1833, 32;
	or.b32  	%r1934, %r1933, 8;
	selp.b32	%r61, %r1934, %r1933, %p71;
	// inline asm
	bfe.u32 %r1837, %r47, %r1903, %r1916;
	// inline asm
	setp.eq.s32	%p72, %r1837, 32;
	selp.u32	%r1935, 1, 0, %p72;
	// inline asm
	bfe.u32 %r1841, %r47, %r1916, %r1916;
	// inline asm
	setp.eq.s32	%p73, %r1841, 32;
	or.b32  	%r1936, %r1935, 2;
	selp.b32	%r1937, %r1936, %r1935, %p73;
	// inline asm
	bfe.u32 %r1845, %r47, %r1911, %r1916;
	// inline asm
	setp.eq.s32	%p74, %r1845, 32;
	or.b32  	%r1938, %r1937, 4;
	selp.b32	%r1939, %r1938, %r1937, %p74;
	// inline asm
	bfe.u32 %r1849, %r47, %r1915, %r1916;
	// inline asm
	setp.eq.s32	%p75, %r1849, 32;
	or.b32  	%r1940, %r1939, 8;
	selp.b32	%r62, %r1940, %r1939, %p75;
	// inline asm
	bfe.u32 %r1853, %r51, %r1903, %r1916;
	// inline asm
	setp.eq.s32	%p76, %r1853, 32;
	selp.u32	%r1941, 1, 0, %p76;
	// inline asm
	bfe.u32 %r1857, %r51, %r1916, %r1916;
	// inline asm
	setp.eq.s32	%p77, %r1857, 32;
	or.b32  	%r1942, %r1941, 2;
	selp.b32	%r1943, %r1942, %r1941, %p77;
	// inline asm
	bfe.u32 %r1861, %r51, %r1911, %r1916;
	// inline asm
	setp.eq.s32	%p78, %r1861, 32;
	or.b32  	%r1944, %r1943, 4;
	selp.b32	%r1945, %r1944, %r1943, %p78;
	// inline asm
	bfe.u32 %r1865, %r51, %r1915, %r1916;
	// inline asm
	setp.eq.s32	%p79, %r1865, 32;
	or.b32  	%r1946, %r1945, 8;
	selp.b32	%r1947, %r1946, %r1945, %p79;
	// inline asm
	bfe.u32 %r1869, %r52, %r1903, %r1916;
	// inline asm
	setp.eq.s32	%p80, %r1869, 32;
	selp.u32	%r1948, 1, 0, %p80;
	// inline asm
	bfe.u32 %r1873, %r52, %r1916, %r1916;
	// inline asm
	setp.eq.s32	%p81, %r1873, 32;
	or.b32  	%r1949, %r1948, 2;
	selp.b32	%r1950, %r1949, %r1948, %p81;
	// inline asm
	bfe.u32 %r1877, %r52, %r1911, %r1916;
	// inline asm
	setp.eq.s32	%p82, %r1877, 32;
	or.b32  	%r1951, %r1950, 4;
	selp.b32	%r1952, %r1951, %r1950, %p82;
	// inline asm
	bfe.u32 %r1881, %r52, %r1915, %r1916;
	// inline asm
	setp.eq.s32	%p83, %r1881, 32;
	or.b32  	%r1953, %r1952, 8;
	selp.b32	%r1954, %r1953, %r1952, %p83;
	// inline asm
	bfe.u32 %r1885, %r53, %r1903, %r1916;
	// inline asm
	setp.eq.s32	%p84, %r1885, 32;
	selp.u32	%r1955, 1, 0, %p84;
	// inline asm
	bfe.u32 %r1889, %r53, %r1916, %r1916;
	// inline asm
	setp.eq.s32	%p85, %r1889, 32;
	or.b32  	%r1956, %r1955, 2;
	selp.b32	%r1957, %r1956, %r1955, %p85;
	// inline asm
	bfe.u32 %r1893, %r53, %r1911, %r1916;
	// inline asm
	setp.eq.s32	%p86, %r1893, 32;
	or.b32  	%r1958, %r1957, 4;
	selp.b32	%r1959, %r1958, %r1957, %p86;
	// inline asm
	bfe.u32 %r1897, %r53, %r1915, %r1916;
	// inline asm
	setp.eq.s32	%p87, %r1897, 32;
	or.b32  	%r1960, %r1959, 8;
	selp.b32	%r1961, %r1960, %r1959, %p87;
	// inline asm
	bfe.u32 %r1901, %r54, %r1903, %r1916;
	// inline asm
	setp.eq.s32	%p88, %r1901, 32;
	selp.u32	%r1962, 1, 0, %p88;
	// inline asm
	bfe.u32 %r1905, %r54, %r1916, %r1916;
	// inline asm
	setp.eq.s32	%p89, %r1905, 32;
	or.b32  	%r1963, %r1962, 2;
	selp.b32	%r1964, %r1963, %r1962, %p89;
	// inline asm
	bfe.u32 %r1909, %r54, %r1911, %r1916;
	// inline asm
	setp.eq.s32	%p90, %r1909, 32;
	or.b32  	%r1965, %r1964, 4;
	selp.b32	%r1966, %r1965, %r1964, %p90;
	// inline asm
	bfe.u32 %r1913, %r54, %r1915, %r1916;
	// inline asm
	setp.eq.s32	%p91, %r1913, 32;
	or.b32  	%r1967, %r1966, 8;
	selp.b32	%r1968, %r1967, %r1966, %p91;
	and.b32  	%r1969, %r50, 1077952576;
	shr.u32 	%r1970, %r1969, 1;
	and.b32  	%r1971, %r50, -2139062144;
	shr.u32 	%r1972, %r1971, 2;
	not.b32 	%r1973, %r1972;
	and.b32  	%r1974, %r1970, %r1973;
	and.b32  	%r1975, %r50, 522133279;
	add.s32 	%r1976, %r1975, 522133279;
	mov.u32 	%r1977, -84215046;
	sub.s32 	%r1978, %r1977, %r1975;
	and.b32  	%r1979, %r1974, %r1978;
	and.b32  	%r1980, %r1979, %r1976;
	or.b32  	%r63, %r1980, %r50;
	and.b32  	%r1981, %r49, 1077952576;
	shr.u32 	%r1982, %r1981, 1;
	and.b32  	%r1983, %r49, -2139062144;
	shr.u32 	%r1984, %r1983, 2;
	not.b32 	%r1985, %r1984;
	and.b32  	%r1986, %r1982, %r1985;
	and.b32  	%r1987, %r49, 522133279;
	add.s32 	%r1988, %r1987, 522133279;
	sub.s32 	%r1989, %r1977, %r1987;
	and.b32  	%r1990, %r1986, %r1989;
	and.b32  	%r1991, %r1990, %r1988;
	or.b32  	%r11741, %r1991, %r49;
	and.b32  	%r1992, %r48, 1077952576;
	shr.u32 	%r1993, %r1992, 1;
	and.b32  	%r1994, %r48, -2139062144;
	shr.u32 	%r1995, %r1994, 2;
	not.b32 	%r1996, %r1995;
	and.b32  	%r1997, %r1993, %r1996;
	and.b32  	%r1998, %r48, 522133279;
	add.s32 	%r1999, %r1998, 522133279;
	sub.s32 	%r2000, %r1977, %r1998;
	and.b32  	%r2001, %r1997, %r2000;
	and.b32  	%r2002, %r2001, %r1999;
	or.b32  	%r11742, %r2002, %r48;
	and.b32  	%r2003, %r47, 1077952576;
	shr.u32 	%r2004, %r2003, 1;
	and.b32  	%r2005, %r47, -2139062144;
	shr.u32 	%r2006, %r2005, 2;
	not.b32 	%r2007, %r2006;
	and.b32  	%r2008, %r2004, %r2007;
	and.b32  	%r2009, %r47, 522133279;
	add.s32 	%r2010, %r2009, 522133279;
	sub.s32 	%r2011, %r1977, %r2009;
	and.b32  	%r2012, %r2008, %r2011;
	and.b32  	%r2013, %r2012, %r2010;
	or.b32  	%r11743, %r2013, %r47;
	and.b32  	%r2014, %r51, 1077952576;
	shr.u32 	%r2015, %r2014, 1;
	and.b32  	%r2016, %r51, -2139062144;
	shr.u32 	%r2017, %r2016, 2;
	not.b32 	%r2018, %r2017;
	and.b32  	%r2019, %r2015, %r2018;
	and.b32  	%r2020, %r51, 522133279;
	add.s32 	%r2021, %r2020, 522133279;
	sub.s32 	%r2022, %r1977, %r2020;
	and.b32  	%r2023, %r2019, %r2022;
	and.b32  	%r2024, %r2023, %r2021;
	or.b32  	%r11739, %r2024, %r51;
	and.b32  	%r2025, %r52, 1077952576;
	shr.u32 	%r2026, %r2025, 1;
	and.b32  	%r2027, %r52, -2139062144;
	shr.u32 	%r2028, %r2027, 2;
	not.b32 	%r2029, %r2028;
	and.b32  	%r2030, %r2026, %r2029;
	and.b32  	%r2031, %r52, 522133279;
	add.s32 	%r2032, %r2031, 522133279;
	sub.s32 	%r2033, %r1977, %r2031;
	and.b32  	%r2034, %r2030, %r2033;
	and.b32  	%r2035, %r2034, %r2032;
	or.b32  	%r11738, %r2035, %r52;
	and.b32  	%r2036, %r53, 1077952576;
	shr.u32 	%r2037, %r2036, 1;
	and.b32  	%r2038, %r53, -2139062144;
	shr.u32 	%r2039, %r2038, 2;
	not.b32 	%r2040, %r2039;
	and.b32  	%r2041, %r2037, %r2040;
	and.b32  	%r2042, %r53, 522133279;
	add.s32 	%r2043, %r2042, 522133279;
	sub.s32 	%r2044, %r1977, %r2042;
	and.b32  	%r2045, %r2041, %r2044;
	and.b32  	%r2046, %r2045, %r2043;
	or.b32  	%r11737, %r2046, %r53;
	and.b32  	%r2047, %r54, 1077952576;
	shr.u32 	%r2048, %r2047, 1;
	and.b32  	%r2049, %r54, -2139062144;
	shr.u32 	%r2050, %r2049, 2;
	not.b32 	%r2051, %r2050;
	and.b32  	%r2052, %r2048, %r2051;
	and.b32  	%r2053, %r54, 522133279;
	add.s32 	%r2054, %r2053, 522133279;
	sub.s32 	%r2055, %r1977, %r2053;
	and.b32  	%r2056, %r2052, %r2055;
	and.b32  	%r2057, %r2056, %r2054;
	or.b32  	%r11736, %r2057, %r54;
	and.b32  	%r2058, %r63, 64;
	shr.u32 	%r2059, %r2058, 1;
	and.b32  	%r2060, %r63, 128;
	shr.u32 	%r2061, %r2060, 2;
	not.b32 	%r2062, %r2061;
	and.b32  	%r2063, %r2059, %r2062;
	and.b32  	%r71, %r63, 522133279;
	add.s32 	%r2064, %r71, 31;
	sub.s32 	%r72, %r1977, %r71;
	and.b32  	%r2065, %r2063, %r72;
	and.b32  	%r2066, %r2065, %r2064;
	not.b32 	%r2067, %r2066;
	or.b32  	%r2068, %r2067, -33;
	and.b32  	%r11740, %r2068, %r63;
	add.s32 	%r2069, %r60, %r59;
	add.s32 	%r2070, %r2069, %r61;
	add.s32 	%r2071, %r2070, %r62;
	add.s32 	%r2072, %r2071, %r1947;
	add.s32 	%r2073, %r2072, %r1954;
	add.s32 	%r2074, %r2073, %r1961;
	neg.s32 	%r2075, %r1968;
	setp.eq.s32	%p92, %r2074, %r2075;
	@%p92 bra 	BB3_1026;

	shl.b32 	%r2076, %r60, 1;
	bfe.u32 	%r2077, %r59, 3, 28;
	or.b32  	%r2078, %r2076, %r2077;
	shr.u32 	%r2079, %r2078, 4;
	shl.b32 	%r2080, %r61, 1;
	or.b32  	%r2081, %r2080, %r2079;
	shr.u32 	%r2082, %r2081, 4;
	shl.b32 	%r2083, %r62, 1;
	or.b32  	%r2084, %r2083, %r2082;
	and.b32  	%r2085, %r63, 1077952576;
	shr.u32 	%r2086, %r2085, 1;
	and.b32  	%r2087, %r63, -2139062144;
	shr.u32 	%r2088, %r2087, 2;
	not.b32 	%r2089, %r2088;
	and.b32  	%r2090, %r2086, %r2089;
	and.b32  	%r2091, %r2090, %r72;
	add.s32 	%r2092, %r71, 522133279;
	and.b32  	%r2093, %r2091, %r2092;
	shl.b32 	%r2094, %r59, 13;
	and.b32  	%r2095, %r2093, %r2094;
	and.b32  	%r2096, %r2095, 8192;
	xor.b32  	%r2097, %r11740, %r2096;
	shl.b32 	%r2098, %r59, 20;
	and.b32  	%r2099, %r2093, %r2098;
	and.b32  	%r2100, %r2099, 2097152;
	xor.b32  	%r2101, %r2097, %r2100;
	shl.b32 	%r2102, %r59, 27;
	and.b32  	%r2103, %r2093, %r2102;
	and.b32  	%r2104, %r2103, 536870912;
	xor.b32  	%r11740, %r2101, %r2104;
	and.b32  	%r2105, %r11741, 1077952576;
	shr.u32 	%r2106, %r2105, 1;
	and.b32  	%r2107, %r11741, -2139062144;
	shr.u32 	%r2108, %r2107, 2;
	not.b32 	%r2109, %r2108;
	and.b32  	%r2110, %r2106, %r2109;
	and.b32  	%r2111, %r11741, 522133279;
	add.s32 	%r2112, %r2111, 522133279;
	sub.s32 	%r2114, %r1977, %r2111;
	and.b32  	%r2115, %r2110, %r2114;
	and.b32  	%r2116, %r2115, %r2112;
	shl.b32 	%r2117, %r59, 2;
	and.b32  	%r2118, %r2116, %r2117;
	and.b32  	%r2119, %r2118, 32;
	xor.b32  	%r2120, %r2119, %r11741;
	shl.b32 	%r2121, %r2078, 12;
	and.b32  	%r2122, %r2116, %r2121;
	and.b32  	%r2123, %r2122, 8192;
	xor.b32  	%r2124, %r2120, %r2123;
	shl.b32 	%r2125, %r2078, 19;
	and.b32  	%r2126, %r2116, %r2125;
	and.b32  	%r2127, %r2126, 2097152;
	xor.b32  	%r2128, %r2124, %r2127;
	shl.b32 	%r2129, %r2078, 26;
	and.b32  	%r2130, %r2116, %r2129;
	and.b32  	%r2131, %r2130, 536870912;
	xor.b32  	%r11741, %r2128, %r2131;
	and.b32  	%r2132, %r11742, 1077952576;
	shr.u32 	%r2133, %r2132, 1;
	and.b32  	%r2134, %r11742, -2139062144;
	shr.u32 	%r2135, %r2134, 2;
	not.b32 	%r2136, %r2135;
	and.b32  	%r2137, %r2133, %r2136;
	and.b32  	%r2138, %r11742, 522133279;
	add.s32 	%r2139, %r2138, 522133279;
	sub.s32 	%r2140, %r1977, %r2138;
	and.b32  	%r2141, %r2137, %r2140;
	and.b32  	%r2142, %r2141, %r2139;
	shl.b32 	%r2143, %r2078, 1;
	and.b32  	%r2144, %r2142, %r2143;
	and.b32  	%r2145, %r2144, 32;
	xor.b32  	%r2146, %r2145, %r11742;
	shl.b32 	%r2147, %r2081, 12;
	and.b32  	%r2148, %r2142, %r2147;
	and.b32  	%r2149, %r2148, 8192;
	xor.b32  	%r2150, %r2146, %r2149;
	shl.b32 	%r2151, %r2081, 19;
	and.b32  	%r2152, %r2142, %r2151;
	and.b32  	%r2153, %r2152, 2097152;
	xor.b32  	%r2154, %r2150, %r2153;
	shl.b32 	%r2155, %r2081, 26;
	and.b32  	%r2156, %r2142, %r2155;
	and.b32  	%r2157, %r2156, 536870912;
	xor.b32  	%r11742, %r2154, %r2157;
	and.b32  	%r2158, %r11743, 1077952576;
	shr.u32 	%r2159, %r2158, 1;
	and.b32  	%r2160, %r11743, -2139062144;
	shr.u32 	%r2161, %r2160, 2;
	not.b32 	%r2162, %r2161;
	and.b32  	%r2163, %r2159, %r2162;
	and.b32  	%r2164, %r11743, 522133279;
	add.s32 	%r2165, %r2164, 522133279;
	sub.s32 	%r2166, %r1977, %r2164;
	and.b32  	%r2167, %r2163, %r2166;
	and.b32  	%r2168, %r2167, %r2165;
	shl.b32 	%r2169, %r2081, 1;
	and.b32  	%r2170, %r2168, %r2169;
	and.b32  	%r2171, %r2170, 32;
	xor.b32  	%r2172, %r2171, %r11743;
	shl.b32 	%r2173, %r2084, 12;
	and.b32  	%r2174, %r2168, %r2173;
	and.b32  	%r2175, %r2174, 8192;
	xor.b32  	%r2176, %r2172, %r2175;
	shl.b32 	%r2177, %r2084, 19;
	and.b32  	%r2178, %r2168, %r2177;
	and.b32  	%r2179, %r2178, 2097152;
	xor.b32  	%r2180, %r2176, %r2179;
	shl.b32 	%r2181, %r2084, 26;
	and.b32  	%r2182, %r2168, %r2181;
	and.b32  	%r2183, %r2182, 536870912;
	xor.b32  	%r11743, %r2180, %r2183;
	and.b32  	%r2184, %r11739, 1077952576;
	shr.u32 	%r2185, %r2184, 1;
	and.b32  	%r2186, %r11739, -2139062144;
	shr.u32 	%r2187, %r2186, 2;
	not.b32 	%r2188, %r2187;
	and.b32  	%r2189, %r2185, %r2188;
	and.b32  	%r2190, %r11739, 522133279;
	add.s32 	%r2191, %r2190, 522133279;
	sub.s32 	%r2192, %r1977, %r2190;
	and.b32  	%r2193, %r2189, %r2192;
	and.b32  	%r2194, %r2193, %r2191;
	and.b32  	%r2195, %r2194, %r2094;
	and.b32  	%r2196, %r2195, 8192;
	xor.b32  	%r2197, %r11739, %r2196;
	and.b32  	%r2198, %r2194, %r2098;
	and.b32  	%r2199, %r2198, 2097152;
	xor.b32  	%r2200, %r2197, %r2199;
	and.b32  	%r2201, %r2194, %r2102;
	and.b32  	%r2202, %r2201, 536870912;
	xor.b32  	%r11739, %r2200, %r2202;
	and.b32  	%r2203, %r11738, 1077952576;
	shr.u32 	%r2204, %r2203, 1;
	and.b32  	%r2205, %r11738, -2139062144;
	shr.u32 	%r2206, %r2205, 2;
	not.b32 	%r2207, %r2206;
	and.b32  	%r2208, %r2204, %r2207;
	and.b32  	%r2209, %r11738, 522133279;
	add.s32 	%r2210, %r2209, 522133279;
	sub.s32 	%r2211, %r1977, %r2209;
	and.b32  	%r2212, %r2208, %r2211;
	and.b32  	%r2213, %r2212, %r2210;
	and.b32  	%r2214, %r2213, %r2117;
	and.b32  	%r2215, %r2214, 32;
	xor.b32  	%r2216, %r2215, %r11738;
	and.b32  	%r2217, %r2213, %r2121;
	and.b32  	%r2218, %r2217, 8192;
	xor.b32  	%r2219, %r2216, %r2218;
	and.b32  	%r2220, %r2213, %r2125;
	and.b32  	%r2221, %r2220, 2097152;
	xor.b32  	%r2222, %r2219, %r2221;
	and.b32  	%r2223, %r2213, %r2129;
	and.b32  	%r2224, %r2223, 536870912;
	xor.b32  	%r11738, %r2222, %r2224;
	and.b32  	%r2225, %r11737, 1077952576;
	shr.u32 	%r2226, %r2225, 1;
	and.b32  	%r2227, %r11737, -2139062144;
	shr.u32 	%r2228, %r2227, 2;
	not.b32 	%r2229, %r2228;
	and.b32  	%r2230, %r2226, %r2229;
	and.b32  	%r2231, %r11737, 522133279;
	add.s32 	%r2232, %r2231, 522133279;
	sub.s32 	%r2233, %r1977, %r2231;
	and.b32  	%r2234, %r2230, %r2233;
	and.b32  	%r2235, %r2234, %r2232;
	and.b32  	%r2236, %r2235, %r2143;
	and.b32  	%r2237, %r2236, 32;
	xor.b32  	%r2238, %r2237, %r11737;
	and.b32  	%r2239, %r2235, %r2147;
	and.b32  	%r2240, %r2239, 8192;
	xor.b32  	%r2241, %r2238, %r2240;
	and.b32  	%r2242, %r2235, %r2151;
	and.b32  	%r2243, %r2242, 2097152;
	xor.b32  	%r2244, %r2241, %r2243;
	and.b32  	%r2245, %r2235, %r2155;
	and.b32  	%r2246, %r2245, 536870912;
	xor.b32  	%r11737, %r2244, %r2246;
	and.b32  	%r2247, %r11736, 1077952576;
	shr.u32 	%r2248, %r2247, 1;
	and.b32  	%r2249, %r11736, -2139062144;
	shr.u32 	%r2250, %r2249, 2;
	not.b32 	%r2251, %r2250;
	and.b32  	%r2252, %r2248, %r2251;
	and.b32  	%r2253, %r11736, 522133279;
	add.s32 	%r2254, %r2253, 522133279;
	sub.s32 	%r2255, %r1977, %r2253;
	and.b32  	%r2256, %r2252, %r2255;
	and.b32  	%r2257, %r2256, %r2254;
	and.b32  	%r2258, %r2257, %r2169;
	and.b32  	%r2259, %r2258, 32;
	xor.b32  	%r2260, %r2259, %r11736;
	and.b32  	%r2261, %r2257, %r2173;
	and.b32  	%r2262, %r2261, 8192;
	xor.b32  	%r2263, %r2260, %r2262;
	and.b32  	%r2264, %r2257, %r2177;
	and.b32  	%r2265, %r2264, 2097152;
	xor.b32  	%r2266, %r2263, %r2265;
	and.b32  	%r2267, %r2257, %r2181;
	and.b32  	%r2268, %r2267, 536870912;
	xor.b32  	%r11736, %r2266, %r2268;
	bra.uni 	BB3_1026;

BB3_84:
	setp.gt.s32	%p6, %r1786, 119;
	@%p6 bra 	BB3_99;

	setp.gt.s32	%p13, %r1786, 114;
	@%p13 bra 	BB3_95;

	setp.eq.s32	%p17, %r1786, 113;
	@%p17 bra 	BB3_335;
	bra.uni 	BB3_87;

BB3_335:
	setp.eq.s32	%p260, %r38, 0;
	add.s32 	%r11752, %r38, %r38;
	setp.gt.u32	%p261, %r11752, 31;
	or.pred  	%p262, %p260, %p261;
	@%p262 bra 	BB3_111;

	and.b32  	%r4104, %r50, 255;
	and.b32  	%r4105, %r50, 65280;
	prmt.b32 	%r11748, %r4105, %r4104, 8452;
	bfe.u32 	%r4106, %r50, 16, 8;
	and.b32  	%r4107, %r50, -16777216;
	shr.u32 	%r4108, %r4107, 8;
	or.b32  	%r11749, %r4106, %r4108;
	and.b32  	%r4109, %r49, 65280;
	and.b32  	%r4110, %r49, 255;
	prmt.b32 	%r11750, %r4109, %r4110, 8452;
	bfe.u32 	%r4111, %r49, 16, 8;
	and.b32  	%r4112, %r49, -16777216;
	shr.u32 	%r4113, %r4112, 8;
	or.b32  	%r11751, %r4111, %r4113;
	and.b32  	%r4114, %r48, 65280;
	and.b32  	%r4115, %r48, 255;
	prmt.b32 	%r11744, %r4114, %r4115, 8452;
	bfe.u32 	%r4116, %r48, 16, 8;
	and.b32  	%r4117, %r48, -16777216;
	shr.u32 	%r4118, %r4117, 8;
	or.b32  	%r11745, %r4116, %r4118;
	and.b32  	%r4119, %r47, 65280;
	and.b32  	%r4120, %r47, 255;
	prmt.b32 	%r11746, %r4119, %r4120, 8452;
	bfe.u32 	%r4121, %r47, 16, 8;
	and.b32  	%r4122, %r47, -16777216;
	shr.u32 	%r4123, %r4122, 8;
	or.b32  	%r11747, %r4121, %r4123;
	shl.b32 	%r4124, %r11748, 8;
	or.b32  	%r11740, %r4124, %r11748;
	shl.b32 	%r4125, %r11749, 8;
	or.b32  	%r11741, %r4125, %r11749;
	shl.b32 	%r4126, %r11750, 8;
	or.b32  	%r11742, %r4126, %r11750;
	shl.b32 	%r4127, %r11751, 8;
	or.b32  	%r11743, %r4127, %r11751;
	shl.b32 	%r4128, %r11744, 8;
	or.b32  	%r11739, %r4128, %r11744;
	shl.b32 	%r4129, %r11745, 8;
	or.b32  	%r11738, %r4129, %r11745;
	shl.b32 	%r4130, %r11746, 8;
	or.b32  	%r11737, %r4130, %r11746;
	shl.b32 	%r4131, %r11747, 8;
	or.b32  	%r11736, %r4131, %r11747;
	bra.uni 	BB3_1027;

BB3_19:
	setp.gt.s32	%p47, %r1786, 63;
	@%p47 bra 	BB3_24;

	setp.eq.s32	%p51, %r1786, 45;
	@%p51 bra 	BB3_326;
	bra.uni 	BB3_21;

BB3_326:
	setp.ge.u32	%p251, %r56, %r38;
	@%p251 bra 	BB3_111;

	and.b32  	%r4004, %r55, 3;
	shl.b32 	%r4005, %r4004, 3;
	mov.u32 	%r4006, 255;
	shl.b32 	%r4007, %r4006, %r4005;
	not.b32 	%r4008, %r4007;
	and.b32  	%r4009, %r4007, 16843009;
	st.local.v4.u32 	[%rd86], {%r50, %r49, %r48, %r47};
	st.local.v4.u32 	[%rd86+16], {%r51, %r52, %r53, %r54};
	ld.local.u32 	%r4010, [%rd89];
	and.b32  	%r4011, %r4010, %r4008;
	and.b32  	%r4012, %r4010, %r4007;
	sub.s32 	%r4013, %r4012, %r4009;
	and.b32  	%r4014, %r4013, %r4007;
	or.b32  	%r4015, %r4014, %r4011;
	st.local.u32 	[%rd89], %r4015;
	bra.uni 	BB3_334;

BB3_75:
	setp.gt.s32	%p20, %r1786, 107;
	@%p20 bra 	BB3_80;

	setp.eq.s32	%p24, %r1786, 105;
	@%p24 bra 	BB3_504;
	bra.uni 	BB3_77;

BB3_504:
	setp.gt.u32	%p471, %r56, %r38;
	add.s32 	%r11752, %r38, 1;
	setp.gt.u32	%p472, %r11752, 31;
	or.pred  	%p473, %p471, %p472;
	@%p473 bra 	BB3_111;

	mov.u32 	%r5356, 24;
	// inline asm
	shf.r.wrap.b32 %r11747, %r53, %r54, %r5356;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r52, %r53, %r5356;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r51, %r52, %r5356;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r47, %r51, %r5356;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r48, %r47, %r5356;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r49, %r48, %r5356;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r50, %r49, %r5356;
	// inline asm
	mov.u32 	%r5354, 0;
	// inline asm
	shf.r.wrap.b32 %r11748, %r5354, %r50, %r5356;
	// inline asm
	and.b32  	%r5358, %r55, 3;
	shl.b32 	%r5359, %r5358, 3;
	shl.b32 	%r723, %r58, %r5359;
	mov.u32 	%r5360, 1;
	shl.b32 	%r5361, %r5360, %r5359;
	add.s32 	%r724, %r5361, -1;
	mov.u32 	%r5362, -256;
	shl.b32 	%r725, %r5362, %r5359;
	shr.u32 	%r5357, %r56, 2;
	setp.gt.s32	%p474, %r5357, 3;
	@%p474 bra 	BB3_514;

	setp.gt.s32	%p480, %r5357, 1;
	@%p480 bra 	BB3_511;

	setp.eq.s32	%p483, %r5357, 0;
	@%p483 bra 	BB3_524;
	bra.uni 	BB3_508;

BB3_524:
	and.b32  	%r5384, %r724, %r50;
	or.b32  	%r5385, %r5384, %r723;
	and.b32  	%r5386, %r11748, %r725;
	or.b32  	%r11740, %r5385, %r5386;
	mov.u32 	%r11736, %r11747;
	mov.u32 	%r11737, %r11746;
	mov.u32 	%r11738, %r11745;
	mov.u32 	%r11739, %r11744;
	mov.u32 	%r11741, %r11749;
	bra.uni 	BB3_510;

BB3_44:
	setp.gt.s32	%p34, %r1786, 89;
	@%p34 bra 	BB3_54;

	setp.eq.s32	%p38, %r1786, 84;
	@%p38 bra 	BB3_1021;
	bra.uni 	BB3_46;

BB3_1021:
	setp.ge.u32	%p790, %r56, %r38;
	@%p790 bra 	BB3_111;

	st.local.v4.u32 	[%rd86], {%r50, %r49, %r48, %r47};
	st.local.v4.u32 	[%rd86+16], {%r51, %r52, %r53, %r54};
	and.b32  	%r8575, %r55, 3;
	shl.b32 	%r8576, %r8575, 3;
	mov.u32 	%r8577, 32;
	shl.b32 	%r8578, %r8577, %r8576;
	ld.local.u32 	%r8579, [%rd89];
	and.b32  	%r8580, %r8579, 1077952576;
	shr.u32 	%r8581, %r8580, 1;
	shr.u32 	%r8582, %r8579, 2;
	not.b32 	%r8583, %r8582;
	and.b32  	%r8584, %r8579, 522133279;
	add.s32 	%r8585, %r8584, 522133279;
	mov.u32 	%r8586, -84215046;
	sub.s32 	%r8587, %r8586, %r8584;
	and.b32  	%r8588, %r8578, %r8583;
	and.b32  	%r8589, %r8588, %r8581;
	and.b32  	%r8590, %r8589, %r8587;
	and.b32  	%r8591, %r8590, %r8585;
	xor.b32  	%r8592, %r8591, %r8579;
	st.local.u32 	[%rd89], %r8592;
	bra.uni 	BB3_334;

BB3_99:
	setp.gt.s32	%p7, %r1786, 121;
	@%p7 bra 	BB3_108;

	setp.eq.s32	%p11, %r1786, 120;
	@%p11 bra 	BB3_639;
	bra.uni 	BB3_101;

BB3_639:
	setp.ge.u32	%p546, %r56, %r38;
	add.s32 	%r5961, %r56, %r58;
	setp.gt.u32	%p547, %r5961, %r38;
	or.pred  	%p548, %p546, %p547;
	@%p548 bra 	BB3_111;

	setp.gt.s32	%p549, %r56, 15;
	@%p549 bra 	BB3_669;

	setp.gt.s32	%p573, %r56, 7;
	@%p573 bra 	BB3_654;

	setp.gt.s32	%p585, %r56, 3;
	@%p585 bra 	BB3_647;

	setp.eq.s32	%p591, %r56, 1;
	@%p591 bra 	BB3_718;

	setp.eq.s32	%p592, %r56, 2;
	@%p592 bra 	BB3_717;
	bra.uni 	BB3_645;

BB3_717:
	mov.u32 	%r6462, 16;
	// inline asm
	shf.r.wrap.b32 %r11740, %r50, %r49, %r6462;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r49, %r48, %r6462;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11742, %r48, %r47, %r6462;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6443, %r47, %r51, %r6462;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11739, %r51, %r52, %r6462;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11738, %r52, %r53, %r6462;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11737, %r53, %r54, %r6462;
	// inline asm
	mov.u32 	%r6461, 0;
	// inline asm
	shf.r.wrap.b32 %r11736, %r54, %r6461, %r6462;
	// inline asm
	mov.u32 	%r54, %r6443;
	bra.uni 	BB3_724;

BB3_15:
	setp.eq.s32	%p54, %r1786, 42;
	@%p54 bra 	BB3_332;

	setp.eq.s32	%p55, %r1786, 43;
	@%p55 bra 	BB3_328;
	bra.uni 	BB3_17;

BB3_328:
	setp.ge.u32	%p252, %r56, %r38;
	@%p252 bra 	BB3_111;

	and.b32  	%r4024, %r55, 3;
	shl.b32 	%r4025, %r4024, 3;
	mov.u32 	%r4026, 255;
	shl.b32 	%r4027, %r4026, %r4025;
	not.b32 	%r4028, %r4027;
	and.b32  	%r4029, %r4027, 16843009;
	st.local.v4.u32 	[%rd86], {%r50, %r49, %r48, %r47};
	st.local.v4.u32 	[%rd86+16], {%r51, %r52, %r53, %r54};
	ld.local.u32 	%r4030, [%rd89];
	and.b32  	%r4031, %r4030, %r4028;
	and.b32  	%r4032, %r4030, %r4027;
	add.s32 	%r4033, %r4032, %r4029;
	and.b32  	%r4034, %r4033, %r4027;
	or.b32  	%r4035, %r4034, %r4031;
	st.local.u32 	[%rd89], %r4035;
	bra.uni 	BB3_334;

BB3_65:
	setp.eq.s32	%p27, %r1786, 100;
	@%p27 bra 	BB3_924;

	setp.eq.s32	%p28, %r1786, 101;
	@%p28 bra 	BB3_121;
	bra.uni 	BB3_67;

BB3_121:
	setp.eq.s32	%p93, %r38, 0;
	mov.u32 	%r11752, 0;
	@%p93 bra 	BB3_114;

	mov.u32 	%r2384, 0;
	mov.u32 	%r2397, 8;
	// inline asm
	bfe.u32 %r2270, %r50, %r2384, %r2397;
	// inline asm
	setp.eq.s32	%p94, %r2270, %r56;
	selp.u32	%r2398, 1, 0, %p94;
	// inline asm
	bfe.u32 %r2274, %r50, %r2397, %r2397;
	// inline asm
	setp.eq.s32	%p95, %r2274, %r56;
	or.b32  	%r2399, %r2398, 2;
	selp.b32	%r2400, %r2399, %r2398, %p95;
	mov.u32 	%r2392, 16;
	// inline asm
	bfe.u32 %r2278, %r50, %r2392, %r2397;
	// inline asm
	setp.eq.s32	%p96, %r2278, %r56;
	or.b32  	%r2401, %r2400, 4;
	selp.b32	%r2402, %r2401, %r2400, %p96;
	mov.u32 	%r2396, 24;
	// inline asm
	bfe.u32 %r2282, %r50, %r2396, %r2397;
	// inline asm
	setp.eq.s32	%p97, %r2282, %r56;
	or.b32  	%r2403, %r2402, 8;
	selp.b32	%r82, %r2403, %r2402, %p97;
	// inline asm
	bfe.u32 %r2286, %r49, %r2384, %r2397;
	// inline asm
	setp.eq.s32	%p98, %r2286, %r56;
	selp.u32	%r2404, 1, 0, %p98;
	// inline asm
	bfe.u32 %r2290, %r49, %r2397, %r2397;
	// inline asm
	setp.eq.s32	%p99, %r2290, %r56;
	or.b32  	%r2405, %r2404, 2;
	selp.b32	%r2406, %r2405, %r2404, %p99;
	// inline asm
	bfe.u32 %r2294, %r49, %r2392, %r2397;
	// inline asm
	setp.eq.s32	%p100, %r2294, %r56;
	or.b32  	%r2407, %r2406, 4;
	selp.b32	%r2408, %r2407, %r2406, %p100;
	// inline asm
	bfe.u32 %r2298, %r49, %r2396, %r2397;
	// inline asm
	setp.eq.s32	%p101, %r2298, %r56;
	or.b32  	%r2409, %r2408, 8;
	selp.b32	%r83, %r2409, %r2408, %p101;
	// inline asm
	bfe.u32 %r2302, %r48, %r2384, %r2397;
	// inline asm
	setp.eq.s32	%p102, %r2302, %r56;
	selp.u32	%r2410, 1, 0, %p102;
	// inline asm
	bfe.u32 %r2306, %r48, %r2397, %r2397;
	// inline asm
	setp.eq.s32	%p103, %r2306, %r56;
	or.b32  	%r2411, %r2410, 2;
	selp.b32	%r2412, %r2411, %r2410, %p103;
	// inline asm
	bfe.u32 %r2310, %r48, %r2392, %r2397;
	// inline asm
	setp.eq.s32	%p104, %r2310, %r56;
	or.b32  	%r2413, %r2412, 4;
	selp.b32	%r2414, %r2413, %r2412, %p104;
	// inline asm
	bfe.u32 %r2314, %r48, %r2396, %r2397;
	// inline asm
	setp.eq.s32	%p105, %r2314, %r56;
	or.b32  	%r2415, %r2414, 8;
	selp.b32	%r84, %r2415, %r2414, %p105;
	// inline asm
	bfe.u32 %r2318, %r47, %r2384, %r2397;
	// inline asm
	setp.eq.s32	%p106, %r2318, %r56;
	selp.u32	%r2416, 1, 0, %p106;
	// inline asm
	bfe.u32 %r2322, %r47, %r2397, %r2397;
	// inline asm
	setp.eq.s32	%p107, %r2322, %r56;
	or.b32  	%r2417, %r2416, 2;
	selp.b32	%r2418, %r2417, %r2416, %p107;
	// inline asm
	bfe.u32 %r2326, %r47, %r2392, %r2397;
	// inline asm
	setp.eq.s32	%p108, %r2326, %r56;
	or.b32  	%r2419, %r2418, 4;
	selp.b32	%r2420, %r2419, %r2418, %p108;
	// inline asm
	bfe.u32 %r2330, %r47, %r2396, %r2397;
	// inline asm
	setp.eq.s32	%p109, %r2330, %r56;
	or.b32  	%r2421, %r2420, 8;
	selp.b32	%r85, %r2421, %r2420, %p109;
	// inline asm
	bfe.u32 %r2334, %r51, %r2384, %r2397;
	// inline asm
	setp.eq.s32	%p110, %r2334, %r56;
	selp.u32	%r2422, 1, 0, %p110;
	// inline asm
	bfe.u32 %r2338, %r51, %r2397, %r2397;
	// inline asm
	setp.eq.s32	%p111, %r2338, %r56;
	or.b32  	%r2423, %r2422, 2;
	selp.b32	%r2424, %r2423, %r2422, %p111;
	// inline asm
	bfe.u32 %r2342, %r51, %r2392, %r2397;
	// inline asm
	setp.eq.s32	%p112, %r2342, %r56;
	or.b32  	%r2425, %r2424, 4;
	selp.b32	%r2426, %r2425, %r2424, %p112;
	// inline asm
	bfe.u32 %r2346, %r51, %r2396, %r2397;
	// inline asm
	setp.eq.s32	%p113, %r2346, %r56;
	or.b32  	%r2427, %r2426, 8;
	selp.b32	%r2428, %r2427, %r2426, %p113;
	// inline asm
	bfe.u32 %r2350, %r52, %r2384, %r2397;
	// inline asm
	setp.eq.s32	%p114, %r2350, %r56;
	selp.u32	%r2429, 1, 0, %p114;
	// inline asm
	bfe.u32 %r2354, %r52, %r2397, %r2397;
	// inline asm
	setp.eq.s32	%p115, %r2354, %r56;
	or.b32  	%r2430, %r2429, 2;
	selp.b32	%r2431, %r2430, %r2429, %p115;
	// inline asm
	bfe.u32 %r2358, %r52, %r2392, %r2397;
	// inline asm
	setp.eq.s32	%p116, %r2358, %r56;
	or.b32  	%r2432, %r2431, 4;
	selp.b32	%r2433, %r2432, %r2431, %p116;
	// inline asm
	bfe.u32 %r2362, %r52, %r2396, %r2397;
	// inline asm
	setp.eq.s32	%p117, %r2362, %r56;
	or.b32  	%r2434, %r2433, 8;
	selp.b32	%r2435, %r2434, %r2433, %p117;
	// inline asm
	bfe.u32 %r2366, %r53, %r2384, %r2397;
	// inline asm
	setp.eq.s32	%p118, %r2366, %r56;
	selp.u32	%r2436, 1, 0, %p118;
	// inline asm
	bfe.u32 %r2370, %r53, %r2397, %r2397;
	// inline asm
	setp.eq.s32	%p119, %r2370, %r56;
	or.b32  	%r2437, %r2436, 2;
	selp.b32	%r2438, %r2437, %r2436, %p119;
	// inline asm
	bfe.u32 %r2374, %r53, %r2392, %r2397;
	// inline asm
	setp.eq.s32	%p120, %r2374, %r56;
	or.b32  	%r2439, %r2438, 4;
	selp.b32	%r2440, %r2439, %r2438, %p120;
	// inline asm
	bfe.u32 %r2378, %r53, %r2396, %r2397;
	// inline asm
	setp.eq.s32	%p121, %r2378, %r56;
	or.b32  	%r2441, %r2440, 8;
	selp.b32	%r2442, %r2441, %r2440, %p121;
	// inline asm
	bfe.u32 %r2382, %r54, %r2384, %r2397;
	// inline asm
	setp.eq.s32	%p122, %r2382, %r56;
	selp.u32	%r2443, 1, 0, %p122;
	// inline asm
	bfe.u32 %r2386, %r54, %r2397, %r2397;
	// inline asm
	setp.eq.s32	%p123, %r2386, %r56;
	or.b32  	%r2444, %r2443, 2;
	selp.b32	%r2445, %r2444, %r2443, %p123;
	// inline asm
	bfe.u32 %r2390, %r54, %r2392, %r2397;
	// inline asm
	setp.eq.s32	%p124, %r2390, %r56;
	or.b32  	%r2446, %r2445, 4;
	selp.b32	%r2447, %r2446, %r2445, %p124;
	// inline asm
	bfe.u32 %r2394, %r54, %r2396, %r2397;
	// inline asm
	setp.eq.s32	%p125, %r2394, %r56;
	or.b32  	%r2448, %r2447, 8;
	selp.b32	%r2449, %r2448, %r2447, %p125;
	and.b32  	%r2450, %r50, 1077952576;
	shr.u32 	%r2451, %r2450, 1;
	and.b32  	%r2452, %r50, -2139062144;
	shr.u32 	%r2453, %r2452, 2;
	not.b32 	%r2454, %r2453;
	and.b32  	%r2455, %r2451, %r2454;
	and.b32  	%r2456, %r50, 522133279;
	add.s32 	%r2457, %r2456, 522133279;
	mov.u32 	%r2458, -84215046;
	sub.s32 	%r2459, %r2458, %r2456;
	and.b32  	%r2460, %r2455, %r2459;
	and.b32  	%r2461, %r2460, %r2457;
	or.b32  	%r86, %r2461, %r50;
	and.b32  	%r2462, %r49, 1077952576;
	shr.u32 	%r2463, %r2462, 1;
	and.b32  	%r2464, %r49, -2139062144;
	shr.u32 	%r2465, %r2464, 2;
	not.b32 	%r2466, %r2465;
	and.b32  	%r2467, %r2463, %r2466;
	and.b32  	%r2468, %r49, 522133279;
	add.s32 	%r2469, %r2468, 522133279;
	sub.s32 	%r2470, %r2458, %r2468;
	and.b32  	%r2471, %r2467, %r2470;
	and.b32  	%r2472, %r2471, %r2469;
	or.b32  	%r11741, %r2472, %r49;
	and.b32  	%r2473, %r48, 1077952576;
	shr.u32 	%r2474, %r2473, 1;
	and.b32  	%r2475, %r48, -2139062144;
	shr.u32 	%r2476, %r2475, 2;
	not.b32 	%r2477, %r2476;
	and.b32  	%r2478, %r2474, %r2477;
	and.b32  	%r2479, %r48, 522133279;
	add.s32 	%r2480, %r2479, 522133279;
	sub.s32 	%r2481, %r2458, %r2479;
	and.b32  	%r2482, %r2478, %r2481;
	and.b32  	%r2483, %r2482, %r2480;
	or.b32  	%r11742, %r2483, %r48;
	and.b32  	%r2484, %r47, 1077952576;
	shr.u32 	%r2485, %r2484, 1;
	and.b32  	%r2486, %r47, -2139062144;
	shr.u32 	%r2487, %r2486, 2;
	not.b32 	%r2488, %r2487;
	and.b32  	%r2489, %r2485, %r2488;
	and.b32  	%r2490, %r47, 522133279;
	add.s32 	%r2491, %r2490, 522133279;
	sub.s32 	%r2492, %r2458, %r2490;
	and.b32  	%r2493, %r2489, %r2492;
	and.b32  	%r2494, %r2493, %r2491;
	or.b32  	%r11743, %r2494, %r47;
	and.b32  	%r2495, %r51, 1077952576;
	shr.u32 	%r2496, %r2495, 1;
	and.b32  	%r2497, %r51, -2139062144;
	shr.u32 	%r2498, %r2497, 2;
	not.b32 	%r2499, %r2498;
	and.b32  	%r2500, %r2496, %r2499;
	and.b32  	%r2501, %r51, 522133279;
	add.s32 	%r2502, %r2501, 522133279;
	sub.s32 	%r2503, %r2458, %r2501;
	and.b32  	%r2504, %r2500, %r2503;
	and.b32  	%r2505, %r2504, %r2502;
	or.b32  	%r11739, %r2505, %r51;
	and.b32  	%r2506, %r52, 1077952576;
	shr.u32 	%r2507, %r2506, 1;
	and.b32  	%r2508, %r52, -2139062144;
	shr.u32 	%r2509, %r2508, 2;
	not.b32 	%r2510, %r2509;
	and.b32  	%r2511, %r2507, %r2510;
	and.b32  	%r2512, %r52, 522133279;
	add.s32 	%r2513, %r2512, 522133279;
	sub.s32 	%r2514, %r2458, %r2512;
	and.b32  	%r2515, %r2511, %r2514;
	and.b32  	%r2516, %r2515, %r2513;
	or.b32  	%r11738, %r2516, %r52;
	and.b32  	%r2517, %r53, 1077952576;
	shr.u32 	%r2518, %r2517, 1;
	and.b32  	%r2519, %r53, -2139062144;
	shr.u32 	%r2520, %r2519, 2;
	not.b32 	%r2521, %r2520;
	and.b32  	%r2522, %r2518, %r2521;
	and.b32  	%r2523, %r53, 522133279;
	add.s32 	%r2524, %r2523, 522133279;
	sub.s32 	%r2525, %r2458, %r2523;
	and.b32  	%r2526, %r2522, %r2525;
	and.b32  	%r2527, %r2526, %r2524;
	or.b32  	%r11737, %r2527, %r53;
	and.b32  	%r2528, %r54, 1077952576;
	shr.u32 	%r2529, %r2528, 1;
	and.b32  	%r2530, %r54, -2139062144;
	shr.u32 	%r2531, %r2530, 2;
	not.b32 	%r2532, %r2531;
	and.b32  	%r2533, %r2529, %r2532;
	and.b32  	%r2534, %r54, 522133279;
	add.s32 	%r2535, %r2534, 522133279;
	sub.s32 	%r2536, %r2458, %r2534;
	and.b32  	%r2537, %r2533, %r2536;
	and.b32  	%r2538, %r2537, %r2535;
	or.b32  	%r11736, %r2538, %r54;
	and.b32  	%r2539, %r86, 64;
	shr.u32 	%r2540, %r2539, 1;
	and.b32  	%r2541, %r86, 128;
	shr.u32 	%r2542, %r2541, 2;
	not.b32 	%r2543, %r2542;
	and.b32  	%r2544, %r2540, %r2543;
	and.b32  	%r94, %r86, 522133279;
	add.s32 	%r2545, %r94, 31;
	sub.s32 	%r95, %r2458, %r94;
	and.b32  	%r2546, %r2544, %r95;
	and.b32  	%r2547, %r2546, %r2545;
	not.b32 	%r2548, %r2547;
	or.b32  	%r2549, %r2548, -33;
	and.b32  	%r11740, %r2549, %r86;
	add.s32 	%r2550, %r83, %r82;
	add.s32 	%r2551, %r2550, %r84;
	add.s32 	%r2552, %r2551, %r85;
	add.s32 	%r2553, %r2552, %r2428;
	add.s32 	%r2554, %r2553, %r2435;
	add.s32 	%r2555, %r2554, %r2442;
	neg.s32 	%r2556, %r2449;
	setp.eq.s32	%p126, %r2555, %r2556;
	@%p126 bra 	BB3_1026;

	shl.b32 	%r2557, %r83, 1;
	bfe.u32 	%r2558, %r82, 3, 28;
	or.b32  	%r2559, %r2557, %r2558;
	shr.u32 	%r2560, %r2559, 4;
	shl.b32 	%r2561, %r84, 1;
	or.b32  	%r2562, %r2561, %r2560;
	shr.u32 	%r2563, %r2562, 4;
	shl.b32 	%r2564, %r85, 1;
	or.b32  	%r2565, %r2564, %r2563;
	and.b32  	%r2566, %r86, 1077952576;
	shr.u32 	%r2567, %r2566, 1;
	and.b32  	%r2568, %r86, -2139062144;
	shr.u32 	%r2569, %r2568, 2;
	not.b32 	%r2570, %r2569;
	and.b32  	%r2571, %r2567, %r2570;
	and.b32  	%r2572, %r2571, %r95;
	add.s32 	%r2573, %r94, 522133279;
	and.b32  	%r2574, %r2572, %r2573;
	shl.b32 	%r2575, %r82, 13;
	and.b32  	%r2576, %r2574, %r2575;
	and.b32  	%r2577, %r2576, 8192;
	xor.b32  	%r2578, %r11740, %r2577;
	shl.b32 	%r2579, %r82, 20;
	and.b32  	%r2580, %r2574, %r2579;
	and.b32  	%r2581, %r2580, 2097152;
	xor.b32  	%r2582, %r2578, %r2581;
	shl.b32 	%r2583, %r82, 27;
	and.b32  	%r2584, %r2574, %r2583;
	and.b32  	%r2585, %r2584, 536870912;
	xor.b32  	%r11740, %r2582, %r2585;
	and.b32  	%r2586, %r11741, 1077952576;
	shr.u32 	%r2587, %r2586, 1;
	and.b32  	%r2588, %r11741, -2139062144;
	shr.u32 	%r2589, %r2588, 2;
	not.b32 	%r2590, %r2589;
	and.b32  	%r2591, %r2587, %r2590;
	and.b32  	%r2592, %r11741, 522133279;
	add.s32 	%r2593, %r2592, 522133279;
	sub.s32 	%r2595, %r2458, %r2592;
	and.b32  	%r2596, %r2591, %r2595;
	and.b32  	%r2597, %r2596, %r2593;
	shl.b32 	%r2598, %r82, 2;
	and.b32  	%r2599, %r2597, %r2598;
	and.b32  	%r2600, %r2599, 32;
	xor.b32  	%r2601, %r2600, %r11741;
	shl.b32 	%r2602, %r2559, 12;
	and.b32  	%r2603, %r2597, %r2602;
	and.b32  	%r2604, %r2603, 8192;
	xor.b32  	%r2605, %r2601, %r2604;
	shl.b32 	%r2606, %r2559, 19;
	and.b32  	%r2607, %r2597, %r2606;
	and.b32  	%r2608, %r2607, 2097152;
	xor.b32  	%r2609, %r2605, %r2608;
	shl.b32 	%r2610, %r2559, 26;
	and.b32  	%r2611, %r2597, %r2610;
	and.b32  	%r2612, %r2611, 536870912;
	xor.b32  	%r11741, %r2609, %r2612;
	and.b32  	%r2613, %r11742, 1077952576;
	shr.u32 	%r2614, %r2613, 1;
	and.b32  	%r2615, %r11742, -2139062144;
	shr.u32 	%r2616, %r2615, 2;
	not.b32 	%r2617, %r2616;
	and.b32  	%r2618, %r2614, %r2617;
	and.b32  	%r2619, %r11742, 522133279;
	add.s32 	%r2620, %r2619, 522133279;
	sub.s32 	%r2621, %r2458, %r2619;
	and.b32  	%r2622, %r2618, %r2621;
	and.b32  	%r2623, %r2622, %r2620;
	shl.b32 	%r2624, %r2559, 1;
	and.b32  	%r2625, %r2623, %r2624;
	and.b32  	%r2626, %r2625, 32;
	xor.b32  	%r2627, %r2626, %r11742;
	shl.b32 	%r2628, %r2562, 12;
	and.b32  	%r2629, %r2623, %r2628;
	and.b32  	%r2630, %r2629, 8192;
	xor.b32  	%r2631, %r2627, %r2630;
	shl.b32 	%r2632, %r2562, 19;
	and.b32  	%r2633, %r2623, %r2632;
	and.b32  	%r2634, %r2633, 2097152;
	xor.b32  	%r2635, %r2631, %r2634;
	shl.b32 	%r2636, %r2562, 26;
	and.b32  	%r2637, %r2623, %r2636;
	and.b32  	%r2638, %r2637, 536870912;
	xor.b32  	%r11742, %r2635, %r2638;
	and.b32  	%r2639, %r11743, 1077952576;
	shr.u32 	%r2640, %r2639, 1;
	and.b32  	%r2641, %r11743, -2139062144;
	shr.u32 	%r2642, %r2641, 2;
	not.b32 	%r2643, %r2642;
	and.b32  	%r2644, %r2640, %r2643;
	and.b32  	%r2645, %r11743, 522133279;
	add.s32 	%r2646, %r2645, 522133279;
	sub.s32 	%r2647, %r2458, %r2645;
	and.b32  	%r2648, %r2644, %r2647;
	and.b32  	%r2649, %r2648, %r2646;
	shl.b32 	%r2650, %r2562, 1;
	and.b32  	%r2651, %r2649, %r2650;
	and.b32  	%r2652, %r2651, 32;
	xor.b32  	%r2653, %r2652, %r11743;
	shl.b32 	%r2654, %r2565, 12;
	and.b32  	%r2655, %r2649, %r2654;
	and.b32  	%r2656, %r2655, 8192;
	xor.b32  	%r2657, %r2653, %r2656;
	shl.b32 	%r2658, %r2565, 19;
	and.b32  	%r2659, %r2649, %r2658;
	and.b32  	%r2660, %r2659, 2097152;
	xor.b32  	%r2661, %r2657, %r2660;
	shl.b32 	%r2662, %r2565, 26;
	and.b32  	%r2663, %r2649, %r2662;
	and.b32  	%r2664, %r2663, 536870912;
	xor.b32  	%r11743, %r2661, %r2664;
	and.b32  	%r2665, %r11739, 1077952576;
	shr.u32 	%r2666, %r2665, 1;
	and.b32  	%r2667, %r11739, -2139062144;
	shr.u32 	%r2668, %r2667, 2;
	not.b32 	%r2669, %r2668;
	and.b32  	%r2670, %r2666, %r2669;
	and.b32  	%r2671, %r11739, 522133279;
	add.s32 	%r2672, %r2671, 522133279;
	sub.s32 	%r2673, %r2458, %r2671;
	and.b32  	%r2674, %r2670, %r2673;
	and.b32  	%r2675, %r2674, %r2672;
	and.b32  	%r2676, %r2675, %r2575;
	and.b32  	%r2677, %r2676, 8192;
	xor.b32  	%r2678, %r11739, %r2677;
	and.b32  	%r2679, %r2675, %r2579;
	and.b32  	%r2680, %r2679, 2097152;
	xor.b32  	%r2681, %r2678, %r2680;
	and.b32  	%r2682, %r2675, %r2583;
	and.b32  	%r2683, %r2682, 536870912;
	xor.b32  	%r11739, %r2681, %r2683;
	and.b32  	%r2684, %r11738, 1077952576;
	shr.u32 	%r2685, %r2684, 1;
	and.b32  	%r2686, %r11738, -2139062144;
	shr.u32 	%r2687, %r2686, 2;
	not.b32 	%r2688, %r2687;
	and.b32  	%r2689, %r2685, %r2688;
	and.b32  	%r2690, %r11738, 522133279;
	add.s32 	%r2691, %r2690, 522133279;
	sub.s32 	%r2692, %r2458, %r2690;
	and.b32  	%r2693, %r2689, %r2692;
	and.b32  	%r2694, %r2693, %r2691;
	and.b32  	%r2695, %r2694, %r2598;
	and.b32  	%r2696, %r2695, 32;
	xor.b32  	%r2697, %r2696, %r11738;
	and.b32  	%r2698, %r2694, %r2602;
	and.b32  	%r2699, %r2698, 8192;
	xor.b32  	%r2700, %r2697, %r2699;
	and.b32  	%r2701, %r2694, %r2606;
	and.b32  	%r2702, %r2701, 2097152;
	xor.b32  	%r2703, %r2700, %r2702;
	and.b32  	%r2704, %r2694, %r2610;
	and.b32  	%r2705, %r2704, 536870912;
	xor.b32  	%r11738, %r2703, %r2705;
	and.b32  	%r2706, %r11737, 1077952576;
	shr.u32 	%r2707, %r2706, 1;
	and.b32  	%r2708, %r11737, -2139062144;
	shr.u32 	%r2709, %r2708, 2;
	not.b32 	%r2710, %r2709;
	and.b32  	%r2711, %r2707, %r2710;
	and.b32  	%r2712, %r11737, 522133279;
	add.s32 	%r2713, %r2712, 522133279;
	sub.s32 	%r2714, %r2458, %r2712;
	and.b32  	%r2715, %r2711, %r2714;
	and.b32  	%r2716, %r2715, %r2713;
	and.b32  	%r2717, %r2716, %r2624;
	and.b32  	%r2718, %r2717, 32;
	xor.b32  	%r2719, %r2718, %r11737;
	and.b32  	%r2720, %r2716, %r2628;
	and.b32  	%r2721, %r2720, 8192;
	xor.b32  	%r2722, %r2719, %r2721;
	and.b32  	%r2723, %r2716, %r2632;
	and.b32  	%r2724, %r2723, 2097152;
	xor.b32  	%r2725, %r2722, %r2724;
	and.b32  	%r2726, %r2716, %r2636;
	and.b32  	%r2727, %r2726, 536870912;
	xor.b32  	%r11737, %r2725, %r2727;
	and.b32  	%r2728, %r11736, 1077952576;
	shr.u32 	%r2729, %r2728, 1;
	and.b32  	%r2730, %r11736, -2139062144;
	shr.u32 	%r2731, %r2730, 2;
	not.b32 	%r2732, %r2731;
	and.b32  	%r2733, %r2729, %r2732;
	and.b32  	%r2734, %r11736, 522133279;
	add.s32 	%r2735, %r2734, 522133279;
	sub.s32 	%r2736, %r2458, %r2734;
	and.b32  	%r2737, %r2733, %r2736;
	and.b32  	%r2738, %r2737, %r2735;
	and.b32  	%r2739, %r2738, %r2650;
	and.b32  	%r2740, %r2739, 32;
	xor.b32  	%r2741, %r2740, %r11736;
	and.b32  	%r2742, %r2738, %r2654;
	and.b32  	%r2743, %r2742, 8192;
	xor.b32  	%r2744, %r2741, %r2743;
	and.b32  	%r2745, %r2738, %r2658;
	and.b32  	%r2746, %r2745, 2097152;
	xor.b32  	%r2747, %r2744, %r2746;
	and.b32  	%r2748, %r2738, %r2662;
	and.b32  	%r2749, %r2748, 536870912;
	xor.b32  	%r11736, %r2747, %r2749;
	bra.uni 	BB3_1026;

BB3_39:
	setp.eq.s32	%p41, %r1786, 76;
	@%p41 bra 	BB3_330;

	setp.eq.s32	%p42, %r1786, 79;
	@%p42 bra 	BB3_525;
	bra.uni 	BB3_41;

BB3_525:
	setp.ge.u32	%p485, %r56, %r38;
	add.s32 	%r5387, %r56, %r58;
	setp.gt.u32	%p486, %r5387, %r38;
	or.pred  	%p487, %p485, %p486;
	@%p487 bra 	BB3_111;

	mov.u32 	%r11747, 0;
	setp.gt.s32	%p488, %r58, 15;
	@%p488 bra 	BB3_559;

	setp.gt.s32	%p512, %r58, 7;
	@%p512 bra 	BB3_543;

	setp.gt.s32	%p524, %r58, 3;
	@%p524 bra 	BB3_536;

	setp.gt.s32	%p530, %r58, 1;
	@%p530 bra 	BB3_533;

	setp.eq.s32	%p533, %r58, 0;
	@%p533 bra 	BB3_611;
	bra.uni 	BB3_531;

BB3_611:
	mov.u32 	%r11744, %r51;
	mov.u32 	%r11745, %r52;
	mov.u32 	%r11746, %r53;
	mov.u32 	%r11747, %r54;
	mov.u32 	%r11748, %r50;
	mov.u32 	%r11749, %r49;
	mov.u32 	%r11750, %r48;
	mov.u32 	%r11751, %r47;
	bra.uni 	BB3_612;

BB3_95:
	setp.eq.s32	%p14, %r1786, 115;
	@%p14 bra 	BB3_478;

	setp.eq.s32	%p15, %r1786, 116;
	@%p15 bra 	BB3_1023;
	bra.uni 	BB3_97;

BB3_1023:
	and.b32  	%r8601, %r50, 1077952576;
	shr.u32 	%r8602, %r8601, 1;
	and.b32  	%r8603, %r50, -2139062144;
	shr.u32 	%r8604, %r8603, 2;
	not.b32 	%r8605, %r8604;
	and.b32  	%r8606, %r8602, %r8605;
	and.b32  	%r8607, %r50, 522133279;
	add.s32 	%r8608, %r8607, 522133279;
	mov.u32 	%r8609, -84215046;
	sub.s32 	%r8610, %r8609, %r8607;
	and.b32  	%r8611, %r8606, %r8610;
	and.b32  	%r8612, %r8611, %r8608;
	xor.b32  	%r11740, %r8612, %r50;
	and.b32  	%r8613, %r49, 1077952576;
	shr.u32 	%r8614, %r8613, 1;
	and.b32  	%r8615, %r49, -2139062144;
	shr.u32 	%r8616, %r8615, 2;
	not.b32 	%r8617, %r8616;
	and.b32  	%r8618, %r8614, %r8617;
	and.b32  	%r8619, %r49, 522133279;
	add.s32 	%r8620, %r8619, 522133279;
	sub.s32 	%r8621, %r8609, %r8619;
	and.b32  	%r8622, %r8618, %r8621;
	and.b32  	%r8623, %r8622, %r8620;
	xor.b32  	%r11741, %r8623, %r49;
	and.b32  	%r8624, %r48, 1077952576;
	shr.u32 	%r8625, %r8624, 1;
	and.b32  	%r8626, %r48, -2139062144;
	shr.u32 	%r8627, %r8626, 2;
	not.b32 	%r8628, %r8627;
	and.b32  	%r8629, %r8625, %r8628;
	and.b32  	%r8630, %r48, 522133279;
	add.s32 	%r8631, %r8630, 522133279;
	sub.s32 	%r8632, %r8609, %r8630;
	and.b32  	%r8633, %r8629, %r8632;
	and.b32  	%r8634, %r8633, %r8631;
	xor.b32  	%r11742, %r8634, %r48;
	and.b32  	%r8635, %r47, 1077952576;
	shr.u32 	%r8636, %r8635, 1;
	and.b32  	%r8637, %r47, -2139062144;
	shr.u32 	%r8638, %r8637, 2;
	not.b32 	%r8639, %r8638;
	and.b32  	%r8640, %r8636, %r8639;
	and.b32  	%r8641, %r47, 522133279;
	add.s32 	%r8642, %r8641, 522133279;
	sub.s32 	%r8643, %r8609, %r8641;
	and.b32  	%r8644, %r8640, %r8643;
	and.b32  	%r8645, %r8644, %r8642;
	xor.b32  	%r11743, %r8645, %r47;
	and.b32  	%r8646, %r51, 1077952576;
	shr.u32 	%r8647, %r8646, 1;
	and.b32  	%r8648, %r51, -2139062144;
	shr.u32 	%r8649, %r8648, 2;
	not.b32 	%r8650, %r8649;
	and.b32  	%r8651, %r8647, %r8650;
	and.b32  	%r8652, %r51, 522133279;
	add.s32 	%r8653, %r8652, 522133279;
	sub.s32 	%r8654, %r8609, %r8652;
	and.b32  	%r8655, %r8651, %r8654;
	and.b32  	%r8656, %r8655, %r8653;
	xor.b32  	%r11739, %r8656, %r51;
	and.b32  	%r8657, %r52, 1077952576;
	shr.u32 	%r8658, %r8657, 1;
	and.b32  	%r8659, %r52, -2139062144;
	shr.u32 	%r8660, %r8659, 2;
	not.b32 	%r8661, %r8660;
	and.b32  	%r8662, %r8658, %r8661;
	and.b32  	%r8663, %r52, 522133279;
	add.s32 	%r8664, %r8663, 522133279;
	sub.s32 	%r8665, %r8609, %r8663;
	and.b32  	%r8666, %r8662, %r8665;
	and.b32  	%r8667, %r8666, %r8664;
	xor.b32  	%r11738, %r8667, %r52;
	and.b32  	%r8668, %r53, 1077952576;
	shr.u32 	%r8669, %r8668, 1;
	and.b32  	%r8670, %r53, -2139062144;
	shr.u32 	%r8671, %r8670, 2;
	not.b32 	%r8672, %r8671;
	and.b32  	%r8673, %r8669, %r8672;
	and.b32  	%r8674, %r53, 522133279;
	add.s32 	%r8675, %r8674, 522133279;
	sub.s32 	%r8676, %r8609, %r8674;
	and.b32  	%r8677, %r8673, %r8676;
	and.b32  	%r8678, %r8677, %r8675;
	xor.b32  	%r11737, %r8678, %r53;
	and.b32  	%r8679, %r54, 1077952576;
	shr.u32 	%r8680, %r8679, 1;
	and.b32  	%r8681, %r54, -2139062144;
	shr.u32 	%r8682, %r8681, 2;
	not.b32 	%r8683, %r8682;
	and.b32  	%r8684, %r8680, %r8683;
	and.b32  	%r8685, %r54, 522133279;
	add.s32 	%r8686, %r8685, 522133279;
	sub.s32 	%r8687, %r8609, %r8685;
	and.b32  	%r8688, %r8684, %r8687;
	and.b32  	%r8689, %r8688, %r8686;
	xor.b32  	%r11736, %r8689, %r54;
	bra.uni 	BB3_1026;

BB3_24:
	setp.eq.s32	%p48, %r1786, 64;
	@%p48 bra 	BB3_452;

	setp.eq.s32	%p49, %r1786, 67;
	@%p49 bra 	BB3_1024;
	bra.uni 	BB3_26;

BB3_1024:
	and.b32  	%r8690, %r50, 1077952576;
	shr.u32 	%r8691, %r8690, 1;
	and.b32  	%r8692, %r50, -2139062144;
	shr.u32 	%r8693, %r8692, 2;
	not.b32 	%r8694, %r8693;
	and.b32  	%r8695, %r8691, %r8694;
	and.b32  	%r8696, %r50, 522133279;
	add.s32 	%r8697, %r8696, 522133279;
	mov.u32 	%r8698, -84215046;
	sub.s32 	%r8699, %r8698, %r8696;
	and.b32  	%r8700, %r8695, %r8699;
	and.b32  	%r8701, %r8700, %r8697;
	not.b32 	%r8702, %r8701;
	and.b32  	%r8703, %r50, %r8702;
	and.b32  	%r8704, %r49, 1077952576;
	shr.u32 	%r8705, %r8704, 1;
	and.b32  	%r8706, %r49, -2139062144;
	shr.u32 	%r8707, %r8706, 2;
	not.b32 	%r8708, %r8707;
	and.b32  	%r8709, %r8705, %r8708;
	and.b32  	%r8710, %r49, 522133279;
	add.s32 	%r8711, %r8710, 522133279;
	sub.s32 	%r8712, %r8698, %r8710;
	and.b32  	%r8713, %r8709, %r8712;
	and.b32  	%r8714, %r8713, %r8711;
	not.b32 	%r8715, %r8714;
	and.b32  	%r11741, %r49, %r8715;
	and.b32  	%r8716, %r48, 1077952576;
	shr.u32 	%r8717, %r8716, 1;
	and.b32  	%r8718, %r48, -2139062144;
	shr.u32 	%r8719, %r8718, 2;
	not.b32 	%r8720, %r8719;
	and.b32  	%r8721, %r8717, %r8720;
	and.b32  	%r8722, %r48, 522133279;
	add.s32 	%r8723, %r8722, 522133279;
	sub.s32 	%r8724, %r8698, %r8722;
	and.b32  	%r8725, %r8721, %r8724;
	and.b32  	%r8726, %r8725, %r8723;
	not.b32 	%r8727, %r8726;
	and.b32  	%r11742, %r48, %r8727;
	and.b32  	%r8728, %r47, 1077952576;
	shr.u32 	%r8729, %r8728, 1;
	and.b32  	%r8730, %r47, -2139062144;
	shr.u32 	%r8731, %r8730, 2;
	not.b32 	%r8732, %r8731;
	and.b32  	%r8733, %r8729, %r8732;
	and.b32  	%r8734, %r47, 522133279;
	add.s32 	%r8735, %r8734, 522133279;
	sub.s32 	%r8736, %r8698, %r8734;
	and.b32  	%r8737, %r8733, %r8736;
	and.b32  	%r8738, %r8737, %r8735;
	not.b32 	%r8739, %r8738;
	and.b32  	%r11743, %r47, %r8739;
	and.b32  	%r8740, %r51, 1077952576;
	shr.u32 	%r8741, %r8740, 1;
	and.b32  	%r8742, %r51, -2139062144;
	shr.u32 	%r8743, %r8742, 2;
	not.b32 	%r8744, %r8743;
	and.b32  	%r8745, %r8741, %r8744;
	and.b32  	%r8746, %r51, 522133279;
	add.s32 	%r8747, %r8746, 522133279;
	sub.s32 	%r8748, %r8698, %r8746;
	and.b32  	%r8749, %r8745, %r8748;
	and.b32  	%r8750, %r8749, %r8747;
	not.b32 	%r8751, %r8750;
	and.b32  	%r11739, %r51, %r8751;
	and.b32  	%r8752, %r52, 1077952576;
	shr.u32 	%r8753, %r8752, 1;
	and.b32  	%r8754, %r52, -2139062144;
	shr.u32 	%r8755, %r8754, 2;
	not.b32 	%r8756, %r8755;
	and.b32  	%r8757, %r8753, %r8756;
	and.b32  	%r8758, %r52, 522133279;
	add.s32 	%r8759, %r8758, 522133279;
	sub.s32 	%r8760, %r8698, %r8758;
	and.b32  	%r8761, %r8757, %r8760;
	and.b32  	%r8762, %r8761, %r8759;
	not.b32 	%r8763, %r8762;
	and.b32  	%r11738, %r52, %r8763;
	and.b32  	%r8764, %r53, 1077952576;
	shr.u32 	%r8765, %r8764, 1;
	and.b32  	%r8766, %r53, -2139062144;
	shr.u32 	%r8767, %r8766, 2;
	not.b32 	%r8768, %r8767;
	and.b32  	%r8769, %r8765, %r8768;
	and.b32  	%r8770, %r53, 522133279;
	add.s32 	%r8771, %r8770, 522133279;
	sub.s32 	%r8772, %r8698, %r8770;
	and.b32  	%r8773, %r8769, %r8772;
	and.b32  	%r8774, %r8773, %r8771;
	not.b32 	%r8775, %r8774;
	and.b32  	%r11737, %r53, %r8775;
	and.b32  	%r8776, %r54, 1077952576;
	shr.u32 	%r8777, %r8776, 1;
	and.b32  	%r8778, %r54, -2139062144;
	shr.u32 	%r8779, %r8778, 2;
	not.b32 	%r8780, %r8779;
	and.b32  	%r8781, %r8777, %r8780;
	and.b32  	%r8782, %r54, 522133279;
	add.s32 	%r8783, %r8782, 522133279;
	sub.s32 	%r8784, %r8698, %r8782;
	and.b32  	%r8785, %r8781, %r8784;
	and.b32  	%r8786, %r8785, %r8783;
	not.b32 	%r8787, %r8786;
	and.b32  	%r11736, %r54, %r8787;
	and.b32  	%r8788, %r8703, 1077952576;
	shr.u32 	%r8789, %r8788, 1;
	and.b32  	%r8790, %r8703, 128;
	shr.u32 	%r8791, %r8790, 2;
	and.b32  	%r8792, %r8703, 522133279;
	add.s32 	%r8793, %r8792, 522133279;
	sub.s32 	%r8794, %r8698, %r8792;
	xor.b32  	%r8795, %r8791, 32;
	and.b32  	%r8796, %r8795, %r8789;
	and.b32  	%r8797, %r8796, %r8794;
	and.b32  	%r8798, %r8797, %r8793;
	or.b32  	%r11740, %r8798, %r8703;
	bra.uni 	BB3_1026;

BB3_80:
	setp.eq.s32	%p21, %r1786, 108;
	@%p21 bra 	BB3_1025;

	setp.eq.s32	%p22, %r1786, 111;
	@%p22 bra 	BB3_502;
	bra.uni 	BB3_82;

BB3_502:
	setp.ge.u32	%p470, %r56, %r38;
	@%p470 bra 	BB3_111;

	and.b32  	%r5308, %r55, 3;
	shl.b32 	%r5309, %r5308, 3;
	shl.b32 	%r5310, %r58, %r5309;
	mov.u32 	%r5311, 255;
	shl.b32 	%r5312, %r5311, %r5309;
	not.b32 	%r5313, %r5312;
	st.local.v4.u32 	[%rd86], {%r50, %r49, %r48, %r47};
	st.local.v4.u32 	[%rd86+16], {%r51, %r52, %r53, %r54};
	ld.local.u32 	%r5314, [%rd89];
	and.b32  	%r5315, %r5314, %r5313;
	or.b32  	%r5316, %r5315, %r5310;
	st.local.u32 	[%rd89], %r5316;
	bra.uni 	BB3_334;

BB3_54:
	setp.eq.s32	%p35, %r1786, 90;
	@%p35 bra 	BB3_337;

	setp.eq.s32	%p36, %r1786, 91;
	@%p36 bra 	BB3_768;
	bra.uni 	BB3_56;

BB3_768:
	setp.eq.s32	%p626, %r38, 0;
	mov.u32 	%r11752, 0;
	@%p626 bra 	BB3_114;

	add.s32 	%r11752, %r38, -1;
	mov.u32 	%r6639, 8;
	// inline asm
	shf.r.wrap.b32 %r11740, %r50, %r49, %r6639;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11741, %r49, %r48, %r6639;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11742, %r48, %r47, %r6639;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11743, %r47, %r51, %r6639;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11739, %r51, %r52, %r6639;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11738, %r52, %r53, %r6639;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11737, %r53, %r54, %r6639;
	// inline asm
	mov.u32 	%r6638, 0;
	// inline asm
	shf.r.wrap.b32 %r11736, %r54, %r6638, %r6639;
	// inline asm
	bra.uni 	BB3_1027;

BB3_108:
	setp.eq.s32	%p8, %r1786, 122;
	@%p8 bra 	BB3_345;

	setp.eq.s32	%p9, %r1786, 123;
	@%p9 bra 	BB3_796;
	bra.uni 	BB3_110;

BB3_796:
	setp.eq.s32	%p645, %r38, 0;
	mov.u32 	%r11752, 0;
	@%p645 bra 	BB3_114;

	add.s32 	%r6760, %r38, -1;
	mov.u32 	%r6759, 8;
	// inline asm
	shf.r.wrap.b32 %r6728, %r50, %r49, %r6759;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6732, %r49, %r48, %r6759;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6736, %r48, %r47, %r6759;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6740, %r47, %r51, %r6759;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6744, %r51, %r52, %r6759;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6748, %r52, %r53, %r6759;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6752, %r53, %r54, %r6759;
	// inline asm
	mov.u32 	%r6758, 0;
	// inline asm
	shf.r.wrap.b32 %r6756, %r54, %r6758, %r6759;
	// inline asm
	mov.u32 	%r6761, 64;
	prmt.b32 	%r6762, %r50, %r50, %r6761;
	mov.u32 	%r6763, 1040;
	prmt.b32 	%r6764, %r6762, %r50, %r6763;
	mov.u32 	%r6765, 16912;
	prmt.b32 	%r6766, %r6764, %r50, %r6765;
	bfe.u32 	%r6767, %r6760, 2, 2;
	and.b32  	%r6768, %r6760, 3;
	shl.b32 	%r6769, %r6768, 3;
	mov.u32 	%r6770, 255;
	shl.b32 	%r6771, %r6770, %r6769;
	setp.eq.s32	%p646, %r6767, 0;
	selp.b32	%r11748, %r6771, 0, %p646;
	setp.eq.s32	%p647, %r6767, 1;
	selp.b32	%r11749, %r6771, 0, %p647;
	setp.eq.s32	%p648, %r6767, 2;
	selp.b32	%r11750, %r6771, 0, %p648;
	setp.eq.s32	%p649, %r6767, 3;
	selp.b32	%r11751, %r6771, 0, %p649;
	shr.u32 	%r6772, %r6760, 4;
	setp.eq.s32	%p650, %r6772, 0;
	selp.b32	%r6773, %r6766, 0, %p650;
	and.b32  	%r6774, %r6773, %r11748;
	or.b32  	%r11740, %r6774, %r6728;
	and.b32  	%r6775, %r6773, %r11749;
	or.b32  	%r11741, %r6775, %r6732;
	and.b32  	%r6776, %r6773, %r11750;
	or.b32  	%r11742, %r6776, %r6736;
	and.b32  	%r6777, %r6773, %r11751;
	or.b32  	%r11743, %r6777, %r6740;
	setp.eq.s32	%p651, %r6772, 1;
	selp.b32	%r6778, %r6766, 0, %p651;
	and.b32  	%r6779, %r6778, %r11748;
	or.b32  	%r11739, %r6779, %r6744;
	and.b32  	%r6780, %r6778, %r11749;
	or.b32  	%r11738, %r6780, %r6748;
	and.b32  	%r6781, %r6778, %r11750;
	or.b32  	%r11737, %r6781, %r6752;
	and.b32  	%r6782, %r6778, %r11751;
	or.b32  	%r11736, %r6782, %r6756;
	bra.uni 	BB3_1026;

BB3_9:
	setp.eq.s32	%p58, %r1786, 39;
	@%p58 bra 	BB3_10;
	bra.uni 	BB3_111;

BB3_10:
	setp.ge.u32	%p458, %r56, %r38;
	@%p458 bra 	BB3_111;

	and.b32  	%r5276, %r55, 3;
	shl.b32 	%r5277, %r5276, 3;
	mov.u32 	%r5278, 1;
	shl.b32 	%r5279, %r5278, %r5277;
	add.s32 	%r697, %r5279, -1;
	shr.u32 	%r5275, %r56, 2;
	setp.gt.s32	%p459, %r5275, 3;
	@%p459 bra 	BB3_486;

	setp.gt.s32	%p465, %r5275, 1;
	@%p465 bra 	BB3_483;

	setp.eq.s32	%p468, %r5275, 0;
	@%p468 bra 	BB3_14;
	bra.uni 	BB3_480;

BB3_14:
	and.b32  	%r11740, %r697, %r50;
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;
	mov.u32 	%r11741, %r11736;
	bra.uni 	BB3_482;

BB3_63:
	setp.eq.s32	%p31, %r1786, 99;
	@%p31 bra 	BB3_64;
	bra.uni 	BB3_111;

BB3_64:
	and.b32  	%r8799, %r50, 1077952576;
	shr.u32 	%r8800, %r8799, 1;
	and.b32  	%r8801, %r50, -2139062144;
	shr.u32 	%r8802, %r8801, 2;
	not.b32 	%r8803, %r8802;
	and.b32  	%r8804, %r8800, %r8803;
	and.b32  	%r8805, %r50, 522133279;
	add.s32 	%r8806, %r8805, 522133279;
	mov.u32 	%r8807, -84215046;
	sub.s32 	%r8808, %r8807, %r8805;
	and.b32  	%r8809, %r8804, %r8808;
	and.b32  	%r8810, %r8809, %r8806;
	or.b32  	%r8811, %r8810, %r50;
	and.b32  	%r8812, %r49, 1077952576;
	shr.u32 	%r8813, %r8812, 1;
	and.b32  	%r8814, %r49, -2139062144;
	shr.u32 	%r8815, %r8814, 2;
	not.b32 	%r8816, %r8815;
	and.b32  	%r8817, %r8813, %r8816;
	and.b32  	%r8818, %r49, 522133279;
	add.s32 	%r8819, %r8818, 522133279;
	sub.s32 	%r8820, %r8807, %r8818;
	and.b32  	%r8821, %r8817, %r8820;
	and.b32  	%r8822, %r8821, %r8819;
	or.b32  	%r11741, %r8822, %r49;
	and.b32  	%r8823, %r48, 1077952576;
	shr.u32 	%r8824, %r8823, 1;
	and.b32  	%r8825, %r48, -2139062144;
	shr.u32 	%r8826, %r8825, 2;
	not.b32 	%r8827, %r8826;
	and.b32  	%r8828, %r8824, %r8827;
	and.b32  	%r8829, %r48, 522133279;
	add.s32 	%r8830, %r8829, 522133279;
	sub.s32 	%r8831, %r8807, %r8829;
	and.b32  	%r8832, %r8828, %r8831;
	and.b32  	%r8833, %r8832, %r8830;
	or.b32  	%r11742, %r8833, %r48;
	and.b32  	%r8834, %r47, 1077952576;
	shr.u32 	%r8835, %r8834, 1;
	and.b32  	%r8836, %r47, -2139062144;
	shr.u32 	%r8837, %r8836, 2;
	not.b32 	%r8838, %r8837;
	and.b32  	%r8839, %r8835, %r8838;
	and.b32  	%r8840, %r47, 522133279;
	add.s32 	%r8841, %r8840, 522133279;
	sub.s32 	%r8842, %r8807, %r8840;
	and.b32  	%r8843, %r8839, %r8842;
	and.b32  	%r8844, %r8843, %r8841;
	or.b32  	%r11743, %r8844, %r47;
	and.b32  	%r8845, %r51, 1077952576;
	shr.u32 	%r8846, %r8845, 1;
	and.b32  	%r8847, %r51, -2139062144;
	shr.u32 	%r8848, %r8847, 2;
	not.b32 	%r8849, %r8848;
	and.b32  	%r8850, %r8846, %r8849;
	and.b32  	%r8851, %r51, 522133279;
	add.s32 	%r8852, %r8851, 522133279;
	sub.s32 	%r8853, %r8807, %r8851;
	and.b32  	%r8854, %r8850, %r8853;
	and.b32  	%r8855, %r8854, %r8852;
	or.b32  	%r11739, %r8855, %r51;
	and.b32  	%r8856, %r52, 1077952576;
	shr.u32 	%r8857, %r8856, 1;
	and.b32  	%r8858, %r52, -2139062144;
	shr.u32 	%r8859, %r8858, 2;
	not.b32 	%r8860, %r8859;
	and.b32  	%r8861, %r8857, %r8860;
	and.b32  	%r8862, %r52, 522133279;
	add.s32 	%r8863, %r8862, 522133279;
	sub.s32 	%r8864, %r8807, %r8862;
	and.b32  	%r8865, %r8861, %r8864;
	and.b32  	%r8866, %r8865, %r8863;
	or.b32  	%r11738, %r8866, %r52;
	and.b32  	%r8867, %r53, 1077952576;
	shr.u32 	%r8868, %r8867, 1;
	and.b32  	%r8869, %r53, -2139062144;
	shr.u32 	%r8870, %r8869, 2;
	not.b32 	%r8871, %r8870;
	and.b32  	%r8872, %r8868, %r8871;
	and.b32  	%r8873, %r53, 522133279;
	add.s32 	%r8874, %r8873, 522133279;
	sub.s32 	%r8875, %r8807, %r8873;
	and.b32  	%r8876, %r8872, %r8875;
	and.b32  	%r8877, %r8876, %r8874;
	or.b32  	%r11737, %r8877, %r53;
	and.b32  	%r8878, %r54, 1077952576;
	shr.u32 	%r8879, %r8878, 1;
	and.b32  	%r8880, %r54, -2139062144;
	shr.u32 	%r8881, %r8880, 2;
	not.b32 	%r8882, %r8881;
	and.b32  	%r8883, %r8879, %r8882;
	and.b32  	%r8884, %r54, 522133279;
	add.s32 	%r8885, %r8884, 522133279;
	sub.s32 	%r8886, %r8807, %r8884;
	and.b32  	%r8887, %r8883, %r8886;
	and.b32  	%r8888, %r8887, %r8885;
	or.b32  	%r11736, %r8888, %r54;
	and.b32  	%r8889, %r8811, 64;
	shr.u32 	%r8890, %r8889, 1;
	and.b32  	%r8891, %r8811, 128;
	shr.u32 	%r8892, %r8891, 2;
	not.b32 	%r8893, %r8892;
	and.b32  	%r8894, %r8890, %r8893;
	and.b32  	%r8895, %r8811, 522133279;
	add.s32 	%r8896, %r8895, 31;
	sub.s32 	%r8897, %r8807, %r8895;
	and.b32  	%r8898, %r8894, %r8897;
	and.b32  	%r8899, %r8898, %r8896;
	not.b32 	%r8900, %r8899;
	or.b32  	%r8901, %r8900, -33;
	and.b32  	%r11740, %r8901, %r8811;
	bra.uni 	BB3_1026;

BB3_36:
	setp.eq.s32	%p45, %r1786, 75;
	@%p45 bra 	BB3_37;
	bra.uni 	BB3_111;

BB3_37:
	setp.lt.u32	%p258, %r38, 2;
	@%p258 bra 	BB3_111;

	st.local.v4.u32 	[%rd86], {%r50, %r49, %r48, %r47};
	st.local.v4.u32 	[%rd86+16], {%r51, %r52, %r53, %r54};
	add.s32 	%r4090, %r38, -2;
	cvt.s64.s32	%rd137, %r4090;
	add.s64 	%rd140, %rd86, %rd137;
	ld.local.u8 	%rs12, [%rd140];
	ld.local.u8 	%rs13, [%rd140+1];
	st.local.u8 	[%rd140], %rs13;
	st.local.u8 	[%rd140+1], %rs12;
	bra.uni 	BB3_334;

BB3_87:
	setp.eq.s32	%p18, %r1786, 114;
	@%p18 bra 	BB3_88;
	bra.uni 	BB3_111;

BB3_88:
	mov.u32 	%r8014, 32;
	sub.s32 	%r8013, %r8014, %r38;
	setp.gt.s32	%p745, %r8013, 15;
	@%p745 bra 	BB3_968;

	setp.gt.s32	%p769, %r8013, 7;
	@%p769 bra 	BB3_952;

	setp.gt.s32	%p781, %r8013, 3;
	@%p781 bra 	BB3_945;

	setp.eq.s32	%p787, %r8013, 1;
	@%p787 bra 	BB3_1017;

	setp.eq.s32	%p788, %r8013, 2;
	@%p788 bra 	BB3_1016;
	bra.uni 	BB3_93;

BB3_1016:
	mov.u32 	%r8515, 16;
	// inline asm
	shf.r.wrap.b32 %r11748, %r53, %r54, %r8515;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r52, %r53, %r8515;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r51, %r52, %r8515;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r47, %r51, %r8515;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r48, %r47, %r8515;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r49, %r48, %r8515;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r50, %r49, %r8515;
	// inline asm
	mov.u32 	%r8513, 0;
	// inline asm
	shf.r.wrap.b32 %r11747, %r8513, %r50, %r8515;
	// inline asm
	bra.uni 	BB3_1020;

BB3_21:
	setp.eq.s32	%p52, %r1786, 46;
	@%p52 bra 	BB3_22;
	bra.uni 	BB3_111;

BB3_22:
	add.s32 	%r3953, %r56, 1;
	setp.ge.u32	%p250, %r3953, %r38;
	@%p250 bra 	BB3_111;

	mov.u32 	%r3985, 8;
	// inline asm
	shf.r.wrap.b32 %r3954, %r50, %r49, %r3985;
	// inline asm
	st.local.u32 	[%rd86], %r3954;
	// inline asm
	shf.r.wrap.b32 %r3958, %r49, %r48, %r3985;
	// inline asm
	st.local.u32 	[%rd86+4], %r3958;
	// inline asm
	shf.r.wrap.b32 %r3962, %r48, %r47, %r3985;
	// inline asm
	st.local.u32 	[%rd86+8], %r3962;
	// inline asm
	shf.r.wrap.b32 %r3966, %r47, %r51, %r3985;
	// inline asm
	st.local.u32 	[%rd86+12], %r3966;
	// inline asm
	shf.r.wrap.b32 %r3970, %r51, %r52, %r3985;
	// inline asm
	st.local.u32 	[%rd86+16], %r3970;
	// inline asm
	shf.r.wrap.b32 %r3974, %r52, %r53, %r3985;
	// inline asm
	st.local.u32 	[%rd86+20], %r3974;
	// inline asm
	shf.r.wrap.b32 %r3978, %r53, %r54, %r3985;
	// inline asm
	st.local.u32 	[%rd86+24], %r3978;
	mov.u32 	%r3984, 0;
	// inline asm
	shf.r.wrap.b32 %r3982, %r54, %r3984, %r3985;
	// inline asm
	st.local.u32 	[%rd86+28], %r3982;
	and.b32  	%r3986, %r55, 3;
	shl.b32 	%r3987, %r3986, 3;
	mov.u32 	%r3988, 255;
	shl.b32 	%r3989, %r3988, %r3987;
	not.b32 	%r3990, %r3989;
	st.local.v4.u32 	[%rd87], {%r50, %r49, %r48, %r47};
	st.local.v4.u32 	[%rd87+16], {%r51, %r52, %r53, %r54};
	ld.local.u32 	%r3991, [%rd88];
	and.b32  	%r3992, %r3991, %r3990;
	ld.local.u32 	%r3993, [%rd89];
	and.b32  	%r3994, %r3993, %r3989;
	or.b32  	%r3995, %r3994, %r3992;
	st.local.u32 	[%rd88], %r3995;
	bra.uni 	BB3_325;

BB3_77:
	setp.eq.s32	%p25, %r1786, 107;
	@%p25 bra 	BB3_78;
	bra.uni 	BB3_111;

BB3_78:
	setp.lt.u32	%p259, %r38, 2;
	@%p259 bra 	BB3_111;

	and.b32  	%r4099, %r50, -65536;
	shl.b32 	%r4100, %r50, 8;
	and.b32  	%r4101, %r4100, 65280;
	or.b32  	%r4102, %r4101, %r4099;
	bfe.u32 	%r4103, %r50, 8, 8;
	or.b32  	%r11740, %r4102, %r4103;
	mov.u32 	%r11736, %r54;
	mov.u32 	%r11737, %r53;
	mov.u32 	%r11738, %r52;
	mov.u32 	%r11739, %r51;
	bra.uni 	BB3_112;

BB3_46:
	setp.eq.s32	%p39, %r1786, 89;
	@%p39 bra 	BB3_47;
	bra.uni 	BB3_111;

BB3_47:
	setp.gt.u32	%p127, %r56, %r38;
	add.s32 	%r11752, %r56, %r38;
	setp.gt.u32	%p128, %r11752, 31;
	or.pred  	%p129, %p127, %p128;
	@%p129 bra 	BB3_111;

	setp.gt.s32	%p130, %r56, 15;
	@%p130 bra 	BB3_152;

	setp.gt.s32	%p154, %r56, 7;
	@%p154 bra 	BB3_136;

	setp.gt.s32	%p166, %r56, 3;
	@%p166 bra 	BB3_129;

	setp.gt.s32	%p172, %r56, 1;
	@%p172 bra 	BB3_126;

	setp.eq.s32	%p175, %r56, 0;
	@%p175 bra 	BB3_53;
	bra.uni 	BB3_124;

BB3_53:
	mov.u32 	%r11744, %r51;
	mov.u32 	%r11745, %r52;
	mov.u32 	%r11746, %r53;
	mov.u32 	%r11747, %r54;
	mov.u32 	%r11748, %r50;
	mov.u32 	%r11749, %r49;
	mov.u32 	%r11750, %r48;
	mov.u32 	%r11751, %r47;
	bra.uni 	BB3_202;

BB3_101:
	setp.eq.s32	%p12, %r1786, 121;
	@%p12 bra 	BB3_102;
	bra.uni 	BB3_111;

BB3_102:
	setp.gt.u32	%p188, %r56, %r38;
	add.s32 	%r11752, %r56, %r38;
	setp.gt.u32	%p189, %r11752, 31;
	or.pred  	%p190, %p188, %p189;
	@%p190 bra 	BB3_111;

	and.b32  	%r3327, %r55, 3;
	shl.b32 	%r3328, %r3327, 3;
	mov.u32 	%r3329, 1;
	shl.b32 	%r3330, %r3329, %r3328;
	add.s32 	%r248, %r3330, -1;
	shr.u32 	%r3326, %r56, 2;
	setp.gt.s32	%p191, %r3326, 3;
	@%p191 bra 	BB3_229;

	setp.gt.s32	%p197, %r3326, 1;
	@%p197 bra 	BB3_226;

	setp.eq.s32	%p200, %r3326, 0;
	@%p200 bra 	BB3_106;
	bra.uni 	BB3_224;

BB3_106:
	and.b32  	%r11748, %r248, %r50;
	mov.u32 	%r11744, 0;
	mov.u32 	%r11745, %r11744;
	mov.u32 	%r11746, %r11744;
	mov.u32 	%r11747, %r11744;
	mov.u32 	%r11749, %r11744;
	bra.uni 	BB3_107;

BB3_332:
	setp.gt.u32	%p255, %r38, %r58;
	setp.lt.u32	%p256, %r56, %r38;
	and.pred  	%p257, %p256, %p255;
	mov.u32 	%r11736, %r54;
	mov.u32 	%r11737, %r53;
	mov.u32 	%r11738, %r52;
	mov.u32 	%r11739, %r51;
	mov.u32 	%r11740, %r50;
	mov.u32 	%r11741, %r49;
	mov.u32 	%r11742, %r48;
	mov.u32 	%r11743, %r47;
	mov.u32 	%r11752, %r38;
	@!%p257 bra 	BB3_1027;
	bra.uni 	BB3_333;

BB3_333:
	st.local.v4.u32 	[%rd86], {%r50, %r49, %r48, %r47};
	st.local.v4.u32 	[%rd86+16], {%r51, %r52, %r53, %r54};
	cvt.u64.u32	%rd133, %r56;
	add.s64 	%rd134, %rd86, %rd133;
	ld.local.u8 	%rs10, [%rd134];
	cvt.u64.u32	%rd135, %r58;
	add.s64 	%rd136, %rd86, %rd135;
	ld.local.u8 	%rs11, [%rd136];
	st.local.u8 	[%rd134], %rs11;
	st.local.u8 	[%rd136], %rs10;
	bra.uni 	BB3_334;

BB3_17:
	setp.eq.s32	%p56, %r1786, 44;
	@%p56 bra 	BB3_18;
	bra.uni 	BB3_111;

BB3_18:
	setp.lt.u32	%p247, %r56, %r38;
	setp.ne.s32	%p248, %r56, 0;
	and.pred  	%p249, %p248, %p247;
	mov.u32 	%r11736, %r54;
	mov.u32 	%r11737, %r53;
	mov.u32 	%r11738, %r52;
	mov.u32 	%r11739, %r51;
	mov.u32 	%r11740, %r50;
	mov.u32 	%r11741, %r49;
	mov.u32 	%r11742, %r48;
	mov.u32 	%r11743, %r47;
	mov.u32 	%r11752, %r38;
	@!%p249 bra 	BB3_1027;
	bra.uni 	BB3_324;

BB3_324:
	mov.u32 	%r3934, 24;
	// inline asm
	shf.r.wrap.b32 %r3903, %r53, %r54, %r3934;
	// inline asm
	st.local.u32 	[%rd86+28], %r3903;
	// inline asm
	shf.r.wrap.b32 %r3907, %r52, %r53, %r3934;
	// inline asm
	st.local.u32 	[%rd86+24], %r3907;
	// inline asm
	shf.r.wrap.b32 %r3911, %r51, %r52, %r3934;
	// inline asm
	st.local.u32 	[%rd86+20], %r3911;
	// inline asm
	shf.r.wrap.b32 %r3915, %r47, %r51, %r3934;
	// inline asm
	st.local.u32 	[%rd86+16], %r3915;
	// inline asm
	shf.r.wrap.b32 %r3919, %r48, %r47, %r3934;
	// inline asm
	st.local.u32 	[%rd86+12], %r3919;
	// inline asm
	shf.r.wrap.b32 %r3923, %r49, %r48, %r3934;
	// inline asm
	st.local.u32 	[%rd86+8], %r3923;
	// inline asm
	shf.r.wrap.b32 %r3927, %r50, %r49, %r3934;
	// inline asm
	st.local.u32 	[%rd86+4], %r3927;
	mov.u32 	%r3932, 0;
	// inline asm
	shf.r.wrap.b32 %r3931, %r3932, %r50, %r3934;
	// inline asm
	st.local.u32 	[%rd86], %r3931;
	and.b32  	%r3935, %r55, 3;
	shl.b32 	%r3936, %r3935, 3;
	mov.u32 	%r3937, 255;
	shl.b32 	%r3938, %r3937, %r3936;
	not.b32 	%r3939, %r3938;
	st.local.v4.u32 	[%rd87], {%r50, %r49, %r48, %r47};
	st.local.v4.u32 	[%rd87+16], {%r51, %r52, %r53, %r54};
	ld.local.u32 	%r3940, [%rd88];
	and.b32  	%r3941, %r3940, %r3939;
	ld.local.u32 	%r3942, [%rd89];
	and.b32  	%r3943, %r3942, %r3938;
	or.b32  	%r3944, %r3943, %r3941;
	st.local.u32 	[%rd88], %r3944;

BB3_325:
	ld.local.v4.u32 	{%r11740, %r11741, %r11742, %r11743}, [%rd87];
	ld.local.v4.u32 	{%r11739, %r11738, %r11737, %r11736}, [%rd87+16];
	bra.uni 	BB3_1026;

BB3_924:
	add.s32 	%r11752, %r38, %r38;
	setp.gt.u32	%p733, %r11752, 31;
	@%p733 bra 	BB3_111;

	shr.u32 	%r7834, %r38, 2;
	and.b32  	%r7835, %r38, 3;
	mov.u32 	%r7836, 4;
	sub.s32 	%r7837, %r7836, %r7835;
	shl.b32 	%r7838, %r7837, 2;
	mov.u32 	%r7839, 1985229328;
	shr.u32 	%r7840, %r7839, %r7838;
	and.b32  	%r1340, %r7840, 65535;
	mov.u32 	%r11720, 0;
	setp.gt.s32	%p734, %r7834, 3;
	@%p734 bra 	BB3_933;

	setp.gt.s32	%p740, %r7834, 1;
	@%p740 bra 	BB3_930;

	setp.eq.s32	%p743, %r7834, 0;
	@%p743 bra 	BB3_943;
	bra.uni 	BB3_928;

BB3_943:
	// inline asm
	prmt.b32 %r11727, %r53, %r54, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11726, %r52, %r53, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11725, %r51, %r52, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11724, %r47, %r51, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11723, %r48, %r47, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11722, %r49, %r48, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11721, %r50, %r49, %r1340;
	// inline asm
	mov.u32 	%r8010, 0;
	// inline asm
	prmt.b32 %r11720, %r8010, %r50, %r1340;
	// inline asm
	bra.uni 	BB3_944;

BB3_67:
	setp.eq.s32	%p29, %r1786, 102;
	@%p29 bra 	BB3_68;
	bra.uni 	BB3_111;

BB3_68:
	add.s32 	%r11752, %r38, %r38;
	setp.gt.u32	%p660, %r11752, 31;
	@%p660 bra 	BB3_111;

	mov.u32 	%r6847, 32;
	sub.s32 	%r6846, %r6847, %r38;
	mov.u32 	%r11694, 0;
	setp.gt.s32	%p661, %r6846, 15;
	@%p661 bra 	BB3_830;

	setp.gt.s32	%p685, %r6846, 7;
	@%p685 bra 	BB3_814;

	setp.gt.s32	%p697, %r6846, 3;
	@%p697 bra 	BB3_807;

	setp.gt.s32	%p703, %r6846, 1;
	@%p703 bra 	BB3_804;

	setp.eq.s32	%p706, %r6846, 0;
	@%p706 bra 	BB3_74;
	bra.uni 	BB3_802;

BB3_74:
	mov.u32 	%r11751, %r51;
	mov.u32 	%r11750, %r52;
	mov.u32 	%r11749, %r53;
	mov.u32 	%r11748, %r54;
	mov.u32 	%r11747, %r50;
	mov.u32 	%r11746, %r49;
	mov.u32 	%r11745, %r48;
	mov.u32 	%r11744, %r47;
	bra.uni 	BB3_881;

BB3_330:
	setp.ge.u32	%p254, %r56, %r38;
	@%p254 bra 	BB3_111;

	and.b32  	%r4063, %r55, 3;
	shl.b32 	%r4064, %r4063, 3;
	mov.u32 	%r4065, 255;
	shl.b32 	%r4066, %r4065, %r4064;
	not.b32 	%r4067, %r4066;
	st.local.v4.u32 	[%rd86], {%r50, %r49, %r48, %r47};
	st.local.v4.u32 	[%rd86+16], {%r51, %r52, %r53, %r54};
	ld.local.u32 	%r4068, [%rd89];
	and.b32  	%r4069, %r4068, %r4067;
	and.b32  	%r4070, %r4068, %r4066;
	shl.b32 	%r4071, %r4070, 1;
	and.b32  	%r4072, %r4071, %r4066;
	or.b32  	%r4073, %r4072, %r4069;
	st.local.u32 	[%rd89], %r4073;
	bra.uni 	BB3_334;

BB3_41:
	setp.eq.s32	%p43, %r1786, 82;
	@%p43 bra 	BB3_42;
	bra.uni 	BB3_111;

BB3_42:
	setp.ge.u32	%p253, %r56, %r38;
	@%p253 bra 	BB3_111;

	and.b32  	%r4044, %r55, 3;
	shl.b32 	%r4045, %r4044, 3;
	mov.u32 	%r4046, 255;
	shl.b32 	%r4047, %r4046, %r4045;
	not.b32 	%r4048, %r4047;
	st.local.v4.u32 	[%rd86], {%r50, %r49, %r48, %r47};
	st.local.v4.u32 	[%rd86+16], {%r51, %r52, %r53, %r54};
	ld.local.u32 	%r4049, [%rd89];
	and.b32  	%r4050, %r4049, %r4048;
	and.b32  	%r4051, %r4049, %r4047;
	shr.u32 	%r4052, %r4051, 1;
	and.b32  	%r4053, %r4052, %r4047;
	or.b32  	%r4054, %r4053, %r4050;
	st.local.u32 	[%rd89], %r4054;

BB3_334:
	ld.local.v4.u32 	{%r11740, %r11741, %r11742, %r11743}, [%rd86];
	ld.local.v4.u32 	{%r11739, %r11738, %r11737, %r11736}, [%rd86+16];
	bra.uni 	BB3_1026;

BB3_478:
	mov.u32 	%r5084, 0;
	mov.u32 	%r5097, 8;
	// inline asm
	bfe.u32 %r4970, %r50, %r5084, %r5097;
	// inline asm
	setp.eq.s32	%p385, %r4970, %r56;
	selp.u32	%r5098, 1, 0, %p385;
	// inline asm
	bfe.u32 %r4974, %r50, %r5097, %r5097;
	// inline asm
	setp.eq.s32	%p386, %r4974, %r56;
	or.b32  	%r5099, %r5098, 2;
	selp.b32	%r5100, %r5099, %r5098, %p386;
	mov.u32 	%r5092, 16;
	// inline asm
	bfe.u32 %r4978, %r50, %r5092, %r5097;
	// inline asm
	setp.eq.s32	%p387, %r4978, %r56;
	or.b32  	%r5101, %r5100, 4;
	selp.b32	%r5102, %r5101, %r5100, %p387;
	mov.u32 	%r5096, 24;
	// inline asm
	bfe.u32 %r4982, %r50, %r5096, %r5097;
	// inline asm
	setp.eq.s32	%p388, %r4982, %r56;
	or.b32  	%r5103, %r5102, 8;
	selp.b32	%r681, %r5103, %r5102, %p388;
	// inline asm
	bfe.u32 %r4986, %r49, %r5084, %r5097;
	// inline asm
	setp.eq.s32	%p389, %r4986, %r56;
	selp.u32	%r5104, 1, 0, %p389;
	// inline asm
	bfe.u32 %r4990, %r49, %r5097, %r5097;
	// inline asm
	setp.eq.s32	%p390, %r4990, %r56;
	or.b32  	%r5105, %r5104, 2;
	selp.b32	%r5106, %r5105, %r5104, %p390;
	// inline asm
	bfe.u32 %r4994, %r49, %r5092, %r5097;
	// inline asm
	setp.eq.s32	%p391, %r4994, %r56;
	or.b32  	%r5107, %r5106, 4;
	selp.b32	%r5108, %r5107, %r5106, %p391;
	// inline asm
	bfe.u32 %r4998, %r49, %r5096, %r5097;
	// inline asm
	setp.eq.s32	%p392, %r4998, %r56;
	or.b32  	%r5109, %r5108, 8;
	selp.b32	%r682, %r5109, %r5108, %p392;
	// inline asm
	bfe.u32 %r5002, %r48, %r5084, %r5097;
	// inline asm
	setp.eq.s32	%p393, %r5002, %r56;
	selp.u32	%r5110, 1, 0, %p393;
	// inline asm
	bfe.u32 %r5006, %r48, %r5097, %r5097;
	// inline asm
	setp.eq.s32	%p394, %r5006, %r56;
	or.b32  	%r5111, %r5110, 2;
	selp.b32	%r5112, %r5111, %r5110, %p394;
	// inline asm
	bfe.u32 %r5010, %r48, %r5092, %r5097;
	// inline asm
	setp.eq.s32	%p395, %r5010, %r56;
	or.b32  	%r5113, %r5112, 4;
	selp.b32	%r5114, %r5113, %r5112, %p395;
	// inline asm
	bfe.u32 %r5014, %r48, %r5096, %r5097;
	// inline asm
	setp.eq.s32	%p396, %r5014, %r56;
	or.b32  	%r5115, %r5114, 8;
	selp.b32	%r683, %r5115, %r5114, %p396;
	// inline asm
	bfe.u32 %r5018, %r47, %r5084, %r5097;
	// inline asm
	setp.eq.s32	%p397, %r5018, %r56;
	selp.u32	%r5116, 1, 0, %p397;
	// inline asm
	bfe.u32 %r5022, %r47, %r5097, %r5097;
	// inline asm
	setp.eq.s32	%p398, %r5022, %r56;
	or.b32  	%r5117, %r5116, 2;
	selp.b32	%r5118, %r5117, %r5116, %p398;
	// inline asm
	bfe.u32 %r5026, %r47, %r5092, %r5097;
	// inline asm
	setp.eq.s32	%p399, %r5026, %r56;
	or.b32  	%r5119, %r5118, 4;
	selp.b32	%r5120, %r5119, %r5118, %p399;
	// inline asm
	bfe.u32 %r5030, %r47, %r5096, %r5097;
	// inline asm
	setp.eq.s32	%p400, %r5030, %r56;
	or.b32  	%r5121, %r5120, 8;
	selp.b32	%r684, %r5121, %r5120, %p400;
	// inline asm
	bfe.u32 %r5034, %r51, %r5084, %r5097;
	// inline asm
	setp.eq.s32	%p401, %r5034, %r56;
	selp.u32	%r5122, 1, 0, %p401;
	// inline asm
	bfe.u32 %r5038, %r51, %r5097, %r5097;
	// inline asm
	setp.eq.s32	%p402, %r5038, %r56;
	or.b32  	%r5123, %r5122, 2;
	selp.b32	%r5124, %r5123, %r5122, %p402;
	// inline asm
	bfe.u32 %r5042, %r51, %r5092, %r5097;
	// inline asm
	setp.eq.s32	%p403, %r5042, %r56;
	or.b32  	%r5125, %r5124, 4;
	selp.b32	%r5126, %r5125, %r5124, %p403;
	// inline asm
	bfe.u32 %r5046, %r51, %r5096, %r5097;
	// inline asm
	setp.eq.s32	%p404, %r5046, %r56;
	or.b32  	%r5127, %r5126, 8;
	selp.b32	%r685, %r5127, %r5126, %p404;
	// inline asm
	bfe.u32 %r5050, %r52, %r5084, %r5097;
	// inline asm
	setp.eq.s32	%p405, %r5050, %r56;
	selp.u32	%r5128, 1, 0, %p405;
	// inline asm
	bfe.u32 %r5054, %r52, %r5097, %r5097;
	// inline asm
	setp.eq.s32	%p406, %r5054, %r56;
	or.b32  	%r5129, %r5128, 2;
	selp.b32	%r5130, %r5129, %r5128, %p406;
	// inline asm
	bfe.u32 %r5058, %r52, %r5092, %r5097;
	// inline asm
	setp.eq.s32	%p407, %r5058, %r56;
	or.b32  	%r5131, %r5130, 4;
	selp.b32	%r5132, %r5131, %r5130, %p407;
	// inline asm
	bfe.u32 %r5062, %r52, %r5096, %r5097;
	// inline asm
	setp.eq.s32	%p408, %r5062, %r56;
	or.b32  	%r5133, %r5132, 8;
	selp.b32	%r686, %r5133, %r5132, %p408;
	// inline asm
	bfe.u32 %r5066, %r53, %r5084, %r5097;
	// inline asm
	setp.eq.s32	%p409, %r5066, %r56;
	selp.u32	%r5134, 1, 0, %p409;
	// inline asm
	bfe.u32 %r5070, %r53, %r5097, %r5097;
	// inline asm
	setp.eq.s32	%p410, %r5070, %r56;
	or.b32  	%r5135, %r5134, 2;
	selp.b32	%r5136, %r5135, %r5134, %p410;
	// inline asm
	bfe.u32 %r5074, %r53, %r5092, %r5097;
	// inline asm
	setp.eq.s32	%p411, %r5074, %r56;
	or.b32  	%r5137, %r5136, 4;
	selp.b32	%r5138, %r5137, %r5136, %p411;
	// inline asm
	bfe.u32 %r5078, %r53, %r5096, %r5097;
	// inline asm
	setp.eq.s32	%p412, %r5078, %r56;
	or.b32  	%r5139, %r5138, 8;
	selp.b32	%r687, %r5139, %r5138, %p412;
	// inline asm
	bfe.u32 %r5082, %r54, %r5084, %r5097;
	// inline asm
	setp.eq.s32	%p413, %r5082, %r56;
	selp.u32	%r5140, 1, 0, %p413;
	// inline asm
	bfe.u32 %r5086, %r54, %r5097, %r5097;
	// inline asm
	setp.eq.s32	%p414, %r5086, %r56;
	or.b32  	%r5141, %r5140, 2;
	selp.b32	%r5142, %r5141, %r5140, %p414;
	// inline asm
	bfe.u32 %r5090, %r54, %r5092, %r5097;
	// inline asm
	setp.eq.s32	%p415, %r5090, %r56;
	or.b32  	%r5143, %r5142, 4;
	selp.b32	%r5144, %r5143, %r5142, %p415;
	// inline asm
	bfe.u32 %r5094, %r54, %r5096, %r5097;
	// inline asm
	setp.eq.s32	%p416, %r5094, %r56;
	or.b32  	%r5145, %r5144, 8;
	selp.b32	%r688, %r5145, %r5144, %p416;
	add.s32 	%r5146, %r682, %r681;
	add.s32 	%r5147, %r5146, %r683;
	add.s32 	%r5148, %r5147, %r684;
	add.s32 	%r5149, %r5148, %r685;
	add.s32 	%r5150, %r5149, %r686;
	add.s32 	%r5151, %r5150, %r687;
	neg.s32 	%r5152, %r688;
	setp.eq.s32	%p417, %r5151, %r5152;
	@%p417 bra 	BB3_111;

	and.b32  	%r5153, %r681, 1;
	setp.eq.b32	%p418, %r5153, 1;
	not.pred 	%p419, %p418;
	and.b32  	%r5154, %r50, -256;
	or.b32  	%r5155, %r58, %r5154;
	selp.b32	%r5156, %r50, %r5155, %p419;
	and.b32  	%r5157, %r681, 2;
	setp.eq.s32	%p420, %r5157, 0;
	and.b32  	%r5158, %r5156, -65281;
	shl.b32 	%r5159, %r58, 8;
	or.b32  	%r5160, %r5158, %r5159;
	selp.b32	%r5161, %r5156, %r5160, %p420;
	and.b32  	%r5162, %r681, 4;
	setp.eq.s32	%p421, %r5162, 0;
	and.b32  	%r5163, %r5161, -16711681;
	shl.b32 	%r5164, %r58, 16;
	or.b32  	%r5165, %r5163, %r5164;
	selp.b32	%r5166, %r5161, %r5165, %p421;
	and.b32  	%r5167, %r681, 8;
	setp.eq.s32	%p422, %r5167, 0;
	and.b32  	%r5168, %r5166, 16777215;
	prmt.b32 	%r5169, %r57, %r5168, 1620;
	selp.b32	%r11740, %r5166, %r5169, %p422;
	and.b32  	%r5170, %r682, 1;
	setp.eq.b32	%p423, %r5170, 1;
	not.pred 	%p424, %p423;
	and.b32  	%r5171, %r49, -256;
	or.b32  	%r5172, %r58, %r5171;
	selp.b32	%r5173, %r49, %r5172, %p424;
	and.b32  	%r5174, %r682, 2;
	setp.eq.s32	%p425, %r5174, 0;
	and.b32  	%r5175, %r5173, -65281;
	or.b32  	%r5176, %r5175, %r5159;
	selp.b32	%r5177, %r5173, %r5176, %p425;
	and.b32  	%r5178, %r682, 4;
	setp.eq.s32	%p426, %r5178, 0;
	and.b32  	%r5179, %r5177, -16711681;
	or.b32  	%r5180, %r5179, %r5164;
	selp.b32	%r5181, %r5177, %r5180, %p426;
	and.b32  	%r5182, %r682, 8;
	setp.eq.s32	%p427, %r5182, 0;
	and.b32  	%r5183, %r5181, 16777215;
	prmt.b32 	%r5184, %r57, %r5183, 1620;
	selp.b32	%r11741, %r5181, %r5184, %p427;
	and.b32  	%r5185, %r683, 1;
	setp.eq.b32	%p428, %r5185, 1;
	not.pred 	%p429, %p428;
	and.b32  	%r5186, %r48, -256;
	or.b32  	%r5187, %r58, %r5186;
	selp.b32	%r5188, %r48, %r5187, %p429;
	and.b32  	%r5189, %r683, 2;
	setp.eq.s32	%p430, %r5189, 0;
	and.b32  	%r5190, %r5188, -65281;
	or.b32  	%r5191, %r5190, %r5159;
	selp.b32	%r5192, %r5188, %r5191, %p430;
	and.b32  	%r5193, %r683, 4;
	setp.eq.s32	%p431, %r5193, 0;
	and.b32  	%r5194, %r5192, -16711681;
	or.b32  	%r5195, %r5194, %r5164;
	selp.b32	%r5196, %r5192, %r5195, %p431;
	and.b32  	%r5197, %r683, 8;
	setp.eq.s32	%p432, %r5197, 0;
	and.b32  	%r5198, %r5196, 16777215;
	prmt.b32 	%r5199, %r57, %r5198, 1620;
	selp.b32	%r11742, %r5196, %r5199, %p432;
	and.b32  	%r5200, %r684, 1;
	setp.eq.b32	%p433, %r5200, 1;
	not.pred 	%p434, %p433;
	and.b32  	%r5201, %r47, -256;
	or.b32  	%r5202, %r58, %r5201;
	selp.b32	%r5203, %r47, %r5202, %p434;
	and.b32  	%r5204, %r684, 2;
	setp.eq.s32	%p435, %r5204, 0;
	and.b32  	%r5205, %r5203, -65281;
	or.b32  	%r5206, %r5205, %r5159;
	selp.b32	%r5207, %r5203, %r5206, %p435;
	and.b32  	%r5208, %r684, 4;
	setp.eq.s32	%p436, %r5208, 0;
	and.b32  	%r5209, %r5207, -16711681;
	or.b32  	%r5210, %r5209, %r5164;
	selp.b32	%r5211, %r5207, %r5210, %p436;
	and.b32  	%r5212, %r684, 8;
	setp.eq.s32	%p437, %r5212, 0;
	and.b32  	%r5213, %r5211, 16777215;
	prmt.b32 	%r5214, %r57, %r5213, 1620;
	selp.b32	%r11743, %r5211, %r5214, %p437;
	and.b32  	%r5215, %r685, 1;
	setp.eq.b32	%p438, %r5215, 1;
	not.pred 	%p439, %p438;
	and.b32  	%r5216, %r51, -256;
	or.b32  	%r5217, %r58, %r5216;
	selp.b32	%r5218, %r51, %r5217, %p439;
	and.b32  	%r5219, %r685, 2;
	setp.eq.s32	%p440, %r5219, 0;
	and.b32  	%r5220, %r5218, -65281;
	or.b32  	%r5221, %r5220, %r5159;
	selp.b32	%r5222, %r5218, %r5221, %p440;
	and.b32  	%r5223, %r685, 4;
	setp.eq.s32	%p441, %r5223, 0;
	and.b32  	%r5224, %r5222, -16711681;
	or.b32  	%r5225, %r5224, %r5164;
	selp.b32	%r5226, %r5222, %r5225, %p441;
	and.b32  	%r5227, %r685, 8;
	setp.eq.s32	%p442, %r5227, 0;
	and.b32  	%r5228, %r5226, 16777215;
	prmt.b32 	%r5229, %r57, %r5228, 1620;
	selp.b32	%r11739, %r5226, %r5229, %p442;
	and.b32  	%r5230, %r686, 1;
	setp.eq.b32	%p443, %r5230, 1;
	not.pred 	%p444, %p443;
	and.b32  	%r5231, %r52, -256;
	or.b32  	%r5232, %r58, %r5231;
	selp.b32	%r5233, %r52, %r5232, %p444;
	and.b32  	%r5234, %r686, 2;
	setp.eq.s32	%p445, %r5234, 0;
	and.b32  	%r5235, %r5233, -65281;
	or.b32  	%r5236, %r5235, %r5159;
	selp.b32	%r5237, %r5233, %r5236, %p445;
	and.b32  	%r5238, %r686, 4;
	setp.eq.s32	%p446, %r5238, 0;
	and.b32  	%r5239, %r5237, -16711681;
	or.b32  	%r5240, %r5239, %r5164;
	selp.b32	%r5241, %r5237, %r5240, %p446;
	and.b32  	%r5242, %r686, 8;
	setp.eq.s32	%p447, %r5242, 0;
	and.b32  	%r5243, %r5241, 16777215;
	prmt.b32 	%r5244, %r57, %r5243, 1620;
	selp.b32	%r11738, %r5241, %r5244, %p447;
	and.b32  	%r5245, %r687, 1;
	setp.eq.b32	%p448, %r5245, 1;
	not.pred 	%p449, %p448;
	and.b32  	%r5246, %r53, -256;
	or.b32  	%r5247, %r58, %r5246;
	selp.b32	%r5248, %r53, %r5247, %p449;
	and.b32  	%r5249, %r687, 2;
	setp.eq.s32	%p450, %r5249, 0;
	and.b32  	%r5250, %r5248, -65281;
	or.b32  	%r5251, %r5250, %r5159;
	selp.b32	%r5252, %r5248, %r5251, %p450;
	and.b32  	%r5253, %r687, 4;
	setp.eq.s32	%p451, %r5253, 0;
	and.b32  	%r5254, %r5252, -16711681;
	or.b32  	%r5255, %r5254, %r5164;
	selp.b32	%r5256, %r5252, %r5255, %p451;
	and.b32  	%r5257, %r687, 8;
	setp.eq.s32	%p452, %r5257, 0;
	and.b32  	%r5258, %r5256, 16777215;
	prmt.b32 	%r5259, %r57, %r5258, 1620;
	selp.b32	%r11737, %r5256, %r5259, %p452;
	and.b32  	%r5260, %r688, 1;
	setp.eq.b32	%p453, %r5260, 1;
	not.pred 	%p454, %p453;
	and.b32  	%r5261, %r54, -256;
	or.b32  	%r5262, %r58, %r5261;
	selp.b32	%r5263, %r54, %r5262, %p454;
	and.b32  	%r5264, %r688, 2;
	setp.eq.s32	%p455, %r5264, 0;
	and.b32  	%r5265, %r5263, -65281;
	or.b32  	%r5266, %r5265, %r5159;
	selp.b32	%r5267, %r5263, %r5266, %p455;
	and.b32  	%r5268, %r688, 4;
	setp.eq.s32	%p456, %r5268, 0;
	and.b32  	%r5269, %r5267, -16711681;
	or.b32  	%r5270, %r5269, %r5164;
	selp.b32	%r5271, %r5267, %r5270, %p456;
	and.b32  	%r5272, %r688, 8;
	setp.eq.s32	%p457, %r5272, 0;
	and.b32  	%r5273, %r5271, 16777215;
	prmt.b32 	%r5274, %r57, %r5273, 1620;
	selp.b32	%r11736, %r5271, %r5274, %p457;
	bra.uni 	BB3_1026;

BB3_97:
	setp.eq.s32	%p16, %r1786, 117;
	@%p16 bra 	BB3_98;
	bra.uni 	BB3_111;

BB3_98:
	and.b32  	%r8902, %r50, 1077952576;
	shr.u32 	%r8903, %r8902, 1;
	and.b32  	%r8904, %r50, -2139062144;
	shr.u32 	%r8905, %r8904, 2;
	not.b32 	%r8906, %r8905;
	and.b32  	%r8907, %r8903, %r8906;
	and.b32  	%r8908, %r50, 522133279;
	add.s32 	%r8909, %r8908, 522133279;
	mov.u32 	%r8910, -84215046;
	sub.s32 	%r8911, %r8910, %r8908;
	and.b32  	%r8912, %r8907, %r8911;
	and.b32  	%r8913, %r8912, %r8909;
	not.b32 	%r8914, %r8913;
	and.b32  	%r11740, %r50, %r8914;
	and.b32  	%r8915, %r49, 1077952576;
	shr.u32 	%r8916, %r8915, 1;
	and.b32  	%r8917, %r49, -2139062144;
	shr.u32 	%r8918, %r8917, 2;
	not.b32 	%r8919, %r8918;
	and.b32  	%r8920, %r8916, %r8919;
	and.b32  	%r8921, %r49, 522133279;
	add.s32 	%r8922, %r8921, 522133279;
	sub.s32 	%r8923, %r8910, %r8921;
	and.b32  	%r8924, %r8920, %r8923;
	and.b32  	%r8925, %r8924, %r8922;
	not.b32 	%r8926, %r8925;
	and.b32  	%r11741, %r49, %r8926;
	and.b32  	%r8927, %r48, 1077952576;
	shr.u32 	%r8928, %r8927, 1;
	and.b32  	%r8929, %r48, -2139062144;
	shr.u32 	%r8930, %r8929, 2;
	not.b32 	%r8931, %r8930;
	and.b32  	%r8932, %r8928, %r8931;
	and.b32  	%r8933, %r48, 522133279;
	add.s32 	%r8934, %r8933, 522133279;
	sub.s32 	%r8935, %r8910, %r8933;
	and.b32  	%r8936, %r8932, %r8935;
	and.b32  	%r8937, %r8936, %r8934;
	not.b32 	%r8938, %r8937;
	and.b32  	%r11742, %r48, %r8938;
	and.b32  	%r8939, %r47, 1077952576;
	shr.u32 	%r8940, %r8939, 1;
	and.b32  	%r8941, %r47, -2139062144;
	shr.u32 	%r8942, %r8941, 2;
	not.b32 	%r8943, %r8942;
	and.b32  	%r8944, %r8940, %r8943;
	and.b32  	%r8945, %r47, 522133279;
	add.s32 	%r8946, %r8945, 522133279;
	sub.s32 	%r8947, %r8910, %r8945;
	and.b32  	%r8948, %r8944, %r8947;
	and.b32  	%r8949, %r8948, %r8946;
	not.b32 	%r8950, %r8949;
	and.b32  	%r11743, %r47, %r8950;
	and.b32  	%r8951, %r51, 1077952576;
	shr.u32 	%r8952, %r8951, 1;
	and.b32  	%r8953, %r51, -2139062144;
	shr.u32 	%r8954, %r8953, 2;
	not.b32 	%r8955, %r8954;
	and.b32  	%r8956, %r8952, %r8955;
	and.b32  	%r8957, %r51, 522133279;
	add.s32 	%r8958, %r8957, 522133279;
	sub.s32 	%r8959, %r8910, %r8957;
	and.b32  	%r8960, %r8956, %r8959;
	and.b32  	%r8961, %r8960, %r8958;
	not.b32 	%r8962, %r8961;
	and.b32  	%r11739, %r51, %r8962;
	and.b32  	%r8963, %r52, 1077952576;
	shr.u32 	%r8964, %r8963, 1;
	and.b32  	%r8965, %r52, -2139062144;
	shr.u32 	%r8966, %r8965, 2;
	not.b32 	%r8967, %r8966;
	and.b32  	%r8968, %r8964, %r8967;
	and.b32  	%r8969, %r52, 522133279;
	add.s32 	%r8970, %r8969, 522133279;
	sub.s32 	%r8971, %r8910, %r8969;
	and.b32  	%r8972, %r8968, %r8971;
	and.b32  	%r8973, %r8972, %r8970;
	not.b32 	%r8974, %r8973;
	and.b32  	%r11738, %r52, %r8974;
	and.b32  	%r8975, %r53, 1077952576;
	shr.u32 	%r8976, %r8975, 1;
	and.b32  	%r8977, %r53, -2139062144;
	shr.u32 	%r8978, %r8977, 2;
	not.b32 	%r8979, %r8978;
	and.b32  	%r8980, %r8976, %r8979;
	and.b32  	%r8981, %r53, 522133279;
	add.s32 	%r8982, %r8981, 522133279;
	sub.s32 	%r8983, %r8910, %r8981;
	and.b32  	%r8984, %r8980, %r8983;
	and.b32  	%r8985, %r8984, %r8982;
	not.b32 	%r8986, %r8985;
	and.b32  	%r11737, %r53, %r8986;
	and.b32  	%r8987, %r54, 1077952576;
	shr.u32 	%r8988, %r8987, 1;
	and.b32  	%r8989, %r54, -2139062144;
	shr.u32 	%r8990, %r8989, 2;
	not.b32 	%r8991, %r8990;
	and.b32  	%r8992, %r8988, %r8991;
	and.b32  	%r8993, %r54, 522133279;
	add.s32 	%r8994, %r8993, 522133279;
	sub.s32 	%r8995, %r8910, %r8993;
	and.b32  	%r8996, %r8992, %r8995;
	and.b32  	%r8997, %r8996, %r8994;
	not.b32 	%r8998, %r8997;
	and.b32  	%r11736, %r54, %r8998;
	bra.uni 	BB3_1026;

BB3_452:
	mov.u32 	%r11752, 0;
	mov.u32 	%r4884, 8;
	// inline asm
	bfe.u32 %r4757, %r50, %r11752, %r4884;
	// inline asm
	setp.eq.s32	%p339, %r4757, %r56;
	selp.u32	%r4885, 1, 0, %p339;
	// inline asm
	bfe.u32 %r4761, %r50, %r4884, %r4884;
	// inline asm
	setp.eq.s32	%p340, %r4761, %r56;
	or.b32  	%r4886, %r4885, 2;
	selp.b32	%r4887, %r4886, %r4885, %p340;
	mov.u32 	%r4879, 16;
	// inline asm
	bfe.u32 %r4765, %r50, %r4879, %r4884;
	// inline asm
	setp.eq.s32	%p341, %r4765, %r56;
	or.b32  	%r4888, %r4887, 4;
	selp.b32	%r4889, %r4888, %r4887, %p341;
	mov.u32 	%r4883, 24;
	// inline asm
	bfe.u32 %r4769, %r50, %r4883, %r4884;
	// inline asm
	setp.eq.s32	%p342, %r4769, %r56;
	or.b32  	%r4890, %r4889, 8;
	selp.b32	%r4891, %r4890, %r4889, %p342;
	// inline asm
	bfe.u32 %r4773, %r49, %r11752, %r4884;
	// inline asm
	setp.eq.s32	%p343, %r4773, %r56;
	selp.u32	%r4892, 1, 0, %p343;
	// inline asm
	bfe.u32 %r4777, %r49, %r4884, %r4884;
	// inline asm
	setp.eq.s32	%p344, %r4777, %r56;
	or.b32  	%r4893, %r4892, 2;
	selp.b32	%r4894, %r4893, %r4892, %p344;
	// inline asm
	bfe.u32 %r4781, %r49, %r4879, %r4884;
	// inline asm
	setp.eq.s32	%p345, %r4781, %r56;
	or.b32  	%r4895, %r4894, 4;
	selp.b32	%r4896, %r4895, %r4894, %p345;
	// inline asm
	bfe.u32 %r4785, %r49, %r4883, %r4884;
	// inline asm
	setp.eq.s32	%p346, %r4785, %r56;
	or.b32  	%r4897, %r4896, 8;
	selp.b32	%r4898, %r4897, %r4896, %p346;
	// inline asm
	bfe.u32 %r4789, %r48, %r11752, %r4884;
	// inline asm
	setp.eq.s32	%p347, %r4789, %r56;
	selp.u32	%r4899, 1, 0, %p347;
	// inline asm
	bfe.u32 %r4793, %r48, %r4884, %r4884;
	// inline asm
	setp.eq.s32	%p348, %r4793, %r56;
	or.b32  	%r4900, %r4899, 2;
	selp.b32	%r4901, %r4900, %r4899, %p348;
	// inline asm
	bfe.u32 %r4797, %r48, %r4879, %r4884;
	// inline asm
	setp.eq.s32	%p349, %r4797, %r56;
	or.b32  	%r4902, %r4901, 4;
	selp.b32	%r4903, %r4902, %r4901, %p349;
	// inline asm
	bfe.u32 %r4801, %r48, %r4883, %r4884;
	// inline asm
	setp.eq.s32	%p350, %r4801, %r56;
	or.b32  	%r4904, %r4903, 8;
	selp.b32	%r4905, %r4904, %r4903, %p350;
	// inline asm
	bfe.u32 %r4805, %r47, %r11752, %r4884;
	// inline asm
	setp.eq.s32	%p351, %r4805, %r56;
	selp.u32	%r4906, 1, 0, %p351;
	// inline asm
	bfe.u32 %r4809, %r47, %r4884, %r4884;
	// inline asm
	setp.eq.s32	%p352, %r4809, %r56;
	or.b32  	%r4907, %r4906, 2;
	selp.b32	%r4908, %r4907, %r4906, %p352;
	// inline asm
	bfe.u32 %r4813, %r47, %r4879, %r4884;
	// inline asm
	setp.eq.s32	%p353, %r4813, %r56;
	or.b32  	%r4909, %r4908, 4;
	selp.b32	%r4910, %r4909, %r4908, %p353;
	// inline asm
	bfe.u32 %r4817, %r47, %r4883, %r4884;
	// inline asm
	setp.eq.s32	%p354, %r4817, %r56;
	or.b32  	%r4911, %r4910, 8;
	selp.b32	%r4912, %r4911, %r4910, %p354;
	// inline asm
	bfe.u32 %r4821, %r51, %r11752, %r4884;
	// inline asm
	setp.eq.s32	%p355, %r4821, %r56;
	selp.u32	%r4913, 1, 0, %p355;
	// inline asm
	bfe.u32 %r4825, %r51, %r4884, %r4884;
	// inline asm
	setp.eq.s32	%p356, %r4825, %r56;
	or.b32  	%r4914, %r4913, 2;
	selp.b32	%r4915, %r4914, %r4913, %p356;
	// inline asm
	bfe.u32 %r4829, %r51, %r4879, %r4884;
	// inline asm
	setp.eq.s32	%p357, %r4829, %r56;
	or.b32  	%r4916, %r4915, 4;
	selp.b32	%r4917, %r4916, %r4915, %p357;
	// inline asm
	bfe.u32 %r4833, %r51, %r4883, %r4884;
	// inline asm
	setp.eq.s32	%p358, %r4833, %r56;
	or.b32  	%r4918, %r4917, 8;
	selp.b32	%r4919, %r4918, %r4917, %p358;
	// inline asm
	bfe.u32 %r4837, %r52, %r11752, %r4884;
	// inline asm
	setp.eq.s32	%p359, %r4837, %r56;
	selp.u32	%r4920, 1, 0, %p359;
	// inline asm
	bfe.u32 %r4841, %r52, %r4884, %r4884;
	// inline asm
	setp.eq.s32	%p360, %r4841, %r56;
	or.b32  	%r4921, %r4920, 2;
	selp.b32	%r4922, %r4921, %r4920, %p360;
	// inline asm
	bfe.u32 %r4845, %r52, %r4879, %r4884;
	// inline asm
	setp.eq.s32	%p361, %r4845, %r56;
	or.b32  	%r4923, %r4922, 4;
	selp.b32	%r4924, %r4923, %r4922, %p361;
	// inline asm
	bfe.u32 %r4849, %r52, %r4883, %r4884;
	// inline asm
	setp.eq.s32	%p362, %r4849, %r56;
	or.b32  	%r4925, %r4924, 8;
	selp.b32	%r4926, %r4925, %r4924, %p362;
	// inline asm
	bfe.u32 %r4853, %r53, %r11752, %r4884;
	// inline asm
	setp.eq.s32	%p363, %r4853, %r56;
	selp.u32	%r4927, 1, 0, %p363;
	// inline asm
	bfe.u32 %r4857, %r53, %r4884, %r4884;
	// inline asm
	setp.eq.s32	%p364, %r4857, %r56;
	or.b32  	%r4928, %r4927, 2;
	selp.b32	%r4929, %r4928, %r4927, %p364;
	// inline asm
	bfe.u32 %r4861, %r53, %r4879, %r4884;
	// inline asm
	setp.eq.s32	%p365, %r4861, %r56;
	or.b32  	%r4930, %r4929, 4;
	selp.b32	%r4931, %r4930, %r4929, %p365;
	// inline asm
	bfe.u32 %r4865, %r53, %r4883, %r4884;
	// inline asm
	setp.eq.s32	%p366, %r4865, %r56;
	or.b32  	%r4932, %r4931, 8;
	selp.b32	%r4933, %r4932, %r4931, %p366;
	// inline asm
	bfe.u32 %r4869, %r54, %r11752, %r4884;
	// inline asm
	setp.eq.s32	%p367, %r4869, %r56;
	selp.u32	%r4934, 1, 0, %p367;
	// inline asm
	bfe.u32 %r4873, %r54, %r4884, %r4884;
	// inline asm
	setp.eq.s32	%p368, %r4873, %r56;
	or.b32  	%r4935, %r4934, 2;
	selp.b32	%r4936, %r4935, %r4934, %p368;
	// inline asm
	bfe.u32 %r4877, %r54, %r4879, %r4884;
	// inline asm
	setp.eq.s32	%p369, %r4877, %r56;
	or.b32  	%r4937, %r4936, 4;
	selp.b32	%r4938, %r4937, %r4936, %p369;
	// inline asm
	bfe.u32 %r4881, %r54, %r4883, %r4884;
	// inline asm
	setp.eq.s32	%p370, %r4881, %r56;
	or.b32  	%r4939, %r4938, 8;
	selp.b32	%r4940, %r4939, %r4938, %p370;
	add.s32 	%r4941, %r4898, %r4891;
	add.s32 	%r4942, %r4941, %r4905;
	add.s32 	%r4943, %r4942, %r4912;
	add.s32 	%r4944, %r4943, %r4919;
	add.s32 	%r4945, %r4944, %r4926;
	add.s32 	%r4946, %r4945, %r4933;
	neg.s32 	%r4947, %r4940;
	setp.eq.s32	%p371, %r4946, %r4947;
	@%p371 bra 	BB3_111;

	mov.u64 	%rd2779, 0;
	st.local.v4.u32 	[%rd86], {%r50, %r49, %r48, %r47};
	st.local.v4.u32 	[%rd86+16], {%r51, %r52, %r53, %r54};
	st.local.v2.u64 	[%rd87], {%rd2779, %rd2779};
	st.local.v2.u64 	[%rd87+16], {%rd2779, %rd2779};
	cvt.u16.u32	%rs23, %r50;
	setp.eq.s32	%p372, %r38, 0;
	@%p372 bra 	BB3_477;

	cvt.u16.u32	%rs2, %r55;
	and.b32  	%r648, %r38, 3;
	setp.eq.s32	%p373, %r648, 0;
	mov.u32 	%r11646, 0;
	mov.u32 	%r11752, %r11646;
	@%p373 bra 	BB3_467;

	setp.eq.s32	%p374, %r648, 1;
	mov.u32 	%r11641, 0;
	@%p374 bra 	BB3_456;
	bra.uni 	BB3_457;

BB3_456:
	mov.u32 	%r11752, %r11641;
	bra.uni 	BB3_464;

BB3_26:
	setp.eq.s32	%p50, %r1786, 68;
	@%p50 bra 	BB3_27;
	bra.uni 	BB3_111;

BB3_27:
	setp.ge.u32	%p605, %r56, %r38;
	@%p605 bra 	BB3_111;

	mov.u32 	%r6570, 8;
	// inline asm
	shf.r.wrap.b32 %r11748, %r50, %r49, %r6570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11749, %r49, %r48, %r6570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11750, %r48, %r47, %r6570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11751, %r47, %r51, %r6570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11744, %r51, %r52, %r6570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11745, %r52, %r53, %r6570;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11746, %r53, %r54, %r6570;
	// inline asm
	mov.u32 	%r6569, 0;
	// inline asm
	shf.r.wrap.b32 %r11747, %r54, %r6569, %r6570;
	// inline asm
	and.b32  	%r6572, %r55, 3;
	shl.b32 	%r6573, %r6572, 3;
	mov.u32 	%r6574, 1;
	shl.b32 	%r6575, %r6574, %r6573;
	add.s32 	%r1002, %r6575, -1;
	neg.s32 	%r1003, %r6575;
	shr.u32 	%r6571, %r56, 2;
	setp.gt.s32	%p606, %r6571, 3;
	@%p606 bra 	BB3_751;

	setp.gt.s32	%p612, %r6571, 1;
	@%p612 bra 	BB3_748;

	setp.eq.s32	%p615, %r6571, 0;
	@%p615 bra 	BB3_31;
	bra.uni 	BB3_746;

BB3_31:
	and.b32  	%r6590, %r1002, %r50;
	and.b32  	%r6591, %r11748, %r1003;
	or.b32  	%r11740, %r6591, %r6590;
	mov.u32 	%r11736, %r11747;
	mov.u32 	%r11737, %r11746;
	mov.u32 	%r11738, %r11745;
	mov.u32 	%r11739, %r11744;
	mov.u32 	%r11741, %r11749;
	bra.uni 	BB3_32;

BB3_1025:
	and.b32  	%r8999, %r50, 1077952576;
	shr.u32 	%r9000, %r8999, 1;
	and.b32  	%r9001, %r50, -2139062144;
	shr.u32 	%r9002, %r9001, 2;
	not.b32 	%r9003, %r9002;
	and.b32  	%r9004, %r9000, %r9003;
	and.b32  	%r9005, %r50, 522133279;
	add.s32 	%r9006, %r9005, 522133279;
	mov.u32 	%r9007, -84215046;
	sub.s32 	%r9008, %r9007, %r9005;
	and.b32  	%r9009, %r9004, %r9008;
	and.b32  	%r9010, %r9009, %r9006;
	or.b32  	%r11740, %r9010, %r50;
	and.b32  	%r9011, %r49, 1077952576;
	shr.u32 	%r9012, %r9011, 1;
	and.b32  	%r9013, %r49, -2139062144;
	shr.u32 	%r9014, %r9013, 2;
	not.b32 	%r9015, %r9014;
	and.b32  	%r9016, %r9012, %r9015;
	and.b32  	%r9017, %r49, 522133279;
	add.s32 	%r9018, %r9017, 522133279;
	sub.s32 	%r9019, %r9007, %r9017;
	and.b32  	%r9020, %r9016, %r9019;
	and.b32  	%r9021, %r9020, %r9018;
	or.b32  	%r11741, %r9021, %r49;
	and.b32  	%r9022, %r48, 1077952576;
	shr.u32 	%r9023, %r9022, 1;
	and.b32  	%r9024, %r48, -2139062144;
	shr.u32 	%r9025, %r9024, 2;
	not.b32 	%r9026, %r9025;
	and.b32  	%r9027, %r9023, %r9026;
	and.b32  	%r9028, %r48, 522133279;
	add.s32 	%r9029, %r9028, 522133279;
	sub.s32 	%r9030, %r9007, %r9028;
	and.b32  	%r9031, %r9027, %r9030;
	and.b32  	%r9032, %r9031, %r9029;
	or.b32  	%r11742, %r9032, %r48;
	and.b32  	%r9033, %r47, 1077952576;
	shr.u32 	%r9034, %r9033, 1;
	and.b32  	%r9035, %r47, -2139062144;
	shr.u32 	%r9036, %r9035, 2;
	not.b32 	%r9037, %r9036;
	and.b32  	%r9038, %r9034, %r9037;
	and.b32  	%r9039, %r47, 522133279;
	add.s32 	%r9040, %r9039, 522133279;
	sub.s32 	%r9041, %r9007, %r9039;
	and.b32  	%r9042, %r9038, %r9041;
	and.b32  	%r9043, %r9042, %r9040;
	or.b32  	%r11743, %r9043, %r47;
	and.b32  	%r9044, %r51, 1077952576;
	shr.u32 	%r9045, %r9044, 1;
	and.b32  	%r9046, %r51, -2139062144;
	shr.u32 	%r9047, %r9046, 2;
	not.b32 	%r9048, %r9047;
	and.b32  	%r9049, %r9045, %r9048;
	and.b32  	%r9050, %r51, 522133279;
	add.s32 	%r9051, %r9050, 522133279;
	sub.s32 	%r9052, %r9007, %r9050;
	and.b32  	%r9053, %r9049, %r9052;
	and.b32  	%r9054, %r9053, %r9051;
	or.b32  	%r11739, %r9054, %r51;
	and.b32  	%r9055, %r52, 1077952576;
	shr.u32 	%r9056, %r9055, 1;
	and.b32  	%r9057, %r52, -2139062144;
	shr.u32 	%r9058, %r9057, 2;
	not.b32 	%r9059, %r9058;
	and.b32  	%r9060, %r9056, %r9059;
	and.b32  	%r9061, %r52, 522133279;
	add.s32 	%r9062, %r9061, 522133279;
	sub.s32 	%r9063, %r9007, %r9061;
	and.b32  	%r9064, %r9060, %r9063;
	and.b32  	%r9065, %r9064, %r9062;
	or.b32  	%r11738, %r9065, %r52;
	and.b32  	%r9066, %r53, 1077952576;
	shr.u32 	%r9067, %r9066, 1;
	and.b32  	%r9068, %r53, -2139062144;
	shr.u32 	%r9069, %r9068, 2;
	not.b32 	%r9070, %r9069;
	and.b32  	%r9071, %r9067, %r9070;
	and.b32  	%r9072, %r53, 522133279;
	add.s32 	%r9073, %r9072, 522133279;
	sub.s32 	%r9074, %r9007, %r9072;
	and.b32  	%r9075, %r9071, %r9074;
	and.b32  	%r9076, %r9075, %r9073;
	or.b32  	%r11737, %r9076, %r53;
	and.b32  	%r9077, %r54, 1077952576;
	shr.u32 	%r9078, %r9077, 1;
	and.b32  	%r9079, %r54, -2139062144;
	shr.u32 	%r9080, %r9079, 2;
	not.b32 	%r9081, %r9080;
	and.b32  	%r9082, %r9078, %r9081;
	and.b32  	%r9083, %r54, 522133279;
	add.s32 	%r9084, %r9083, 522133279;
	sub.s32 	%r9085, %r9007, %r9083;
	and.b32  	%r9086, %r9082, %r9085;
	and.b32  	%r9087, %r9086, %r9084;
	or.b32  	%r11736, %r9087, %r54;
	bra.uni 	BB3_1026;

BB3_82:
	setp.eq.s32	%p23, %r1786, 112;
	@%p23 bra 	BB3_83;
	bra.uni 	BB3_111;

BB3_83:
	mad.lo.s32 	%r7637, %r56, %r38, %r38;
	setp.gt.u32	%p719, %r7637, 31;
	@%p719 bra 	BB3_111;

	setp.eq.s32	%p720, %r56, 0;
	mov.u32 	%r7638, 0;
	mov.u32 	%r11702, %r7638;
	mov.u32 	%r11752, %r38;
	mov.u32 	%r11743, %r47;
	mov.u32 	%r11742, %r48;
	mov.u32 	%r11741, %r49;
	mov.u32 	%r11740, %r50;
	mov.u32 	%r11739, %r51;
	mov.u32 	%r11738, %r52;
	mov.u32 	%r11737, %r53;
	mov.u32 	%r11736, %r54;
	@%p720 bra 	BB3_902;
	bra.uni 	BB3_903;

BB3_902:
	mov.u32 	%r11736, %r54;
	mov.u32 	%r11737, %r53;
	mov.u32 	%r11738, %r52;
	mov.u32 	%r11739, %r51;
	mov.u32 	%r11740, %r50;
	mov.u32 	%r11741, %r49;
	mov.u32 	%r11742, %r48;
	mov.u32 	%r11743, %r47;
	mov.u32 	%r11744, %r51;
	mov.u32 	%r11745, %r52;
	mov.u32 	%r11746, %r53;
	mov.u32 	%r11747, %r54;
	mov.u32 	%r11748, %r50;
	mov.u32 	%r11749, %r49;
	mov.u32 	%r11750, %r48;
	mov.u32 	%r11751, %r47;
	bra.uni 	BB3_1026;

BB3_903:
	and.b32  	%r7648, %r11752, 3;
	mov.u32 	%r7649, 4;
	sub.s32 	%r7650, %r7649, %r7648;
	shl.b32 	%r7651, %r7650, 2;
	mov.u32 	%r7652, 1985229328;
	shr.u32 	%r7653, %r7652, %r7651;
	and.b32  	%r1284, %r7653, 65535;
	shr.u32 	%r7647, %r11752, 2;
	setp.gt.s32	%p721, %r7647, 3;
	@%p721 bra 	BB3_911;

	setp.gt.s32	%p727, %r7647, 1;
	@%p727 bra 	BB3_908;

	setp.eq.s32	%p730, %r7647, 0;
	@%p730 bra 	BB3_921;
	bra.uni 	BB3_906;

BB3_921:
	// inline asm
	prmt.b32 %r11719, %r53, %r54, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11718, %r52, %r53, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11717, %r51, %r52, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11716, %r47, %r51, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11715, %r48, %r47, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11714, %r49, %r48, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11713, %r50, %r49, %r1284;
	// inline asm
	mov.u32 	%r7823, 0;
	// inline asm
	prmt.b32 %r11712, %r7823, %r50, %r1284;
	// inline asm
	bra.uni 	BB3_922;

BB3_911:
	setp.gt.s32	%p722, %r7647, 5;
	@%p722 bra 	BB3_915;

	setp.eq.s32	%p725, %r7647, 4;
	@%p725 bra 	BB3_919;
	bra.uni 	BB3_913;

BB3_919:
	// inline asm
	prmt.b32 %r11719, %r48, %r47, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11718, %r49, %r48, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11717, %r50, %r49, %r1284;
	// inline asm
	mov.u32 	%r11712, 0;
	// inline asm
	prmt.b32 %r11716, %r11712, %r50, %r1284;
	// inline asm
	mov.u32 	%r11713, %r11712;
	mov.u32 	%r11714, %r11712;
	mov.u32 	%r11715, %r11712;
	bra.uni 	BB3_922;

BB3_908:
	setp.eq.s32	%p728, %r7647, 2;
	@%p728 bra 	BB3_920;
	bra.uni 	BB3_909;

BB3_920:
	// inline asm
	prmt.b32 %r11719, %r51, %r52, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11718, %r47, %r51, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11717, %r48, %r47, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11716, %r49, %r48, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11715, %r50, %r49, %r1284;
	// inline asm
	mov.u32 	%r11712, 0;
	// inline asm
	prmt.b32 %r11714, %r11712, %r50, %r1284;
	// inline asm
	mov.u32 	%r11713, %r11712;
	bra.uni 	BB3_922;

BB3_915:
	setp.eq.s32	%p723, %r7647, 6;
	@%p723 bra 	BB3_918;
	bra.uni 	BB3_916;

BB3_918:
	// inline asm
	prmt.b32 %r11719, %r50, %r49, %r1284;
	// inline asm
	mov.u32 	%r11712, 0;
	// inline asm
	prmt.b32 %r11718, %r11712, %r50, %r1284;
	// inline asm
	mov.u32 	%r11713, %r11712;
	mov.u32 	%r11714, %r11712;
	mov.u32 	%r11715, %r11712;
	mov.u32 	%r11716, %r11712;
	mov.u32 	%r11717, %r11712;
	bra.uni 	BB3_922;

BB3_906:
	setp.eq.s32	%p731, %r7647, 1;
	mov.u32 	%r11712, %r7638;
	mov.u32 	%r11713, %r7638;
	mov.u32 	%r11714, %r7638;
	mov.u32 	%r11715, %r7638;
	mov.u32 	%r11716, %r7638;
	mov.u32 	%r11717, %r7638;
	mov.u32 	%r11718, %r7638;
	mov.u32 	%r11719, %r7638;
	@%p731 bra 	BB3_907;
	bra.uni 	BB3_922;

BB3_907:
	// inline asm
	prmt.b32 %r11719, %r52, %r53, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11718, %r51, %r52, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11717, %r47, %r51, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11716, %r48, %r47, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11715, %r49, %r48, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11714, %r50, %r49, %r1284;
	// inline asm
	mov.u32 	%r11712, 0;
	// inline asm
	prmt.b32 %r11713, %r11712, %r50, %r1284;
	// inline asm
	bra.uni 	BB3_922;

BB3_913:
	setp.eq.s32	%p726, %r7647, 5;
	mov.u32 	%r11712, %r7638;
	mov.u32 	%r11713, %r7638;
	mov.u32 	%r11714, %r7638;
	mov.u32 	%r11715, %r7638;
	mov.u32 	%r11716, %r7638;
	mov.u32 	%r11717, %r7638;
	mov.u32 	%r11718, %r7638;
	mov.u32 	%r11719, %r7638;
	@%p726 bra 	BB3_914;
	bra.uni 	BB3_922;

BB3_914:
	// inline asm
	prmt.b32 %r11719, %r49, %r48, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11718, %r50, %r49, %r1284;
	// inline asm
	mov.u32 	%r11712, 0;
	// inline asm
	prmt.b32 %r11717, %r11712, %r50, %r1284;
	// inline asm
	mov.u32 	%r11713, %r11712;
	mov.u32 	%r11714, %r11712;
	mov.u32 	%r11715, %r11712;
	mov.u32 	%r11716, %r11712;
	bra.uni 	BB3_922;

BB3_909:
	setp.eq.s32	%p729, %r7647, 3;
	mov.u32 	%r11712, %r7638;
	mov.u32 	%r11713, %r7638;
	mov.u32 	%r11714, %r7638;
	mov.u32 	%r11715, %r7638;
	mov.u32 	%r11716, %r7638;
	mov.u32 	%r11717, %r7638;
	mov.u32 	%r11718, %r7638;
	mov.u32 	%r11719, %r7638;
	@%p729 bra 	BB3_910;
	bra.uni 	BB3_922;

BB3_910:
	// inline asm
	prmt.b32 %r11719, %r47, %r51, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11718, %r48, %r47, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11717, %r49, %r48, %r1284;
	// inline asm
	// inline asm
	prmt.b32 %r11716, %r50, %r49, %r1284;
	// inline asm
	mov.u32 	%r11712, 0;
	// inline asm
	prmt.b32 %r11715, %r11712, %r50, %r1284;
	// inline asm
	mov.u32 	%r11713, %r11712;
	mov.u32 	%r11714, %r11712;
	bra.uni 	BB3_922;

BB3_916:
	setp.ne.s32	%p724, %r7647, 7;
	mov.u32 	%r11712, %r7638;
	mov.u32 	%r11713, %r7638;
	mov.u32 	%r11714, %r7638;
	mov.u32 	%r11715, %r7638;
	mov.u32 	%r11716, %r7638;
	mov.u32 	%r11717, %r7638;
	mov.u32 	%r11718, %r7638;
	mov.u32 	%r11719, %r7638;
	@%p724 bra 	BB3_922;

	mov.u32 	%r11712, 0;
	// inline asm
	prmt.b32 %r11719, %r11712, %r50, %r1284;
	// inline asm
	mov.u32 	%r11713, %r11712;
	mov.u32 	%r11714, %r11712;
	mov.u32 	%r11715, %r11712;
	mov.u32 	%r11716, %r11712;
	mov.u32 	%r11717, %r11712;
	mov.u32 	%r11718, %r11712;

BB3_922:
	or.b32  	%r11740, %r11712, %r11740;
	or.b32  	%r11741, %r11713, %r11741;
	or.b32  	%r11742, %r11714, %r11742;
	or.b32  	%r11743, %r11715, %r11743;
	or.b32  	%r11739, %r11716, %r11739;
	or.b32  	%r11738, %r11717, %r11738;
	or.b32  	%r11737, %r11718, %r11737;
	or.b32  	%r11736, %r11719, %r11736;
	add.s32 	%r11752, %r11752, %r38;
	add.s32 	%r11702, %r11702, 1;
	setp.lt.u32	%p732, %r11702, %r56;
	@%p732 bra 	BB3_903;

	mov.u32 	%r11744, %r51;
	mov.u32 	%r11745, %r52;
	mov.u32 	%r11746, %r53;
	mov.u32 	%r11747, %r54;
	mov.u32 	%r11748, %r50;
	mov.u32 	%r11749, %r49;
	mov.u32 	%r11750, %r48;
	mov.u32 	%r11751, %r47;
	bra.uni 	BB3_1027;

BB3_337:
	setp.eq.s32	%p263, %r38, 0;
	add.s32 	%r4132, %r56, %r38;
	setp.gt.u32	%p264, %r4132, 31;
	or.pred  	%p265, %p263, %p264;
	@%p265 bra 	BB3_111;

	add.s32 	%r4135, %r38, -1;
	and.b32  	%r4136, %r4135, 3;
	shl.b32 	%r471, %r4136, 3;
	bfe.u32 	%r4137, %r4135, 2, 2;
	mov.u32 	%r4138, 255;
	shl.b32 	%r4139, %r4138, %r471;
	setp.eq.s32	%p266, %r4137, 0;
	selp.b32	%r11744, %r4139, 0, %p266;
	setp.eq.s32	%p267, %r4137, 1;
	selp.b32	%r11745, %r4139, 0, %p267;
	setp.eq.s32	%p268, %r4137, 2;
	selp.b32	%r11746, %r4139, 0, %p268;
	setp.eq.s32	%p269, %r4137, 3;
	selp.b32	%r11747, %r4139, 0, %p269;
	shr.u32 	%r4134, %r4135, 4;
	mov.u32 	%r11611, 0;
	setp.eq.s32	%p270, %r4134, 0;
	@%p270 bra 	BB3_341;
	bra.uni 	BB3_339;

BB3_341:
	and.b32  	%r4146, %r11744, %r50;
	and.b32  	%r4147, %r11745, %r49;
	or.b32  	%r4148, %r4146, %r4147;
	and.b32  	%r4149, %r11746, %r48;
	or.b32  	%r4150, %r4148, %r4149;
	and.b32  	%r4151, %r11747, %r47;
	or.b32  	%r11611, %r4150, %r4151;
	bra.uni 	BB3_342;

BB3_56:
	setp.eq.s32	%p37, %r1786, 93;
	@%p37 bra 	BB3_57;
	bra.uni 	BB3_111;

BB3_57:
	setp.eq.s32	%p617, %r38, 0;
	mov.u32 	%r11752, 0;
	@%p617 bra 	BB3_114;

	add.s32 	%r11752, %r38, -1;
	and.b32  	%r6593, %r11752, 3;
	shl.b32 	%r6594, %r6593, 3;
	mov.u32 	%r6595, 1;
	shl.b32 	%r6596, %r6595, %r6594;
	add.s32 	%r6597, %r6596, -1;
	setp.lt.u32	%p618, %r11752, 4;
	selp.b32	%r6598, %r6597, -1, %p618;
	and.b32  	%r11740, %r6598, %r50;
	and.b32  	%r6599, %r11752, -4;
	setp.eq.s32	%p619, %r6599, 4;
	selp.b32	%r6600, %r6597, -1, %p619;
	and.b32  	%r11741, %r6600, %r49;
	setp.eq.s32	%p620, %r6599, 8;
	selp.b32	%r6601, %r6597, -1, %p620;
	and.b32  	%r11742, %r6601, %r48;
	setp.eq.s32	%p621, %r6599, 12;
	selp.b32	%r6602, %r6597, -1, %p621;
	and.b32  	%r11743, %r6602, %r47;
	setp.eq.s32	%p622, %r6599, 16;
	selp.b32	%r6603, %r6597, -1, %p622;
	and.b32  	%r11739, %r6603, %r51;
	setp.eq.s32	%p623, %r6599, 20;
	selp.b32	%r6604, %r6597, -1, %p623;
	and.b32  	%r11738, %r6604, %r52;
	setp.eq.s32	%p624, %r6599, 24;
	selp.b32	%r6605, %r6597, -1, %p624;
	and.b32  	%r11737, %r6605, %r53;
	setp.gt.u32	%p625, %r11752, 27;
	selp.b32	%r6606, %r6597, -1, %p625;
	and.b32  	%r11736, %r6606, %r54;
	bra.uni 	BB3_1027;

BB3_345:
	setp.eq.s32	%p280, %r38, 0;
	add.s32 	%r11752, %r56, %r38;
	setp.gt.u32	%p281, %r11752, 31;
	or.pred  	%p282, %p280, %p281;
	@%p282 bra 	BB3_111;

	mov.u32 	%r4175, 64;
	prmt.b32 	%r4176, %r50, %r50, %r4175;
	mov.u32 	%r4177, 1040;
	prmt.b32 	%r4178, %r4176, %r50, %r4177;
	mov.u32 	%r4179, 16912;
	prmt.b32 	%r11748, %r4178, %r50, %r4179;
	setp.gt.s32	%p283, %r56, 15;
	@%p283 bra 	BB3_375;

	setp.gt.s32	%p307, %r56, 7;
	@%p307 bra 	BB3_360;

	setp.gt.s32	%p319, %r56, 3;
	@%p319 bra 	BB3_353;

	setp.eq.s32	%p325, %r56, 1;
	@%p325 bra 	BB3_425;

	setp.eq.s32	%p326, %r56, 2;
	@%p326 bra 	BB3_424;
	bra.uni 	BB3_351;

BB3_424:
	mov.u32 	%r4680, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r53, %r54, %r4680;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r52, %r53, %r4680;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r51, %r52, %r4680;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r51, %r47, %r51, %r4680;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4665, %r48, %r47, %r4680;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11628, %r49, %r48, %r4680;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11627, %r50, %r49, %r4680;
	// inline asm
	mov.u32 	%r4678, 0;
	// inline asm
	shf.r.wrap.b32 %r11626, %r4678, %r50, %r4680;
	// inline asm
	mov.u32 	%r50, %r4665;
	bra.uni 	BB3_428;

BB3_110:
	setp.eq.s32	%p10, %r1786, 125;
	@%p10 bra 	BB3_770;
	bra.uni 	BB3_111;

BB3_770:
	setp.eq.s32	%p627, %r38, 0;
	mov.u32 	%r11752, 0;
	@%p627 bra 	BB3_114;

	add.s32 	%r6643, %r38, -1;
	and.b32  	%r6644, %r6643, 3;
	shl.b32 	%r1039, %r6644, 3;
	bfe.u32 	%r6645, %r6643, 2, 2;
	mov.u32 	%r6646, 255;
	shl.b32 	%r6647, %r6646, %r1039;
	setp.eq.s32	%p628, %r6645, 0;
	selp.b32	%r11748, %r6647, 0, %p628;
	setp.eq.s32	%p629, %r6645, 1;
	selp.b32	%r11749, %r6647, 0, %p629;
	setp.eq.s32	%p630, %r6645, 2;
	selp.b32	%r11750, %r6647, 0, %p630;
	setp.eq.s32	%p631, %r6645, 3;
	selp.b32	%r11751, %r6647, 0, %p631;
	shr.u32 	%r6642, %r6643, 4;
	mov.u32 	%r6641, 0;
	setp.eq.s32	%p632, %r6642, 0;
	@%p632 bra 	BB3_774;
	bra.uni 	BB3_772;

BB3_774:
	and.b32  	%r6654, %r11748, %r50;
	and.b32  	%r6655, %r11749, %r49;
	or.b32  	%r6656, %r6654, %r6655;
	and.b32  	%r6657, %r11750, %r48;
	or.b32  	%r6658, %r6656, %r6657;
	and.b32  	%r6659, %r11751, %r47;
	or.b32  	%r11685, %r6658, %r6659;
	bra.uni 	BB3_775;

BB3_514:
	setp.gt.s32	%p475, %r5357, 5;
	@%p475 bra 	BB3_518;

	setp.eq.s32	%p478, %r5357, 4;
	@%p478 bra 	BB3_522;
	bra.uni 	BB3_516;

BB3_522:
	and.b32  	%r5372, %r724, %r51;
	or.b32  	%r5373, %r5372, %r723;
	and.b32  	%r5374, %r11744, %r725;
	or.b32  	%r11739, %r5373, %r5374;
	mov.u32 	%r11736, %r11747;
	mov.u32 	%r11737, %r11746;
	mov.u32 	%r11738, %r11745;
	bra.uni 	BB3_118;

BB3_669:
	setp.gt.s32	%p550, %r56, 23;
	@%p550 bra 	BB3_687;

	setp.gt.s32	%p562, %r56, 19;
	@%p562 bra 	BB3_679;

	setp.gt.s32	%p568, %r56, 17;
	@%p568 bra 	BB3_675;

	setp.eq.s32	%p571, %r56, 16;
	@%p571 bra 	BB3_710;
	bra.uni 	BB3_673;

BB3_710:
	mov.u32 	%r11736, 0;
	mov.u32 	%r11737, %r11736;
	mov.u32 	%r11738, %r11736;
	mov.u32 	%r11739, %r11736;
	mov.u32 	%r11740, %r51;
	mov.u32 	%r11741, %r52;
	mov.u32 	%r11742, %r53;
	bra.uni 	BB3_724;

BB3_968:
	setp.gt.s32	%p746, %r8013, 23;
	@%p746 bra 	BB3_984;

	setp.gt.s32	%p758, %r8013, 19;
	@%p758 bra 	BB3_977;

	setp.gt.s32	%p764, %r8013, 17;
	@%p764 bra 	BB3_974;

	setp.eq.s32	%p767, %r8013, 16;
	@%p767 bra 	BB3_1007;
	bra.uni 	BB3_972;

BB3_1007:
	mov.u32 	%r11747, 0;
	mov.u32 	%r11748, %r47;
	mov.u32 	%r11749, %r48;
	mov.u32 	%r11750, %r49;
	mov.u32 	%r11751, %r50;
	bra.uni 	BB3_1008;

BB3_559:
	setp.gt.s32	%p489, %r58, 23;
	@%p489 bra 	BB3_577;

	setp.gt.s32	%p501, %r58, 19;
	@%p501 bra 	BB3_569;

	setp.gt.s32	%p507, %r58, 17;
	@%p507 bra 	BB3_565;

	setp.eq.s32	%p510, %r58, 16;
	@%p510 bra 	BB3_603;
	bra.uni 	BB3_563;

BB3_603:
	mov.u32 	%r11744, %r11747;
	mov.u32 	%r11745, %r11747;
	mov.u32 	%r11746, %r11747;
	mov.u32 	%r11748, %r51;
	mov.u32 	%r11749, %r52;
	mov.u32 	%r11750, %r53;
	mov.u32 	%r11751, %r54;
	bra.uni 	BB3_612;

BB3_933:
	setp.gt.s32	%p735, %r7834, 5;
	@%p735 bra 	BB3_937;

	setp.eq.s32	%p738, %r7834, 4;
	@%p738 bra 	BB3_941;
	bra.uni 	BB3_935;

BB3_941:
	// inline asm
	prmt.b32 %r11727, %r48, %r47, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11726, %r49, %r48, %r1340;
	// inline asm
	// inline asm
	prmt.b32 %r11725, %r50, %r49, %r1340;
	// inline asm
	mov.u32 	%r11720, 0;
	// inline asm
	prmt.b32 %r11724, %r11720, %r50, %r1340;
	// inline asm
	mov.u32 	%r11721, %r11720;
	mov.u32 	%r11722, %r11720;
	mov.u32 	%r11723, %r11720;
	bra.uni 	BB3_944;

BB3_339:
	setp.ne.s32	%p271, %r4134, 1;
	@%p271 bra 	BB3_342;

	and.b32  	%r4140, %r11744, %r51;
	and.b32  	%r4141, %r11745, %r52;
	or.b32  	%r4142, %r4140, %r4141;
	and.b32  	%r4143, %r11746, %r53;
	or.b32  	%r4144, %r4142, %r4143;
	and.b32  	%r4145, %r11747, %r54;
	or.b32  	%r11611, %r4144, %r4145;

BB3_342:
	shr.u32 	%r479, %r11611, %r471;
	setp.eq.s32	%p272, %r56, 0;
	@%p272 bra 	BB3_111;

	and.b32  	%r4153, %r479, 255;
	mov.u32 	%r4154, 64;
	prmt.b32 	%r4155, %r4153, %r4153, %r4154;
	mov.u32 	%r4156, 1040;
	prmt.b32 	%r4157, %r4155, %r4153, %r4156;
	mov.u32 	%r4158, 16912;
	prmt.b32 	%r480, %r4157, %r4153, %r4158;
	mov.u32 	%r11612, 0;

BB3_344:
	bfe.u32 	%r4159, %r38, 2, 2;
	and.b32  	%r4160, %r38, 3;
	shl.b32 	%r4161, %r4160, 3;
	shl.b32 	%r4163, %r4138, %r4161;
	setp.eq.s32	%p273, %r4159, 0;
	selp.b32	%r11748, %r4163, 0, %p273;
	setp.eq.s32	%p274, %r4159, 1;
	selp.b32	%r11749, %r4163, 0, %p274;
	setp.eq.s32	%p275, %r4159, 2;
	selp.b32	%r11750, %r4163, 0, %p275;
	setp.eq.s32	%p276, %r4159, 3;
	selp.b32	%r11751, %r4163, 0, %p276;
	shr.u32 	%r4164, %r38, 4;
	setp.eq.s32	%p277, %r4164, 0;
	selp.b32	%r4165, %r480, 0, %p277;
	and.b32  	%r4166, %r4165, %r11748;
	or.b32  	%r50, %r4166, %r50;
	and.b32  	%r4167, %r4165, %r11749;
	or.b32  	%r49, %r4167, %r49;
	and.b32  	%r4168, %r4165, %r11750;
	or.b32  	%r48, %r4168, %r48;
	and.b32  	%r4169, %r4165, %r11751;
	or.b32  	%r47, %r4169, %r47;
	setp.eq.s32	%p278, %r4164, 1;
	selp.b32	%r4170, %r480, 0, %p278;
	and.b32  	%r4171, %r4170, %r11748;
	or.b32  	%r51, %r4171, %r51;
	and.b32  	%r4172, %r4170, %r11749;
	or.b32  	%r52, %r4172, %r52;
	and.b32  	%r4173, %r4170, %r11750;
	or.b32  	%r53, %r4173, %r53;
	and.b32  	%r4174, %r4170, %r11751;
	or.b32  	%r54, %r4174, %r54;
	add.s32 	%r38, %r38, 1;
	add.s32 	%r11612, %r11612, 1;
	setp.lt.u32	%p279, %r11612, %r56;
	@%p279 bra 	BB3_344;

BB3_111:
	mov.u32 	%r11736, %r54;
	mov.u32 	%r11737, %r53;
	mov.u32 	%r11738, %r52;
	mov.u32 	%r11739, %r51;
	mov.u32 	%r11740, %r50;

BB3_112:
	mov.u32 	%r11741, %r49;
	mov.u32 	%r11742, %r48;
	mov.u32 	%r11743, %r47;

BB3_1026:
	mov.u32 	%r11752, %r38;

BB3_1027:
	add.s32 	%r11561, %r11561, 1;
	shl.b64 	%rd187, %rd85, 7;
	add.s64 	%rd188, %rd93, %rd187;
	mul.wide.u32 	%rd189, %r11561, 4;
	add.s64 	%rd190, %rd188, %rd189;
	ld.const.u32 	%r11560, [%rd190];
	setp.ne.s32	%p791, %r11560, 0;
	@%p791 bra 	BB3_4;

BB3_1028:
	and.b32  	%r9089, %r11752, 3;
	mov.u32 	%r9090, 4;
	sub.s32 	%r9091, %r9090, %r9089;
	shl.b32 	%r9092, %r9091, 2;
	mov.u32 	%r9093, 1985229328;
	shr.u32 	%r9094, %r9093, %r9092;
	and.b32  	%r1601, %r9094, 65535;
	shr.u32 	%r9088, %r11752, 2;
	setp.gt.s32	%p792, %r9088, 7;
	@%p792 bra 	BB3_1044;

	setp.gt.s32	%p804, %r9088, 3;
	@%p804 bra 	BB3_1037;

	setp.gt.s32	%p810, %r9088, 1;
	@%p810 bra 	BB3_1034;

	setp.eq.s32	%p813, %r9088, 0;
	@%p813 bra 	BB3_1069;
	bra.uni 	BB3_1032;

BB3_1069:
	// inline asm
	prmt.b32 %r11765, %r23, %r24, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11766, %r22, %r23, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11761, %r21, %r22, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11762, %r20, %r21, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11763, %r19, %r20, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11764, %r18, %r19, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11757, %r17, %r18, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11758, %r16, %r17, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11759, %r15, %r16, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11760, %r14, %r15, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11753, %r13, %r14, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11754, %r12, %r13, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11755, %r11, %r12, %r1601;
	// inline asm
	mov.u32 	%r9631, 0;
	// inline asm
	prmt.b32 %r11756, %r9631, %r11, %r1601;
	// inline asm
	bra.uni 	BB3_1070;

BB3_1044:
	setp.gt.s32	%p793, %r9088, 11;
	@%p793 bra 	BB3_1052;

	setp.gt.s32	%p799, %r9088, 9;
	@%p799 bra 	BB3_1049;

	setp.eq.s32	%p802, %r9088, 8;
	@%p802 bra 	BB3_1063;
	bra.uni 	BB3_1047;

BB3_1063:
	// inline asm
	prmt.b32 %r11765, %r15, %r16, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11766, %r14, %r15, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11761, %r13, %r14, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11762, %r12, %r13, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11763, %r11, %r12, %r1601;
	// inline asm
	mov.u32 	%r11753, 0;
	// inline asm
	prmt.b32 %r11764, %r11753, %r11, %r1601;
	// inline asm
	mov.u32 	%r11754, %r11753;
	mov.u32 	%r11755, %r11753;
	mov.u32 	%r11756, %r11753;
	mov.u32 	%r11757, %r11753;
	bra.uni 	BB3_1064;

BB3_1037:
	setp.gt.s32	%p805, %r9088, 5;
	@%p805 bra 	BB3_1041;

	setp.eq.s32	%p808, %r9088, 4;
	@%p808 bra 	BB3_1067;
	bra.uni 	BB3_1039;

BB3_1067:
	// inline asm
	prmt.b32 %r11765, %r19, %r20, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11766, %r18, %r19, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11761, %r17, %r18, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11762, %r16, %r17, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11763, %r15, %r16, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11764, %r14, %r15, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11757, %r13, %r14, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11758, %r12, %r13, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11759, %r11, %r12, %r1601;
	// inline asm
	mov.u32 	%r11753, 0;
	// inline asm
	prmt.b32 %r11760, %r11753, %r11, %r1601;
	// inline asm
	mov.u32 	%r11754, %r11753;
	mov.u32 	%r11755, %r11753;
	mov.u32 	%r11756, %r11753;
	bra.uni 	BB3_1070;

BB3_1052:
	setp.gt.s32	%p794, %r9088, 13;
	@%p794 bra 	BB3_1056;

	setp.eq.s32	%p797, %r9088, 12;
	@%p797 bra 	BB3_1059;
	bra.uni 	BB3_1054;

BB3_1059:
	// inline asm
	prmt.b32 %r11765, %r11, %r12, %r1601;
	// inline asm
	mov.u32 	%r11753, 0;
	// inline asm
	prmt.b32 %r11766, %r11753, %r11, %r1601;
	// inline asm
	mov.u32 	%r11754, %r11753;
	mov.u32 	%r11755, %r11753;
	mov.u32 	%r11756, %r11753;
	mov.u32 	%r11757, %r11753;
	mov.u32 	%r11758, %r11753;
	mov.u32 	%r11759, %r11753;
	mov.u32 	%r11760, %r11753;
	mov.u32 	%r11761, %r11753;
	bra.uni 	BB3_1060;

BB3_1034:
	setp.eq.s32	%p811, %r9088, 2;
	@%p811 bra 	BB3_1068;
	bra.uni 	BB3_1035;

BB3_1068:
	// inline asm
	prmt.b32 %r11765, %r21, %r22, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11766, %r20, %r21, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11761, %r19, %r20, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11762, %r18, %r19, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11763, %r17, %r18, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11764, %r16, %r17, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11757, %r15, %r16, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11758, %r14, %r15, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11759, %r13, %r14, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11760, %r12, %r13, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11753, %r11, %r12, %r1601;
	// inline asm
	mov.u32 	%r11755, 0;
	// inline asm
	prmt.b32 %r11754, %r11755, %r11, %r1601;
	// inline asm
	mov.u32 	%r11756, %r11755;
	bra.uni 	BB3_1070;

BB3_1049:
	setp.eq.s32	%p800, %r9088, 10;
	@%p800 bra 	BB3_1062;
	bra.uni 	BB3_1050;

BB3_1062:
	// inline asm
	prmt.b32 %r11765, %r13, %r14, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11766, %r12, %r13, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11761, %r11, %r12, %r1601;
	// inline asm
	mov.u32 	%r11753, 0;
	// inline asm
	prmt.b32 %r11762, %r11753, %r11, %r1601;
	// inline asm
	mov.u32 	%r11754, %r11753;
	mov.u32 	%r11755, %r11753;
	mov.u32 	%r11756, %r11753;
	mov.u32 	%r11757, %r11753;
	mov.u32 	%r11758, %r11753;
	mov.u32 	%r11759, %r11753;
	mov.u32 	%r11760, %r11753;
	bra.uni 	BB3_1061;

BB3_1041:
	setp.eq.s32	%p806, %r9088, 6;
	@%p806 bra 	BB3_1066;
	bra.uni 	BB3_1042;

BB3_1066:
	// inline asm
	prmt.b32 %r11765, %r17, %r18, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11766, %r16, %r17, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11761, %r15, %r16, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11762, %r14, %r15, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11763, %r13, %r14, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11764, %r12, %r13, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11757, %r11, %r12, %r1601;
	// inline asm
	mov.u32 	%r11753, 0;
	// inline asm
	prmt.b32 %r11758, %r11753, %r11, %r1601;
	// inline asm
	mov.u32 	%r11754, %r11753;
	mov.u32 	%r11755, %r11753;
	mov.u32 	%r11756, %r11753;
	bra.uni 	BB3_1065;

BB3_1056:
	setp.eq.s32	%p795, %r9088, 14;
	@%p795 bra 	BB3_1058;

	setp.ne.s32	%p796, %r9088, 15;
	mov.u32 	%r11753, %r14;
	mov.u32 	%r11754, %r13;
	mov.u32 	%r11755, %r12;
	mov.u32 	%r11756, %r11;
	mov.u32 	%r11757, %r18;
	mov.u32 	%r11758, %r17;
	mov.u32 	%r11759, %r16;
	mov.u32 	%r11760, %r15;
	mov.u32 	%r11761, %r22;
	mov.u32 	%r11762, %r21;
	mov.u32 	%r11763, %r20;
	mov.u32 	%r11764, %r19;
	mov.u32 	%r11765, %r24;
	mov.u32 	%r11766, %r23;
	@%p796 bra 	BB3_1070;

BB3_1058:
	mov.u32 	%r11753, 0;
	mov.u32 	%r11754, %r11753;
	mov.u32 	%r11755, %r11753;
	mov.u32 	%r11756, %r11753;
	mov.u32 	%r11757, %r11753;
	mov.u32 	%r11758, %r11753;
	mov.u32 	%r11759, %r11753;
	mov.u32 	%r11760, %r11753;
	mov.u32 	%r11761, %r11753;
	mov.u32 	%r11762, %r11753;
	mov.u32 	%r11763, %r11753;
	mov.u32 	%r11764, %r11753;
	mov.u32 	%r11765, %r11753;
	mov.u32 	%r11766, %r11753;
	bra.uni 	BB3_1070;

BB3_1032:
	setp.eq.s32	%p814, %r9088, 1;
	mov.u32 	%r11753, %r14;
	mov.u32 	%r11754, %r13;
	mov.u32 	%r11755, %r12;
	mov.u32 	%r11756, %r11;
	mov.u32 	%r11757, %r18;
	mov.u32 	%r11758, %r17;
	mov.u32 	%r11759, %r16;
	mov.u32 	%r11760, %r15;
	mov.u32 	%r11761, %r22;
	mov.u32 	%r11762, %r21;
	mov.u32 	%r11763, %r20;
	mov.u32 	%r11764, %r19;
	mov.u32 	%r11765, %r24;
	mov.u32 	%r11766, %r23;
	@%p814 bra 	BB3_1033;
	bra.uni 	BB3_1070;

BB3_1033:
	// inline asm
	prmt.b32 %r11765, %r22, %r23, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11766, %r21, %r22, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11761, %r20, %r21, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11762, %r19, %r20, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11763, %r18, %r19, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11764, %r17, %r18, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11757, %r16, %r17, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11758, %r15, %r16, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11759, %r14, %r15, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11760, %r13, %r14, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11753, %r12, %r13, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11754, %r11, %r12, %r1601;
	// inline asm
	mov.u32 	%r11756, 0;
	// inline asm
	prmt.b32 %r11755, %r11756, %r11, %r1601;
	// inline asm
	bra.uni 	BB3_1070;

BB3_1047:
	setp.eq.s32	%p803, %r9088, 9;
	mov.u32 	%r11753, %r14;
	mov.u32 	%r11754, %r13;
	mov.u32 	%r11755, %r12;
	mov.u32 	%r11756, %r11;
	mov.u32 	%r11757, %r18;
	mov.u32 	%r11758, %r17;
	mov.u32 	%r11759, %r16;
	mov.u32 	%r11760, %r15;
	mov.u32 	%r11761, %r22;
	mov.u32 	%r11762, %r21;
	mov.u32 	%r11763, %r20;
	mov.u32 	%r11764, %r19;
	mov.u32 	%r11765, %r24;
	mov.u32 	%r11766, %r23;
	@%p803 bra 	BB3_1048;
	bra.uni 	BB3_1070;

BB3_1048:
	// inline asm
	prmt.b32 %r11765, %r14, %r15, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11766, %r13, %r14, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11761, %r12, %r13, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11762, %r11, %r12, %r1601;
	// inline asm
	mov.u32 	%r11753, 0;
	// inline asm
	prmt.b32 %r11763, %r11753, %r11, %r1601;
	// inline asm
	mov.u32 	%r11754, %r11753;
	mov.u32 	%r11755, %r11753;
	mov.u32 	%r11756, %r11753;
	mov.u32 	%r11757, %r11753;
	mov.u32 	%r11758, %r11753;
	mov.u32 	%r11759, %r11753;
	mov.u32 	%r11760, %r11753;
	mov.u32 	%r11764, %r11753;
	bra.uni 	BB3_1070;

BB3_1039:
	setp.eq.s32	%p809, %r9088, 5;
	mov.u32 	%r11753, %r14;
	mov.u32 	%r11754, %r13;
	mov.u32 	%r11755, %r12;
	mov.u32 	%r11756, %r11;
	mov.u32 	%r11757, %r18;
	mov.u32 	%r11758, %r17;
	mov.u32 	%r11759, %r16;
	mov.u32 	%r11760, %r15;
	mov.u32 	%r11761, %r22;
	mov.u32 	%r11762, %r21;
	mov.u32 	%r11763, %r20;
	mov.u32 	%r11764, %r19;
	mov.u32 	%r11765, %r24;
	mov.u32 	%r11766, %r23;
	@%p809 bra 	BB3_1040;
	bra.uni 	BB3_1070;

BB3_1040:
	// inline asm
	prmt.b32 %r11765, %r18, %r19, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11766, %r17, %r18, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11761, %r16, %r17, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11762, %r15, %r16, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11763, %r14, %r15, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11764, %r13, %r14, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11757, %r12, %r13, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11758, %r11, %r12, %r1601;
	// inline asm
	mov.u32 	%r11753, 0;
	// inline asm
	prmt.b32 %r11759, %r11753, %r11, %r1601;
	// inline asm
	mov.u32 	%r11754, %r11753;
	mov.u32 	%r11755, %r11753;
	mov.u32 	%r11756, %r11753;
	mov.u32 	%r11760, %r11753;
	bra.uni 	BB3_1070;

BB3_1054:
	setp.eq.s32	%p798, %r9088, 13;
	mov.u32 	%r11753, %r14;
	mov.u32 	%r11754, %r13;
	mov.u32 	%r11755, %r12;
	mov.u32 	%r11756, %r11;
	mov.u32 	%r11757, %r18;
	mov.u32 	%r11758, %r17;
	mov.u32 	%r11759, %r16;
	mov.u32 	%r11760, %r15;
	mov.u32 	%r11761, %r22;
	mov.u32 	%r11762, %r21;
	mov.u32 	%r11763, %r20;
	mov.u32 	%r11764, %r19;
	mov.u32 	%r11765, %r24;
	mov.u32 	%r11766, %r23;
	@%p798 bra 	BB3_1055;
	bra.uni 	BB3_1070;

BB3_1055:
	mov.u32 	%r11753, 0;
	// inline asm
	prmt.b32 %r11765, %r11753, %r11, %r1601;
	// inline asm
	mov.u32 	%r11754, %r11753;
	mov.u32 	%r11755, %r11753;
	mov.u32 	%r11756, %r11753;
	mov.u32 	%r11757, %r11753;
	mov.u32 	%r11758, %r11753;
	mov.u32 	%r11759, %r11753;
	mov.u32 	%r11760, %r11753;
	mov.u32 	%r11761, %r11753;
	mov.u32 	%r11762, %r11753;
	mov.u32 	%r11763, %r11753;
	mov.u32 	%r11764, %r11753;
	mov.u32 	%r11766, %r11753;
	bra.uni 	BB3_1070;

BB3_1035:
	setp.eq.s32	%p812, %r9088, 3;
	mov.u32 	%r11753, %r14;
	mov.u32 	%r11754, %r13;
	mov.u32 	%r11755, %r12;
	mov.u32 	%r11756, %r11;
	mov.u32 	%r11757, %r18;
	mov.u32 	%r11758, %r17;
	mov.u32 	%r11759, %r16;
	mov.u32 	%r11760, %r15;
	mov.u32 	%r11761, %r22;
	mov.u32 	%r11762, %r21;
	mov.u32 	%r11763, %r20;
	mov.u32 	%r11764, %r19;
	mov.u32 	%r11765, %r24;
	mov.u32 	%r11766, %r23;
	@%p812 bra 	BB3_1036;
	bra.uni 	BB3_1070;

BB3_1036:
	// inline asm
	prmt.b32 %r11765, %r20, %r21, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11766, %r19, %r20, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11761, %r18, %r19, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11762, %r17, %r18, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11763, %r16, %r17, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11764, %r15, %r16, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11757, %r14, %r15, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11758, %r13, %r14, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11759, %r12, %r13, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11760, %r11, %r12, %r1601;
	// inline asm
	mov.u32 	%r11754, 0;
	// inline asm
	prmt.b32 %r11753, %r11754, %r11, %r1601;
	// inline asm
	mov.u32 	%r11755, %r11754;
	mov.u32 	%r11756, %r11754;
	bra.uni 	BB3_1070;

BB3_1050:
	setp.eq.s32	%p801, %r9088, 11;
	mov.u32 	%r11753, %r14;
	mov.u32 	%r11754, %r13;
	mov.u32 	%r11755, %r12;
	mov.u32 	%r11756, %r11;
	mov.u32 	%r11757, %r18;
	mov.u32 	%r11758, %r17;
	mov.u32 	%r11759, %r16;
	mov.u32 	%r11760, %r15;
	mov.u32 	%r11761, %r22;
	mov.u32 	%r11762, %r21;
	mov.u32 	%r11763, %r20;
	mov.u32 	%r11764, %r19;
	mov.u32 	%r11765, %r24;
	mov.u32 	%r11766, %r23;
	@%p801 bra 	BB3_1051;
	bra.uni 	BB3_1070;

BB3_1051:
	// inline asm
	prmt.b32 %r11765, %r12, %r13, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11766, %r11, %r12, %r1601;
	// inline asm
	mov.u32 	%r11753, 0;
	// inline asm
	prmt.b32 %r11761, %r11753, %r11, %r1601;
	// inline asm
	mov.u32 	%r11754, %r11753;
	mov.u32 	%r11755, %r11753;
	mov.u32 	%r11756, %r11753;
	mov.u32 	%r11757, %r11753;
	mov.u32 	%r11758, %r11753;
	mov.u32 	%r11759, %r11753;
	mov.u32 	%r11760, %r11753;

BB3_1060:
	mov.u32 	%r11762, %r11753;

BB3_1061:
	mov.u32 	%r11763, %r11753;
	mov.u32 	%r11764, %r11753;
	bra.uni 	BB3_1070;

BB3_1042:
	setp.eq.s32	%p807, %r9088, 7;
	mov.u32 	%r11753, %r14;
	mov.u32 	%r11754, %r13;
	mov.u32 	%r11755, %r12;
	mov.u32 	%r11756, %r11;
	mov.u32 	%r11757, %r18;
	mov.u32 	%r11758, %r17;
	mov.u32 	%r11759, %r16;
	mov.u32 	%r11760, %r15;
	mov.u32 	%r11761, %r22;
	mov.u32 	%r11762, %r21;
	mov.u32 	%r11763, %r20;
	mov.u32 	%r11764, %r19;
	mov.u32 	%r11765, %r24;
	mov.u32 	%r11766, %r23;
	@%p807 bra 	BB3_1043;
	bra.uni 	BB3_1070;

BB3_1043:
	// inline asm
	prmt.b32 %r11765, %r16, %r17, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11766, %r15, %r16, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11761, %r14, %r15, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11762, %r13, %r14, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11763, %r12, %r13, %r1601;
	// inline asm
	// inline asm
	prmt.b32 %r11764, %r11, %r12, %r1601;
	// inline asm
	mov.u32 	%r11753, 0;
	// inline asm
	prmt.b32 %r11757, %r11753, %r11, %r1601;
	// inline asm
	mov.u32 	%r11754, %r11753;
	mov.u32 	%r11755, %r11753;
	mov.u32 	%r11756, %r11753;

BB3_1064:
	mov.u32 	%r11758, %r11753;

BB3_1065:
	mov.u32 	%r11759, %r11753;
	mov.u32 	%r11760, %r11753;

BB3_1070:
	ld.param.u64 	%rd2864, [m01710_m04_param_6];
	ld.param.u32 	%r11534, [m01710_m04_param_25];
	ld.const.u64 	%rd2863, [k_sha512+608];
	ld.const.u64 	%rd2862, [k_sha512+600];
	ld.const.u64 	%rd2861, [k_sha512+592];
	ld.const.u64 	%rd2853, [k_sha512+584];
	ld.const.u64 	%rd2852, [k_sha512+576];
	ld.const.u64 	%rd2851, [k_sha512+568];
	ld.const.u64 	%rd2850, [k_sha512+560];
	ld.const.u64 	%rd2849, [k_sha512+552];
	ld.const.u64 	%rd2848, [k_sha512+544];
	ld.const.u64 	%rd2847, [k_sha512+536];
	ld.const.u64 	%rd2846, [k_sha512+528];
	ld.const.u64 	%rd2845, [k_sha512+520];
	ld.const.u64 	%rd2844, [k_sha512+512];
	ld.const.u64 	%rd2843, [k_sha512+504];
	ld.const.u64 	%rd2842, [k_sha512+496];
	ld.const.u64 	%rd2841, [k_sha512+488];
	ld.const.u64 	%rd2840, [k_sha512+480];
	ld.const.u64 	%rd2839, [k_sha512+472];
	ld.const.u64 	%rd2838, [k_sha512+464];
	ld.const.u64 	%rd2837, [k_sha512+456];
	ld.const.u64 	%rd2836, [k_sha512+448];
	ld.const.u64 	%rd2835, [k_sha512+440];
	ld.const.u64 	%rd2834, [k_sha512+432];
	ld.const.u64 	%rd2833, [k_sha512+424];
	ld.const.u64 	%rd2832, [k_sha512+416];
	ld.const.u64 	%rd2831, [k_sha512+408];
	ld.const.u64 	%rd2830, [k_sha512+400];
	ld.const.u64 	%rd2829, [k_sha512+392];
	ld.const.u64 	%rd2828, [k_sha512+384];
	ld.const.u64 	%rd2827, [k_sha512+376];
	ld.const.u64 	%rd2826, [k_sha512+368];
	ld.const.u64 	%rd2825, [k_sha512+360];
	ld.const.u64 	%rd2824, [k_sha512+352];
	ld.const.u64 	%rd2823, [k_sha512+344];
	ld.const.u64 	%rd2822, [k_sha512+336];
	ld.const.u64 	%rd2821, [k_sha512+328];
	ld.const.u64 	%rd2820, [k_sha512+320];
	ld.const.u64 	%rd2819, [k_sha512+312];
	ld.const.u64 	%rd2818, [k_sha512+304];
	ld.const.u64 	%rd2817, [k_sha512+296];
	ld.const.u64 	%rd2816, [k_sha512+288];
	ld.const.u64 	%rd2815, [k_sha512+280];
	ld.const.u64 	%rd2814, [k_sha512+272];
	ld.const.u64 	%rd2813, [k_sha512+264];
	ld.const.u64 	%rd2812, [k_sha512+256];
	ld.const.u64 	%rd2811, [k_sha512+248];
	ld.const.u64 	%rd2810, [k_sha512+240];
	ld.const.u64 	%rd2809, [k_sha512+232];
	ld.const.u64 	%rd2808, [k_sha512+224];
	ld.const.u64 	%rd2807, [k_sha512+216];
	ld.const.u64 	%rd2806, [k_sha512+208];
	ld.const.u64 	%rd2805, [k_sha512+200];
	ld.const.u64 	%rd2804, [k_sha512+192];
	ld.const.u64 	%rd2803, [k_sha512+184];
	ld.const.u64 	%rd2802, [k_sha512+176];
	ld.const.u64 	%rd2801, [k_sha512+168];
	ld.const.u64 	%rd2800, [k_sha512+160];
	ld.const.u64 	%rd2799, [k_sha512+152];
	ld.const.u64 	%rd2798, [k_sha512+144];
	ld.const.u64 	%rd2797, [k_sha512+136];
	ld.const.u64 	%rd2796, [k_sha512+128];
	ld.const.u64 	%rd2795, [k_sha512+120];
	ld.const.u64 	%rd2794, [k_sha512+112];
	ld.const.u64 	%rd2793, [k_sha512+104];
	ld.const.u64 	%rd2792, [k_sha512+96];
	ld.const.u64 	%rd2791, [k_sha512+88];
	ld.const.u64 	%rd2790, [k_sha512+80];
	ld.const.u64 	%rd2789, [k_sha512+72];
	ld.const.u64 	%rd2788, [k_sha512+64];
	ld.const.u64 	%rd2787, [k_sha512+56];
	ld.const.u64 	%rd2786, [k_sha512+48];
	ld.const.u64 	%rd2785, [k_sha512+40];
	ld.const.u64 	%rd2784, [k_sha512+32];
	ld.const.u64 	%rd2783, [k_sha512+24];
	ld.const.u64 	%rd2782, [k_sha512+16];
	ld.const.u64 	%rd2781, [k_sha512+8];
	ld.const.u64 	%rd2780, [k_sha512];
	or.b32  	%r9635, %r11756, %r11740;
	or.b32  	%r9637, %r11755, %r11741;
	or.b32  	%r9639, %r11754, %r11742;
	or.b32  	%r9641, %r11753, %r11743;
	or.b32  	%r9643, %r11760, %r11739;
	or.b32  	%r9645, %r11759, %r11738;
	or.b32  	%r9647, %r11758, %r11737;
	or.b32  	%r9649, %r11757, %r11736;
	add.s32 	%r9662, %r11752, %r25;
	// inline asm
	prmt.b32 %r9634, %r9635, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r9636, %r9637, 0, 0x0123;
	// inline asm
	mov.b64	%rd191, {%r9636, %r9634};
	// inline asm
	prmt.b32 %r9638, %r9639, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r9640, %r9641, 0, 0x0123;
	// inline asm
	mov.b64	%rd192, {%r9640, %r9638};
	// inline asm
	prmt.b32 %r9642, %r9643, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r9644, %r9645, 0, 0x0123;
	// inline asm
	mov.b64	%rd193, {%r9644, %r9642};
	// inline asm
	prmt.b32 %r9646, %r9647, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r9648, %r9649, 0, 0x0123;
	// inline asm
	mov.b64	%rd194, {%r9648, %r9646};
	// inline asm
	prmt.b32 %r9650, %r11764, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r9652, %r11763, 0, 0x0123;
	// inline asm
	mov.b64	%rd195, {%r9652, %r9650};
	// inline asm
	prmt.b32 %r9654, %r11762, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r9656, %r11761, 0, 0x0123;
	// inline asm
	mov.b64	%rd196, {%r9656, %r9654};
	// inline asm
	prmt.b32 %r9658, %r11766, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r9660, %r11765, 0, 0x0123;
	// inline asm
	mov.b64	%rd197, {%r9660, %r9658};
	shl.b32 	%r9663, %r9662, 3;
	mov.u32 	%r9664, 0;
	mov.b64	%rd198, {%r9663, %r9664};
	add.s64 	%rd199, %rd191, %rd2780;
	add.s64 	%rd200, %rd199, %rd4;
	add.s64 	%rd201, %rd200, %rd5;
	add.s64 	%rd202, %rd200, 2357225248857953701;
	add.s64 	%rd203, %rd201, -5343946410804754465;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9665,%dummy}, %rd202;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9666}, %rd202;
	}
	shf.r.wrap.b32 	%r9667, %r9666, %r9665, 14;
	shf.r.wrap.b32 	%r9668, %r9665, %r9666, 14;
	mov.b64 	%rd204, {%r9668, %r9667};
	shf.r.wrap.b32 	%r9669, %r9666, %r9665, 18;
	shf.r.wrap.b32 	%r9670, %r9665, %r9666, 18;
	mov.b64 	%rd205, {%r9670, %r9669};
	xor.b64  	%rd206, %rd205, %rd204;
	shf.l.wrap.b32 	%r9671, %r9665, %r9666, 23;
	shf.l.wrap.b32 	%r9672, %r9666, %r9665, 23;
	mov.b64 	%rd207, {%r9672, %r9671};
	xor.b64  	%rd208, %rd206, %rd207;
	and.b64  	%rd209, %rd202, -3887949035690463538;
	xor.b64  	%rd210, %rd209, -7276294671716946913;
	add.s64 	%rd211, %rd192, %rd2781;
	add.s64 	%rd212, %rd211, %rd210;
	add.s64 	%rd213, %rd212, %rd208;
	xor.b64  	%rd214, %rd203, -4942790177534073029;
	xor.b64  	%rd215, %rd203, 7640891576956012808;
	and.b64  	%rd216, %rd215, %rd214;
	xor.b64  	%rd217, %rd216, %rd203;
	add.s64 	%rd218, %rd213, %rd217;
	add.s64 	%rd219, %rd213, 6625583534739731862;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9673,%dummy}, %rd203;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9674}, %rd203;
	}
	shf.r.wrap.b32 	%r9675, %r9674, %r9673, 28;
	shf.r.wrap.b32 	%r9676, %r9673, %r9674, 28;
	mov.b64 	%rd220, {%r9676, %r9675};
	shf.l.wrap.b32 	%r9677, %r9673, %r9674, 30;
	shf.l.wrap.b32 	%r9678, %r9674, %r9673, 30;
	mov.b64 	%rd221, {%r9678, %r9677};
	xor.b64  	%rd222, %rd221, %rd220;
	shf.l.wrap.b32 	%r9679, %r9673, %r9674, 25;
	shf.l.wrap.b32 	%r9680, %r9674, %r9673, 25;
	mov.b64 	%rd223, {%r9680, %r9679};
	xor.b64  	%rd224, %rd222, %rd223;
	add.s64 	%rd225, %rd218, %rd224;
	add.s64 	%rd226, %rd225, 2270897969802886507;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9681,%dummy}, %rd219;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9682}, %rd219;
	}
	shf.r.wrap.b32 	%r9683, %r9682, %r9681, 14;
	shf.r.wrap.b32 	%r9684, %r9681, %r9682, 14;
	mov.b64 	%rd227, {%r9684, %r9683};
	shf.r.wrap.b32 	%r9685, %r9682, %r9681, 18;
	shf.r.wrap.b32 	%r9686, %r9681, %r9682, 18;
	mov.b64 	%rd228, {%r9686, %r9685};
	xor.b64  	%rd229, %rd228, %rd227;
	shf.l.wrap.b32 	%r9687, %r9681, %r9682, 23;
	shf.l.wrap.b32 	%r9688, %r9682, %r9681, 23;
	mov.b64 	%rd230, {%r9688, %r9687};
	xor.b64  	%rd231, %rd229, %rd230;
	xor.b64  	%rd232, %rd202, 5840696475078001361;
	and.b64  	%rd233, %rd219, %rd232;
	xor.b64  	%rd234, %rd233, 5840696475078001361;
	add.s64 	%rd235, %rd193, %rd2782;
	add.s64 	%rd236, %rd235, %rd234;
	add.s64 	%rd237, %rd236, %rd231;
	xor.b64  	%rd238, %rd226, 7640891576956012808;
	xor.b64  	%rd239, %rd226, %rd203;
	and.b64  	%rd240, %rd239, %rd238;
	xor.b64  	%rd241, %rd240, %rd226;
	add.s64 	%rd242, %rd237, %rd241;
	add.s64 	%rd243, %rd237, 6227659224458531674;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9689,%dummy}, %rd226;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9690}, %rd226;
	}
	shf.r.wrap.b32 	%r9691, %r9690, %r9689, 28;
	shf.r.wrap.b32 	%r9692, %r9689, %r9690, 28;
	mov.b64 	%rd244, {%r9692, %r9691};
	shf.l.wrap.b32 	%r9693, %r9689, %r9690, 30;
	shf.l.wrap.b32 	%r9694, %r9690, %r9689, 30;
	mov.b64 	%rd245, {%r9694, %r9693};
	xor.b64  	%rd246, %rd245, %rd244;
	shf.l.wrap.b32 	%r9695, %r9689, %r9690, 25;
	shf.l.wrap.b32 	%r9696, %r9690, %r9689, 25;
	mov.b64 	%rd247, {%r9696, %r9695};
	xor.b64  	%rd248, %rd246, %rd247;
	add.s64 	%rd249, %rd242, %rd248;
	add.s64 	%rd250, %rd249, -7276294671716946913;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9697,%dummy}, %rd243;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9698}, %rd243;
	}
	shf.r.wrap.b32 	%r9699, %r9698, %r9697, 14;
	shf.r.wrap.b32 	%r9700, %r9697, %r9698, 14;
	mov.b64 	%rd251, {%r9700, %r9699};
	shf.r.wrap.b32 	%r9701, %r9698, %r9697, 18;
	shf.r.wrap.b32 	%r9702, %r9697, %r9698, 18;
	mov.b64 	%rd252, {%r9702, %r9701};
	xor.b64  	%rd253, %rd252, %rd251;
	shf.l.wrap.b32 	%r9703, %r9697, %r9698, 23;
	shf.l.wrap.b32 	%r9704, %r9698, %r9697, 23;
	mov.b64 	%rd254, {%r9704, %r9703};
	xor.b64  	%rd255, %rd253, %rd254;
	xor.b64  	%rd256, %rd219, %rd202;
	and.b64  	%rd257, %rd243, %rd256;
	xor.b64  	%rd258, %rd257, %rd202;
	add.s64 	%rd259, %rd194, %rd2783;
	add.s64 	%rd260, %rd259, %rd258;
	add.s64 	%rd261, %rd260, %rd255;
	xor.b64  	%rd262, %rd250, %rd203;
	xor.b64  	%rd263, %rd250, %rd226;
	and.b64  	%rd264, %rd263, %rd262;
	xor.b64  	%rd265, %rd264, %rd250;
	add.s64 	%rd266, %rd261, %rd265;
	add.s64 	%rd267, %rd261, -4965156021675537447;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9705,%dummy}, %rd250;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9706}, %rd250;
	}
	shf.r.wrap.b32 	%r9707, %r9706, %r9705, 28;
	shf.r.wrap.b32 	%r9708, %r9705, %r9706, 28;
	mov.b64 	%rd268, {%r9708, %r9707};
	shf.l.wrap.b32 	%r9709, %r9705, %r9706, 30;
	shf.l.wrap.b32 	%r9710, %r9706, %r9705, 30;
	mov.b64 	%rd269, {%r9710, %r9709};
	xor.b64  	%rd270, %rd269, %rd268;
	shf.l.wrap.b32 	%r9711, %r9705, %r9706, 25;
	shf.l.wrap.b32 	%r9712, %r9706, %r9705, 25;
	mov.b64 	%rd271, {%r9712, %r9711};
	xor.b64  	%rd272, %rd270, %rd271;
	add.s64 	%rd273, %rd266, %rd272;
	add.s64 	%rd274, %rd273, 5840696475078001361;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9713,%dummy}, %rd267;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9714}, %rd267;
	}
	shf.r.wrap.b32 	%r9715, %r9714, %r9713, 14;
	shf.r.wrap.b32 	%r9716, %r9713, %r9714, 14;
	mov.b64 	%rd275, {%r9716, %r9715};
	shf.r.wrap.b32 	%r9717, %r9714, %r9713, 18;
	shf.r.wrap.b32 	%r9718, %r9713, %r9714, 18;
	mov.b64 	%rd276, {%r9718, %r9717};
	xor.b64  	%rd277, %rd276, %rd275;
	shf.l.wrap.b32 	%r9719, %r9713, %r9714, 23;
	shf.l.wrap.b32 	%r9720, %r9714, %r9713, 23;
	mov.b64 	%rd278, {%r9720, %r9719};
	xor.b64  	%rd279, %rd277, %rd278;
	xor.b64  	%rd280, %rd243, %rd219;
	and.b64  	%rd281, %rd267, %rd280;
	xor.b64  	%rd282, %rd281, %rd219;
	add.s64 	%rd283, %rd202, %rd195;
	add.s64 	%rd284, %rd283, %rd2784;
	add.s64 	%rd285, %rd284, %rd282;
	add.s64 	%rd286, %rd285, %rd279;
	add.s64 	%rd287, %rd286, %rd203;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9721,%dummy}, %rd274;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9722}, %rd274;
	}
	shf.r.wrap.b32 	%r9723, %r9722, %r9721, 28;
	shf.r.wrap.b32 	%r9724, %r9721, %r9722, 28;
	mov.b64 	%rd288, {%r9724, %r9723};
	shf.l.wrap.b32 	%r9725, %r9721, %r9722, 30;
	shf.l.wrap.b32 	%r9726, %r9722, %r9721, 30;
	mov.b64 	%rd289, {%r9726, %r9725};
	xor.b64  	%rd290, %rd289, %rd288;
	shf.l.wrap.b32 	%r9727, %r9721, %r9722, 25;
	shf.l.wrap.b32 	%r9728, %r9722, %r9721, 25;
	mov.b64 	%rd291, {%r9728, %r9727};
	xor.b64  	%rd292, %rd290, %rd291;
	xor.b64  	%rd293, %rd274, %rd226;
	xor.b64  	%rd294, %rd274, %rd250;
	and.b64  	%rd295, %rd294, %rd293;
	xor.b64  	%rd296, %rd295, %rd274;
	add.s64 	%rd297, %rd286, %rd296;
	add.s64 	%rd298, %rd297, %rd292;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9729,%dummy}, %rd287;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9730}, %rd287;
	}
	shf.r.wrap.b32 	%r9731, %r9730, %r9729, 14;
	shf.r.wrap.b32 	%r9732, %r9729, %r9730, 14;
	mov.b64 	%rd299, {%r9732, %r9731};
	shf.r.wrap.b32 	%r9733, %r9730, %r9729, 18;
	shf.r.wrap.b32 	%r9734, %r9729, %r9730, 18;
	mov.b64 	%rd300, {%r9734, %r9733};
	xor.b64  	%rd301, %rd300, %rd299;
	shf.l.wrap.b32 	%r9735, %r9729, %r9730, 23;
	shf.l.wrap.b32 	%r9736, %r9730, %r9729, 23;
	mov.b64 	%rd302, {%r9736, %r9735};
	xor.b64  	%rd303, %rd301, %rd302;
	xor.b64  	%rd304, %rd267, %rd243;
	and.b64  	%rd305, %rd287, %rd304;
	xor.b64  	%rd306, %rd305, %rd243;
	add.s64 	%rd307, %rd219, %rd196;
	add.s64 	%rd308, %rd307, %rd2785;
	add.s64 	%rd309, %rd308, %rd306;
	add.s64 	%rd310, %rd309, %rd303;
	add.s64 	%rd311, %rd310, %rd226;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9737,%dummy}, %rd298;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9738}, %rd298;
	}
	shf.r.wrap.b32 	%r9739, %r9738, %r9737, 28;
	shf.r.wrap.b32 	%r9740, %r9737, %r9738, 28;
	mov.b64 	%rd312, {%r9740, %r9739};
	shf.l.wrap.b32 	%r9741, %r9737, %r9738, 30;
	shf.l.wrap.b32 	%r9742, %r9738, %r9737, 30;
	mov.b64 	%rd313, {%r9742, %r9741};
	xor.b64  	%rd314, %rd313, %rd312;
	shf.l.wrap.b32 	%r9743, %r9737, %r9738, 25;
	shf.l.wrap.b32 	%r9744, %r9738, %r9737, 25;
	mov.b64 	%rd315, {%r9744, %r9743};
	xor.b64  	%rd316, %rd314, %rd315;
	xor.b64  	%rd317, %rd298, %rd250;
	xor.b64  	%rd318, %rd298, %rd274;
	and.b64  	%rd319, %rd318, %rd317;
	xor.b64  	%rd320, %rd319, %rd298;
	add.s64 	%rd321, %rd310, %rd320;
	add.s64 	%rd322, %rd321, %rd316;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9745,%dummy}, %rd311;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9746}, %rd311;
	}
	shf.r.wrap.b32 	%r9747, %r9746, %r9745, 14;
	shf.r.wrap.b32 	%r9748, %r9745, %r9746, 14;
	mov.b64 	%rd323, {%r9748, %r9747};
	shf.r.wrap.b32 	%r9749, %r9746, %r9745, 18;
	shf.r.wrap.b32 	%r9750, %r9745, %r9746, 18;
	mov.b64 	%rd324, {%r9750, %r9749};
	xor.b64  	%rd325, %rd324, %rd323;
	shf.l.wrap.b32 	%r9751, %r9745, %r9746, 23;
	shf.l.wrap.b32 	%r9752, %r9746, %r9745, 23;
	mov.b64 	%rd326, {%r9752, %r9751};
	xor.b64  	%rd327, %rd325, %rd326;
	xor.b64  	%rd328, %rd287, %rd267;
	and.b64  	%rd329, %rd311, %rd328;
	xor.b64  	%rd330, %rd329, %rd267;
	add.s64 	%rd331, %rd243, %rd197;
	add.s64 	%rd332, %rd331, %rd2786;
	add.s64 	%rd333, %rd332, %rd330;
	add.s64 	%rd334, %rd333, %rd327;
	add.s64 	%rd335, %rd334, %rd250;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9753,%dummy}, %rd322;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9754}, %rd322;
	}
	shf.r.wrap.b32 	%r9755, %r9754, %r9753, 28;
	shf.r.wrap.b32 	%r9756, %r9753, %r9754, 28;
	mov.b64 	%rd336, {%r9756, %r9755};
	shf.l.wrap.b32 	%r9757, %r9753, %r9754, 30;
	shf.l.wrap.b32 	%r9758, %r9754, %r9753, 30;
	mov.b64 	%rd337, {%r9758, %r9757};
	xor.b64  	%rd338, %rd337, %rd336;
	shf.l.wrap.b32 	%r9759, %r9753, %r9754, 25;
	shf.l.wrap.b32 	%r9760, %r9754, %r9753, 25;
	mov.b64 	%rd339, {%r9760, %r9759};
	xor.b64  	%rd340, %rd338, %rd339;
	xor.b64  	%rd341, %rd322, %rd274;
	xor.b64  	%rd342, %rd322, %rd298;
	and.b64  	%rd343, %rd342, %rd341;
	xor.b64  	%rd344, %rd343, %rd322;
	add.s64 	%rd345, %rd334, %rd344;
	add.s64 	%rd346, %rd345, %rd340;
	add.s64 	%rd347, %rd2787, %rd267;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9761,%dummy}, %rd335;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9762}, %rd335;
	}
	shf.r.wrap.b32 	%r9763, %r9762, %r9761, 14;
	shf.r.wrap.b32 	%r9764, %r9761, %r9762, 14;
	mov.b64 	%rd348, {%r9764, %r9763};
	shf.r.wrap.b32 	%r9765, %r9762, %r9761, 18;
	shf.r.wrap.b32 	%r9766, %r9761, %r9762, 18;
	mov.b64 	%rd349, {%r9766, %r9765};
	xor.b64  	%rd350, %rd349, %rd348;
	shf.l.wrap.b32 	%r9767, %r9761, %r9762, 23;
	shf.l.wrap.b32 	%r9768, %r9762, %r9761, 23;
	mov.b64 	%rd351, {%r9768, %r9767};
	xor.b64  	%rd352, %rd350, %rd351;
	xor.b64  	%rd353, %rd311, %rd287;
	and.b64  	%rd354, %rd335, %rd353;
	xor.b64  	%rd355, %rd354, %rd287;
	add.s64 	%rd356, %rd347, %rd355;
	add.s64 	%rd357, %rd356, %rd352;
	add.s64 	%rd358, %rd357, %rd274;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9769,%dummy}, %rd346;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9770}, %rd346;
	}
	shf.r.wrap.b32 	%r9771, %r9770, %r9769, 28;
	shf.r.wrap.b32 	%r9772, %r9769, %r9770, 28;
	mov.b64 	%rd359, {%r9772, %r9771};
	shf.l.wrap.b32 	%r9773, %r9769, %r9770, 30;
	shf.l.wrap.b32 	%r9774, %r9770, %r9769, 30;
	mov.b64 	%rd360, {%r9774, %r9773};
	xor.b64  	%rd361, %rd360, %rd359;
	shf.l.wrap.b32 	%r9775, %r9769, %r9770, 25;
	shf.l.wrap.b32 	%r9776, %r9770, %r9769, 25;
	mov.b64 	%rd362, {%r9776, %r9775};
	xor.b64  	%rd363, %rd361, %rd362;
	xor.b64  	%rd364, %rd346, %rd298;
	xor.b64  	%rd365, %rd346, %rd322;
	and.b64  	%rd366, %rd365, %rd364;
	xor.b64  	%rd367, %rd366, %rd346;
	add.s64 	%rd368, %rd357, %rd367;
	add.s64 	%rd369, %rd368, %rd363;
	add.s64 	%rd370, %rd2788, %rd287;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9777,%dummy}, %rd358;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9778}, %rd358;
	}
	shf.r.wrap.b32 	%r9779, %r9778, %r9777, 14;
	shf.r.wrap.b32 	%r9780, %r9777, %r9778, 14;
	mov.b64 	%rd371, {%r9780, %r9779};
	shf.r.wrap.b32 	%r9781, %r9778, %r9777, 18;
	shf.r.wrap.b32 	%r9782, %r9777, %r9778, 18;
	mov.b64 	%rd372, {%r9782, %r9781};
	xor.b64  	%rd373, %rd372, %rd371;
	shf.l.wrap.b32 	%r9783, %r9777, %r9778, 23;
	shf.l.wrap.b32 	%r9784, %r9778, %r9777, 23;
	mov.b64 	%rd374, {%r9784, %r9783};
	xor.b64  	%rd375, %rd373, %rd374;
	xor.b64  	%rd376, %rd335, %rd311;
	and.b64  	%rd377, %rd358, %rd376;
	xor.b64  	%rd378, %rd377, %rd311;
	add.s64 	%rd379, %rd370, %rd378;
	add.s64 	%rd380, %rd379, %rd375;
	add.s64 	%rd381, %rd380, %rd298;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9785,%dummy}, %rd369;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9786}, %rd369;
	}
	shf.r.wrap.b32 	%r9787, %r9786, %r9785, 28;
	shf.r.wrap.b32 	%r9788, %r9785, %r9786, 28;
	mov.b64 	%rd382, {%r9788, %r9787};
	shf.l.wrap.b32 	%r9789, %r9785, %r9786, 30;
	shf.l.wrap.b32 	%r9790, %r9786, %r9785, 30;
	mov.b64 	%rd383, {%r9790, %r9789};
	xor.b64  	%rd384, %rd383, %rd382;
	shf.l.wrap.b32 	%r9791, %r9785, %r9786, 25;
	shf.l.wrap.b32 	%r9792, %r9786, %r9785, 25;
	mov.b64 	%rd385, {%r9792, %r9791};
	xor.b64  	%rd386, %rd384, %rd385;
	xor.b64  	%rd387, %rd369, %rd322;
	xor.b64  	%rd388, %rd369, %rd346;
	and.b64  	%rd389, %rd388, %rd387;
	xor.b64  	%rd390, %rd389, %rd369;
	add.s64 	%rd391, %rd380, %rd390;
	add.s64 	%rd392, %rd391, %rd386;
	add.s64 	%rd393, %rd2789, %rd311;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9793,%dummy}, %rd381;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9794}, %rd381;
	}
	shf.r.wrap.b32 	%r9795, %r9794, %r9793, 14;
	shf.r.wrap.b32 	%r9796, %r9793, %r9794, 14;
	mov.b64 	%rd394, {%r9796, %r9795};
	shf.r.wrap.b32 	%r9797, %r9794, %r9793, 18;
	shf.r.wrap.b32 	%r9798, %r9793, %r9794, 18;
	mov.b64 	%rd395, {%r9798, %r9797};
	xor.b64  	%rd396, %rd395, %rd394;
	shf.l.wrap.b32 	%r9799, %r9793, %r9794, 23;
	shf.l.wrap.b32 	%r9800, %r9794, %r9793, 23;
	mov.b64 	%rd397, {%r9800, %r9799};
	xor.b64  	%rd398, %rd396, %rd397;
	xor.b64  	%rd399, %rd358, %rd335;
	and.b64  	%rd400, %rd381, %rd399;
	xor.b64  	%rd401, %rd400, %rd335;
	add.s64 	%rd402, %rd393, %rd401;
	add.s64 	%rd403, %rd402, %rd398;
	add.s64 	%rd404, %rd403, %rd322;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9801,%dummy}, %rd392;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9802}, %rd392;
	}
	shf.r.wrap.b32 	%r9803, %r9802, %r9801, 28;
	shf.r.wrap.b32 	%r9804, %r9801, %r9802, 28;
	mov.b64 	%rd405, {%r9804, %r9803};
	shf.l.wrap.b32 	%r9805, %r9801, %r9802, 30;
	shf.l.wrap.b32 	%r9806, %r9802, %r9801, 30;
	mov.b64 	%rd406, {%r9806, %r9805};
	xor.b64  	%rd407, %rd406, %rd405;
	shf.l.wrap.b32 	%r9807, %r9801, %r9802, 25;
	shf.l.wrap.b32 	%r9808, %r9802, %r9801, 25;
	mov.b64 	%rd408, {%r9808, %r9807};
	xor.b64  	%rd409, %rd407, %rd408;
	xor.b64  	%rd410, %rd392, %rd346;
	xor.b64  	%rd411, %rd392, %rd369;
	and.b64  	%rd412, %rd411, %rd410;
	xor.b64  	%rd413, %rd412, %rd392;
	add.s64 	%rd414, %rd403, %rd413;
	add.s64 	%rd415, %rd414, %rd409;
	add.s64 	%rd416, %rd2790, %rd335;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9809,%dummy}, %rd404;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9810}, %rd404;
	}
	shf.r.wrap.b32 	%r9811, %r9810, %r9809, 14;
	shf.r.wrap.b32 	%r9812, %r9809, %r9810, 14;
	mov.b64 	%rd417, {%r9812, %r9811};
	shf.r.wrap.b32 	%r9813, %r9810, %r9809, 18;
	shf.r.wrap.b32 	%r9814, %r9809, %r9810, 18;
	mov.b64 	%rd418, {%r9814, %r9813};
	xor.b64  	%rd419, %rd418, %rd417;
	shf.l.wrap.b32 	%r9815, %r9809, %r9810, 23;
	shf.l.wrap.b32 	%r9816, %r9810, %r9809, 23;
	mov.b64 	%rd420, {%r9816, %r9815};
	xor.b64  	%rd421, %rd419, %rd420;
	xor.b64  	%rd422, %rd381, %rd358;
	and.b64  	%rd423, %rd404, %rd422;
	xor.b64  	%rd424, %rd423, %rd358;
	add.s64 	%rd425, %rd416, %rd424;
	add.s64 	%rd426, %rd425, %rd421;
	add.s64 	%rd427, %rd426, %rd346;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9817,%dummy}, %rd415;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9818}, %rd415;
	}
	shf.r.wrap.b32 	%r9819, %r9818, %r9817, 28;
	shf.r.wrap.b32 	%r9820, %r9817, %r9818, 28;
	mov.b64 	%rd428, {%r9820, %r9819};
	shf.l.wrap.b32 	%r9821, %r9817, %r9818, 30;
	shf.l.wrap.b32 	%r9822, %r9818, %r9817, 30;
	mov.b64 	%rd429, {%r9822, %r9821};
	xor.b64  	%rd430, %rd429, %rd428;
	shf.l.wrap.b32 	%r9823, %r9817, %r9818, 25;
	shf.l.wrap.b32 	%r9824, %r9818, %r9817, 25;
	mov.b64 	%rd431, {%r9824, %r9823};
	xor.b64  	%rd432, %rd430, %rd431;
	xor.b64  	%rd433, %rd415, %rd369;
	xor.b64  	%rd434, %rd415, %rd392;
	and.b64  	%rd435, %rd434, %rd433;
	xor.b64  	%rd436, %rd435, %rd415;
	add.s64 	%rd437, %rd426, %rd436;
	add.s64 	%rd438, %rd437, %rd432;
	add.s64 	%rd439, %rd2791, %rd358;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9825,%dummy}, %rd427;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9826}, %rd427;
	}
	shf.r.wrap.b32 	%r9827, %r9826, %r9825, 14;
	shf.r.wrap.b32 	%r9828, %r9825, %r9826, 14;
	mov.b64 	%rd440, {%r9828, %r9827};
	shf.r.wrap.b32 	%r9829, %r9826, %r9825, 18;
	shf.r.wrap.b32 	%r9830, %r9825, %r9826, 18;
	mov.b64 	%rd441, {%r9830, %r9829};
	xor.b64  	%rd442, %rd441, %rd440;
	shf.l.wrap.b32 	%r9831, %r9825, %r9826, 23;
	shf.l.wrap.b32 	%r9832, %r9826, %r9825, 23;
	mov.b64 	%rd443, {%r9832, %r9831};
	xor.b64  	%rd444, %rd442, %rd443;
	xor.b64  	%rd445, %rd404, %rd381;
	and.b64  	%rd446, %rd427, %rd445;
	xor.b64  	%rd447, %rd446, %rd381;
	add.s64 	%rd448, %rd439, %rd447;
	add.s64 	%rd449, %rd448, %rd444;
	add.s64 	%rd450, %rd449, %rd369;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9833,%dummy}, %rd438;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9834}, %rd438;
	}
	shf.r.wrap.b32 	%r9835, %r9834, %r9833, 28;
	shf.r.wrap.b32 	%r9836, %r9833, %r9834, 28;
	mov.b64 	%rd451, {%r9836, %r9835};
	shf.l.wrap.b32 	%r9837, %r9833, %r9834, 30;
	shf.l.wrap.b32 	%r9838, %r9834, %r9833, 30;
	mov.b64 	%rd452, {%r9838, %r9837};
	xor.b64  	%rd453, %rd452, %rd451;
	shf.l.wrap.b32 	%r9839, %r9833, %r9834, 25;
	shf.l.wrap.b32 	%r9840, %r9834, %r9833, 25;
	mov.b64 	%rd454, {%r9840, %r9839};
	xor.b64  	%rd455, %rd453, %rd454;
	xor.b64  	%rd456, %rd438, %rd392;
	xor.b64  	%rd457, %rd438, %rd415;
	and.b64  	%rd458, %rd457, %rd456;
	xor.b64  	%rd459, %rd458, %rd438;
	add.s64 	%rd460, %rd449, %rd459;
	add.s64 	%rd461, %rd460, %rd455;
	add.s64 	%rd462, %rd2792, %rd381;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9841,%dummy}, %rd450;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9842}, %rd450;
	}
	shf.r.wrap.b32 	%r9843, %r9842, %r9841, 14;
	shf.r.wrap.b32 	%r9844, %r9841, %r9842, 14;
	mov.b64 	%rd463, {%r9844, %r9843};
	shf.r.wrap.b32 	%r9845, %r9842, %r9841, 18;
	shf.r.wrap.b32 	%r9846, %r9841, %r9842, 18;
	mov.b64 	%rd464, {%r9846, %r9845};
	xor.b64  	%rd465, %rd464, %rd463;
	shf.l.wrap.b32 	%r9847, %r9841, %r9842, 23;
	shf.l.wrap.b32 	%r9848, %r9842, %r9841, 23;
	mov.b64 	%rd466, {%r9848, %r9847};
	xor.b64  	%rd467, %rd465, %rd466;
	xor.b64  	%rd468, %rd427, %rd404;
	and.b64  	%rd469, %rd450, %rd468;
	xor.b64  	%rd470, %rd469, %rd404;
	add.s64 	%rd471, %rd462, %rd470;
	add.s64 	%rd472, %rd471, %rd467;
	add.s64 	%rd473, %rd472, %rd392;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9849,%dummy}, %rd461;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9850}, %rd461;
	}
	shf.r.wrap.b32 	%r9851, %r9850, %r9849, 28;
	shf.r.wrap.b32 	%r9852, %r9849, %r9850, 28;
	mov.b64 	%rd474, {%r9852, %r9851};
	shf.l.wrap.b32 	%r9853, %r9849, %r9850, 30;
	shf.l.wrap.b32 	%r9854, %r9850, %r9849, 30;
	mov.b64 	%rd475, {%r9854, %r9853};
	xor.b64  	%rd476, %rd475, %rd474;
	shf.l.wrap.b32 	%r9855, %r9849, %r9850, 25;
	shf.l.wrap.b32 	%r9856, %r9850, %r9849, 25;
	mov.b64 	%rd477, {%r9856, %r9855};
	xor.b64  	%rd478, %rd476, %rd477;
	xor.b64  	%rd479, %rd461, %rd415;
	xor.b64  	%rd480, %rd461, %rd438;
	and.b64  	%rd481, %rd480, %rd479;
	xor.b64  	%rd482, %rd481, %rd461;
	add.s64 	%rd483, %rd472, %rd482;
	add.s64 	%rd484, %rd483, %rd478;
	add.s64 	%rd485, %rd2793, %rd404;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9857,%dummy}, %rd473;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9858}, %rd473;
	}
	shf.r.wrap.b32 	%r9859, %r9858, %r9857, 14;
	shf.r.wrap.b32 	%r9860, %r9857, %r9858, 14;
	mov.b64 	%rd486, {%r9860, %r9859};
	shf.r.wrap.b32 	%r9861, %r9858, %r9857, 18;
	shf.r.wrap.b32 	%r9862, %r9857, %r9858, 18;
	mov.b64 	%rd487, {%r9862, %r9861};
	xor.b64  	%rd488, %rd487, %rd486;
	shf.l.wrap.b32 	%r9863, %r9857, %r9858, 23;
	shf.l.wrap.b32 	%r9864, %r9858, %r9857, 23;
	mov.b64 	%rd489, {%r9864, %r9863};
	xor.b64  	%rd490, %rd488, %rd489;
	xor.b64  	%rd491, %rd450, %rd427;
	and.b64  	%rd492, %rd473, %rd491;
	xor.b64  	%rd493, %rd492, %rd427;
	add.s64 	%rd494, %rd485, %rd493;
	add.s64 	%rd495, %rd494, %rd490;
	add.s64 	%rd496, %rd495, %rd415;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9865,%dummy}, %rd484;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9866}, %rd484;
	}
	shf.r.wrap.b32 	%r9867, %r9866, %r9865, 28;
	shf.r.wrap.b32 	%r9868, %r9865, %r9866, 28;
	mov.b64 	%rd497, {%r9868, %r9867};
	shf.l.wrap.b32 	%r9869, %r9865, %r9866, 30;
	shf.l.wrap.b32 	%r9870, %r9866, %r9865, 30;
	mov.b64 	%rd498, {%r9870, %r9869};
	xor.b64  	%rd499, %rd498, %rd497;
	shf.l.wrap.b32 	%r9871, %r9865, %r9866, 25;
	shf.l.wrap.b32 	%r9872, %r9866, %r9865, 25;
	mov.b64 	%rd500, {%r9872, %r9871};
	xor.b64  	%rd501, %rd499, %rd500;
	xor.b64  	%rd502, %rd484, %rd438;
	xor.b64  	%rd503, %rd484, %rd461;
	and.b64  	%rd504, %rd503, %rd502;
	xor.b64  	%rd505, %rd504, %rd484;
	add.s64 	%rd506, %rd495, %rd505;
	add.s64 	%rd507, %rd506, %rd501;
	add.s64 	%rd508, %rd2794, %rd427;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9873,%dummy}, %rd496;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9874}, %rd496;
	}
	shf.r.wrap.b32 	%r9875, %r9874, %r9873, 14;
	shf.r.wrap.b32 	%r9876, %r9873, %r9874, 14;
	mov.b64 	%rd509, {%r9876, %r9875};
	shf.r.wrap.b32 	%r9877, %r9874, %r9873, 18;
	shf.r.wrap.b32 	%r9878, %r9873, %r9874, 18;
	mov.b64 	%rd510, {%r9878, %r9877};
	xor.b64  	%rd511, %rd510, %rd509;
	shf.l.wrap.b32 	%r9879, %r9873, %r9874, 23;
	shf.l.wrap.b32 	%r9880, %r9874, %r9873, 23;
	mov.b64 	%rd512, {%r9880, %r9879};
	xor.b64  	%rd513, %rd511, %rd512;
	xor.b64  	%rd514, %rd473, %rd450;
	and.b64  	%rd515, %rd496, %rd514;
	xor.b64  	%rd516, %rd515, %rd450;
	add.s64 	%rd517, %rd508, %rd516;
	add.s64 	%rd518, %rd517, %rd513;
	add.s64 	%rd519, %rd518, %rd438;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9881,%dummy}, %rd507;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9882}, %rd507;
	}
	shf.r.wrap.b32 	%r9883, %r9882, %r9881, 28;
	shf.r.wrap.b32 	%r9884, %r9881, %r9882, 28;
	mov.b64 	%rd520, {%r9884, %r9883};
	shf.l.wrap.b32 	%r9885, %r9881, %r9882, 30;
	shf.l.wrap.b32 	%r9886, %r9882, %r9881, 30;
	mov.b64 	%rd521, {%r9886, %r9885};
	xor.b64  	%rd522, %rd521, %rd520;
	shf.l.wrap.b32 	%r9887, %r9881, %r9882, 25;
	shf.l.wrap.b32 	%r9888, %r9882, %r9881, 25;
	mov.b64 	%rd523, {%r9888, %r9887};
	xor.b64  	%rd524, %rd522, %rd523;
	xor.b64  	%rd525, %rd507, %rd461;
	xor.b64  	%rd526, %rd507, %rd484;
	and.b64  	%rd527, %rd526, %rd525;
	xor.b64  	%rd528, %rd527, %rd507;
	add.s64 	%rd529, %rd518, %rd528;
	add.s64 	%rd530, %rd529, %rd524;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9889,%dummy}, %rd519;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9890}, %rd519;
	}
	shf.r.wrap.b32 	%r9891, %r9890, %r9889, 14;
	shf.r.wrap.b32 	%r9892, %r9889, %r9890, 14;
	mov.b64 	%rd531, {%r9892, %r9891};
	shf.r.wrap.b32 	%r9893, %r9890, %r9889, 18;
	shf.r.wrap.b32 	%r9894, %r9889, %r9890, 18;
	mov.b64 	%rd532, {%r9894, %r9893};
	xor.b64  	%rd533, %rd532, %rd531;
	shf.l.wrap.b32 	%r9895, %r9889, %r9890, 23;
	shf.l.wrap.b32 	%r9896, %r9890, %r9889, 23;
	mov.b64 	%rd534, {%r9896, %r9895};
	xor.b64  	%rd535, %rd533, %rd534;
	xor.b64  	%rd536, %rd496, %rd473;
	and.b64  	%rd537, %rd519, %rd536;
	xor.b64  	%rd538, %rd537, %rd473;
	add.s64 	%rd539, %rd450, %rd198;
	add.s64 	%rd540, %rd539, %rd2795;
	add.s64 	%rd541, %rd540, %rd538;
	add.s64 	%rd542, %rd541, %rd535;
	add.s64 	%rd543, %rd542, %rd461;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9897,%dummy}, %rd530;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9898}, %rd530;
	}
	shf.r.wrap.b32 	%r9899, %r9898, %r9897, 28;
	shf.r.wrap.b32 	%r9900, %r9897, %r9898, 28;
	mov.b64 	%rd544, {%r9900, %r9899};
	shf.l.wrap.b32 	%r9901, %r9897, %r9898, 30;
	shf.l.wrap.b32 	%r9902, %r9898, %r9897, 30;
	mov.b64 	%rd545, {%r9902, %r9901};
	xor.b64  	%rd546, %rd545, %rd544;
	shf.l.wrap.b32 	%r9903, %r9897, %r9898, 25;
	shf.l.wrap.b32 	%r9904, %r9898, %r9897, 25;
	mov.b64 	%rd547, {%r9904, %r9903};
	xor.b64  	%rd548, %rd546, %rd547;
	xor.b64  	%rd549, %rd530, %rd484;
	xor.b64  	%rd550, %rd530, %rd507;
	and.b64  	%rd551, %rd550, %rd549;
	xor.b64  	%rd552, %rd551, %rd530;
	add.s64 	%rd553, %rd542, %rd552;
	add.s64 	%rd554, %rd553, %rd548;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9905,%dummy}, %rd192;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9906}, %rd192;
	}
	shf.r.wrap.b32 	%r9907, %r9906, %r9905, 1;
	shf.r.wrap.b32 	%r9908, %r9905, %r9906, 1;
	mov.b64 	%rd555, {%r9908, %r9907};
	shf.r.wrap.b32 	%r9909, %r9906, %r9905, 8;
	shf.r.wrap.b32 	%r9910, %r9905, %r9906, 8;
	mov.b64 	%rd556, {%r9910, %r9909};
	shr.u64 	%rd557, %rd192, 7;
	xor.b64  	%rd558, %rd555, %rd557;
	xor.b64  	%rd559, %rd558, %rd556;
	add.s64 	%rd560, %rd191, %rd22;
	add.s64 	%rd561, %rd560, %rd559;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9911,%dummy}, %rd198;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9912}, %rd198;
	}
	shf.r.wrap.b32 	%r9913, %r9912, %r9911, 19;
	shf.r.wrap.b32 	%r9914, %r9911, %r9912, 19;
	mov.b64 	%rd562, {%r9914, %r9913};
	shf.l.wrap.b32 	%r9915, %r9911, %r9912, 3;
	shf.l.wrap.b32 	%r9916, %r9912, %r9911, 3;
	mov.b64 	%rd563, {%r9916, %r9915};
	shr.u64 	%rd564, %rd198, 6;
	xor.b64  	%rd565, %rd562, %rd564;
	xor.b64  	%rd566, %rd565, %rd563;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9917,%dummy}, %rd193;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9918}, %rd193;
	}
	shf.r.wrap.b32 	%r9919, %r9918, %r9917, 1;
	shf.r.wrap.b32 	%r9920, %r9917, %r9918, 1;
	mov.b64 	%rd567, {%r9920, %r9919};
	shf.r.wrap.b32 	%r9921, %r9918, %r9917, 8;
	shf.r.wrap.b32 	%r9922, %r9917, %r9918, 8;
	mov.b64 	%rd568, {%r9922, %r9921};
	shr.u64 	%rd569, %rd193, 7;
	xor.b64  	%rd570, %rd567, %rd569;
	xor.b64  	%rd571, %rd570, %rd568;
	add.s64 	%rd572, %rd192, %rd566;
	add.s64 	%rd573, %rd572, %rd571;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9923,%dummy}, %rd561;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9924}, %rd561;
	}
	shf.r.wrap.b32 	%r9925, %r9924, %r9923, 19;
	shf.r.wrap.b32 	%r9926, %r9923, %r9924, 19;
	mov.b64 	%rd574, {%r9926, %r9925};
	shf.l.wrap.b32 	%r9927, %r9923, %r9924, 3;
	shf.l.wrap.b32 	%r9928, %r9924, %r9923, 3;
	mov.b64 	%rd575, {%r9928, %r9927};
	shr.u64 	%rd576, %rd561, 6;
	xor.b64  	%rd577, %rd574, %rd576;
	xor.b64  	%rd578, %rd577, %rd575;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9929,%dummy}, %rd194;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9930}, %rd194;
	}
	shf.r.wrap.b32 	%r9931, %r9930, %r9929, 1;
	shf.r.wrap.b32 	%r9932, %r9929, %r9930, 1;
	mov.b64 	%rd579, {%r9932, %r9931};
	shf.r.wrap.b32 	%r9933, %r9930, %r9929, 8;
	shf.r.wrap.b32 	%r9934, %r9929, %r9930, 8;
	mov.b64 	%rd580, {%r9934, %r9933};
	shr.u64 	%rd581, %rd194, 7;
	xor.b64  	%rd582, %rd579, %rd581;
	xor.b64  	%rd583, %rd582, %rd580;
	add.s64 	%rd584, %rd193, %rd578;
	add.s64 	%rd585, %rd584, %rd583;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9935,%dummy}, %rd573;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9936}, %rd573;
	}
	shf.r.wrap.b32 	%r9937, %r9936, %r9935, 19;
	shf.r.wrap.b32 	%r9938, %r9935, %r9936, 19;
	mov.b64 	%rd586, {%r9938, %r9937};
	shf.l.wrap.b32 	%r9939, %r9935, %r9936, 3;
	shf.l.wrap.b32 	%r9940, %r9936, %r9935, 3;
	mov.b64 	%rd587, {%r9940, %r9939};
	shr.u64 	%rd588, %rd573, 6;
	xor.b64  	%rd589, %rd586, %rd588;
	xor.b64  	%rd590, %rd589, %rd587;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9941,%dummy}, %rd195;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9942}, %rd195;
	}
	shf.r.wrap.b32 	%r9943, %r9942, %r9941, 1;
	shf.r.wrap.b32 	%r9944, %r9941, %r9942, 1;
	mov.b64 	%rd591, {%r9944, %r9943};
	shf.r.wrap.b32 	%r9945, %r9942, %r9941, 8;
	shf.r.wrap.b32 	%r9946, %r9941, %r9942, 8;
	mov.b64 	%rd592, {%r9946, %r9945};
	shr.u64 	%rd593, %rd195, 7;
	xor.b64  	%rd594, %rd591, %rd593;
	xor.b64  	%rd595, %rd594, %rd592;
	add.s64 	%rd596, %rd194, %rd590;
	add.s64 	%rd597, %rd596, %rd595;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9947,%dummy}, %rd585;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9948}, %rd585;
	}
	shf.r.wrap.b32 	%r9949, %r9948, %r9947, 19;
	shf.r.wrap.b32 	%r9950, %r9947, %r9948, 19;
	mov.b64 	%rd598, {%r9950, %r9949};
	shf.l.wrap.b32 	%r9951, %r9947, %r9948, 3;
	shf.l.wrap.b32 	%r9952, %r9948, %r9947, 3;
	mov.b64 	%rd599, {%r9952, %r9951};
	shr.u64 	%rd600, %rd585, 6;
	xor.b64  	%rd601, %rd598, %rd600;
	xor.b64  	%rd602, %rd601, %rd599;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9953,%dummy}, %rd196;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9954}, %rd196;
	}
	shf.r.wrap.b32 	%r9955, %r9954, %r9953, 1;
	shf.r.wrap.b32 	%r9956, %r9953, %r9954, 1;
	mov.b64 	%rd603, {%r9956, %r9955};
	shf.r.wrap.b32 	%r9957, %r9954, %r9953, 8;
	shf.r.wrap.b32 	%r9958, %r9953, %r9954, 8;
	mov.b64 	%rd604, {%r9958, %r9957};
	shr.u64 	%rd605, %rd196, 7;
	xor.b64  	%rd606, %rd603, %rd605;
	xor.b64  	%rd607, %rd606, %rd604;
	add.s64 	%rd608, %rd195, %rd602;
	add.s64 	%rd609, %rd608, %rd607;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9959,%dummy}, %rd597;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9960}, %rd597;
	}
	shf.r.wrap.b32 	%r9961, %r9960, %r9959, 19;
	shf.r.wrap.b32 	%r9962, %r9959, %r9960, 19;
	mov.b64 	%rd610, {%r9962, %r9961};
	shf.l.wrap.b32 	%r9963, %r9959, %r9960, 3;
	shf.l.wrap.b32 	%r9964, %r9960, %r9959, 3;
	mov.b64 	%rd611, {%r9964, %r9963};
	shr.u64 	%rd612, %rd597, 6;
	xor.b64  	%rd613, %rd610, %rd612;
	xor.b64  	%rd614, %rd613, %rd611;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9965,%dummy}, %rd197;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9966}, %rd197;
	}
	shf.r.wrap.b32 	%r9967, %r9966, %r9965, 1;
	shf.r.wrap.b32 	%r9968, %r9965, %r9966, 1;
	mov.b64 	%rd615, {%r9968, %r9967};
	shf.r.wrap.b32 	%r9969, %r9966, %r9965, 8;
	shf.r.wrap.b32 	%r9970, %r9965, %r9966, 8;
	mov.b64 	%rd616, {%r9970, %r9969};
	shr.u64 	%rd617, %rd197, 7;
	xor.b64  	%rd618, %rd615, %rd617;
	xor.b64  	%rd619, %rd618, %rd616;
	add.s64 	%rd620, %rd196, %rd614;
	add.s64 	%rd621, %rd620, %rd619;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9971,%dummy}, %rd609;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9972}, %rd609;
	}
	shf.r.wrap.b32 	%r9973, %r9972, %r9971, 19;
	shf.r.wrap.b32 	%r9974, %r9971, %r9972, 19;
	mov.b64 	%rd622, {%r9974, %r9973};
	shf.l.wrap.b32 	%r9975, %r9971, %r9972, 3;
	shf.l.wrap.b32 	%r9976, %r9972, %r9971, 3;
	mov.b64 	%rd623, {%r9976, %r9975};
	shr.u64 	%rd624, %rd609, 6;
	xor.b64  	%rd625, %rd622, %rd624;
	xor.b64  	%rd626, %rd625, %rd623;
	add.s64 	%rd627, %rd197, %rd198;
	add.s64 	%rd628, %rd627, %rd626;
	add.s64 	%rd629, %rd628, %rd23;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9977,%dummy}, %rd621;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9978}, %rd621;
	}
	shf.r.wrap.b32 	%r9979, %r9978, %r9977, 19;
	shf.r.wrap.b32 	%r9980, %r9977, %r9978, 19;
	mov.b64 	%rd630, {%r9980, %r9979};
	shf.l.wrap.b32 	%r9981, %r9977, %r9978, 3;
	shf.l.wrap.b32 	%r9982, %r9978, %r9977, 3;
	mov.b64 	%rd631, {%r9982, %r9981};
	shr.u64 	%rd632, %rd621, 6;
	xor.b64  	%rd633, %rd630, %rd632;
	xor.b64  	%rd634, %rd633, %rd631;
	add.s64 	%rd635, %rd561, %rd634;
	add.s64 	%rd636, %rd635, %rd23;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9983,%dummy}, %rd629;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9984}, %rd629;
	}
	shf.r.wrap.b32 	%r9985, %r9984, %r9983, 19;
	shf.r.wrap.b32 	%r9986, %r9983, %r9984, 19;
	mov.b64 	%rd637, {%r9986, %r9985};
	shf.l.wrap.b32 	%r9987, %r9983, %r9984, 3;
	shf.l.wrap.b32 	%r9988, %r9984, %r9983, 3;
	mov.b64 	%rd638, {%r9988, %r9987};
	shr.u64 	%rd639, %rd629, 6;
	xor.b64  	%rd640, %rd637, %rd639;
	xor.b64  	%rd641, %rd640, %rd638;
	add.s64 	%rd642, %rd573, %rd641;
	add.s64 	%rd643, %rd642, %rd23;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9989,%dummy}, %rd636;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9990}, %rd636;
	}
	shf.r.wrap.b32 	%r9991, %r9990, %r9989, 19;
	shf.r.wrap.b32 	%r9992, %r9989, %r9990, 19;
	mov.b64 	%rd644, {%r9992, %r9991};
	shf.l.wrap.b32 	%r9993, %r9989, %r9990, 3;
	shf.l.wrap.b32 	%r9994, %r9990, %r9989, 3;
	mov.b64 	%rd645, {%r9994, %r9993};
	shr.u64 	%rd646, %rd636, 6;
	xor.b64  	%rd647, %rd644, %rd646;
	xor.b64  	%rd648, %rd647, %rd645;
	add.s64 	%rd649, %rd585, %rd648;
	add.s64 	%rd650, %rd649, %rd23;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9995,%dummy}, %rd643;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9996}, %rd643;
	}
	shf.r.wrap.b32 	%r9997, %r9996, %r9995, 19;
	shf.r.wrap.b32 	%r9998, %r9995, %r9996, 19;
	mov.b64 	%rd651, {%r9998, %r9997};
	shf.l.wrap.b32 	%r9999, %r9995, %r9996, 3;
	shf.l.wrap.b32 	%r10000, %r9996, %r9995, 3;
	mov.b64 	%rd652, {%r10000, %r9999};
	shr.u64 	%rd653, %rd643, 6;
	xor.b64  	%rd654, %rd651, %rd653;
	xor.b64  	%rd655, %rd654, %rd652;
	add.s64 	%rd656, %rd597, %rd655;
	add.s64 	%rd657, %rd656, %rd23;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10001,%dummy}, %rd650;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10002}, %rd650;
	}
	shf.r.wrap.b32 	%r10003, %r10002, %r10001, 19;
	shf.r.wrap.b32 	%r10004, %r10001, %r10002, 19;
	mov.b64 	%rd658, {%r10004, %r10003};
	shf.l.wrap.b32 	%r10005, %r10001, %r10002, 3;
	shf.l.wrap.b32 	%r10006, %r10002, %r10001, 3;
	mov.b64 	%rd659, {%r10006, %r10005};
	shr.u64 	%rd660, %rd650, 6;
	xor.b64  	%rd661, %rd658, %rd660;
	xor.b64  	%rd662, %rd661, %rd659;
	add.s64 	%rd663, %rd609, %rd662;
	add.s64 	%rd664, %rd663, %rd23;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10007,%dummy}, %rd657;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10008}, %rd657;
	}
	shf.r.wrap.b32 	%r10009, %r10008, %r10007, 19;
	shf.r.wrap.b32 	%r10010, %r10007, %r10008, 19;
	mov.b64 	%rd665, {%r10010, %r10009};
	shf.l.wrap.b32 	%r10011, %r10007, %r10008, 3;
	shf.l.wrap.b32 	%r10012, %r10008, %r10007, 3;
	mov.b64 	%rd666, {%r10012, %r10011};
	shr.u64 	%rd667, %rd657, 6;
	xor.b64  	%rd668, %rd665, %rd667;
	xor.b64  	%rd669, %rd668, %rd666;
	add.s64 	%rd670, %rd621, %rd669;
	add.s64 	%rd671, %rd670, %rd23;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10013,%dummy}, %rd664;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10014}, %rd664;
	}
	shf.r.wrap.b32 	%r10015, %r10014, %r10013, 19;
	shf.r.wrap.b32 	%r10016, %r10013, %r10014, 19;
	mov.b64 	%rd672, {%r10016, %r10015};
	shf.l.wrap.b32 	%r10017, %r10013, %r10014, 3;
	shf.l.wrap.b32 	%r10018, %r10014, %r10013, 3;
	mov.b64 	%rd673, {%r10018, %r10017};
	shr.u64 	%rd674, %rd664, 6;
	xor.b64  	%rd675, %rd672, %rd674;
	xor.b64  	%rd676, %rd675, %rd673;
	add.s64 	%rd677, %rd629, %rd676;
	add.s64 	%rd678, %rd677, %rd23;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10019,%dummy}, %rd671;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10020}, %rd671;
	}
	shf.r.wrap.b32 	%r10021, %r10020, %r10019, 19;
	shf.r.wrap.b32 	%r10022, %r10019, %r10020, 19;
	mov.b64 	%rd679, {%r10022, %r10021};
	shf.l.wrap.b32 	%r10023, %r10019, %r10020, 3;
	shf.l.wrap.b32 	%r10024, %r10020, %r10019, 3;
	mov.b64 	%rd680, {%r10024, %r10023};
	shr.u64 	%rd681, %rd671, 6;
	xor.b64  	%rd682, %rd679, %rd681;
	xor.b64  	%rd683, %rd682, %rd680;
	shf.r.wrap.b32 	%r10025, %r9912, %r9911, 1;
	shf.r.wrap.b32 	%r10026, %r9911, %r9912, 1;
	mov.b64 	%rd684, {%r10026, %r10025};
	shf.r.wrap.b32 	%r10027, %r9912, %r9911, 8;
	shf.r.wrap.b32 	%r10028, %r9911, %r9912, 8;
	mov.b64 	%rd685, {%r10028, %r10027};
	shr.u64 	%rd686, %rd198, 7;
	xor.b64  	%rd687, %rd684, %rd686;
	xor.b64  	%rd688, %rd687, %rd685;
	add.s64 	%rd689, %rd636, %rd683;
	add.s64 	%rd690, %rd689, %rd688;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10029,%dummy}, %rd678;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10030}, %rd678;
	}
	shf.r.wrap.b32 	%r10031, %r10030, %r10029, 19;
	shf.r.wrap.b32 	%r10032, %r10029, %r10030, 19;
	mov.b64 	%rd691, {%r10032, %r10031};
	shf.l.wrap.b32 	%r10033, %r10029, %r10030, 3;
	shf.l.wrap.b32 	%r10034, %r10030, %r10029, 3;
	mov.b64 	%rd692, {%r10034, %r10033};
	shr.u64 	%rd693, %rd678, 6;
	xor.b64  	%rd694, %rd691, %rd693;
	xor.b64  	%rd695, %rd694, %rd692;
	shf.r.wrap.b32 	%r10035, %r9924, %r9923, 1;
	shf.r.wrap.b32 	%r10036, %r9923, %r9924, 1;
	mov.b64 	%rd696, {%r10036, %r10035};
	shf.r.wrap.b32 	%r10037, %r9924, %r9923, 8;
	shf.r.wrap.b32 	%r10038, %r9923, %r9924, 8;
	mov.b64 	%rd697, {%r10038, %r10037};
	shr.u64 	%rd698, %rd561, 7;
	xor.b64  	%rd699, %rd696, %rd698;
	xor.b64  	%rd700, %rd699, %rd697;
	add.s64 	%rd701, %rd643, %rd198;
	add.s64 	%rd702, %rd701, %rd695;
	add.s64 	%rd703, %rd702, %rd700;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10039,%dummy}, %rd543;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10040}, %rd543;
	}
	shf.r.wrap.b32 	%r10041, %r10040, %r10039, 14;
	shf.r.wrap.b32 	%r10042, %r10039, %r10040, 14;
	mov.b64 	%rd704, {%r10042, %r10041};
	shf.r.wrap.b32 	%r10043, %r10040, %r10039, 18;
	shf.r.wrap.b32 	%r10044, %r10039, %r10040, 18;
	mov.b64 	%rd705, {%r10044, %r10043};
	xor.b64  	%rd706, %rd705, %rd704;
	shf.l.wrap.b32 	%r10045, %r10039, %r10040, 23;
	shf.l.wrap.b32 	%r10046, %r10040, %r10039, 23;
	mov.b64 	%rd707, {%r10046, %r10045};
	xor.b64  	%rd708, %rd706, %rd707;
	xor.b64  	%rd709, %rd496, %rd519;
	and.b64  	%rd710, %rd709, %rd543;
	xor.b64  	%rd711, %rd710, %rd496;
	add.s64 	%rd712, %rd711, %rd473;
	add.s64 	%rd713, %rd712, %rd561;
	add.s64 	%rd714, %rd713, %rd2796;
	add.s64 	%rd715, %rd714, %rd708;
	add.s64 	%rd716, %rd715, %rd484;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10047,%dummy}, %rd554;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10048}, %rd554;
	}
	shf.r.wrap.b32 	%r10049, %r10048, %r10047, 28;
	shf.r.wrap.b32 	%r10050, %r10047, %r10048, 28;
	mov.b64 	%rd717, {%r10050, %r10049};
	shf.l.wrap.b32 	%r10051, %r10047, %r10048, 30;
	shf.l.wrap.b32 	%r10052, %r10048, %r10047, 30;
	mov.b64 	%rd718, {%r10052, %r10051};
	xor.b64  	%rd719, %rd718, %rd717;
	shf.l.wrap.b32 	%r10053, %r10047, %r10048, 25;
	shf.l.wrap.b32 	%r10054, %r10048, %r10047, 25;
	mov.b64 	%rd720, {%r10054, %r10053};
	xor.b64  	%rd721, %rd719, %rd720;
	xor.b64  	%rd722, %rd554, %rd507;
	xor.b64  	%rd723, %rd554, %rd530;
	and.b64  	%rd724, %rd723, %rd722;
	xor.b64  	%rd725, %rd724, %rd554;
	add.s64 	%rd726, %rd715, %rd725;
	add.s64 	%rd727, %rd726, %rd721;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10055,%dummy}, %rd716;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10056}, %rd716;
	}
	shf.r.wrap.b32 	%r10057, %r10056, %r10055, 14;
	shf.r.wrap.b32 	%r10058, %r10055, %r10056, 14;
	mov.b64 	%rd728, {%r10058, %r10057};
	shf.r.wrap.b32 	%r10059, %r10056, %r10055, 18;
	shf.r.wrap.b32 	%r10060, %r10055, %r10056, 18;
	mov.b64 	%rd729, {%r10060, %r10059};
	xor.b64  	%rd730, %rd729, %rd728;
	shf.l.wrap.b32 	%r10061, %r10055, %r10056, 23;
	shf.l.wrap.b32 	%r10062, %r10056, %r10055, 23;
	mov.b64 	%rd731, {%r10062, %r10061};
	xor.b64  	%rd732, %rd730, %rd731;
	xor.b64  	%rd733, %rd519, %rd543;
	and.b64  	%rd734, %rd716, %rd733;
	xor.b64  	%rd735, %rd734, %rd519;
	add.s64 	%rd736, %rd573, %rd496;
	add.s64 	%rd737, %rd736, %rd2797;
	add.s64 	%rd738, %rd737, %rd735;
	add.s64 	%rd739, %rd738, %rd732;
	add.s64 	%rd740, %rd739, %rd507;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10063,%dummy}, %rd727;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10064}, %rd727;
	}
	shf.r.wrap.b32 	%r10065, %r10064, %r10063, 28;
	shf.r.wrap.b32 	%r10066, %r10063, %r10064, 28;
	mov.b64 	%rd741, {%r10066, %r10065};
	shf.l.wrap.b32 	%r10067, %r10063, %r10064, 30;
	shf.l.wrap.b32 	%r10068, %r10064, %r10063, 30;
	mov.b64 	%rd742, {%r10068, %r10067};
	xor.b64  	%rd743, %rd742, %rd741;
	shf.l.wrap.b32 	%r10069, %r10063, %r10064, 25;
	shf.l.wrap.b32 	%r10070, %r10064, %r10063, 25;
	mov.b64 	%rd744, {%r10070, %r10069};
	xor.b64  	%rd745, %rd743, %rd744;
	xor.b64  	%rd746, %rd727, %rd530;
	xor.b64  	%rd747, %rd727, %rd554;
	and.b64  	%rd748, %rd747, %rd746;
	xor.b64  	%rd749, %rd748, %rd727;
	add.s64 	%rd750, %rd739, %rd749;
	add.s64 	%rd751, %rd750, %rd745;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10071,%dummy}, %rd740;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10072}, %rd740;
	}
	shf.r.wrap.b32 	%r10073, %r10072, %r10071, 14;
	shf.r.wrap.b32 	%r10074, %r10071, %r10072, 14;
	mov.b64 	%rd752, {%r10074, %r10073};
	shf.r.wrap.b32 	%r10075, %r10072, %r10071, 18;
	shf.r.wrap.b32 	%r10076, %r10071, %r10072, 18;
	mov.b64 	%rd753, {%r10076, %r10075};
	xor.b64  	%rd754, %rd753, %rd752;
	shf.l.wrap.b32 	%r10077, %r10071, %r10072, 23;
	shf.l.wrap.b32 	%r10078, %r10072, %r10071, 23;
	mov.b64 	%rd755, {%r10078, %r10077};
	xor.b64  	%rd756, %rd754, %rd755;
	xor.b64  	%rd757, %rd716, %rd543;
	and.b64  	%rd758, %rd740, %rd757;
	xor.b64  	%rd759, %rd758, %rd543;
	add.s64 	%rd760, %rd585, %rd519;
	add.s64 	%rd761, %rd760, %rd2798;
	add.s64 	%rd762, %rd761, %rd759;
	add.s64 	%rd763, %rd762, %rd756;
	add.s64 	%rd764, %rd763, %rd530;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10079,%dummy}, %rd751;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10080}, %rd751;
	}
	shf.r.wrap.b32 	%r10081, %r10080, %r10079, 28;
	shf.r.wrap.b32 	%r10082, %r10079, %r10080, 28;
	mov.b64 	%rd765, {%r10082, %r10081};
	shf.l.wrap.b32 	%r10083, %r10079, %r10080, 30;
	shf.l.wrap.b32 	%r10084, %r10080, %r10079, 30;
	mov.b64 	%rd766, {%r10084, %r10083};
	xor.b64  	%rd767, %rd766, %rd765;
	shf.l.wrap.b32 	%r10085, %r10079, %r10080, 25;
	shf.l.wrap.b32 	%r10086, %r10080, %r10079, 25;
	mov.b64 	%rd768, {%r10086, %r10085};
	xor.b64  	%rd769, %rd767, %rd768;
	xor.b64  	%rd770, %rd751, %rd554;
	xor.b64  	%rd771, %rd751, %rd727;
	and.b64  	%rd772, %rd771, %rd770;
	xor.b64  	%rd773, %rd772, %rd751;
	add.s64 	%rd774, %rd763, %rd773;
	add.s64 	%rd775, %rd774, %rd769;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10087,%dummy}, %rd764;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10088}, %rd764;
	}
	shf.r.wrap.b32 	%r10089, %r10088, %r10087, 14;
	shf.r.wrap.b32 	%r10090, %r10087, %r10088, 14;
	mov.b64 	%rd776, {%r10090, %r10089};
	shf.r.wrap.b32 	%r10091, %r10088, %r10087, 18;
	shf.r.wrap.b32 	%r10092, %r10087, %r10088, 18;
	mov.b64 	%rd777, {%r10092, %r10091};
	xor.b64  	%rd778, %rd777, %rd776;
	shf.l.wrap.b32 	%r10093, %r10087, %r10088, 23;
	shf.l.wrap.b32 	%r10094, %r10088, %r10087, 23;
	mov.b64 	%rd779, {%r10094, %r10093};
	xor.b64  	%rd780, %rd778, %rd779;
	xor.b64  	%rd781, %rd740, %rd716;
	and.b64  	%rd782, %rd764, %rd781;
	xor.b64  	%rd783, %rd782, %rd716;
	add.s64 	%rd784, %rd597, %rd543;
	add.s64 	%rd785, %rd784, %rd2799;
	add.s64 	%rd786, %rd785, %rd783;
	add.s64 	%rd787, %rd786, %rd780;
	add.s64 	%rd788, %rd787, %rd554;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10095,%dummy}, %rd775;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10096}, %rd775;
	}
	shf.r.wrap.b32 	%r10097, %r10096, %r10095, 28;
	shf.r.wrap.b32 	%r10098, %r10095, %r10096, 28;
	mov.b64 	%rd789, {%r10098, %r10097};
	shf.l.wrap.b32 	%r10099, %r10095, %r10096, 30;
	shf.l.wrap.b32 	%r10100, %r10096, %r10095, 30;
	mov.b64 	%rd790, {%r10100, %r10099};
	xor.b64  	%rd791, %rd790, %rd789;
	shf.l.wrap.b32 	%r10101, %r10095, %r10096, 25;
	shf.l.wrap.b32 	%r10102, %r10096, %r10095, 25;
	mov.b64 	%rd792, {%r10102, %r10101};
	xor.b64  	%rd793, %rd791, %rd792;
	xor.b64  	%rd794, %rd775, %rd727;
	xor.b64  	%rd795, %rd775, %rd751;
	and.b64  	%rd796, %rd795, %rd794;
	xor.b64  	%rd797, %rd796, %rd775;
	add.s64 	%rd798, %rd787, %rd797;
	add.s64 	%rd799, %rd798, %rd793;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10103,%dummy}, %rd788;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10104}, %rd788;
	}
	shf.r.wrap.b32 	%r10105, %r10104, %r10103, 14;
	shf.r.wrap.b32 	%r10106, %r10103, %r10104, 14;
	mov.b64 	%rd800, {%r10106, %r10105};
	shf.r.wrap.b32 	%r10107, %r10104, %r10103, 18;
	shf.r.wrap.b32 	%r10108, %r10103, %r10104, 18;
	mov.b64 	%rd801, {%r10108, %r10107};
	xor.b64  	%rd802, %rd801, %rd800;
	shf.l.wrap.b32 	%r10109, %r10103, %r10104, 23;
	shf.l.wrap.b32 	%r10110, %r10104, %r10103, 23;
	mov.b64 	%rd803, {%r10110, %r10109};
	xor.b64  	%rd804, %rd802, %rd803;
	xor.b64  	%rd805, %rd764, %rd740;
	and.b64  	%rd806, %rd788, %rd805;
	xor.b64  	%rd807, %rd806, %rd740;
	add.s64 	%rd808, %rd716, %rd609;
	add.s64 	%rd809, %rd808, %rd2800;
	add.s64 	%rd810, %rd809, %rd807;
	add.s64 	%rd811, %rd810, %rd804;
	add.s64 	%rd812, %rd811, %rd727;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10111,%dummy}, %rd799;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10112}, %rd799;
	}
	shf.r.wrap.b32 	%r10113, %r10112, %r10111, 28;
	shf.r.wrap.b32 	%r10114, %r10111, %r10112, 28;
	mov.b64 	%rd813, {%r10114, %r10113};
	shf.l.wrap.b32 	%r10115, %r10111, %r10112, 30;
	shf.l.wrap.b32 	%r10116, %r10112, %r10111, 30;
	mov.b64 	%rd814, {%r10116, %r10115};
	xor.b64  	%rd815, %rd814, %rd813;
	shf.l.wrap.b32 	%r10117, %r10111, %r10112, 25;
	shf.l.wrap.b32 	%r10118, %r10112, %r10111, 25;
	mov.b64 	%rd816, {%r10118, %r10117};
	xor.b64  	%rd817, %rd815, %rd816;
	xor.b64  	%rd818, %rd799, %rd751;
	xor.b64  	%rd819, %rd799, %rd775;
	and.b64  	%rd820, %rd819, %rd818;
	xor.b64  	%rd821, %rd820, %rd799;
	add.s64 	%rd822, %rd811, %rd821;
	add.s64 	%rd823, %rd822, %rd817;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10119,%dummy}, %rd812;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10120}, %rd812;
	}
	shf.r.wrap.b32 	%r10121, %r10120, %r10119, 14;
	shf.r.wrap.b32 	%r10122, %r10119, %r10120, 14;
	mov.b64 	%rd824, {%r10122, %r10121};
	shf.r.wrap.b32 	%r10123, %r10120, %r10119, 18;
	shf.r.wrap.b32 	%r10124, %r10119, %r10120, 18;
	mov.b64 	%rd825, {%r10124, %r10123};
	xor.b64  	%rd826, %rd825, %rd824;
	shf.l.wrap.b32 	%r10125, %r10119, %r10120, 23;
	shf.l.wrap.b32 	%r10126, %r10120, %r10119, 23;
	mov.b64 	%rd827, {%r10126, %r10125};
	xor.b64  	%rd828, %rd826, %rd827;
	xor.b64  	%rd829, %rd788, %rd764;
	and.b64  	%rd830, %rd812, %rd829;
	xor.b64  	%rd831, %rd830, %rd764;
	add.s64 	%rd832, %rd740, %rd621;
	add.s64 	%rd833, %rd832, %rd2801;
	add.s64 	%rd834, %rd833, %rd831;
	add.s64 	%rd835, %rd834, %rd828;
	add.s64 	%rd836, %rd835, %rd751;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10127,%dummy}, %rd823;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10128}, %rd823;
	}
	shf.r.wrap.b32 	%r10129, %r10128, %r10127, 28;
	shf.r.wrap.b32 	%r10130, %r10127, %r10128, 28;
	mov.b64 	%rd837, {%r10130, %r10129};
	shf.l.wrap.b32 	%r10131, %r10127, %r10128, 30;
	shf.l.wrap.b32 	%r10132, %r10128, %r10127, 30;
	mov.b64 	%rd838, {%r10132, %r10131};
	xor.b64  	%rd839, %rd838, %rd837;
	shf.l.wrap.b32 	%r10133, %r10127, %r10128, 25;
	shf.l.wrap.b32 	%r10134, %r10128, %r10127, 25;
	mov.b64 	%rd840, {%r10134, %r10133};
	xor.b64  	%rd841, %rd839, %rd840;
	xor.b64  	%rd842, %rd823, %rd775;
	xor.b64  	%rd843, %rd823, %rd799;
	and.b64  	%rd844, %rd843, %rd842;
	xor.b64  	%rd845, %rd844, %rd823;
	add.s64 	%rd846, %rd835, %rd845;
	add.s64 	%rd847, %rd846, %rd841;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10135,%dummy}, %rd836;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10136}, %rd836;
	}
	shf.r.wrap.b32 	%r10137, %r10136, %r10135, 14;
	shf.r.wrap.b32 	%r10138, %r10135, %r10136, 14;
	mov.b64 	%rd848, {%r10138, %r10137};
	shf.r.wrap.b32 	%r10139, %r10136, %r10135, 18;
	shf.r.wrap.b32 	%r10140, %r10135, %r10136, 18;
	mov.b64 	%rd849, {%r10140, %r10139};
	xor.b64  	%rd850, %rd849, %rd848;
	shf.l.wrap.b32 	%r10141, %r10135, %r10136, 23;
	shf.l.wrap.b32 	%r10142, %r10136, %r10135, 23;
	mov.b64 	%rd851, {%r10142, %r10141};
	xor.b64  	%rd852, %rd850, %rd851;
	xor.b64  	%rd853, %rd812, %rd788;
	and.b64  	%rd854, %rd836, %rd853;
	xor.b64  	%rd855, %rd854, %rd788;
	add.s64 	%rd856, %rd764, %rd629;
	add.s64 	%rd857, %rd856, %rd2802;
	add.s64 	%rd858, %rd857, %rd855;
	add.s64 	%rd859, %rd858, %rd852;
	add.s64 	%rd860, %rd859, %rd775;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10143,%dummy}, %rd847;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10144}, %rd847;
	}
	shf.r.wrap.b32 	%r10145, %r10144, %r10143, 28;
	shf.r.wrap.b32 	%r10146, %r10143, %r10144, 28;
	mov.b64 	%rd861, {%r10146, %r10145};
	shf.l.wrap.b32 	%r10147, %r10143, %r10144, 30;
	shf.l.wrap.b32 	%r10148, %r10144, %r10143, 30;
	mov.b64 	%rd862, {%r10148, %r10147};
	xor.b64  	%rd863, %rd862, %rd861;
	shf.l.wrap.b32 	%r10149, %r10143, %r10144, 25;
	shf.l.wrap.b32 	%r10150, %r10144, %r10143, 25;
	mov.b64 	%rd864, {%r10150, %r10149};
	xor.b64  	%rd865, %rd863, %rd864;
	xor.b64  	%rd866, %rd847, %rd799;
	xor.b64  	%rd867, %rd847, %rd823;
	and.b64  	%rd868, %rd867, %rd866;
	xor.b64  	%rd869, %rd868, %rd847;
	add.s64 	%rd870, %rd859, %rd869;
	add.s64 	%rd871, %rd870, %rd865;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10151,%dummy}, %rd860;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10152}, %rd860;
	}
	shf.r.wrap.b32 	%r10153, %r10152, %r10151, 14;
	shf.r.wrap.b32 	%r10154, %r10151, %r10152, 14;
	mov.b64 	%rd872, {%r10154, %r10153};
	shf.r.wrap.b32 	%r10155, %r10152, %r10151, 18;
	shf.r.wrap.b32 	%r10156, %r10151, %r10152, 18;
	mov.b64 	%rd873, {%r10156, %r10155};
	xor.b64  	%rd874, %rd873, %rd872;
	shf.l.wrap.b32 	%r10157, %r10151, %r10152, 23;
	shf.l.wrap.b32 	%r10158, %r10152, %r10151, 23;
	mov.b64 	%rd875, {%r10158, %r10157};
	xor.b64  	%rd876, %rd874, %rd875;
	xor.b64  	%rd877, %rd836, %rd812;
	and.b64  	%rd878, %rd860, %rd877;
	xor.b64  	%rd879, %rd878, %rd812;
	add.s64 	%rd880, %rd788, %rd636;
	add.s64 	%rd881, %rd880, %rd2803;
	add.s64 	%rd882, %rd881, %rd879;
	add.s64 	%rd883, %rd882, %rd876;
	add.s64 	%rd884, %rd883, %rd799;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10159,%dummy}, %rd871;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10160}, %rd871;
	}
	shf.r.wrap.b32 	%r10161, %r10160, %r10159, 28;
	shf.r.wrap.b32 	%r10162, %r10159, %r10160, 28;
	mov.b64 	%rd885, {%r10162, %r10161};
	shf.l.wrap.b32 	%r10163, %r10159, %r10160, 30;
	shf.l.wrap.b32 	%r10164, %r10160, %r10159, 30;
	mov.b64 	%rd886, {%r10164, %r10163};
	xor.b64  	%rd887, %rd886, %rd885;
	shf.l.wrap.b32 	%r10165, %r10159, %r10160, 25;
	shf.l.wrap.b32 	%r10166, %r10160, %r10159, 25;
	mov.b64 	%rd888, {%r10166, %r10165};
	xor.b64  	%rd889, %rd887, %rd888;
	xor.b64  	%rd890, %rd871, %rd823;
	xor.b64  	%rd891, %rd871, %rd847;
	and.b64  	%rd892, %rd891, %rd890;
	xor.b64  	%rd893, %rd892, %rd871;
	add.s64 	%rd894, %rd883, %rd893;
	add.s64 	%rd895, %rd894, %rd889;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10167,%dummy}, %rd884;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10168}, %rd884;
	}
	shf.r.wrap.b32 	%r10169, %r10168, %r10167, 14;
	shf.r.wrap.b32 	%r10170, %r10167, %r10168, 14;
	mov.b64 	%rd896, {%r10170, %r10169};
	shf.r.wrap.b32 	%r10171, %r10168, %r10167, 18;
	shf.r.wrap.b32 	%r10172, %r10167, %r10168, 18;
	mov.b64 	%rd897, {%r10172, %r10171};
	xor.b64  	%rd898, %rd897, %rd896;
	shf.l.wrap.b32 	%r10173, %r10167, %r10168, 23;
	shf.l.wrap.b32 	%r10174, %r10168, %r10167, 23;
	mov.b64 	%rd899, {%r10174, %r10173};
	xor.b64  	%rd900, %rd898, %rd899;
	xor.b64  	%rd901, %rd860, %rd836;
	and.b64  	%rd902, %rd884, %rd901;
	xor.b64  	%rd903, %rd902, %rd836;
	add.s64 	%rd904, %rd812, %rd643;
	add.s64 	%rd905, %rd904, %rd2804;
	add.s64 	%rd906, %rd905, %rd903;
	add.s64 	%rd907, %rd906, %rd900;
	add.s64 	%rd908, %rd907, %rd823;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10175,%dummy}, %rd895;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10176}, %rd895;
	}
	shf.r.wrap.b32 	%r10177, %r10176, %r10175, 28;
	shf.r.wrap.b32 	%r10178, %r10175, %r10176, 28;
	mov.b64 	%rd909, {%r10178, %r10177};
	shf.l.wrap.b32 	%r10179, %r10175, %r10176, 30;
	shf.l.wrap.b32 	%r10180, %r10176, %r10175, 30;
	mov.b64 	%rd910, {%r10180, %r10179};
	xor.b64  	%rd911, %rd910, %rd909;
	shf.l.wrap.b32 	%r10181, %r10175, %r10176, 25;
	shf.l.wrap.b32 	%r10182, %r10176, %r10175, 25;
	mov.b64 	%rd912, {%r10182, %r10181};
	xor.b64  	%rd913, %rd911, %rd912;
	xor.b64  	%rd914, %rd895, %rd847;
	xor.b64  	%rd915, %rd895, %rd871;
	and.b64  	%rd916, %rd915, %rd914;
	xor.b64  	%rd917, %rd916, %rd895;
	add.s64 	%rd918, %rd907, %rd917;
	add.s64 	%rd919, %rd918, %rd913;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10183,%dummy}, %rd908;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10184}, %rd908;
	}
	shf.r.wrap.b32 	%r10185, %r10184, %r10183, 14;
	shf.r.wrap.b32 	%r10186, %r10183, %r10184, 14;
	mov.b64 	%rd920, {%r10186, %r10185};
	shf.r.wrap.b32 	%r10187, %r10184, %r10183, 18;
	shf.r.wrap.b32 	%r10188, %r10183, %r10184, 18;
	mov.b64 	%rd921, {%r10188, %r10187};
	xor.b64  	%rd922, %rd921, %rd920;
	shf.l.wrap.b32 	%r10189, %r10183, %r10184, 23;
	shf.l.wrap.b32 	%r10190, %r10184, %r10183, 23;
	mov.b64 	%rd923, {%r10190, %r10189};
	xor.b64  	%rd924, %rd922, %rd923;
	xor.b64  	%rd925, %rd884, %rd860;
	and.b64  	%rd926, %rd908, %rd925;
	xor.b64  	%rd927, %rd926, %rd860;
	add.s64 	%rd928, %rd836, %rd650;
	add.s64 	%rd929, %rd928, %rd2805;
	add.s64 	%rd930, %rd929, %rd927;
	add.s64 	%rd931, %rd930, %rd924;
	add.s64 	%rd932, %rd931, %rd847;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10191,%dummy}, %rd919;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10192}, %rd919;
	}
	shf.r.wrap.b32 	%r10193, %r10192, %r10191, 28;
	shf.r.wrap.b32 	%r10194, %r10191, %r10192, 28;
	mov.b64 	%rd933, {%r10194, %r10193};
	shf.l.wrap.b32 	%r10195, %r10191, %r10192, 30;
	shf.l.wrap.b32 	%r10196, %r10192, %r10191, 30;
	mov.b64 	%rd934, {%r10196, %r10195};
	xor.b64  	%rd935, %rd934, %rd933;
	shf.l.wrap.b32 	%r10197, %r10191, %r10192, 25;
	shf.l.wrap.b32 	%r10198, %r10192, %r10191, 25;
	mov.b64 	%rd936, {%r10198, %r10197};
	xor.b64  	%rd937, %rd935, %rd936;
	xor.b64  	%rd938, %rd919, %rd871;
	xor.b64  	%rd939, %rd919, %rd895;
	and.b64  	%rd940, %rd939, %rd938;
	xor.b64  	%rd941, %rd940, %rd919;
	add.s64 	%rd942, %rd931, %rd941;
	add.s64 	%rd943, %rd942, %rd937;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10199,%dummy}, %rd932;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10200}, %rd932;
	}
	shf.r.wrap.b32 	%r10201, %r10200, %r10199, 14;
	shf.r.wrap.b32 	%r10202, %r10199, %r10200, 14;
	mov.b64 	%rd944, {%r10202, %r10201};
	shf.r.wrap.b32 	%r10203, %r10200, %r10199, 18;
	shf.r.wrap.b32 	%r10204, %r10199, %r10200, 18;
	mov.b64 	%rd945, {%r10204, %r10203};
	xor.b64  	%rd946, %rd945, %rd944;
	shf.l.wrap.b32 	%r10205, %r10199, %r10200, 23;
	shf.l.wrap.b32 	%r10206, %r10200, %r10199, 23;
	mov.b64 	%rd947, {%r10206, %r10205};
	xor.b64  	%rd948, %rd946, %rd947;
	xor.b64  	%rd949, %rd908, %rd884;
	and.b64  	%rd950, %rd932, %rd949;
	xor.b64  	%rd951, %rd950, %rd884;
	add.s64 	%rd952, %rd860, %rd657;
	add.s64 	%rd953, %rd952, %rd2806;
	add.s64 	%rd954, %rd953, %rd951;
	add.s64 	%rd955, %rd954, %rd948;
	add.s64 	%rd956, %rd955, %rd871;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10207,%dummy}, %rd943;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10208}, %rd943;
	}
	shf.r.wrap.b32 	%r10209, %r10208, %r10207, 28;
	shf.r.wrap.b32 	%r10210, %r10207, %r10208, 28;
	mov.b64 	%rd957, {%r10210, %r10209};
	shf.l.wrap.b32 	%r10211, %r10207, %r10208, 30;
	shf.l.wrap.b32 	%r10212, %r10208, %r10207, 30;
	mov.b64 	%rd958, {%r10212, %r10211};
	xor.b64  	%rd959, %rd958, %rd957;
	shf.l.wrap.b32 	%r10213, %r10207, %r10208, 25;
	shf.l.wrap.b32 	%r10214, %r10208, %r10207, 25;
	mov.b64 	%rd960, {%r10214, %r10213};
	xor.b64  	%rd961, %rd959, %rd960;
	xor.b64  	%rd962, %rd943, %rd895;
	xor.b64  	%rd963, %rd943, %rd919;
	and.b64  	%rd964, %rd963, %rd962;
	xor.b64  	%rd965, %rd964, %rd943;
	add.s64 	%rd966, %rd955, %rd965;
	add.s64 	%rd967, %rd966, %rd961;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10215,%dummy}, %rd956;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10216}, %rd956;
	}
	shf.r.wrap.b32 	%r10217, %r10216, %r10215, 14;
	shf.r.wrap.b32 	%r10218, %r10215, %r10216, 14;
	mov.b64 	%rd968, {%r10218, %r10217};
	shf.r.wrap.b32 	%r10219, %r10216, %r10215, 18;
	shf.r.wrap.b32 	%r10220, %r10215, %r10216, 18;
	mov.b64 	%rd969, {%r10220, %r10219};
	xor.b64  	%rd970, %rd969, %rd968;
	shf.l.wrap.b32 	%r10221, %r10215, %r10216, 23;
	shf.l.wrap.b32 	%r10222, %r10216, %r10215, 23;
	mov.b64 	%rd971, {%r10222, %r10221};
	xor.b64  	%rd972, %rd970, %rd971;
	xor.b64  	%rd973, %rd932, %rd908;
	and.b64  	%rd974, %rd956, %rd973;
	xor.b64  	%rd975, %rd974, %rd908;
	add.s64 	%rd976, %rd884, %rd664;
	add.s64 	%rd977, %rd976, %rd2807;
	add.s64 	%rd978, %rd977, %rd975;
	add.s64 	%rd979, %rd978, %rd972;
	add.s64 	%rd980, %rd979, %rd895;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10223,%dummy}, %rd967;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10224}, %rd967;
	}
	shf.r.wrap.b32 	%r10225, %r10224, %r10223, 28;
	shf.r.wrap.b32 	%r10226, %r10223, %r10224, 28;
	mov.b64 	%rd981, {%r10226, %r10225};
	shf.l.wrap.b32 	%r10227, %r10223, %r10224, 30;
	shf.l.wrap.b32 	%r10228, %r10224, %r10223, 30;
	mov.b64 	%rd982, {%r10228, %r10227};
	xor.b64  	%rd983, %rd982, %rd981;
	shf.l.wrap.b32 	%r10229, %r10223, %r10224, 25;
	shf.l.wrap.b32 	%r10230, %r10224, %r10223, 25;
	mov.b64 	%rd984, {%r10230, %r10229};
	xor.b64  	%rd985, %rd983, %rd984;
	xor.b64  	%rd986, %rd967, %rd919;
	xor.b64  	%rd987, %rd967, %rd943;
	and.b64  	%rd988, %rd987, %rd986;
	xor.b64  	%rd989, %rd988, %rd967;
	add.s64 	%rd990, %rd979, %rd989;
	add.s64 	%rd991, %rd990, %rd985;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10231,%dummy}, %rd980;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10232}, %rd980;
	}
	shf.r.wrap.b32 	%r10233, %r10232, %r10231, 14;
	shf.r.wrap.b32 	%r10234, %r10231, %r10232, 14;
	mov.b64 	%rd992, {%r10234, %r10233};
	shf.r.wrap.b32 	%r10235, %r10232, %r10231, 18;
	shf.r.wrap.b32 	%r10236, %r10231, %r10232, 18;
	mov.b64 	%rd993, {%r10236, %r10235};
	xor.b64  	%rd994, %rd993, %rd992;
	shf.l.wrap.b32 	%r10237, %r10231, %r10232, 23;
	shf.l.wrap.b32 	%r10238, %r10232, %r10231, 23;
	mov.b64 	%rd995, {%r10238, %r10237};
	xor.b64  	%rd996, %rd994, %rd995;
	xor.b64  	%rd997, %rd956, %rd932;
	and.b64  	%rd998, %rd980, %rd997;
	xor.b64  	%rd999, %rd998, %rd932;
	add.s64 	%rd1000, %rd908, %rd671;
	add.s64 	%rd1001, %rd1000, %rd2808;
	add.s64 	%rd1002, %rd1001, %rd999;
	add.s64 	%rd1003, %rd1002, %rd996;
	add.s64 	%rd1004, %rd1003, %rd919;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10239,%dummy}, %rd991;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10240}, %rd991;
	}
	shf.r.wrap.b32 	%r10241, %r10240, %r10239, 28;
	shf.r.wrap.b32 	%r10242, %r10239, %r10240, 28;
	mov.b64 	%rd1005, {%r10242, %r10241};
	shf.l.wrap.b32 	%r10243, %r10239, %r10240, 30;
	shf.l.wrap.b32 	%r10244, %r10240, %r10239, 30;
	mov.b64 	%rd1006, {%r10244, %r10243};
	xor.b64  	%rd1007, %rd1006, %rd1005;
	shf.l.wrap.b32 	%r10245, %r10239, %r10240, 25;
	shf.l.wrap.b32 	%r10246, %r10240, %r10239, 25;
	mov.b64 	%rd1008, {%r10246, %r10245};
	xor.b64  	%rd1009, %rd1007, %rd1008;
	xor.b64  	%rd1010, %rd991, %rd943;
	xor.b64  	%rd1011, %rd991, %rd967;
	and.b64  	%rd1012, %rd1011, %rd1010;
	xor.b64  	%rd1013, %rd1012, %rd991;
	add.s64 	%rd1014, %rd1003, %rd1013;
	add.s64 	%rd1015, %rd1014, %rd1009;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10247,%dummy}, %rd1004;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10248}, %rd1004;
	}
	shf.r.wrap.b32 	%r10249, %r10248, %r10247, 14;
	shf.r.wrap.b32 	%r10250, %r10247, %r10248, 14;
	mov.b64 	%rd1016, {%r10250, %r10249};
	shf.r.wrap.b32 	%r10251, %r10248, %r10247, 18;
	shf.r.wrap.b32 	%r10252, %r10247, %r10248, 18;
	mov.b64 	%rd1017, {%r10252, %r10251};
	xor.b64  	%rd1018, %rd1017, %rd1016;
	shf.l.wrap.b32 	%r10253, %r10247, %r10248, 23;
	shf.l.wrap.b32 	%r10254, %r10248, %r10247, 23;
	mov.b64 	%rd1019, {%r10254, %r10253};
	xor.b64  	%rd1020, %rd1018, %rd1019;
	xor.b64  	%rd1021, %rd980, %rd956;
	and.b64  	%rd1022, %rd1004, %rd1021;
	xor.b64  	%rd1023, %rd1022, %rd956;
	add.s64 	%rd1024, %rd932, %rd678;
	add.s64 	%rd1025, %rd1024, %rd2809;
	add.s64 	%rd1026, %rd1025, %rd1023;
	add.s64 	%rd1027, %rd1026, %rd1020;
	add.s64 	%rd1028, %rd1027, %rd943;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10255,%dummy}, %rd1015;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10256}, %rd1015;
	}
	shf.r.wrap.b32 	%r10257, %r10256, %r10255, 28;
	shf.r.wrap.b32 	%r10258, %r10255, %r10256, 28;
	mov.b64 	%rd1029, {%r10258, %r10257};
	shf.l.wrap.b32 	%r10259, %r10255, %r10256, 30;
	shf.l.wrap.b32 	%r10260, %r10256, %r10255, 30;
	mov.b64 	%rd1030, {%r10260, %r10259};
	xor.b64  	%rd1031, %rd1030, %rd1029;
	shf.l.wrap.b32 	%r10261, %r10255, %r10256, 25;
	shf.l.wrap.b32 	%r10262, %r10256, %r10255, 25;
	mov.b64 	%rd1032, {%r10262, %r10261};
	xor.b64  	%rd1033, %rd1031, %rd1032;
	xor.b64  	%rd1034, %rd1015, %rd967;
	xor.b64  	%rd1035, %rd1015, %rd991;
	and.b64  	%rd1036, %rd1035, %rd1034;
	xor.b64  	%rd1037, %rd1036, %rd1015;
	add.s64 	%rd1038, %rd1027, %rd1037;
	add.s64 	%rd1039, %rd1038, %rd1033;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10263,%dummy}, %rd1028;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10264}, %rd1028;
	}
	shf.r.wrap.b32 	%r10265, %r10264, %r10263, 14;
	shf.r.wrap.b32 	%r10266, %r10263, %r10264, 14;
	mov.b64 	%rd1040, {%r10266, %r10265};
	shf.r.wrap.b32 	%r10267, %r10264, %r10263, 18;
	shf.r.wrap.b32 	%r10268, %r10263, %r10264, 18;
	mov.b64 	%rd1041, {%r10268, %r10267};
	xor.b64  	%rd1042, %rd1041, %rd1040;
	shf.l.wrap.b32 	%r10269, %r10263, %r10264, 23;
	shf.l.wrap.b32 	%r10270, %r10264, %r10263, 23;
	mov.b64 	%rd1043, {%r10270, %r10269};
	xor.b64  	%rd1044, %rd1042, %rd1043;
	xor.b64  	%rd1045, %rd1004, %rd980;
	and.b64  	%rd1046, %rd1028, %rd1045;
	xor.b64  	%rd1047, %rd1046, %rd980;
	add.s64 	%rd1048, %rd956, %rd690;
	add.s64 	%rd1049, %rd1048, %rd2810;
	add.s64 	%rd1050, %rd1049, %rd1047;
	add.s64 	%rd1051, %rd1050, %rd1044;
	add.s64 	%rd1052, %rd1051, %rd967;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10271,%dummy}, %rd1039;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10272}, %rd1039;
	}
	shf.r.wrap.b32 	%r10273, %r10272, %r10271, 28;
	shf.r.wrap.b32 	%r10274, %r10271, %r10272, 28;
	mov.b64 	%rd1053, {%r10274, %r10273};
	shf.l.wrap.b32 	%r10275, %r10271, %r10272, 30;
	shf.l.wrap.b32 	%r10276, %r10272, %r10271, 30;
	mov.b64 	%rd1054, {%r10276, %r10275};
	xor.b64  	%rd1055, %rd1054, %rd1053;
	shf.l.wrap.b32 	%r10277, %r10271, %r10272, 25;
	shf.l.wrap.b32 	%r10278, %r10272, %r10271, 25;
	mov.b64 	%rd1056, {%r10278, %r10277};
	xor.b64  	%rd1057, %rd1055, %rd1056;
	xor.b64  	%rd1058, %rd1039, %rd991;
	xor.b64  	%rd1059, %rd1039, %rd1015;
	and.b64  	%rd1060, %rd1059, %rd1058;
	xor.b64  	%rd1061, %rd1060, %rd1039;
	add.s64 	%rd1062, %rd1051, %rd1061;
	add.s64 	%rd1063, %rd1062, %rd1057;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10279,%dummy}, %rd1052;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10280}, %rd1052;
	}
	shf.r.wrap.b32 	%r10281, %r10280, %r10279, 14;
	shf.r.wrap.b32 	%r10282, %r10279, %r10280, 14;
	mov.b64 	%rd1064, {%r10282, %r10281};
	shf.r.wrap.b32 	%r10283, %r10280, %r10279, 18;
	shf.r.wrap.b32 	%r10284, %r10279, %r10280, 18;
	mov.b64 	%rd1065, {%r10284, %r10283};
	xor.b64  	%rd1066, %rd1065, %rd1064;
	shf.l.wrap.b32 	%r10285, %r10279, %r10280, 23;
	shf.l.wrap.b32 	%r10286, %r10280, %r10279, 23;
	mov.b64 	%rd1067, {%r10286, %r10285};
	xor.b64  	%rd1068, %rd1066, %rd1067;
	xor.b64  	%rd1069, %rd1028, %rd1004;
	and.b64  	%rd1070, %rd1052, %rd1069;
	xor.b64  	%rd1071, %rd1070, %rd1004;
	add.s64 	%rd1072, %rd980, %rd703;
	add.s64 	%rd1073, %rd1072, %rd2811;
	add.s64 	%rd1074, %rd1073, %rd1071;
	add.s64 	%rd1075, %rd1074, %rd1068;
	add.s64 	%rd1076, %rd1075, %rd991;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10287,%dummy}, %rd1063;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10288}, %rd1063;
	}
	shf.r.wrap.b32 	%r10289, %r10288, %r10287, 28;
	shf.r.wrap.b32 	%r10290, %r10287, %r10288, 28;
	mov.b64 	%rd1077, {%r10290, %r10289};
	shf.l.wrap.b32 	%r10291, %r10287, %r10288, 30;
	shf.l.wrap.b32 	%r10292, %r10288, %r10287, 30;
	mov.b64 	%rd1078, {%r10292, %r10291};
	xor.b64  	%rd1079, %rd1078, %rd1077;
	shf.l.wrap.b32 	%r10293, %r10287, %r10288, 25;
	shf.l.wrap.b32 	%r10294, %r10288, %r10287, 25;
	mov.b64 	%rd1080, {%r10294, %r10293};
	xor.b64  	%rd1081, %rd1079, %rd1080;
	xor.b64  	%rd1082, %rd1063, %rd1015;
	xor.b64  	%rd1083, %rd1063, %rd1039;
	and.b64  	%rd1084, %rd1083, %rd1082;
	xor.b64  	%rd1085, %rd1084, %rd1063;
	add.s64 	%rd1086, %rd1075, %rd1085;
	add.s64 	%rd1087, %rd1086, %rd1081;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10295,%dummy}, %rd690;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10296}, %rd690;
	}
	shf.r.wrap.b32 	%r10297, %r10296, %r10295, 19;
	shf.r.wrap.b32 	%r10298, %r10295, %r10296, 19;
	mov.b64 	%rd1088, {%r10298, %r10297};
	shf.l.wrap.b32 	%r10299, %r10295, %r10296, 3;
	shf.l.wrap.b32 	%r10300, %r10296, %r10295, 3;
	mov.b64 	%rd1089, {%r10300, %r10299};
	shr.u64 	%rd1090, %rd690, 6;
	xor.b64  	%rd1091, %rd1088, %rd1090;
	xor.b64  	%rd1092, %rd1091, %rd1089;
	shf.r.wrap.b32 	%r10301, %r9936, %r9935, 1;
	shf.r.wrap.b32 	%r10302, %r9935, %r9936, 1;
	mov.b64 	%rd1093, {%r10302, %r10301};
	shf.r.wrap.b32 	%r10303, %r9936, %r9935, 8;
	shf.r.wrap.b32 	%r10304, %r9935, %r9936, 8;
	mov.b64 	%rd1094, {%r10304, %r10303};
	shr.u64 	%rd1095, %rd573, 7;
	xor.b64  	%rd1096, %rd1093, %rd1095;
	xor.b64  	%rd1097, %rd1096, %rd1094;
	add.s64 	%rd1098, %rd561, %rd650;
	add.s64 	%rd1099, %rd1098, %rd1092;
	add.s64 	%rd1100, %rd1099, %rd1097;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10305,%dummy}, %rd703;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10306}, %rd703;
	}
	shf.r.wrap.b32 	%r10307, %r10306, %r10305, 19;
	shf.r.wrap.b32 	%r10308, %r10305, %r10306, 19;
	mov.b64 	%rd1101, {%r10308, %r10307};
	shf.l.wrap.b32 	%r10309, %r10305, %r10306, 3;
	shf.l.wrap.b32 	%r10310, %r10306, %r10305, 3;
	mov.b64 	%rd1102, {%r10310, %r10309};
	shr.u64 	%rd1103, %rd703, 6;
	xor.b64  	%rd1104, %rd1101, %rd1103;
	xor.b64  	%rd1105, %rd1104, %rd1102;
	shf.r.wrap.b32 	%r10311, %r9948, %r9947, 1;
	shf.r.wrap.b32 	%r10312, %r9947, %r9948, 1;
	mov.b64 	%rd1106, {%r10312, %r10311};
	shf.r.wrap.b32 	%r10313, %r9948, %r9947, 8;
	shf.r.wrap.b32 	%r10314, %r9947, %r9948, 8;
	mov.b64 	%rd1107, {%r10314, %r10313};
	shr.u64 	%rd1108, %rd585, 7;
	xor.b64  	%rd1109, %rd1106, %rd1108;
	xor.b64  	%rd1110, %rd1109, %rd1107;
	add.s64 	%rd1111, %rd573, %rd657;
	add.s64 	%rd1112, %rd1111, %rd1105;
	add.s64 	%rd1113, %rd1112, %rd1110;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10315,%dummy}, %rd1100;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10316}, %rd1100;
	}
	shf.r.wrap.b32 	%r10317, %r10316, %r10315, 19;
	shf.r.wrap.b32 	%r10318, %r10315, %r10316, 19;
	mov.b64 	%rd1114, {%r10318, %r10317};
	shf.l.wrap.b32 	%r10319, %r10315, %r10316, 3;
	shf.l.wrap.b32 	%r10320, %r10316, %r10315, 3;
	mov.b64 	%rd1115, {%r10320, %r10319};
	shr.u64 	%rd1116, %rd1100, 6;
	xor.b64  	%rd1117, %rd1114, %rd1116;
	xor.b64  	%rd1118, %rd1117, %rd1115;
	shf.r.wrap.b32 	%r10321, %r9960, %r9959, 1;
	shf.r.wrap.b32 	%r10322, %r9959, %r9960, 1;
	mov.b64 	%rd1119, {%r10322, %r10321};
	shf.r.wrap.b32 	%r10323, %r9960, %r9959, 8;
	shf.r.wrap.b32 	%r10324, %r9959, %r9960, 8;
	mov.b64 	%rd1120, {%r10324, %r10323};
	shr.u64 	%rd1121, %rd597, 7;
	xor.b64  	%rd1122, %rd1119, %rd1121;
	xor.b64  	%rd1123, %rd1122, %rd1120;
	add.s64 	%rd1124, %rd585, %rd664;
	add.s64 	%rd1125, %rd1124, %rd1118;
	add.s64 	%rd1126, %rd1125, %rd1123;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10325,%dummy}, %rd1113;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10326}, %rd1113;
	}
	shf.r.wrap.b32 	%r10327, %r10326, %r10325, 19;
	shf.r.wrap.b32 	%r10328, %r10325, %r10326, 19;
	mov.b64 	%rd1127, {%r10328, %r10327};
	shf.l.wrap.b32 	%r10329, %r10325, %r10326, 3;
	shf.l.wrap.b32 	%r10330, %r10326, %r10325, 3;
	mov.b64 	%rd1128, {%r10330, %r10329};
	shr.u64 	%rd1129, %rd1113, 6;
	xor.b64  	%rd1130, %rd1127, %rd1129;
	xor.b64  	%rd1131, %rd1130, %rd1128;
	shf.r.wrap.b32 	%r10331, %r9972, %r9971, 1;
	shf.r.wrap.b32 	%r10332, %r9971, %r9972, 1;
	mov.b64 	%rd1132, {%r10332, %r10331};
	shf.r.wrap.b32 	%r10333, %r9972, %r9971, 8;
	shf.r.wrap.b32 	%r10334, %r9971, %r9972, 8;
	mov.b64 	%rd1133, {%r10334, %r10333};
	shr.u64 	%rd1134, %rd609, 7;
	xor.b64  	%rd1135, %rd1132, %rd1134;
	xor.b64  	%rd1136, %rd1135, %rd1133;
	add.s64 	%rd1137, %rd597, %rd671;
	add.s64 	%rd1138, %rd1137, %rd1131;
	add.s64 	%rd1139, %rd1138, %rd1136;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10335,%dummy}, %rd1126;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10336}, %rd1126;
	}
	shf.r.wrap.b32 	%r10337, %r10336, %r10335, 19;
	shf.r.wrap.b32 	%r10338, %r10335, %r10336, 19;
	mov.b64 	%rd1140, {%r10338, %r10337};
	shf.l.wrap.b32 	%r10339, %r10335, %r10336, 3;
	shf.l.wrap.b32 	%r10340, %r10336, %r10335, 3;
	mov.b64 	%rd1141, {%r10340, %r10339};
	shr.u64 	%rd1142, %rd1126, 6;
	xor.b64  	%rd1143, %rd1140, %rd1142;
	xor.b64  	%rd1144, %rd1143, %rd1141;
	shf.r.wrap.b32 	%r10341, %r9978, %r9977, 1;
	shf.r.wrap.b32 	%r10342, %r9977, %r9978, 1;
	mov.b64 	%rd1145, {%r10342, %r10341};
	shf.r.wrap.b32 	%r10343, %r9978, %r9977, 8;
	shf.r.wrap.b32 	%r10344, %r9977, %r9978, 8;
	mov.b64 	%rd1146, {%r10344, %r10343};
	shr.u64 	%rd1147, %rd621, 7;
	xor.b64  	%rd1148, %rd1145, %rd1147;
	xor.b64  	%rd1149, %rd1148, %rd1146;
	add.s64 	%rd1150, %rd609, %rd678;
	add.s64 	%rd1151, %rd1150, %rd1144;
	add.s64 	%rd1152, %rd1151, %rd1149;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10345,%dummy}, %rd1139;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10346}, %rd1139;
	}
	shf.r.wrap.b32 	%r10347, %r10346, %r10345, 19;
	shf.r.wrap.b32 	%r10348, %r10345, %r10346, 19;
	mov.b64 	%rd1153, {%r10348, %r10347};
	shf.l.wrap.b32 	%r10349, %r10345, %r10346, 3;
	shf.l.wrap.b32 	%r10350, %r10346, %r10345, 3;
	mov.b64 	%rd1154, {%r10350, %r10349};
	shr.u64 	%rd1155, %rd1139, 6;
	xor.b64  	%rd1156, %rd1153, %rd1155;
	xor.b64  	%rd1157, %rd1156, %rd1154;
	shf.r.wrap.b32 	%r10351, %r9984, %r9983, 1;
	shf.r.wrap.b32 	%r10352, %r9983, %r9984, 1;
	mov.b64 	%rd1158, {%r10352, %r10351};
	shf.r.wrap.b32 	%r10353, %r9984, %r9983, 8;
	shf.r.wrap.b32 	%r10354, %r9983, %r9984, 8;
	mov.b64 	%rd1159, {%r10354, %r10353};
	shr.u64 	%rd1160, %rd629, 7;
	xor.b64  	%rd1161, %rd1158, %rd1160;
	xor.b64  	%rd1162, %rd1161, %rd1159;
	add.s64 	%rd1163, %rd621, %rd690;
	add.s64 	%rd1164, %rd1163, %rd1157;
	add.s64 	%rd1165, %rd1164, %rd1162;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10355,%dummy}, %rd1152;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10356}, %rd1152;
	}
	shf.r.wrap.b32 	%r10357, %r10356, %r10355, 19;
	shf.r.wrap.b32 	%r10358, %r10355, %r10356, 19;
	mov.b64 	%rd1166, {%r10358, %r10357};
	shf.l.wrap.b32 	%r10359, %r10355, %r10356, 3;
	shf.l.wrap.b32 	%r10360, %r10356, %r10355, 3;
	mov.b64 	%rd1167, {%r10360, %r10359};
	shr.u64 	%rd1168, %rd1152, 6;
	xor.b64  	%rd1169, %rd1166, %rd1168;
	xor.b64  	%rd1170, %rd1169, %rd1167;
	shf.r.wrap.b32 	%r10361, %r9990, %r9989, 1;
	shf.r.wrap.b32 	%r10362, %r9989, %r9990, 1;
	mov.b64 	%rd1171, {%r10362, %r10361};
	shf.r.wrap.b32 	%r10363, %r9990, %r9989, 8;
	shf.r.wrap.b32 	%r10364, %r9989, %r9990, 8;
	mov.b64 	%rd1172, {%r10364, %r10363};
	shr.u64 	%rd1173, %rd636, 7;
	xor.b64  	%rd1174, %rd1171, %rd1173;
	xor.b64  	%rd1175, %rd1174, %rd1172;
	add.s64 	%rd1176, %rd629, %rd703;
	add.s64 	%rd1177, %rd1176, %rd1170;
	add.s64 	%rd1178, %rd1177, %rd1175;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10365,%dummy}, %rd1165;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10366}, %rd1165;
	}
	shf.r.wrap.b32 	%r10367, %r10366, %r10365, 19;
	shf.r.wrap.b32 	%r10368, %r10365, %r10366, 19;
	mov.b64 	%rd1179, {%r10368, %r10367};
	shf.l.wrap.b32 	%r10369, %r10365, %r10366, 3;
	shf.l.wrap.b32 	%r10370, %r10366, %r10365, 3;
	mov.b64 	%rd1180, {%r10370, %r10369};
	shr.u64 	%rd1181, %rd1165, 6;
	xor.b64  	%rd1182, %rd1179, %rd1181;
	xor.b64  	%rd1183, %rd1182, %rd1180;
	shf.r.wrap.b32 	%r10371, %r9996, %r9995, 1;
	shf.r.wrap.b32 	%r10372, %r9995, %r9996, 1;
	mov.b64 	%rd1184, {%r10372, %r10371};
	shf.r.wrap.b32 	%r10373, %r9996, %r9995, 8;
	shf.r.wrap.b32 	%r10374, %r9995, %r9996, 8;
	mov.b64 	%rd1185, {%r10374, %r10373};
	shr.u64 	%rd1186, %rd643, 7;
	xor.b64  	%rd1187, %rd1184, %rd1186;
	xor.b64  	%rd1188, %rd1187, %rd1185;
	add.s64 	%rd1189, %rd1100, %rd636;
	add.s64 	%rd1190, %rd1189, %rd1183;
	add.s64 	%rd1191, %rd1190, %rd1188;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10375,%dummy}, %rd1178;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10376}, %rd1178;
	}
	shf.r.wrap.b32 	%r10377, %r10376, %r10375, 19;
	shf.r.wrap.b32 	%r10378, %r10375, %r10376, 19;
	mov.b64 	%rd1192, {%r10378, %r10377};
	shf.l.wrap.b32 	%r10379, %r10375, %r10376, 3;
	shf.l.wrap.b32 	%r10380, %r10376, %r10375, 3;
	mov.b64 	%rd1193, {%r10380, %r10379};
	shr.u64 	%rd1194, %rd1178, 6;
	xor.b64  	%rd1195, %rd1192, %rd1194;
	xor.b64  	%rd1196, %rd1195, %rd1193;
	shf.r.wrap.b32 	%r10381, %r10002, %r10001, 1;
	shf.r.wrap.b32 	%r10382, %r10001, %r10002, 1;
	mov.b64 	%rd1197, {%r10382, %r10381};
	shf.r.wrap.b32 	%r10383, %r10002, %r10001, 8;
	shf.r.wrap.b32 	%r10384, %r10001, %r10002, 8;
	mov.b64 	%rd1198, {%r10384, %r10383};
	shr.u64 	%rd1199, %rd650, 7;
	xor.b64  	%rd1200, %rd1197, %rd1199;
	xor.b64  	%rd1201, %rd1200, %rd1198;
	add.s64 	%rd1202, %rd1113, %rd643;
	add.s64 	%rd1203, %rd1202, %rd1196;
	add.s64 	%rd1204, %rd1203, %rd1201;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10385,%dummy}, %rd1191;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10386}, %rd1191;
	}
	shf.r.wrap.b32 	%r10387, %r10386, %r10385, 19;
	shf.r.wrap.b32 	%r10388, %r10385, %r10386, 19;
	mov.b64 	%rd1205, {%r10388, %r10387};
	shf.l.wrap.b32 	%r10389, %r10385, %r10386, 3;
	shf.l.wrap.b32 	%r10390, %r10386, %r10385, 3;
	mov.b64 	%rd1206, {%r10390, %r10389};
	shr.u64 	%rd1207, %rd1191, 6;
	xor.b64  	%rd1208, %rd1205, %rd1207;
	xor.b64  	%rd1209, %rd1208, %rd1206;
	shf.r.wrap.b32 	%r10391, %r10008, %r10007, 1;
	shf.r.wrap.b32 	%r10392, %r10007, %r10008, 1;
	mov.b64 	%rd1210, {%r10392, %r10391};
	shf.r.wrap.b32 	%r10393, %r10008, %r10007, 8;
	shf.r.wrap.b32 	%r10394, %r10007, %r10008, 8;
	mov.b64 	%rd1211, {%r10394, %r10393};
	shr.u64 	%rd1212, %rd657, 7;
	xor.b64  	%rd1213, %rd1210, %rd1212;
	xor.b64  	%rd1214, %rd1213, %rd1211;
	add.s64 	%rd1215, %rd1126, %rd650;
	add.s64 	%rd1216, %rd1215, %rd1209;
	add.s64 	%rd1217, %rd1216, %rd1214;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10395,%dummy}, %rd1204;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10396}, %rd1204;
	}
	shf.r.wrap.b32 	%r10397, %r10396, %r10395, 19;
	shf.r.wrap.b32 	%r10398, %r10395, %r10396, 19;
	mov.b64 	%rd1218, {%r10398, %r10397};
	shf.l.wrap.b32 	%r10399, %r10395, %r10396, 3;
	shf.l.wrap.b32 	%r10400, %r10396, %r10395, 3;
	mov.b64 	%rd1219, {%r10400, %r10399};
	shr.u64 	%rd1220, %rd1204, 6;
	xor.b64  	%rd1221, %rd1218, %rd1220;
	xor.b64  	%rd1222, %rd1221, %rd1219;
	shf.r.wrap.b32 	%r10401, %r10014, %r10013, 1;
	shf.r.wrap.b32 	%r10402, %r10013, %r10014, 1;
	mov.b64 	%rd1223, {%r10402, %r10401};
	shf.r.wrap.b32 	%r10403, %r10014, %r10013, 8;
	shf.r.wrap.b32 	%r10404, %r10013, %r10014, 8;
	mov.b64 	%rd1224, {%r10404, %r10403};
	shr.u64 	%rd1225, %rd664, 7;
	xor.b64  	%rd1226, %rd1223, %rd1225;
	xor.b64  	%rd1227, %rd1226, %rd1224;
	add.s64 	%rd1228, %rd1139, %rd657;
	add.s64 	%rd1229, %rd1228, %rd1222;
	add.s64 	%rd1230, %rd1229, %rd1227;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10405,%dummy}, %rd1217;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10406}, %rd1217;
	}
	shf.r.wrap.b32 	%r10407, %r10406, %r10405, 19;
	shf.r.wrap.b32 	%r10408, %r10405, %r10406, 19;
	mov.b64 	%rd1231, {%r10408, %r10407};
	shf.l.wrap.b32 	%r10409, %r10405, %r10406, 3;
	shf.l.wrap.b32 	%r10410, %r10406, %r10405, 3;
	mov.b64 	%rd1232, {%r10410, %r10409};
	shr.u64 	%rd1233, %rd1217, 6;
	xor.b64  	%rd1234, %rd1231, %rd1233;
	xor.b64  	%rd1235, %rd1234, %rd1232;
	shf.r.wrap.b32 	%r10411, %r10020, %r10019, 1;
	shf.r.wrap.b32 	%r10412, %r10019, %r10020, 1;
	mov.b64 	%rd1236, {%r10412, %r10411};
	shf.r.wrap.b32 	%r10413, %r10020, %r10019, 8;
	shf.r.wrap.b32 	%r10414, %r10019, %r10020, 8;
	mov.b64 	%rd1237, {%r10414, %r10413};
	shr.u64 	%rd1238, %rd671, 7;
	xor.b64  	%rd1239, %rd1236, %rd1238;
	xor.b64  	%rd1240, %rd1239, %rd1237;
	add.s64 	%rd1241, %rd1152, %rd664;
	add.s64 	%rd1242, %rd1241, %rd1235;
	add.s64 	%rd1243, %rd1242, %rd1240;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10415,%dummy}, %rd1230;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10416}, %rd1230;
	}
	shf.r.wrap.b32 	%r10417, %r10416, %r10415, 19;
	shf.r.wrap.b32 	%r10418, %r10415, %r10416, 19;
	mov.b64 	%rd1244, {%r10418, %r10417};
	shf.l.wrap.b32 	%r10419, %r10415, %r10416, 3;
	shf.l.wrap.b32 	%r10420, %r10416, %r10415, 3;
	mov.b64 	%rd1245, {%r10420, %r10419};
	shr.u64 	%rd1246, %rd1230, 6;
	xor.b64  	%rd1247, %rd1244, %rd1246;
	xor.b64  	%rd1248, %rd1247, %rd1245;
	shf.r.wrap.b32 	%r10421, %r10030, %r10029, 1;
	shf.r.wrap.b32 	%r10422, %r10029, %r10030, 1;
	mov.b64 	%rd1249, {%r10422, %r10421};
	shf.r.wrap.b32 	%r10423, %r10030, %r10029, 8;
	shf.r.wrap.b32 	%r10424, %r10029, %r10030, 8;
	mov.b64 	%rd1250, {%r10424, %r10423};
	shr.u64 	%rd1251, %rd678, 7;
	xor.b64  	%rd1252, %rd1249, %rd1251;
	xor.b64  	%rd1253, %rd1252, %rd1250;
	add.s64 	%rd1254, %rd1165, %rd671;
	add.s64 	%rd1255, %rd1254, %rd1248;
	add.s64 	%rd1256, %rd1255, %rd1253;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10425,%dummy}, %rd1243;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10426}, %rd1243;
	}
	shf.r.wrap.b32 	%r10427, %r10426, %r10425, 19;
	shf.r.wrap.b32 	%r10428, %r10425, %r10426, 19;
	mov.b64 	%rd1257, {%r10428, %r10427};
	shf.l.wrap.b32 	%r10429, %r10425, %r10426, 3;
	shf.l.wrap.b32 	%r10430, %r10426, %r10425, 3;
	mov.b64 	%rd1258, {%r10430, %r10429};
	shr.u64 	%rd1259, %rd1243, 6;
	xor.b64  	%rd1260, %rd1257, %rd1259;
	xor.b64  	%rd1261, %rd1260, %rd1258;
	shf.r.wrap.b32 	%r10431, %r10296, %r10295, 1;
	shf.r.wrap.b32 	%r10432, %r10295, %r10296, 1;
	mov.b64 	%rd1262, {%r10432, %r10431};
	shf.r.wrap.b32 	%r10433, %r10296, %r10295, 8;
	shf.r.wrap.b32 	%r10434, %r10295, %r10296, 8;
	mov.b64 	%rd1263, {%r10434, %r10433};
	shr.u64 	%rd1264, %rd690, 7;
	xor.b64  	%rd1265, %rd1262, %rd1264;
	xor.b64  	%rd1266, %rd1265, %rd1263;
	add.s64 	%rd1267, %rd1178, %rd678;
	add.s64 	%rd1268, %rd1267, %rd1261;
	add.s64 	%rd1269, %rd1268, %rd1266;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10435,%dummy}, %rd1256;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10436}, %rd1256;
	}
	shf.r.wrap.b32 	%r10437, %r10436, %r10435, 19;
	shf.r.wrap.b32 	%r10438, %r10435, %r10436, 19;
	mov.b64 	%rd1270, {%r10438, %r10437};
	shf.l.wrap.b32 	%r10439, %r10435, %r10436, 3;
	shf.l.wrap.b32 	%r10440, %r10436, %r10435, 3;
	mov.b64 	%rd1271, {%r10440, %r10439};
	shr.u64 	%rd1272, %rd1256, 6;
	xor.b64  	%rd1273, %rd1270, %rd1272;
	xor.b64  	%rd1274, %rd1273, %rd1271;
	shf.r.wrap.b32 	%r10441, %r10306, %r10305, 1;
	shf.r.wrap.b32 	%r10442, %r10305, %r10306, 1;
	mov.b64 	%rd1275, {%r10442, %r10441};
	shf.r.wrap.b32 	%r10443, %r10306, %r10305, 8;
	shf.r.wrap.b32 	%r10444, %r10305, %r10306, 8;
	mov.b64 	%rd1276, {%r10444, %r10443};
	shr.u64 	%rd1277, %rd703, 7;
	xor.b64  	%rd1278, %rd1275, %rd1277;
	xor.b64  	%rd1279, %rd1278, %rd1276;
	add.s64 	%rd1280, %rd1191, %rd690;
	add.s64 	%rd1281, %rd1280, %rd1274;
	add.s64 	%rd1282, %rd1281, %rd1279;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10445,%dummy}, %rd1269;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10446}, %rd1269;
	}
	shf.r.wrap.b32 	%r10447, %r10446, %r10445, 19;
	shf.r.wrap.b32 	%r10448, %r10445, %r10446, 19;
	mov.b64 	%rd1283, {%r10448, %r10447};
	shf.l.wrap.b32 	%r10449, %r10445, %r10446, 3;
	shf.l.wrap.b32 	%r10450, %r10446, %r10445, 3;
	mov.b64 	%rd1284, {%r10450, %r10449};
	shr.u64 	%rd1285, %rd1269, 6;
	xor.b64  	%rd1286, %rd1283, %rd1285;
	xor.b64  	%rd1287, %rd1286, %rd1284;
	shf.r.wrap.b32 	%r10451, %r10316, %r10315, 1;
	shf.r.wrap.b32 	%r10452, %r10315, %r10316, 1;
	mov.b64 	%rd1288, {%r10452, %r10451};
	shf.r.wrap.b32 	%r10453, %r10316, %r10315, 8;
	shf.r.wrap.b32 	%r10454, %r10315, %r10316, 8;
	mov.b64 	%rd1289, {%r10454, %r10453};
	shr.u64 	%rd1290, %rd1100, 7;
	xor.b64  	%rd1291, %rd1288, %rd1290;
	xor.b64  	%rd1292, %rd1291, %rd1289;
	add.s64 	%rd1293, %rd1204, %rd703;
	add.s64 	%rd1294, %rd1293, %rd1287;
	add.s64 	%rd1295, %rd1294, %rd1292;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10455,%dummy}, %rd1076;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10456}, %rd1076;
	}
	shf.r.wrap.b32 	%r10457, %r10456, %r10455, 14;
	shf.r.wrap.b32 	%r10458, %r10455, %r10456, 14;
	mov.b64 	%rd1296, {%r10458, %r10457};
	shf.r.wrap.b32 	%r10459, %r10456, %r10455, 18;
	shf.r.wrap.b32 	%r10460, %r10455, %r10456, 18;
	mov.b64 	%rd1297, {%r10460, %r10459};
	xor.b64  	%rd1298, %rd1297, %rd1296;
	shf.l.wrap.b32 	%r10461, %r10455, %r10456, 23;
	shf.l.wrap.b32 	%r10462, %r10456, %r10455, 23;
	mov.b64 	%rd1299, {%r10462, %r10461};
	xor.b64  	%rd1300, %rd1298, %rd1299;
	xor.b64  	%rd1301, %rd1028, %rd1052;
	and.b64  	%rd1302, %rd1301, %rd1076;
	xor.b64  	%rd1303, %rd1302, %rd1028;
	add.s64 	%rd1304, %rd1303, %rd1004;
	add.s64 	%rd1305, %rd1304, %rd1100;
	add.s64 	%rd1306, %rd1305, %rd2812;
	add.s64 	%rd1307, %rd1306, %rd1300;
	add.s64 	%rd1308, %rd1307, %rd1015;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10463,%dummy}, %rd1087;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10464}, %rd1087;
	}
	shf.r.wrap.b32 	%r10465, %r10464, %r10463, 28;
	shf.r.wrap.b32 	%r10466, %r10463, %r10464, 28;
	mov.b64 	%rd1309, {%r10466, %r10465};
	shf.l.wrap.b32 	%r10467, %r10463, %r10464, 30;
	shf.l.wrap.b32 	%r10468, %r10464, %r10463, 30;
	mov.b64 	%rd1310, {%r10468, %r10467};
	xor.b64  	%rd1311, %rd1310, %rd1309;
	shf.l.wrap.b32 	%r10469, %r10463, %r10464, 25;
	shf.l.wrap.b32 	%r10470, %r10464, %r10463, 25;
	mov.b64 	%rd1312, {%r10470, %r10469};
	xor.b64  	%rd1313, %rd1311, %rd1312;
	xor.b64  	%rd1314, %rd1087, %rd1039;
	xor.b64  	%rd1315, %rd1087, %rd1063;
	and.b64  	%rd1316, %rd1315, %rd1314;
	xor.b64  	%rd1317, %rd1316, %rd1087;
	add.s64 	%rd1318, %rd1307, %rd1317;
	add.s64 	%rd1319, %rd1318, %rd1313;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10471,%dummy}, %rd1308;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10472}, %rd1308;
	}
	shf.r.wrap.b32 	%r10473, %r10472, %r10471, 14;
	shf.r.wrap.b32 	%r10474, %r10471, %r10472, 14;
	mov.b64 	%rd1320, {%r10474, %r10473};
	shf.r.wrap.b32 	%r10475, %r10472, %r10471, 18;
	shf.r.wrap.b32 	%r10476, %r10471, %r10472, 18;
	mov.b64 	%rd1321, {%r10476, %r10475};
	xor.b64  	%rd1322, %rd1321, %rd1320;
	shf.l.wrap.b32 	%r10477, %r10471, %r10472, 23;
	shf.l.wrap.b32 	%r10478, %r10472, %r10471, 23;
	mov.b64 	%rd1323, {%r10478, %r10477};
	xor.b64  	%rd1324, %rd1322, %rd1323;
	xor.b64  	%rd1325, %rd1052, %rd1076;
	and.b64  	%rd1326, %rd1308, %rd1325;
	xor.b64  	%rd1327, %rd1326, %rd1052;
	add.s64 	%rd1328, %rd1113, %rd1028;
	add.s64 	%rd1329, %rd1328, %rd2813;
	add.s64 	%rd1330, %rd1329, %rd1327;
	add.s64 	%rd1331, %rd1330, %rd1324;
	add.s64 	%rd1332, %rd1331, %rd1039;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10479,%dummy}, %rd1319;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10480}, %rd1319;
	}
	shf.r.wrap.b32 	%r10481, %r10480, %r10479, 28;
	shf.r.wrap.b32 	%r10482, %r10479, %r10480, 28;
	mov.b64 	%rd1333, {%r10482, %r10481};
	shf.l.wrap.b32 	%r10483, %r10479, %r10480, 30;
	shf.l.wrap.b32 	%r10484, %r10480, %r10479, 30;
	mov.b64 	%rd1334, {%r10484, %r10483};
	xor.b64  	%rd1335, %rd1334, %rd1333;
	shf.l.wrap.b32 	%r10485, %r10479, %r10480, 25;
	shf.l.wrap.b32 	%r10486, %r10480, %r10479, 25;
	mov.b64 	%rd1336, {%r10486, %r10485};
	xor.b64  	%rd1337, %rd1335, %rd1336;
	xor.b64  	%rd1338, %rd1319, %rd1063;
	xor.b64  	%rd1339, %rd1319, %rd1087;
	and.b64  	%rd1340, %rd1339, %rd1338;
	xor.b64  	%rd1341, %rd1340, %rd1319;
	add.s64 	%rd1342, %rd1331, %rd1341;
	add.s64 	%rd1343, %rd1342, %rd1337;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10487,%dummy}, %rd1332;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10488}, %rd1332;
	}
	shf.r.wrap.b32 	%r10489, %r10488, %r10487, 14;
	shf.r.wrap.b32 	%r10490, %r10487, %r10488, 14;
	mov.b64 	%rd1344, {%r10490, %r10489};
	shf.r.wrap.b32 	%r10491, %r10488, %r10487, 18;
	shf.r.wrap.b32 	%r10492, %r10487, %r10488, 18;
	mov.b64 	%rd1345, {%r10492, %r10491};
	xor.b64  	%rd1346, %rd1345, %rd1344;
	shf.l.wrap.b32 	%r10493, %r10487, %r10488, 23;
	shf.l.wrap.b32 	%r10494, %r10488, %r10487, 23;
	mov.b64 	%rd1347, {%r10494, %r10493};
	xor.b64  	%rd1348, %rd1346, %rd1347;
	xor.b64  	%rd1349, %rd1308, %rd1076;
	and.b64  	%rd1350, %rd1332, %rd1349;
	xor.b64  	%rd1351, %rd1350, %rd1076;
	add.s64 	%rd1352, %rd1126, %rd1052;
	add.s64 	%rd1353, %rd1352, %rd2814;
	add.s64 	%rd1354, %rd1353, %rd1351;
	add.s64 	%rd1355, %rd1354, %rd1348;
	add.s64 	%rd1356, %rd1355, %rd1063;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10495,%dummy}, %rd1343;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10496}, %rd1343;
	}
	shf.r.wrap.b32 	%r10497, %r10496, %r10495, 28;
	shf.r.wrap.b32 	%r10498, %r10495, %r10496, 28;
	mov.b64 	%rd1357, {%r10498, %r10497};
	shf.l.wrap.b32 	%r10499, %r10495, %r10496, 30;
	shf.l.wrap.b32 	%r10500, %r10496, %r10495, 30;
	mov.b64 	%rd1358, {%r10500, %r10499};
	xor.b64  	%rd1359, %rd1358, %rd1357;
	shf.l.wrap.b32 	%r10501, %r10495, %r10496, 25;
	shf.l.wrap.b32 	%r10502, %r10496, %r10495, 25;
	mov.b64 	%rd1360, {%r10502, %r10501};
	xor.b64  	%rd1361, %rd1359, %rd1360;
	xor.b64  	%rd1362, %rd1343, %rd1087;
	xor.b64  	%rd1363, %rd1343, %rd1319;
	and.b64  	%rd1364, %rd1363, %rd1362;
	xor.b64  	%rd1365, %rd1364, %rd1343;
	add.s64 	%rd1366, %rd1355, %rd1365;
	add.s64 	%rd1367, %rd1366, %rd1361;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10503,%dummy}, %rd1356;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10504}, %rd1356;
	}
	shf.r.wrap.b32 	%r10505, %r10504, %r10503, 14;
	shf.r.wrap.b32 	%r10506, %r10503, %r10504, 14;
	mov.b64 	%rd1368, {%r10506, %r10505};
	shf.r.wrap.b32 	%r10507, %r10504, %r10503, 18;
	shf.r.wrap.b32 	%r10508, %r10503, %r10504, 18;
	mov.b64 	%rd1369, {%r10508, %r10507};
	xor.b64  	%rd1370, %rd1369, %rd1368;
	shf.l.wrap.b32 	%r10509, %r10503, %r10504, 23;
	shf.l.wrap.b32 	%r10510, %r10504, %r10503, 23;
	mov.b64 	%rd1371, {%r10510, %r10509};
	xor.b64  	%rd1372, %rd1370, %rd1371;
	xor.b64  	%rd1373, %rd1332, %rd1308;
	and.b64  	%rd1374, %rd1356, %rd1373;
	xor.b64  	%rd1375, %rd1374, %rd1308;
	add.s64 	%rd1376, %rd1139, %rd1076;
	add.s64 	%rd1377, %rd1376, %rd2815;
	add.s64 	%rd1378, %rd1377, %rd1375;
	add.s64 	%rd1379, %rd1378, %rd1372;
	add.s64 	%rd1380, %rd1379, %rd1087;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10511,%dummy}, %rd1367;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10512}, %rd1367;
	}
	shf.r.wrap.b32 	%r10513, %r10512, %r10511, 28;
	shf.r.wrap.b32 	%r10514, %r10511, %r10512, 28;
	mov.b64 	%rd1381, {%r10514, %r10513};
	shf.l.wrap.b32 	%r10515, %r10511, %r10512, 30;
	shf.l.wrap.b32 	%r10516, %r10512, %r10511, 30;
	mov.b64 	%rd1382, {%r10516, %r10515};
	xor.b64  	%rd1383, %rd1382, %rd1381;
	shf.l.wrap.b32 	%r10517, %r10511, %r10512, 25;
	shf.l.wrap.b32 	%r10518, %r10512, %r10511, 25;
	mov.b64 	%rd1384, {%r10518, %r10517};
	xor.b64  	%rd1385, %rd1383, %rd1384;
	xor.b64  	%rd1386, %rd1367, %rd1319;
	xor.b64  	%rd1387, %rd1367, %rd1343;
	and.b64  	%rd1388, %rd1387, %rd1386;
	xor.b64  	%rd1389, %rd1388, %rd1367;
	add.s64 	%rd1390, %rd1379, %rd1389;
	add.s64 	%rd1391, %rd1390, %rd1385;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10519,%dummy}, %rd1380;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10520}, %rd1380;
	}
	shf.r.wrap.b32 	%r10521, %r10520, %r10519, 14;
	shf.r.wrap.b32 	%r10522, %r10519, %r10520, 14;
	mov.b64 	%rd1392, {%r10522, %r10521};
	shf.r.wrap.b32 	%r10523, %r10520, %r10519, 18;
	shf.r.wrap.b32 	%r10524, %r10519, %r10520, 18;
	mov.b64 	%rd1393, {%r10524, %r10523};
	xor.b64  	%rd1394, %rd1393, %rd1392;
	shf.l.wrap.b32 	%r10525, %r10519, %r10520, 23;
	shf.l.wrap.b32 	%r10526, %r10520, %r10519, 23;
	mov.b64 	%rd1395, {%r10526, %r10525};
	xor.b64  	%rd1396, %rd1394, %rd1395;
	xor.b64  	%rd1397, %rd1356, %rd1332;
	and.b64  	%rd1398, %rd1380, %rd1397;
	xor.b64  	%rd1399, %rd1398, %rd1332;
	add.s64 	%rd1400, %rd1308, %rd1152;
	add.s64 	%rd1401, %rd1400, %rd2816;
	add.s64 	%rd1402, %rd1401, %rd1399;
	add.s64 	%rd1403, %rd1402, %rd1396;
	add.s64 	%rd1404, %rd1403, %rd1319;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10527,%dummy}, %rd1391;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10528}, %rd1391;
	}
	shf.r.wrap.b32 	%r10529, %r10528, %r10527, 28;
	shf.r.wrap.b32 	%r10530, %r10527, %r10528, 28;
	mov.b64 	%rd1405, {%r10530, %r10529};
	shf.l.wrap.b32 	%r10531, %r10527, %r10528, 30;
	shf.l.wrap.b32 	%r10532, %r10528, %r10527, 30;
	mov.b64 	%rd1406, {%r10532, %r10531};
	xor.b64  	%rd1407, %rd1406, %rd1405;
	shf.l.wrap.b32 	%r10533, %r10527, %r10528, 25;
	shf.l.wrap.b32 	%r10534, %r10528, %r10527, 25;
	mov.b64 	%rd1408, {%r10534, %r10533};
	xor.b64  	%rd1409, %rd1407, %rd1408;
	xor.b64  	%rd1410, %rd1391, %rd1343;
	xor.b64  	%rd1411, %rd1391, %rd1367;
	and.b64  	%rd1412, %rd1411, %rd1410;
	xor.b64  	%rd1413, %rd1412, %rd1391;
	add.s64 	%rd1414, %rd1403, %rd1413;
	add.s64 	%rd1415, %rd1414, %rd1409;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10535,%dummy}, %rd1404;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10536}, %rd1404;
	}
	shf.r.wrap.b32 	%r10537, %r10536, %r10535, 14;
	shf.r.wrap.b32 	%r10538, %r10535, %r10536, 14;
	mov.b64 	%rd1416, {%r10538, %r10537};
	shf.r.wrap.b32 	%r10539, %r10536, %r10535, 18;
	shf.r.wrap.b32 	%r10540, %r10535, %r10536, 18;
	mov.b64 	%rd1417, {%r10540, %r10539};
	xor.b64  	%rd1418, %rd1417, %rd1416;
	shf.l.wrap.b32 	%r10541, %r10535, %r10536, 23;
	shf.l.wrap.b32 	%r10542, %r10536, %r10535, 23;
	mov.b64 	%rd1419, {%r10542, %r10541};
	xor.b64  	%rd1420, %rd1418, %rd1419;
	xor.b64  	%rd1421, %rd1380, %rd1356;
	and.b64  	%rd1422, %rd1404, %rd1421;
	xor.b64  	%rd1423, %rd1422, %rd1356;
	add.s64 	%rd1424, %rd1332, %rd1165;
	add.s64 	%rd1425, %rd1424, %rd2817;
	add.s64 	%rd1426, %rd1425, %rd1423;
	add.s64 	%rd1427, %rd1426, %rd1420;
	add.s64 	%rd1428, %rd1427, %rd1343;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10543,%dummy}, %rd1415;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10544}, %rd1415;
	}
	shf.r.wrap.b32 	%r10545, %r10544, %r10543, 28;
	shf.r.wrap.b32 	%r10546, %r10543, %r10544, 28;
	mov.b64 	%rd1429, {%r10546, %r10545};
	shf.l.wrap.b32 	%r10547, %r10543, %r10544, 30;
	shf.l.wrap.b32 	%r10548, %r10544, %r10543, 30;
	mov.b64 	%rd1430, {%r10548, %r10547};
	xor.b64  	%rd1431, %rd1430, %rd1429;
	shf.l.wrap.b32 	%r10549, %r10543, %r10544, 25;
	shf.l.wrap.b32 	%r10550, %r10544, %r10543, 25;
	mov.b64 	%rd1432, {%r10550, %r10549};
	xor.b64  	%rd1433, %rd1431, %rd1432;
	xor.b64  	%rd1434, %rd1415, %rd1367;
	xor.b64  	%rd1435, %rd1415, %rd1391;
	and.b64  	%rd1436, %rd1435, %rd1434;
	xor.b64  	%rd1437, %rd1436, %rd1415;
	add.s64 	%rd1438, %rd1427, %rd1437;
	add.s64 	%rd1439, %rd1438, %rd1433;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10551,%dummy}, %rd1428;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10552}, %rd1428;
	}
	shf.r.wrap.b32 	%r10553, %r10552, %r10551, 14;
	shf.r.wrap.b32 	%r10554, %r10551, %r10552, 14;
	mov.b64 	%rd1440, {%r10554, %r10553};
	shf.r.wrap.b32 	%r10555, %r10552, %r10551, 18;
	shf.r.wrap.b32 	%r10556, %r10551, %r10552, 18;
	mov.b64 	%rd1441, {%r10556, %r10555};
	xor.b64  	%rd1442, %rd1441, %rd1440;
	shf.l.wrap.b32 	%r10557, %r10551, %r10552, 23;
	shf.l.wrap.b32 	%r10558, %r10552, %r10551, 23;
	mov.b64 	%rd1443, {%r10558, %r10557};
	xor.b64  	%rd1444, %rd1442, %rd1443;
	xor.b64  	%rd1445, %rd1404, %rd1380;
	and.b64  	%rd1446, %rd1428, %rd1445;
	xor.b64  	%rd1447, %rd1446, %rd1380;
	add.s64 	%rd1448, %rd1356, %rd1178;
	add.s64 	%rd1449, %rd1448, %rd2818;
	add.s64 	%rd1450, %rd1449, %rd1447;
	add.s64 	%rd1451, %rd1450, %rd1444;
	add.s64 	%rd1452, %rd1451, %rd1367;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10559,%dummy}, %rd1439;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10560}, %rd1439;
	}
	shf.r.wrap.b32 	%r10561, %r10560, %r10559, 28;
	shf.r.wrap.b32 	%r10562, %r10559, %r10560, 28;
	mov.b64 	%rd1453, {%r10562, %r10561};
	shf.l.wrap.b32 	%r10563, %r10559, %r10560, 30;
	shf.l.wrap.b32 	%r10564, %r10560, %r10559, 30;
	mov.b64 	%rd1454, {%r10564, %r10563};
	xor.b64  	%rd1455, %rd1454, %rd1453;
	shf.l.wrap.b32 	%r10565, %r10559, %r10560, 25;
	shf.l.wrap.b32 	%r10566, %r10560, %r10559, 25;
	mov.b64 	%rd1456, {%r10566, %r10565};
	xor.b64  	%rd1457, %rd1455, %rd1456;
	xor.b64  	%rd1458, %rd1439, %rd1391;
	xor.b64  	%rd1459, %rd1439, %rd1415;
	and.b64  	%rd1460, %rd1459, %rd1458;
	xor.b64  	%rd1461, %rd1460, %rd1439;
	add.s64 	%rd1462, %rd1451, %rd1461;
	add.s64 	%rd1463, %rd1462, %rd1457;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10567,%dummy}, %rd1452;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10568}, %rd1452;
	}
	shf.r.wrap.b32 	%r10569, %r10568, %r10567, 14;
	shf.r.wrap.b32 	%r10570, %r10567, %r10568, 14;
	mov.b64 	%rd1464, {%r10570, %r10569};
	shf.r.wrap.b32 	%r10571, %r10568, %r10567, 18;
	shf.r.wrap.b32 	%r10572, %r10567, %r10568, 18;
	mov.b64 	%rd1465, {%r10572, %r10571};
	xor.b64  	%rd1466, %rd1465, %rd1464;
	shf.l.wrap.b32 	%r10573, %r10567, %r10568, 23;
	shf.l.wrap.b32 	%r10574, %r10568, %r10567, 23;
	mov.b64 	%rd1467, {%r10574, %r10573};
	xor.b64  	%rd1468, %rd1466, %rd1467;
	xor.b64  	%rd1469, %rd1428, %rd1404;
	and.b64  	%rd1470, %rd1452, %rd1469;
	xor.b64  	%rd1471, %rd1470, %rd1404;
	add.s64 	%rd1472, %rd1380, %rd1191;
	add.s64 	%rd1473, %rd1472, %rd2819;
	add.s64 	%rd1474, %rd1473, %rd1471;
	add.s64 	%rd1475, %rd1474, %rd1468;
	add.s64 	%rd1476, %rd1475, %rd1391;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10575,%dummy}, %rd1463;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10576}, %rd1463;
	}
	shf.r.wrap.b32 	%r10577, %r10576, %r10575, 28;
	shf.r.wrap.b32 	%r10578, %r10575, %r10576, 28;
	mov.b64 	%rd1477, {%r10578, %r10577};
	shf.l.wrap.b32 	%r10579, %r10575, %r10576, 30;
	shf.l.wrap.b32 	%r10580, %r10576, %r10575, 30;
	mov.b64 	%rd1478, {%r10580, %r10579};
	xor.b64  	%rd1479, %rd1478, %rd1477;
	shf.l.wrap.b32 	%r10581, %r10575, %r10576, 25;
	shf.l.wrap.b32 	%r10582, %r10576, %r10575, 25;
	mov.b64 	%rd1480, {%r10582, %r10581};
	xor.b64  	%rd1481, %rd1479, %rd1480;
	xor.b64  	%rd1482, %rd1463, %rd1415;
	xor.b64  	%rd1483, %rd1463, %rd1439;
	and.b64  	%rd1484, %rd1483, %rd1482;
	xor.b64  	%rd1485, %rd1484, %rd1463;
	add.s64 	%rd1486, %rd1475, %rd1485;
	add.s64 	%rd1487, %rd1486, %rd1481;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10583,%dummy}, %rd1476;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10584}, %rd1476;
	}
	shf.r.wrap.b32 	%r10585, %r10584, %r10583, 14;
	shf.r.wrap.b32 	%r10586, %r10583, %r10584, 14;
	mov.b64 	%rd1488, {%r10586, %r10585};
	shf.r.wrap.b32 	%r10587, %r10584, %r10583, 18;
	shf.r.wrap.b32 	%r10588, %r10583, %r10584, 18;
	mov.b64 	%rd1489, {%r10588, %r10587};
	xor.b64  	%rd1490, %rd1489, %rd1488;
	shf.l.wrap.b32 	%r10589, %r10583, %r10584, 23;
	shf.l.wrap.b32 	%r10590, %r10584, %r10583, 23;
	mov.b64 	%rd1491, {%r10590, %r10589};
	xor.b64  	%rd1492, %rd1490, %rd1491;
	xor.b64  	%rd1493, %rd1452, %rd1428;
	and.b64  	%rd1494, %rd1476, %rd1493;
	xor.b64  	%rd1495, %rd1494, %rd1428;
	add.s64 	%rd1496, %rd1404, %rd1204;
	add.s64 	%rd1497, %rd1496, %rd2820;
	add.s64 	%rd1498, %rd1497, %rd1495;
	add.s64 	%rd1499, %rd1498, %rd1492;
	add.s64 	%rd1500, %rd1499, %rd1415;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10591,%dummy}, %rd1487;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10592}, %rd1487;
	}
	shf.r.wrap.b32 	%r10593, %r10592, %r10591, 28;
	shf.r.wrap.b32 	%r10594, %r10591, %r10592, 28;
	mov.b64 	%rd1501, {%r10594, %r10593};
	shf.l.wrap.b32 	%r10595, %r10591, %r10592, 30;
	shf.l.wrap.b32 	%r10596, %r10592, %r10591, 30;
	mov.b64 	%rd1502, {%r10596, %r10595};
	xor.b64  	%rd1503, %rd1502, %rd1501;
	shf.l.wrap.b32 	%r10597, %r10591, %r10592, 25;
	shf.l.wrap.b32 	%r10598, %r10592, %r10591, 25;
	mov.b64 	%rd1504, {%r10598, %r10597};
	xor.b64  	%rd1505, %rd1503, %rd1504;
	xor.b64  	%rd1506, %rd1487, %rd1439;
	xor.b64  	%rd1507, %rd1487, %rd1463;
	and.b64  	%rd1508, %rd1507, %rd1506;
	xor.b64  	%rd1509, %rd1508, %rd1487;
	add.s64 	%rd1510, %rd1499, %rd1509;
	add.s64 	%rd1511, %rd1510, %rd1505;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10599,%dummy}, %rd1500;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10600}, %rd1500;
	}
	shf.r.wrap.b32 	%r10601, %r10600, %r10599, 14;
	shf.r.wrap.b32 	%r10602, %r10599, %r10600, 14;
	mov.b64 	%rd1512, {%r10602, %r10601};
	shf.r.wrap.b32 	%r10603, %r10600, %r10599, 18;
	shf.r.wrap.b32 	%r10604, %r10599, %r10600, 18;
	mov.b64 	%rd1513, {%r10604, %r10603};
	xor.b64  	%rd1514, %rd1513, %rd1512;
	shf.l.wrap.b32 	%r10605, %r10599, %r10600, 23;
	shf.l.wrap.b32 	%r10606, %r10600, %r10599, 23;
	mov.b64 	%rd1515, {%r10606, %r10605};
	xor.b64  	%rd1516, %rd1514, %rd1515;
	xor.b64  	%rd1517, %rd1476, %rd1452;
	and.b64  	%rd1518, %rd1500, %rd1517;
	xor.b64  	%rd1519, %rd1518, %rd1452;
	add.s64 	%rd1520, %rd1428, %rd1217;
	add.s64 	%rd1521, %rd1520, %rd2821;
	add.s64 	%rd1522, %rd1521, %rd1519;
	add.s64 	%rd1523, %rd1522, %rd1516;
	add.s64 	%rd1524, %rd1523, %rd1439;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10607,%dummy}, %rd1511;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10608}, %rd1511;
	}
	shf.r.wrap.b32 	%r10609, %r10608, %r10607, 28;
	shf.r.wrap.b32 	%r10610, %r10607, %r10608, 28;
	mov.b64 	%rd1525, {%r10610, %r10609};
	shf.l.wrap.b32 	%r10611, %r10607, %r10608, 30;
	shf.l.wrap.b32 	%r10612, %r10608, %r10607, 30;
	mov.b64 	%rd1526, {%r10612, %r10611};
	xor.b64  	%rd1527, %rd1526, %rd1525;
	shf.l.wrap.b32 	%r10613, %r10607, %r10608, 25;
	shf.l.wrap.b32 	%r10614, %r10608, %r10607, 25;
	mov.b64 	%rd1528, {%r10614, %r10613};
	xor.b64  	%rd1529, %rd1527, %rd1528;
	xor.b64  	%rd1530, %rd1511, %rd1463;
	xor.b64  	%rd1531, %rd1511, %rd1487;
	and.b64  	%rd1532, %rd1531, %rd1530;
	xor.b64  	%rd1533, %rd1532, %rd1511;
	add.s64 	%rd1534, %rd1523, %rd1533;
	add.s64 	%rd1535, %rd1534, %rd1529;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10615,%dummy}, %rd1524;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10616}, %rd1524;
	}
	shf.r.wrap.b32 	%r10617, %r10616, %r10615, 14;
	shf.r.wrap.b32 	%r10618, %r10615, %r10616, 14;
	mov.b64 	%rd1536, {%r10618, %r10617};
	shf.r.wrap.b32 	%r10619, %r10616, %r10615, 18;
	shf.r.wrap.b32 	%r10620, %r10615, %r10616, 18;
	mov.b64 	%rd1537, {%r10620, %r10619};
	xor.b64  	%rd1538, %rd1537, %rd1536;
	shf.l.wrap.b32 	%r10621, %r10615, %r10616, 23;
	shf.l.wrap.b32 	%r10622, %r10616, %r10615, 23;
	mov.b64 	%rd1539, {%r10622, %r10621};
	xor.b64  	%rd1540, %rd1538, %rd1539;
	xor.b64  	%rd1541, %rd1500, %rd1476;
	and.b64  	%rd1542, %rd1524, %rd1541;
	xor.b64  	%rd1543, %rd1542, %rd1476;
	add.s64 	%rd1544, %rd1452, %rd1230;
	add.s64 	%rd1545, %rd1544, %rd2822;
	add.s64 	%rd1546, %rd1545, %rd1543;
	add.s64 	%rd1547, %rd1546, %rd1540;
	add.s64 	%rd1548, %rd1547, %rd1463;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10623,%dummy}, %rd1535;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10624}, %rd1535;
	}
	shf.r.wrap.b32 	%r10625, %r10624, %r10623, 28;
	shf.r.wrap.b32 	%r10626, %r10623, %r10624, 28;
	mov.b64 	%rd1549, {%r10626, %r10625};
	shf.l.wrap.b32 	%r10627, %r10623, %r10624, 30;
	shf.l.wrap.b32 	%r10628, %r10624, %r10623, 30;
	mov.b64 	%rd1550, {%r10628, %r10627};
	xor.b64  	%rd1551, %rd1550, %rd1549;
	shf.l.wrap.b32 	%r10629, %r10623, %r10624, 25;
	shf.l.wrap.b32 	%r10630, %r10624, %r10623, 25;
	mov.b64 	%rd1552, {%r10630, %r10629};
	xor.b64  	%rd1553, %rd1551, %rd1552;
	xor.b64  	%rd1554, %rd1535, %rd1487;
	xor.b64  	%rd1555, %rd1535, %rd1511;
	and.b64  	%rd1556, %rd1555, %rd1554;
	xor.b64  	%rd1557, %rd1556, %rd1535;
	add.s64 	%rd1558, %rd1547, %rd1557;
	add.s64 	%rd1559, %rd1558, %rd1553;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10631,%dummy}, %rd1548;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10632}, %rd1548;
	}
	shf.r.wrap.b32 	%r10633, %r10632, %r10631, 14;
	shf.r.wrap.b32 	%r10634, %r10631, %r10632, 14;
	mov.b64 	%rd1560, {%r10634, %r10633};
	shf.r.wrap.b32 	%r10635, %r10632, %r10631, 18;
	shf.r.wrap.b32 	%r10636, %r10631, %r10632, 18;
	mov.b64 	%rd1561, {%r10636, %r10635};
	xor.b64  	%rd1562, %rd1561, %rd1560;
	shf.l.wrap.b32 	%r10637, %r10631, %r10632, 23;
	shf.l.wrap.b32 	%r10638, %r10632, %r10631, 23;
	mov.b64 	%rd1563, {%r10638, %r10637};
	xor.b64  	%rd1564, %rd1562, %rd1563;
	xor.b64  	%rd1565, %rd1524, %rd1500;
	and.b64  	%rd1566, %rd1548, %rd1565;
	xor.b64  	%rd1567, %rd1566, %rd1500;
	add.s64 	%rd1568, %rd1476, %rd1243;
	add.s64 	%rd1569, %rd1568, %rd2823;
	add.s64 	%rd1570, %rd1569, %rd1567;
	add.s64 	%rd1571, %rd1570, %rd1564;
	add.s64 	%rd1572, %rd1571, %rd1487;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10639,%dummy}, %rd1559;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10640}, %rd1559;
	}
	shf.r.wrap.b32 	%r10641, %r10640, %r10639, 28;
	shf.r.wrap.b32 	%r10642, %r10639, %r10640, 28;
	mov.b64 	%rd1573, {%r10642, %r10641};
	shf.l.wrap.b32 	%r10643, %r10639, %r10640, 30;
	shf.l.wrap.b32 	%r10644, %r10640, %r10639, 30;
	mov.b64 	%rd1574, {%r10644, %r10643};
	xor.b64  	%rd1575, %rd1574, %rd1573;
	shf.l.wrap.b32 	%r10645, %r10639, %r10640, 25;
	shf.l.wrap.b32 	%r10646, %r10640, %r10639, 25;
	mov.b64 	%rd1576, {%r10646, %r10645};
	xor.b64  	%rd1577, %rd1575, %rd1576;
	xor.b64  	%rd1578, %rd1559, %rd1511;
	xor.b64  	%rd1579, %rd1559, %rd1535;
	and.b64  	%rd1580, %rd1579, %rd1578;
	xor.b64  	%rd1581, %rd1580, %rd1559;
	add.s64 	%rd1582, %rd1571, %rd1581;
	add.s64 	%rd1583, %rd1582, %rd1577;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10647,%dummy}, %rd1572;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10648}, %rd1572;
	}
	shf.r.wrap.b32 	%r10649, %r10648, %r10647, 14;
	shf.r.wrap.b32 	%r10650, %r10647, %r10648, 14;
	mov.b64 	%rd1584, {%r10650, %r10649};
	shf.r.wrap.b32 	%r10651, %r10648, %r10647, 18;
	shf.r.wrap.b32 	%r10652, %r10647, %r10648, 18;
	mov.b64 	%rd1585, {%r10652, %r10651};
	xor.b64  	%rd1586, %rd1585, %rd1584;
	shf.l.wrap.b32 	%r10653, %r10647, %r10648, 23;
	shf.l.wrap.b32 	%r10654, %r10648, %r10647, 23;
	mov.b64 	%rd1587, {%r10654, %r10653};
	xor.b64  	%rd1588, %rd1586, %rd1587;
	xor.b64  	%rd1589, %rd1548, %rd1524;
	and.b64  	%rd1590, %rd1572, %rd1589;
	xor.b64  	%rd1591, %rd1590, %rd1524;
	add.s64 	%rd1592, %rd1500, %rd1256;
	add.s64 	%rd1593, %rd1592, %rd2824;
	add.s64 	%rd1594, %rd1593, %rd1591;
	add.s64 	%rd1595, %rd1594, %rd1588;
	add.s64 	%rd1596, %rd1595, %rd1511;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10655,%dummy}, %rd1583;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10656}, %rd1583;
	}
	shf.r.wrap.b32 	%r10657, %r10656, %r10655, 28;
	shf.r.wrap.b32 	%r10658, %r10655, %r10656, 28;
	mov.b64 	%rd1597, {%r10658, %r10657};
	shf.l.wrap.b32 	%r10659, %r10655, %r10656, 30;
	shf.l.wrap.b32 	%r10660, %r10656, %r10655, 30;
	mov.b64 	%rd1598, {%r10660, %r10659};
	xor.b64  	%rd1599, %rd1598, %rd1597;
	shf.l.wrap.b32 	%r10661, %r10655, %r10656, 25;
	shf.l.wrap.b32 	%r10662, %r10656, %r10655, 25;
	mov.b64 	%rd1600, {%r10662, %r10661};
	xor.b64  	%rd1601, %rd1599, %rd1600;
	xor.b64  	%rd1602, %rd1583, %rd1535;
	xor.b64  	%rd1603, %rd1583, %rd1559;
	and.b64  	%rd1604, %rd1603, %rd1602;
	xor.b64  	%rd1605, %rd1604, %rd1583;
	add.s64 	%rd1606, %rd1595, %rd1605;
	add.s64 	%rd1607, %rd1606, %rd1601;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10663,%dummy}, %rd1596;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10664}, %rd1596;
	}
	shf.r.wrap.b32 	%r10665, %r10664, %r10663, 14;
	shf.r.wrap.b32 	%r10666, %r10663, %r10664, 14;
	mov.b64 	%rd1608, {%r10666, %r10665};
	shf.r.wrap.b32 	%r10667, %r10664, %r10663, 18;
	shf.r.wrap.b32 	%r10668, %r10663, %r10664, 18;
	mov.b64 	%rd1609, {%r10668, %r10667};
	xor.b64  	%rd1610, %rd1609, %rd1608;
	shf.l.wrap.b32 	%r10669, %r10663, %r10664, 23;
	shf.l.wrap.b32 	%r10670, %r10664, %r10663, 23;
	mov.b64 	%rd1611, {%r10670, %r10669};
	xor.b64  	%rd1612, %rd1610, %rd1611;
	xor.b64  	%rd1613, %rd1572, %rd1548;
	and.b64  	%rd1614, %rd1596, %rd1613;
	xor.b64  	%rd1615, %rd1614, %rd1548;
	add.s64 	%rd1616, %rd1524, %rd1269;
	add.s64 	%rd1617, %rd1616, %rd2825;
	add.s64 	%rd1618, %rd1617, %rd1615;
	add.s64 	%rd1619, %rd1618, %rd1612;
	add.s64 	%rd1620, %rd1619, %rd1535;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10671,%dummy}, %rd1607;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10672}, %rd1607;
	}
	shf.r.wrap.b32 	%r10673, %r10672, %r10671, 28;
	shf.r.wrap.b32 	%r10674, %r10671, %r10672, 28;
	mov.b64 	%rd1621, {%r10674, %r10673};
	shf.l.wrap.b32 	%r10675, %r10671, %r10672, 30;
	shf.l.wrap.b32 	%r10676, %r10672, %r10671, 30;
	mov.b64 	%rd1622, {%r10676, %r10675};
	xor.b64  	%rd1623, %rd1622, %rd1621;
	shf.l.wrap.b32 	%r10677, %r10671, %r10672, 25;
	shf.l.wrap.b32 	%r10678, %r10672, %r10671, 25;
	mov.b64 	%rd1624, {%r10678, %r10677};
	xor.b64  	%rd1625, %rd1623, %rd1624;
	xor.b64  	%rd1626, %rd1607, %rd1559;
	xor.b64  	%rd1627, %rd1607, %rd1583;
	and.b64  	%rd1628, %rd1627, %rd1626;
	xor.b64  	%rd1629, %rd1628, %rd1607;
	add.s64 	%rd1630, %rd1619, %rd1629;
	add.s64 	%rd1631, %rd1630, %rd1625;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10679,%dummy}, %rd1620;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10680}, %rd1620;
	}
	shf.r.wrap.b32 	%r10681, %r10680, %r10679, 14;
	shf.r.wrap.b32 	%r10682, %r10679, %r10680, 14;
	mov.b64 	%rd1632, {%r10682, %r10681};
	shf.r.wrap.b32 	%r10683, %r10680, %r10679, 18;
	shf.r.wrap.b32 	%r10684, %r10679, %r10680, 18;
	mov.b64 	%rd1633, {%r10684, %r10683};
	xor.b64  	%rd1634, %rd1633, %rd1632;
	shf.l.wrap.b32 	%r10685, %r10679, %r10680, 23;
	shf.l.wrap.b32 	%r10686, %r10680, %r10679, 23;
	mov.b64 	%rd1635, {%r10686, %r10685};
	xor.b64  	%rd1636, %rd1634, %rd1635;
	xor.b64  	%rd1637, %rd1596, %rd1572;
	and.b64  	%rd1638, %rd1620, %rd1637;
	xor.b64  	%rd1639, %rd1638, %rd1572;
	add.s64 	%rd1640, %rd1548, %rd1282;
	add.s64 	%rd1641, %rd1640, %rd2826;
	add.s64 	%rd1642, %rd1641, %rd1639;
	add.s64 	%rd1643, %rd1642, %rd1636;
	add.s64 	%rd1644, %rd1643, %rd1559;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10687,%dummy}, %rd1631;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10688}, %rd1631;
	}
	shf.r.wrap.b32 	%r10689, %r10688, %r10687, 28;
	shf.r.wrap.b32 	%r10690, %r10687, %r10688, 28;
	mov.b64 	%rd1645, {%r10690, %r10689};
	shf.l.wrap.b32 	%r10691, %r10687, %r10688, 30;
	shf.l.wrap.b32 	%r10692, %r10688, %r10687, 30;
	mov.b64 	%rd1646, {%r10692, %r10691};
	xor.b64  	%rd1647, %rd1646, %rd1645;
	shf.l.wrap.b32 	%r10693, %r10687, %r10688, 25;
	shf.l.wrap.b32 	%r10694, %r10688, %r10687, 25;
	mov.b64 	%rd1648, {%r10694, %r10693};
	xor.b64  	%rd1649, %rd1647, %rd1648;
	xor.b64  	%rd1650, %rd1631, %rd1583;
	xor.b64  	%rd1651, %rd1631, %rd1607;
	and.b64  	%rd1652, %rd1651, %rd1650;
	xor.b64  	%rd1653, %rd1652, %rd1631;
	add.s64 	%rd1654, %rd1643, %rd1653;
	add.s64 	%rd1655, %rd1654, %rd1649;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10695,%dummy}, %rd1644;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10696}, %rd1644;
	}
	shf.r.wrap.b32 	%r10697, %r10696, %r10695, 14;
	shf.r.wrap.b32 	%r10698, %r10695, %r10696, 14;
	mov.b64 	%rd1656, {%r10698, %r10697};
	shf.r.wrap.b32 	%r10699, %r10696, %r10695, 18;
	shf.r.wrap.b32 	%r10700, %r10695, %r10696, 18;
	mov.b64 	%rd1657, {%r10700, %r10699};
	xor.b64  	%rd1658, %rd1657, %rd1656;
	shf.l.wrap.b32 	%r10701, %r10695, %r10696, 23;
	shf.l.wrap.b32 	%r10702, %r10696, %r10695, 23;
	mov.b64 	%rd1659, {%r10702, %r10701};
	xor.b64  	%rd1660, %rd1658, %rd1659;
	xor.b64  	%rd1661, %rd1620, %rd1596;
	and.b64  	%rd1662, %rd1644, %rd1661;
	xor.b64  	%rd1663, %rd1662, %rd1596;
	add.s64 	%rd1664, %rd1572, %rd1295;
	add.s64 	%rd1665, %rd1664, %rd2827;
	add.s64 	%rd1666, %rd1665, %rd1663;
	add.s64 	%rd1667, %rd1666, %rd1660;
	add.s64 	%rd1668, %rd1667, %rd1583;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10703,%dummy}, %rd1655;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10704}, %rd1655;
	}
	shf.r.wrap.b32 	%r10705, %r10704, %r10703, 28;
	shf.r.wrap.b32 	%r10706, %r10703, %r10704, 28;
	mov.b64 	%rd1669, {%r10706, %r10705};
	shf.l.wrap.b32 	%r10707, %r10703, %r10704, 30;
	shf.l.wrap.b32 	%r10708, %r10704, %r10703, 30;
	mov.b64 	%rd1670, {%r10708, %r10707};
	xor.b64  	%rd1671, %rd1670, %rd1669;
	shf.l.wrap.b32 	%r10709, %r10703, %r10704, 25;
	shf.l.wrap.b32 	%r10710, %r10704, %r10703, 25;
	mov.b64 	%rd1672, {%r10710, %r10709};
	xor.b64  	%rd1673, %rd1671, %rd1672;
	xor.b64  	%rd1674, %rd1655, %rd1607;
	xor.b64  	%rd1675, %rd1655, %rd1631;
	and.b64  	%rd1676, %rd1675, %rd1674;
	xor.b64  	%rd1677, %rd1676, %rd1655;
	add.s64 	%rd1678, %rd1667, %rd1677;
	add.s64 	%rd1679, %rd1678, %rd1673;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10711,%dummy}, %rd1282;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10712}, %rd1282;
	}
	shf.r.wrap.b32 	%r10713, %r10712, %r10711, 19;
	shf.r.wrap.b32 	%r10714, %r10711, %r10712, 19;
	mov.b64 	%rd1680, {%r10714, %r10713};
	shf.l.wrap.b32 	%r10715, %r10711, %r10712, 3;
	shf.l.wrap.b32 	%r10716, %r10712, %r10711, 3;
	mov.b64 	%rd1681, {%r10716, %r10715};
	shr.u64 	%rd1682, %rd1282, 6;
	xor.b64  	%rd1683, %rd1680, %rd1682;
	xor.b64  	%rd1684, %rd1683, %rd1681;
	shf.r.wrap.b32 	%r10717, %r10326, %r10325, 1;
	shf.r.wrap.b32 	%r10718, %r10325, %r10326, 1;
	mov.b64 	%rd1685, {%r10718, %r10717};
	shf.r.wrap.b32 	%r10719, %r10326, %r10325, 8;
	shf.r.wrap.b32 	%r10720, %r10325, %r10326, 8;
	mov.b64 	%rd1686, {%r10720, %r10719};
	shr.u64 	%rd1687, %rd1113, 7;
	xor.b64  	%rd1688, %rd1685, %rd1687;
	xor.b64  	%rd1689, %rd1688, %rd1686;
	add.s64 	%rd1690, %rd1100, %rd1217;
	add.s64 	%rd1691, %rd1690, %rd1684;
	add.s64 	%rd1692, %rd1691, %rd1689;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10721,%dummy}, %rd1295;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10722}, %rd1295;
	}
	shf.r.wrap.b32 	%r10723, %r10722, %r10721, 19;
	shf.r.wrap.b32 	%r10724, %r10721, %r10722, 19;
	mov.b64 	%rd1693, {%r10724, %r10723};
	shf.l.wrap.b32 	%r10725, %r10721, %r10722, 3;
	shf.l.wrap.b32 	%r10726, %r10722, %r10721, 3;
	mov.b64 	%rd1694, {%r10726, %r10725};
	shr.u64 	%rd1695, %rd1295, 6;
	xor.b64  	%rd1696, %rd1693, %rd1695;
	xor.b64  	%rd1697, %rd1696, %rd1694;
	shf.r.wrap.b32 	%r10727, %r10336, %r10335, 1;
	shf.r.wrap.b32 	%r10728, %r10335, %r10336, 1;
	mov.b64 	%rd1698, {%r10728, %r10727};
	shf.r.wrap.b32 	%r10729, %r10336, %r10335, 8;
	shf.r.wrap.b32 	%r10730, %r10335, %r10336, 8;
	mov.b64 	%rd1699, {%r10730, %r10729};
	shr.u64 	%rd1700, %rd1126, 7;
	xor.b64  	%rd1701, %rd1698, %rd1700;
	xor.b64  	%rd1702, %rd1701, %rd1699;
	add.s64 	%rd1703, %rd1113, %rd1230;
	add.s64 	%rd1704, %rd1703, %rd1697;
	add.s64 	%rd1705, %rd1704, %rd1702;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10731,%dummy}, %rd1692;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10732}, %rd1692;
	}
	shf.r.wrap.b32 	%r10733, %r10732, %r10731, 19;
	shf.r.wrap.b32 	%r10734, %r10731, %r10732, 19;
	mov.b64 	%rd1706, {%r10734, %r10733};
	shf.l.wrap.b32 	%r10735, %r10731, %r10732, 3;
	shf.l.wrap.b32 	%r10736, %r10732, %r10731, 3;
	mov.b64 	%rd1707, {%r10736, %r10735};
	shr.u64 	%rd1708, %rd1692, 6;
	xor.b64  	%rd1709, %rd1706, %rd1708;
	xor.b64  	%rd1710, %rd1709, %rd1707;
	shf.r.wrap.b32 	%r10737, %r10346, %r10345, 1;
	shf.r.wrap.b32 	%r10738, %r10345, %r10346, 1;
	mov.b64 	%rd1711, {%r10738, %r10737};
	shf.r.wrap.b32 	%r10739, %r10346, %r10345, 8;
	shf.r.wrap.b32 	%r10740, %r10345, %r10346, 8;
	mov.b64 	%rd1712, {%r10740, %r10739};
	shr.u64 	%rd1713, %rd1139, 7;
	xor.b64  	%rd1714, %rd1711, %rd1713;
	xor.b64  	%rd1715, %rd1714, %rd1712;
	add.s64 	%rd1716, %rd1126, %rd1243;
	add.s64 	%rd1717, %rd1716, %rd1710;
	add.s64 	%rd1718, %rd1717, %rd1715;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10741,%dummy}, %rd1705;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10742}, %rd1705;
	}
	shf.r.wrap.b32 	%r10743, %r10742, %r10741, 19;
	shf.r.wrap.b32 	%r10744, %r10741, %r10742, 19;
	mov.b64 	%rd1719, {%r10744, %r10743};
	shf.l.wrap.b32 	%r10745, %r10741, %r10742, 3;
	shf.l.wrap.b32 	%r10746, %r10742, %r10741, 3;
	mov.b64 	%rd1720, {%r10746, %r10745};
	shr.u64 	%rd1721, %rd1705, 6;
	xor.b64  	%rd1722, %rd1719, %rd1721;
	xor.b64  	%rd1723, %rd1722, %rd1720;
	shf.r.wrap.b32 	%r10747, %r10356, %r10355, 1;
	shf.r.wrap.b32 	%r10748, %r10355, %r10356, 1;
	mov.b64 	%rd1724, {%r10748, %r10747};
	shf.r.wrap.b32 	%r10749, %r10356, %r10355, 8;
	shf.r.wrap.b32 	%r10750, %r10355, %r10356, 8;
	mov.b64 	%rd1725, {%r10750, %r10749};
	shr.u64 	%rd1726, %rd1152, 7;
	xor.b64  	%rd1727, %rd1724, %rd1726;
	xor.b64  	%rd1728, %rd1727, %rd1725;
	add.s64 	%rd1729, %rd1139, %rd1256;
	add.s64 	%rd1730, %rd1729, %rd1723;
	add.s64 	%rd1731, %rd1730, %rd1728;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10751,%dummy}, %rd1718;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10752}, %rd1718;
	}
	shf.r.wrap.b32 	%r10753, %r10752, %r10751, 19;
	shf.r.wrap.b32 	%r10754, %r10751, %r10752, 19;
	mov.b64 	%rd1732, {%r10754, %r10753};
	shf.l.wrap.b32 	%r10755, %r10751, %r10752, 3;
	shf.l.wrap.b32 	%r10756, %r10752, %r10751, 3;
	mov.b64 	%rd1733, {%r10756, %r10755};
	shr.u64 	%rd1734, %rd1718, 6;
	xor.b64  	%rd1735, %rd1732, %rd1734;
	xor.b64  	%rd1736, %rd1735, %rd1733;
	shf.r.wrap.b32 	%r10757, %r10366, %r10365, 1;
	shf.r.wrap.b32 	%r10758, %r10365, %r10366, 1;
	mov.b64 	%rd1737, {%r10758, %r10757};
	shf.r.wrap.b32 	%r10759, %r10366, %r10365, 8;
	shf.r.wrap.b32 	%r10760, %r10365, %r10366, 8;
	mov.b64 	%rd1738, {%r10760, %r10759};
	shr.u64 	%rd1739, %rd1165, 7;
	xor.b64  	%rd1740, %rd1737, %rd1739;
	xor.b64  	%rd1741, %rd1740, %rd1738;
	add.s64 	%rd1742, %rd1152, %rd1269;
	add.s64 	%rd1743, %rd1742, %rd1736;
	add.s64 	%rd1744, %rd1743, %rd1741;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10761,%dummy}, %rd1731;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10762}, %rd1731;
	}
	shf.r.wrap.b32 	%r10763, %r10762, %r10761, 19;
	shf.r.wrap.b32 	%r10764, %r10761, %r10762, 19;
	mov.b64 	%rd1745, {%r10764, %r10763};
	shf.l.wrap.b32 	%r10765, %r10761, %r10762, 3;
	shf.l.wrap.b32 	%r10766, %r10762, %r10761, 3;
	mov.b64 	%rd1746, {%r10766, %r10765};
	shr.u64 	%rd1747, %rd1731, 6;
	xor.b64  	%rd1748, %rd1745, %rd1747;
	xor.b64  	%rd1749, %rd1748, %rd1746;
	shf.r.wrap.b32 	%r10767, %r10376, %r10375, 1;
	shf.r.wrap.b32 	%r10768, %r10375, %r10376, 1;
	mov.b64 	%rd1750, {%r10768, %r10767};
	shf.r.wrap.b32 	%r10769, %r10376, %r10375, 8;
	shf.r.wrap.b32 	%r10770, %r10375, %r10376, 8;
	mov.b64 	%rd1751, {%r10770, %r10769};
	shr.u64 	%rd1752, %rd1178, 7;
	xor.b64  	%rd1753, %rd1750, %rd1752;
	xor.b64  	%rd1754, %rd1753, %rd1751;
	add.s64 	%rd1755, %rd1165, %rd1282;
	add.s64 	%rd1756, %rd1755, %rd1749;
	add.s64 	%rd1757, %rd1756, %rd1754;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10771,%dummy}, %rd1744;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10772}, %rd1744;
	}
	shf.r.wrap.b32 	%r10773, %r10772, %r10771, 19;
	shf.r.wrap.b32 	%r10774, %r10771, %r10772, 19;
	mov.b64 	%rd1758, {%r10774, %r10773};
	shf.l.wrap.b32 	%r10775, %r10771, %r10772, 3;
	shf.l.wrap.b32 	%r10776, %r10772, %r10771, 3;
	mov.b64 	%rd1759, {%r10776, %r10775};
	shr.u64 	%rd1760, %rd1744, 6;
	xor.b64  	%rd1761, %rd1758, %rd1760;
	xor.b64  	%rd1762, %rd1761, %rd1759;
	shf.r.wrap.b32 	%r10777, %r10386, %r10385, 1;
	shf.r.wrap.b32 	%r10778, %r10385, %r10386, 1;
	mov.b64 	%rd1763, {%r10778, %r10777};
	shf.r.wrap.b32 	%r10779, %r10386, %r10385, 8;
	shf.r.wrap.b32 	%r10780, %r10385, %r10386, 8;
	mov.b64 	%rd1764, {%r10780, %r10779};
	shr.u64 	%rd1765, %rd1191, 7;
	xor.b64  	%rd1766, %rd1763, %rd1765;
	xor.b64  	%rd1767, %rd1766, %rd1764;
	add.s64 	%rd1768, %rd1178, %rd1295;
	add.s64 	%rd1769, %rd1768, %rd1762;
	add.s64 	%rd1770, %rd1769, %rd1767;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10781,%dummy}, %rd1757;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10782}, %rd1757;
	}
	shf.r.wrap.b32 	%r10783, %r10782, %r10781, 19;
	shf.r.wrap.b32 	%r10784, %r10781, %r10782, 19;
	mov.b64 	%rd1771, {%r10784, %r10783};
	shf.l.wrap.b32 	%r10785, %r10781, %r10782, 3;
	shf.l.wrap.b32 	%r10786, %r10782, %r10781, 3;
	mov.b64 	%rd1772, {%r10786, %r10785};
	shr.u64 	%rd1773, %rd1757, 6;
	xor.b64  	%rd1774, %rd1771, %rd1773;
	xor.b64  	%rd1775, %rd1774, %rd1772;
	shf.r.wrap.b32 	%r10787, %r10396, %r10395, 1;
	shf.r.wrap.b32 	%r10788, %r10395, %r10396, 1;
	mov.b64 	%rd1776, {%r10788, %r10787};
	shf.r.wrap.b32 	%r10789, %r10396, %r10395, 8;
	shf.r.wrap.b32 	%r10790, %r10395, %r10396, 8;
	mov.b64 	%rd1777, {%r10790, %r10789};
	shr.u64 	%rd1778, %rd1204, 7;
	xor.b64  	%rd1779, %rd1776, %rd1778;
	xor.b64  	%rd1780, %rd1779, %rd1777;
	add.s64 	%rd1781, %rd1692, %rd1191;
	add.s64 	%rd1782, %rd1781, %rd1775;
	add.s64 	%rd1783, %rd1782, %rd1780;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10791,%dummy}, %rd1770;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10792}, %rd1770;
	}
	shf.r.wrap.b32 	%r10793, %r10792, %r10791, 19;
	shf.r.wrap.b32 	%r10794, %r10791, %r10792, 19;
	mov.b64 	%rd1784, {%r10794, %r10793};
	shf.l.wrap.b32 	%r10795, %r10791, %r10792, 3;
	shf.l.wrap.b32 	%r10796, %r10792, %r10791, 3;
	mov.b64 	%rd1785, {%r10796, %r10795};
	shr.u64 	%rd1786, %rd1770, 6;
	xor.b64  	%rd1787, %rd1784, %rd1786;
	xor.b64  	%rd1788, %rd1787, %rd1785;
	shf.r.wrap.b32 	%r10797, %r10406, %r10405, 1;
	shf.r.wrap.b32 	%r10798, %r10405, %r10406, 1;
	mov.b64 	%rd1789, {%r10798, %r10797};
	shf.r.wrap.b32 	%r10799, %r10406, %r10405, 8;
	shf.r.wrap.b32 	%r10800, %r10405, %r10406, 8;
	mov.b64 	%rd1790, {%r10800, %r10799};
	shr.u64 	%rd1791, %rd1217, 7;
	xor.b64  	%rd1792, %rd1789, %rd1791;
	xor.b64  	%rd1793, %rd1792, %rd1790;
	add.s64 	%rd1794, %rd1705, %rd1204;
	add.s64 	%rd1795, %rd1794, %rd1788;
	add.s64 	%rd1796, %rd1795, %rd1793;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10801,%dummy}, %rd1783;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10802}, %rd1783;
	}
	shf.r.wrap.b32 	%r10803, %r10802, %r10801, 19;
	shf.r.wrap.b32 	%r10804, %r10801, %r10802, 19;
	mov.b64 	%rd1797, {%r10804, %r10803};
	shf.l.wrap.b32 	%r10805, %r10801, %r10802, 3;
	shf.l.wrap.b32 	%r10806, %r10802, %r10801, 3;
	mov.b64 	%rd1798, {%r10806, %r10805};
	shr.u64 	%rd1799, %rd1783, 6;
	xor.b64  	%rd1800, %rd1797, %rd1799;
	xor.b64  	%rd1801, %rd1800, %rd1798;
	shf.r.wrap.b32 	%r10807, %r10416, %r10415, 1;
	shf.r.wrap.b32 	%r10808, %r10415, %r10416, 1;
	mov.b64 	%rd1802, {%r10808, %r10807};
	shf.r.wrap.b32 	%r10809, %r10416, %r10415, 8;
	shf.r.wrap.b32 	%r10810, %r10415, %r10416, 8;
	mov.b64 	%rd1803, {%r10810, %r10809};
	shr.u64 	%rd1804, %rd1230, 7;
	xor.b64  	%rd1805, %rd1802, %rd1804;
	xor.b64  	%rd1806, %rd1805, %rd1803;
	add.s64 	%rd1807, %rd1718, %rd1217;
	add.s64 	%rd1808, %rd1807, %rd1801;
	add.s64 	%rd1809, %rd1808, %rd1806;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10811,%dummy}, %rd1796;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10812}, %rd1796;
	}
	shf.r.wrap.b32 	%r10813, %r10812, %r10811, 19;
	shf.r.wrap.b32 	%r10814, %r10811, %r10812, 19;
	mov.b64 	%rd1810, {%r10814, %r10813};
	shf.l.wrap.b32 	%r10815, %r10811, %r10812, 3;
	shf.l.wrap.b32 	%r10816, %r10812, %r10811, 3;
	mov.b64 	%rd1811, {%r10816, %r10815};
	shr.u64 	%rd1812, %rd1796, 6;
	xor.b64  	%rd1813, %rd1810, %rd1812;
	xor.b64  	%rd1814, %rd1813, %rd1811;
	shf.r.wrap.b32 	%r10817, %r10426, %r10425, 1;
	shf.r.wrap.b32 	%r10818, %r10425, %r10426, 1;
	mov.b64 	%rd1815, {%r10818, %r10817};
	shf.r.wrap.b32 	%r10819, %r10426, %r10425, 8;
	shf.r.wrap.b32 	%r10820, %r10425, %r10426, 8;
	mov.b64 	%rd1816, {%r10820, %r10819};
	shr.u64 	%rd1817, %rd1243, 7;
	xor.b64  	%rd1818, %rd1815, %rd1817;
	xor.b64  	%rd1819, %rd1818, %rd1816;
	add.s64 	%rd1820, %rd1731, %rd1230;
	add.s64 	%rd1821, %rd1820, %rd1814;
	add.s64 	%rd1822, %rd1821, %rd1819;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10821,%dummy}, %rd1809;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10822}, %rd1809;
	}
	shf.r.wrap.b32 	%r10823, %r10822, %r10821, 19;
	shf.r.wrap.b32 	%r10824, %r10821, %r10822, 19;
	mov.b64 	%rd1823, {%r10824, %r10823};
	shf.l.wrap.b32 	%r10825, %r10821, %r10822, 3;
	shf.l.wrap.b32 	%r10826, %r10822, %r10821, 3;
	mov.b64 	%rd1824, {%r10826, %r10825};
	shr.u64 	%rd1825, %rd1809, 6;
	xor.b64  	%rd1826, %rd1823, %rd1825;
	xor.b64  	%rd1827, %rd1826, %rd1824;
	shf.r.wrap.b32 	%r10827, %r10436, %r10435, 1;
	shf.r.wrap.b32 	%r10828, %r10435, %r10436, 1;
	mov.b64 	%rd1828, {%r10828, %r10827};
	shf.r.wrap.b32 	%r10829, %r10436, %r10435, 8;
	shf.r.wrap.b32 	%r10830, %r10435, %r10436, 8;
	mov.b64 	%rd1829, {%r10830, %r10829};
	shr.u64 	%rd1830, %rd1256, 7;
	xor.b64  	%rd1831, %rd1828, %rd1830;
	xor.b64  	%rd1832, %rd1831, %rd1829;
	add.s64 	%rd1833, %rd1744, %rd1243;
	add.s64 	%rd1834, %rd1833, %rd1827;
	add.s64 	%rd1835, %rd1834, %rd1832;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10831,%dummy}, %rd1822;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10832}, %rd1822;
	}
	shf.r.wrap.b32 	%r10833, %r10832, %r10831, 19;
	shf.r.wrap.b32 	%r10834, %r10831, %r10832, 19;
	mov.b64 	%rd1836, {%r10834, %r10833};
	shf.l.wrap.b32 	%r10835, %r10831, %r10832, 3;
	shf.l.wrap.b32 	%r10836, %r10832, %r10831, 3;
	mov.b64 	%rd1837, {%r10836, %r10835};
	shr.u64 	%rd1838, %rd1822, 6;
	xor.b64  	%rd1839, %rd1836, %rd1838;
	xor.b64  	%rd1840, %rd1839, %rd1837;
	shf.r.wrap.b32 	%r10837, %r10446, %r10445, 1;
	shf.r.wrap.b32 	%r10838, %r10445, %r10446, 1;
	mov.b64 	%rd1841, {%r10838, %r10837};
	shf.r.wrap.b32 	%r10839, %r10446, %r10445, 8;
	shf.r.wrap.b32 	%r10840, %r10445, %r10446, 8;
	mov.b64 	%rd1842, {%r10840, %r10839};
	shr.u64 	%rd1843, %rd1269, 7;
	xor.b64  	%rd1844, %rd1841, %rd1843;
	xor.b64  	%rd1845, %rd1844, %rd1842;
	add.s64 	%rd1846, %rd1757, %rd1256;
	add.s64 	%rd1847, %rd1846, %rd1840;
	add.s64 	%rd1848, %rd1847, %rd1845;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10841,%dummy}, %rd1835;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10842}, %rd1835;
	}
	shf.r.wrap.b32 	%r10843, %r10842, %r10841, 19;
	shf.r.wrap.b32 	%r10844, %r10841, %r10842, 19;
	mov.b64 	%rd1849, {%r10844, %r10843};
	shf.l.wrap.b32 	%r10845, %r10841, %r10842, 3;
	shf.l.wrap.b32 	%r10846, %r10842, %r10841, 3;
	mov.b64 	%rd1850, {%r10846, %r10845};
	shr.u64 	%rd1851, %rd1835, 6;
	xor.b64  	%rd1852, %rd1849, %rd1851;
	xor.b64  	%rd1853, %rd1852, %rd1850;
	shf.r.wrap.b32 	%r10847, %r10712, %r10711, 1;
	shf.r.wrap.b32 	%r10848, %r10711, %r10712, 1;
	mov.b64 	%rd1854, {%r10848, %r10847};
	shf.r.wrap.b32 	%r10849, %r10712, %r10711, 8;
	shf.r.wrap.b32 	%r10850, %r10711, %r10712, 8;
	mov.b64 	%rd1855, {%r10850, %r10849};
	shr.u64 	%rd1856, %rd1282, 7;
	xor.b64  	%rd1857, %rd1854, %rd1856;
	xor.b64  	%rd1858, %rd1857, %rd1855;
	add.s64 	%rd1859, %rd1770, %rd1269;
	add.s64 	%rd1860, %rd1859, %rd1853;
	add.s64 	%rd1861, %rd1860, %rd1858;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10851,%dummy}, %rd1848;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10852}, %rd1848;
	}
	shf.r.wrap.b32 	%r10853, %r10852, %r10851, 19;
	shf.r.wrap.b32 	%r10854, %r10851, %r10852, 19;
	mov.b64 	%rd1862, {%r10854, %r10853};
	shf.l.wrap.b32 	%r10855, %r10851, %r10852, 3;
	shf.l.wrap.b32 	%r10856, %r10852, %r10851, 3;
	mov.b64 	%rd1863, {%r10856, %r10855};
	shr.u64 	%rd1864, %rd1848, 6;
	xor.b64  	%rd1865, %rd1862, %rd1864;
	xor.b64  	%rd1866, %rd1865, %rd1863;
	shf.r.wrap.b32 	%r10857, %r10722, %r10721, 1;
	shf.r.wrap.b32 	%r10858, %r10721, %r10722, 1;
	mov.b64 	%rd1867, {%r10858, %r10857};
	shf.r.wrap.b32 	%r10859, %r10722, %r10721, 8;
	shf.r.wrap.b32 	%r10860, %r10721, %r10722, 8;
	mov.b64 	%rd1868, {%r10860, %r10859};
	shr.u64 	%rd1869, %rd1295, 7;
	xor.b64  	%rd1870, %rd1867, %rd1869;
	xor.b64  	%rd1871, %rd1870, %rd1868;
	add.s64 	%rd1872, %rd1783, %rd1282;
	add.s64 	%rd1873, %rd1872, %rd1866;
	add.s64 	%rd1874, %rd1873, %rd1871;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10861,%dummy}, %rd1861;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10862}, %rd1861;
	}
	shf.r.wrap.b32 	%r10863, %r10862, %r10861, 19;
	shf.r.wrap.b32 	%r10864, %r10861, %r10862, 19;
	mov.b64 	%rd1875, {%r10864, %r10863};
	shf.l.wrap.b32 	%r10865, %r10861, %r10862, 3;
	shf.l.wrap.b32 	%r10866, %r10862, %r10861, 3;
	mov.b64 	%rd1876, {%r10866, %r10865};
	shr.u64 	%rd1877, %rd1861, 6;
	xor.b64  	%rd1878, %rd1875, %rd1877;
	xor.b64  	%rd1879, %rd1878, %rd1876;
	shf.r.wrap.b32 	%r10867, %r10732, %r10731, 1;
	shf.r.wrap.b32 	%r10868, %r10731, %r10732, 1;
	mov.b64 	%rd1880, {%r10868, %r10867};
	shf.r.wrap.b32 	%r10869, %r10732, %r10731, 8;
	shf.r.wrap.b32 	%r10870, %r10731, %r10732, 8;
	mov.b64 	%rd1881, {%r10870, %r10869};
	shr.u64 	%rd1882, %rd1692, 7;
	xor.b64  	%rd1883, %rd1880, %rd1882;
	xor.b64  	%rd1884, %rd1883, %rd1881;
	add.s64 	%rd1885, %rd1796, %rd1295;
	add.s64 	%rd1886, %rd1885, %rd1879;
	add.s64 	%rd1887, %rd1886, %rd1884;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10871,%dummy}, %rd1668;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10872}, %rd1668;
	}
	shf.r.wrap.b32 	%r10873, %r10872, %r10871, 14;
	shf.r.wrap.b32 	%r10874, %r10871, %r10872, 14;
	mov.b64 	%rd1888, {%r10874, %r10873};
	shf.r.wrap.b32 	%r10875, %r10872, %r10871, 18;
	shf.r.wrap.b32 	%r10876, %r10871, %r10872, 18;
	mov.b64 	%rd1889, {%r10876, %r10875};
	xor.b64  	%rd1890, %rd1889, %rd1888;
	shf.l.wrap.b32 	%r10877, %r10871, %r10872, 23;
	shf.l.wrap.b32 	%r10878, %r10872, %r10871, 23;
	mov.b64 	%rd1891, {%r10878, %r10877};
	xor.b64  	%rd1892, %rd1890, %rd1891;
	xor.b64  	%rd1893, %rd1620, %rd1644;
	and.b64  	%rd1894, %rd1893, %rd1668;
	xor.b64  	%rd1895, %rd1894, %rd1620;
	add.s64 	%rd1896, %rd1895, %rd1596;
	add.s64 	%rd1897, %rd1896, %rd1692;
	add.s64 	%rd1898, %rd1897, %rd2828;
	add.s64 	%rd1899, %rd1898, %rd1892;
	add.s64 	%rd1900, %rd1899, %rd1607;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10879,%dummy}, %rd1679;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10880}, %rd1679;
	}
	shf.r.wrap.b32 	%r10881, %r10880, %r10879, 28;
	shf.r.wrap.b32 	%r10882, %r10879, %r10880, 28;
	mov.b64 	%rd1901, {%r10882, %r10881};
	shf.l.wrap.b32 	%r10883, %r10879, %r10880, 30;
	shf.l.wrap.b32 	%r10884, %r10880, %r10879, 30;
	mov.b64 	%rd1902, {%r10884, %r10883};
	xor.b64  	%rd1903, %rd1902, %rd1901;
	shf.l.wrap.b32 	%r10885, %r10879, %r10880, 25;
	shf.l.wrap.b32 	%r10886, %r10880, %r10879, 25;
	mov.b64 	%rd1904, {%r10886, %r10885};
	xor.b64  	%rd1905, %rd1903, %rd1904;
	xor.b64  	%rd1906, %rd1679, %rd1631;
	xor.b64  	%rd1907, %rd1679, %rd1655;
	and.b64  	%rd1908, %rd1907, %rd1906;
	xor.b64  	%rd1909, %rd1908, %rd1679;
	add.s64 	%rd1910, %rd1899, %rd1909;
	add.s64 	%rd1911, %rd1910, %rd1905;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10887,%dummy}, %rd1900;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10888}, %rd1900;
	}
	shf.r.wrap.b32 	%r10889, %r10888, %r10887, 14;
	shf.r.wrap.b32 	%r10890, %r10887, %r10888, 14;
	mov.b64 	%rd1912, {%r10890, %r10889};
	shf.r.wrap.b32 	%r10891, %r10888, %r10887, 18;
	shf.r.wrap.b32 	%r10892, %r10887, %r10888, 18;
	mov.b64 	%rd1913, {%r10892, %r10891};
	xor.b64  	%rd1914, %rd1913, %rd1912;
	shf.l.wrap.b32 	%r10893, %r10887, %r10888, 23;
	shf.l.wrap.b32 	%r10894, %r10888, %r10887, 23;
	mov.b64 	%rd1915, {%r10894, %r10893};
	xor.b64  	%rd1916, %rd1914, %rd1915;
	xor.b64  	%rd1917, %rd1644, %rd1668;
	and.b64  	%rd1918, %rd1900, %rd1917;
	xor.b64  	%rd1919, %rd1918, %rd1644;
	add.s64 	%rd1920, %rd1705, %rd1620;
	add.s64 	%rd1921, %rd1920, %rd2829;
	add.s64 	%rd1922, %rd1921, %rd1919;
	add.s64 	%rd1923, %rd1922, %rd1916;
	add.s64 	%rd1924, %rd1923, %rd1631;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10895,%dummy}, %rd1911;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10896}, %rd1911;
	}
	shf.r.wrap.b32 	%r10897, %r10896, %r10895, 28;
	shf.r.wrap.b32 	%r10898, %r10895, %r10896, 28;
	mov.b64 	%rd1925, {%r10898, %r10897};
	shf.l.wrap.b32 	%r10899, %r10895, %r10896, 30;
	shf.l.wrap.b32 	%r10900, %r10896, %r10895, 30;
	mov.b64 	%rd1926, {%r10900, %r10899};
	xor.b64  	%rd1927, %rd1926, %rd1925;
	shf.l.wrap.b32 	%r10901, %r10895, %r10896, 25;
	shf.l.wrap.b32 	%r10902, %r10896, %r10895, 25;
	mov.b64 	%rd1928, {%r10902, %r10901};
	xor.b64  	%rd1929, %rd1927, %rd1928;
	xor.b64  	%rd1930, %rd1911, %rd1655;
	xor.b64  	%rd1931, %rd1911, %rd1679;
	and.b64  	%rd1932, %rd1931, %rd1930;
	xor.b64  	%rd1933, %rd1932, %rd1911;
	add.s64 	%rd1934, %rd1923, %rd1933;
	add.s64 	%rd1935, %rd1934, %rd1929;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10903,%dummy}, %rd1924;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10904}, %rd1924;
	}
	shf.r.wrap.b32 	%r10905, %r10904, %r10903, 14;
	shf.r.wrap.b32 	%r10906, %r10903, %r10904, 14;
	mov.b64 	%rd1936, {%r10906, %r10905};
	shf.r.wrap.b32 	%r10907, %r10904, %r10903, 18;
	shf.r.wrap.b32 	%r10908, %r10903, %r10904, 18;
	mov.b64 	%rd1937, {%r10908, %r10907};
	xor.b64  	%rd1938, %rd1937, %rd1936;
	shf.l.wrap.b32 	%r10909, %r10903, %r10904, 23;
	shf.l.wrap.b32 	%r10910, %r10904, %r10903, 23;
	mov.b64 	%rd1939, {%r10910, %r10909};
	xor.b64  	%rd1940, %rd1938, %rd1939;
	xor.b64  	%rd1941, %rd1900, %rd1668;
	and.b64  	%rd1942, %rd1924, %rd1941;
	xor.b64  	%rd1943, %rd1942, %rd1668;
	add.s64 	%rd1944, %rd1718, %rd1644;
	add.s64 	%rd1945, %rd1944, %rd2830;
	add.s64 	%rd1946, %rd1945, %rd1943;
	add.s64 	%rd1947, %rd1946, %rd1940;
	add.s64 	%rd1948, %rd1947, %rd1655;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10911,%dummy}, %rd1935;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10912}, %rd1935;
	}
	shf.r.wrap.b32 	%r10913, %r10912, %r10911, 28;
	shf.r.wrap.b32 	%r10914, %r10911, %r10912, 28;
	mov.b64 	%rd1949, {%r10914, %r10913};
	shf.l.wrap.b32 	%r10915, %r10911, %r10912, 30;
	shf.l.wrap.b32 	%r10916, %r10912, %r10911, 30;
	mov.b64 	%rd1950, {%r10916, %r10915};
	xor.b64  	%rd1951, %rd1950, %rd1949;
	shf.l.wrap.b32 	%r10917, %r10911, %r10912, 25;
	shf.l.wrap.b32 	%r10918, %r10912, %r10911, 25;
	mov.b64 	%rd1952, {%r10918, %r10917};
	xor.b64  	%rd1953, %rd1951, %rd1952;
	xor.b64  	%rd1954, %rd1935, %rd1679;
	xor.b64  	%rd1955, %rd1935, %rd1911;
	and.b64  	%rd1956, %rd1955, %rd1954;
	xor.b64  	%rd1957, %rd1956, %rd1935;
	add.s64 	%rd1958, %rd1947, %rd1957;
	add.s64 	%rd1959, %rd1958, %rd1953;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10919,%dummy}, %rd1948;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10920}, %rd1948;
	}
	shf.r.wrap.b32 	%r10921, %r10920, %r10919, 14;
	shf.r.wrap.b32 	%r10922, %r10919, %r10920, 14;
	mov.b64 	%rd1960, {%r10922, %r10921};
	shf.r.wrap.b32 	%r10923, %r10920, %r10919, 18;
	shf.r.wrap.b32 	%r10924, %r10919, %r10920, 18;
	mov.b64 	%rd1961, {%r10924, %r10923};
	xor.b64  	%rd1962, %rd1961, %rd1960;
	shf.l.wrap.b32 	%r10925, %r10919, %r10920, 23;
	shf.l.wrap.b32 	%r10926, %r10920, %r10919, 23;
	mov.b64 	%rd1963, {%r10926, %r10925};
	xor.b64  	%rd1964, %rd1962, %rd1963;
	xor.b64  	%rd1965, %rd1924, %rd1900;
	and.b64  	%rd1966, %rd1948, %rd1965;
	xor.b64  	%rd1967, %rd1966, %rd1900;
	add.s64 	%rd1968, %rd1731, %rd1668;
	add.s64 	%rd1969, %rd1968, %rd2831;
	add.s64 	%rd1970, %rd1969, %rd1967;
	add.s64 	%rd1971, %rd1970, %rd1964;
	add.s64 	%rd1972, %rd1971, %rd1679;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10927,%dummy}, %rd1959;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10928}, %rd1959;
	}
	shf.r.wrap.b32 	%r10929, %r10928, %r10927, 28;
	shf.r.wrap.b32 	%r10930, %r10927, %r10928, 28;
	mov.b64 	%rd1973, {%r10930, %r10929};
	shf.l.wrap.b32 	%r10931, %r10927, %r10928, 30;
	shf.l.wrap.b32 	%r10932, %r10928, %r10927, 30;
	mov.b64 	%rd1974, {%r10932, %r10931};
	xor.b64  	%rd1975, %rd1974, %rd1973;
	shf.l.wrap.b32 	%r10933, %r10927, %r10928, 25;
	shf.l.wrap.b32 	%r10934, %r10928, %r10927, 25;
	mov.b64 	%rd1976, {%r10934, %r10933};
	xor.b64  	%rd1977, %rd1975, %rd1976;
	xor.b64  	%rd1978, %rd1959, %rd1911;
	xor.b64  	%rd1979, %rd1959, %rd1935;
	and.b64  	%rd1980, %rd1979, %rd1978;
	xor.b64  	%rd1981, %rd1980, %rd1959;
	add.s64 	%rd1982, %rd1971, %rd1981;
	add.s64 	%rd1983, %rd1982, %rd1977;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10935,%dummy}, %rd1972;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10936}, %rd1972;
	}
	shf.r.wrap.b32 	%r10937, %r10936, %r10935, 14;
	shf.r.wrap.b32 	%r10938, %r10935, %r10936, 14;
	mov.b64 	%rd1984, {%r10938, %r10937};
	shf.r.wrap.b32 	%r10939, %r10936, %r10935, 18;
	shf.r.wrap.b32 	%r10940, %r10935, %r10936, 18;
	mov.b64 	%rd1985, {%r10940, %r10939};
	xor.b64  	%rd1986, %rd1985, %rd1984;
	shf.l.wrap.b32 	%r10941, %r10935, %r10936, 23;
	shf.l.wrap.b32 	%r10942, %r10936, %r10935, 23;
	mov.b64 	%rd1987, {%r10942, %r10941};
	xor.b64  	%rd1988, %rd1986, %rd1987;
	xor.b64  	%rd1989, %rd1948, %rd1924;
	and.b64  	%rd1990, %rd1972, %rd1989;
	xor.b64  	%rd1991, %rd1990, %rd1924;
	add.s64 	%rd1992, %rd1900, %rd1744;
	add.s64 	%rd1993, %rd1992, %rd2832;
	add.s64 	%rd1994, %rd1993, %rd1991;
	add.s64 	%rd1995, %rd1994, %rd1988;
	add.s64 	%rd1996, %rd1995, %rd1911;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10943,%dummy}, %rd1983;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10944}, %rd1983;
	}
	shf.r.wrap.b32 	%r10945, %r10944, %r10943, 28;
	shf.r.wrap.b32 	%r10946, %r10943, %r10944, 28;
	mov.b64 	%rd1997, {%r10946, %r10945};
	shf.l.wrap.b32 	%r10947, %r10943, %r10944, 30;
	shf.l.wrap.b32 	%r10948, %r10944, %r10943, 30;
	mov.b64 	%rd1998, {%r10948, %r10947};
	xor.b64  	%rd1999, %rd1998, %rd1997;
	shf.l.wrap.b32 	%r10949, %r10943, %r10944, 25;
	shf.l.wrap.b32 	%r10950, %r10944, %r10943, 25;
	mov.b64 	%rd2000, {%r10950, %r10949};
	xor.b64  	%rd2001, %rd1999, %rd2000;
	xor.b64  	%rd2002, %rd1983, %rd1935;
	xor.b64  	%rd2003, %rd1983, %rd1959;
	and.b64  	%rd2004, %rd2003, %rd2002;
	xor.b64  	%rd2005, %rd2004, %rd1983;
	add.s64 	%rd2006, %rd1995, %rd2005;
	add.s64 	%rd2007, %rd2006, %rd2001;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10951,%dummy}, %rd1996;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10952}, %rd1996;
	}
	shf.r.wrap.b32 	%r10953, %r10952, %r10951, 14;
	shf.r.wrap.b32 	%r10954, %r10951, %r10952, 14;
	mov.b64 	%rd2008, {%r10954, %r10953};
	shf.r.wrap.b32 	%r10955, %r10952, %r10951, 18;
	shf.r.wrap.b32 	%r10956, %r10951, %r10952, 18;
	mov.b64 	%rd2009, {%r10956, %r10955};
	xor.b64  	%rd2010, %rd2009, %rd2008;
	shf.l.wrap.b32 	%r10957, %r10951, %r10952, 23;
	shf.l.wrap.b32 	%r10958, %r10952, %r10951, 23;
	mov.b64 	%rd2011, {%r10958, %r10957};
	xor.b64  	%rd2012, %rd2010, %rd2011;
	xor.b64  	%rd2013, %rd1972, %rd1948;
	and.b64  	%rd2014, %rd1996, %rd2013;
	xor.b64  	%rd2015, %rd2014, %rd1948;
	add.s64 	%rd2016, %rd1924, %rd1757;
	add.s64 	%rd2017, %rd2016, %rd2833;
	add.s64 	%rd2018, %rd2017, %rd2015;
	add.s64 	%rd2019, %rd2018, %rd2012;
	add.s64 	%rd2020, %rd2019, %rd1935;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10959,%dummy}, %rd2007;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10960}, %rd2007;
	}
	shf.r.wrap.b32 	%r10961, %r10960, %r10959, 28;
	shf.r.wrap.b32 	%r10962, %r10959, %r10960, 28;
	mov.b64 	%rd2021, {%r10962, %r10961};
	shf.l.wrap.b32 	%r10963, %r10959, %r10960, 30;
	shf.l.wrap.b32 	%r10964, %r10960, %r10959, 30;
	mov.b64 	%rd2022, {%r10964, %r10963};
	xor.b64  	%rd2023, %rd2022, %rd2021;
	shf.l.wrap.b32 	%r10965, %r10959, %r10960, 25;
	shf.l.wrap.b32 	%r10966, %r10960, %r10959, 25;
	mov.b64 	%rd2024, {%r10966, %r10965};
	xor.b64  	%rd2025, %rd2023, %rd2024;
	xor.b64  	%rd2026, %rd2007, %rd1959;
	xor.b64  	%rd2027, %rd2007, %rd1983;
	and.b64  	%rd2028, %rd2027, %rd2026;
	xor.b64  	%rd2029, %rd2028, %rd2007;
	add.s64 	%rd2030, %rd2019, %rd2029;
	add.s64 	%rd2031, %rd2030, %rd2025;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10967,%dummy}, %rd2020;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10968}, %rd2020;
	}
	shf.r.wrap.b32 	%r10969, %r10968, %r10967, 14;
	shf.r.wrap.b32 	%r10970, %r10967, %r10968, 14;
	mov.b64 	%rd2032, {%r10970, %r10969};
	shf.r.wrap.b32 	%r10971, %r10968, %r10967, 18;
	shf.r.wrap.b32 	%r10972, %r10967, %r10968, 18;
	mov.b64 	%rd2033, {%r10972, %r10971};
	xor.b64  	%rd2034, %rd2033, %rd2032;
	shf.l.wrap.b32 	%r10973, %r10967, %r10968, 23;
	shf.l.wrap.b32 	%r10974, %r10968, %r10967, 23;
	mov.b64 	%rd2035, {%r10974, %r10973};
	xor.b64  	%rd2036, %rd2034, %rd2035;
	xor.b64  	%rd2037, %rd1996, %rd1972;
	and.b64  	%rd2038, %rd2020, %rd2037;
	xor.b64  	%rd2039, %rd2038, %rd1972;
	add.s64 	%rd2040, %rd1948, %rd1770;
	add.s64 	%rd2041, %rd2040, %rd2834;
	add.s64 	%rd2042, %rd2041, %rd2039;
	add.s64 	%rd2043, %rd2042, %rd2036;
	add.s64 	%rd2044, %rd2043, %rd1959;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10975,%dummy}, %rd2031;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10976}, %rd2031;
	}
	shf.r.wrap.b32 	%r10977, %r10976, %r10975, 28;
	shf.r.wrap.b32 	%r10978, %r10975, %r10976, 28;
	mov.b64 	%rd2045, {%r10978, %r10977};
	shf.l.wrap.b32 	%r10979, %r10975, %r10976, 30;
	shf.l.wrap.b32 	%r10980, %r10976, %r10975, 30;
	mov.b64 	%rd2046, {%r10980, %r10979};
	xor.b64  	%rd2047, %rd2046, %rd2045;
	shf.l.wrap.b32 	%r10981, %r10975, %r10976, 25;
	shf.l.wrap.b32 	%r10982, %r10976, %r10975, 25;
	mov.b64 	%rd2048, {%r10982, %r10981};
	xor.b64  	%rd2049, %rd2047, %rd2048;
	xor.b64  	%rd2050, %rd2031, %rd1983;
	xor.b64  	%rd2051, %rd2031, %rd2007;
	and.b64  	%rd2052, %rd2051, %rd2050;
	xor.b64  	%rd2053, %rd2052, %rd2031;
	add.s64 	%rd2054, %rd2043, %rd2053;
	add.s64 	%rd2055, %rd2054, %rd2049;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10983,%dummy}, %rd2044;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10984}, %rd2044;
	}
	shf.r.wrap.b32 	%r10985, %r10984, %r10983, 14;
	shf.r.wrap.b32 	%r10986, %r10983, %r10984, 14;
	mov.b64 	%rd2056, {%r10986, %r10985};
	shf.r.wrap.b32 	%r10987, %r10984, %r10983, 18;
	shf.r.wrap.b32 	%r10988, %r10983, %r10984, 18;
	mov.b64 	%rd2057, {%r10988, %r10987};
	xor.b64  	%rd2058, %rd2057, %rd2056;
	shf.l.wrap.b32 	%r10989, %r10983, %r10984, 23;
	shf.l.wrap.b32 	%r10990, %r10984, %r10983, 23;
	mov.b64 	%rd2059, {%r10990, %r10989};
	xor.b64  	%rd2060, %rd2058, %rd2059;
	xor.b64  	%rd2061, %rd2020, %rd1996;
	and.b64  	%rd2062, %rd2044, %rd2061;
	xor.b64  	%rd2063, %rd2062, %rd1996;
	add.s64 	%rd2064, %rd1972, %rd1783;
	add.s64 	%rd2065, %rd2064, %rd2835;
	add.s64 	%rd2066, %rd2065, %rd2063;
	add.s64 	%rd2067, %rd2066, %rd2060;
	add.s64 	%rd2068, %rd2067, %rd1983;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10991,%dummy}, %rd2055;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10992}, %rd2055;
	}
	shf.r.wrap.b32 	%r10993, %r10992, %r10991, 28;
	shf.r.wrap.b32 	%r10994, %r10991, %r10992, 28;
	mov.b64 	%rd2069, {%r10994, %r10993};
	shf.l.wrap.b32 	%r10995, %r10991, %r10992, 30;
	shf.l.wrap.b32 	%r10996, %r10992, %r10991, 30;
	mov.b64 	%rd2070, {%r10996, %r10995};
	xor.b64  	%rd2071, %rd2070, %rd2069;
	shf.l.wrap.b32 	%r10997, %r10991, %r10992, 25;
	shf.l.wrap.b32 	%r10998, %r10992, %r10991, 25;
	mov.b64 	%rd2072, {%r10998, %r10997};
	xor.b64  	%rd2073, %rd2071, %rd2072;
	xor.b64  	%rd2074, %rd2055, %rd2007;
	xor.b64  	%rd2075, %rd2055, %rd2031;
	and.b64  	%rd2076, %rd2075, %rd2074;
	xor.b64  	%rd2077, %rd2076, %rd2055;
	add.s64 	%rd2078, %rd2067, %rd2077;
	add.s64 	%rd2079, %rd2078, %rd2073;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10999,%dummy}, %rd2068;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11000}, %rd2068;
	}
	shf.r.wrap.b32 	%r11001, %r11000, %r10999, 14;
	shf.r.wrap.b32 	%r11002, %r10999, %r11000, 14;
	mov.b64 	%rd2080, {%r11002, %r11001};
	shf.r.wrap.b32 	%r11003, %r11000, %r10999, 18;
	shf.r.wrap.b32 	%r11004, %r10999, %r11000, 18;
	mov.b64 	%rd2081, {%r11004, %r11003};
	xor.b64  	%rd2082, %rd2081, %rd2080;
	shf.l.wrap.b32 	%r11005, %r10999, %r11000, 23;
	shf.l.wrap.b32 	%r11006, %r11000, %r10999, 23;
	mov.b64 	%rd2083, {%r11006, %r11005};
	xor.b64  	%rd2084, %rd2082, %rd2083;
	xor.b64  	%rd2085, %rd2044, %rd2020;
	and.b64  	%rd2086, %rd2068, %rd2085;
	xor.b64  	%rd2087, %rd2086, %rd2020;
	add.s64 	%rd2088, %rd1996, %rd1796;
	add.s64 	%rd2089, %rd2088, %rd2836;
	add.s64 	%rd2090, %rd2089, %rd2087;
	add.s64 	%rd2091, %rd2090, %rd2084;
	add.s64 	%rd2092, %rd2091, %rd2007;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11007,%dummy}, %rd2079;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11008}, %rd2079;
	}
	shf.r.wrap.b32 	%r11009, %r11008, %r11007, 28;
	shf.r.wrap.b32 	%r11010, %r11007, %r11008, 28;
	mov.b64 	%rd2093, {%r11010, %r11009};
	shf.l.wrap.b32 	%r11011, %r11007, %r11008, 30;
	shf.l.wrap.b32 	%r11012, %r11008, %r11007, 30;
	mov.b64 	%rd2094, {%r11012, %r11011};
	xor.b64  	%rd2095, %rd2094, %rd2093;
	shf.l.wrap.b32 	%r11013, %r11007, %r11008, 25;
	shf.l.wrap.b32 	%r11014, %r11008, %r11007, 25;
	mov.b64 	%rd2096, {%r11014, %r11013};
	xor.b64  	%rd2097, %rd2095, %rd2096;
	xor.b64  	%rd2098, %rd2079, %rd2031;
	xor.b64  	%rd2099, %rd2079, %rd2055;
	and.b64  	%rd2100, %rd2099, %rd2098;
	xor.b64  	%rd2101, %rd2100, %rd2079;
	add.s64 	%rd2102, %rd2091, %rd2101;
	add.s64 	%rd2103, %rd2102, %rd2097;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11015,%dummy}, %rd2092;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11016}, %rd2092;
	}
	shf.r.wrap.b32 	%r11017, %r11016, %r11015, 14;
	shf.r.wrap.b32 	%r11018, %r11015, %r11016, 14;
	mov.b64 	%rd2104, {%r11018, %r11017};
	shf.r.wrap.b32 	%r11019, %r11016, %r11015, 18;
	shf.r.wrap.b32 	%r11020, %r11015, %r11016, 18;
	mov.b64 	%rd2105, {%r11020, %r11019};
	xor.b64  	%rd2106, %rd2105, %rd2104;
	shf.l.wrap.b32 	%r11021, %r11015, %r11016, 23;
	shf.l.wrap.b32 	%r11022, %r11016, %r11015, 23;
	mov.b64 	%rd2107, {%r11022, %r11021};
	xor.b64  	%rd2108, %rd2106, %rd2107;
	xor.b64  	%rd2109, %rd2068, %rd2044;
	and.b64  	%rd2110, %rd2092, %rd2109;
	xor.b64  	%rd2111, %rd2110, %rd2044;
	add.s64 	%rd2112, %rd2020, %rd1809;
	add.s64 	%rd2113, %rd2112, %rd2837;
	add.s64 	%rd2114, %rd2113, %rd2111;
	add.s64 	%rd2115, %rd2114, %rd2108;
	add.s64 	%rd2116, %rd2115, %rd2031;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11023,%dummy}, %rd2103;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11024}, %rd2103;
	}
	shf.r.wrap.b32 	%r11025, %r11024, %r11023, 28;
	shf.r.wrap.b32 	%r11026, %r11023, %r11024, 28;
	mov.b64 	%rd2117, {%r11026, %r11025};
	shf.l.wrap.b32 	%r11027, %r11023, %r11024, 30;
	shf.l.wrap.b32 	%r11028, %r11024, %r11023, 30;
	mov.b64 	%rd2118, {%r11028, %r11027};
	xor.b64  	%rd2119, %rd2118, %rd2117;
	shf.l.wrap.b32 	%r11029, %r11023, %r11024, 25;
	shf.l.wrap.b32 	%r11030, %r11024, %r11023, 25;
	mov.b64 	%rd2120, {%r11030, %r11029};
	xor.b64  	%rd2121, %rd2119, %rd2120;
	xor.b64  	%rd2122, %rd2103, %rd2055;
	xor.b64  	%rd2123, %rd2103, %rd2079;
	and.b64  	%rd2124, %rd2123, %rd2122;
	xor.b64  	%rd2125, %rd2124, %rd2103;
	add.s64 	%rd2126, %rd2115, %rd2125;
	add.s64 	%rd2127, %rd2126, %rd2121;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11031,%dummy}, %rd2116;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11032}, %rd2116;
	}
	shf.r.wrap.b32 	%r11033, %r11032, %r11031, 14;
	shf.r.wrap.b32 	%r11034, %r11031, %r11032, 14;
	mov.b64 	%rd2128, {%r11034, %r11033};
	shf.r.wrap.b32 	%r11035, %r11032, %r11031, 18;
	shf.r.wrap.b32 	%r11036, %r11031, %r11032, 18;
	mov.b64 	%rd2129, {%r11036, %r11035};
	xor.b64  	%rd2130, %rd2129, %rd2128;
	shf.l.wrap.b32 	%r11037, %r11031, %r11032, 23;
	shf.l.wrap.b32 	%r11038, %r11032, %r11031, 23;
	mov.b64 	%rd2131, {%r11038, %r11037};
	xor.b64  	%rd2132, %rd2130, %rd2131;
	xor.b64  	%rd2133, %rd2092, %rd2068;
	and.b64  	%rd2134, %rd2116, %rd2133;
	xor.b64  	%rd2135, %rd2134, %rd2068;
	add.s64 	%rd2136, %rd2044, %rd1822;
	add.s64 	%rd2137, %rd2136, %rd2838;
	add.s64 	%rd2138, %rd2137, %rd2135;
	add.s64 	%rd2139, %rd2138, %rd2132;
	add.s64 	%rd2140, %rd2139, %rd2055;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11039,%dummy}, %rd2127;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11040}, %rd2127;
	}
	shf.r.wrap.b32 	%r11041, %r11040, %r11039, 28;
	shf.r.wrap.b32 	%r11042, %r11039, %r11040, 28;
	mov.b64 	%rd2141, {%r11042, %r11041};
	shf.l.wrap.b32 	%r11043, %r11039, %r11040, 30;
	shf.l.wrap.b32 	%r11044, %r11040, %r11039, 30;
	mov.b64 	%rd2142, {%r11044, %r11043};
	xor.b64  	%rd2143, %rd2142, %rd2141;
	shf.l.wrap.b32 	%r11045, %r11039, %r11040, 25;
	shf.l.wrap.b32 	%r11046, %r11040, %r11039, 25;
	mov.b64 	%rd2144, {%r11046, %r11045};
	xor.b64  	%rd2145, %rd2143, %rd2144;
	xor.b64  	%rd2146, %rd2127, %rd2079;
	xor.b64  	%rd2147, %rd2127, %rd2103;
	and.b64  	%rd2148, %rd2147, %rd2146;
	xor.b64  	%rd2149, %rd2148, %rd2127;
	add.s64 	%rd2150, %rd2139, %rd2149;
	add.s64 	%rd2151, %rd2150, %rd2145;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11047,%dummy}, %rd2140;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11048}, %rd2140;
	}
	shf.r.wrap.b32 	%r11049, %r11048, %r11047, 14;
	shf.r.wrap.b32 	%r11050, %r11047, %r11048, 14;
	mov.b64 	%rd2152, {%r11050, %r11049};
	shf.r.wrap.b32 	%r11051, %r11048, %r11047, 18;
	shf.r.wrap.b32 	%r11052, %r11047, %r11048, 18;
	mov.b64 	%rd2153, {%r11052, %r11051};
	xor.b64  	%rd2154, %rd2153, %rd2152;
	shf.l.wrap.b32 	%r11053, %r11047, %r11048, 23;
	shf.l.wrap.b32 	%r11054, %r11048, %r11047, 23;
	mov.b64 	%rd2155, {%r11054, %r11053};
	xor.b64  	%rd2156, %rd2154, %rd2155;
	xor.b64  	%rd2157, %rd2116, %rd2092;
	and.b64  	%rd2158, %rd2140, %rd2157;
	xor.b64  	%rd2159, %rd2158, %rd2092;
	add.s64 	%rd2160, %rd2068, %rd1835;
	add.s64 	%rd2161, %rd2160, %rd2839;
	add.s64 	%rd2162, %rd2161, %rd2159;
	add.s64 	%rd2163, %rd2162, %rd2156;
	add.s64 	%rd2164, %rd2163, %rd2079;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11055,%dummy}, %rd2151;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11056}, %rd2151;
	}
	shf.r.wrap.b32 	%r11057, %r11056, %r11055, 28;
	shf.r.wrap.b32 	%r11058, %r11055, %r11056, 28;
	mov.b64 	%rd2165, {%r11058, %r11057};
	shf.l.wrap.b32 	%r11059, %r11055, %r11056, 30;
	shf.l.wrap.b32 	%r11060, %r11056, %r11055, 30;
	mov.b64 	%rd2166, {%r11060, %r11059};
	xor.b64  	%rd2167, %rd2166, %rd2165;
	shf.l.wrap.b32 	%r11061, %r11055, %r11056, 25;
	shf.l.wrap.b32 	%r11062, %r11056, %r11055, 25;
	mov.b64 	%rd2168, {%r11062, %r11061};
	xor.b64  	%rd2169, %rd2167, %rd2168;
	xor.b64  	%rd2170, %rd2151, %rd2103;
	xor.b64  	%rd2171, %rd2151, %rd2127;
	and.b64  	%rd2172, %rd2171, %rd2170;
	xor.b64  	%rd2173, %rd2172, %rd2151;
	add.s64 	%rd2174, %rd2163, %rd2173;
	add.s64 	%rd2175, %rd2174, %rd2169;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11063,%dummy}, %rd2164;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11064}, %rd2164;
	}
	shf.r.wrap.b32 	%r11065, %r11064, %r11063, 14;
	shf.r.wrap.b32 	%r11066, %r11063, %r11064, 14;
	mov.b64 	%rd2176, {%r11066, %r11065};
	shf.r.wrap.b32 	%r11067, %r11064, %r11063, 18;
	shf.r.wrap.b32 	%r11068, %r11063, %r11064, 18;
	mov.b64 	%rd2177, {%r11068, %r11067};
	xor.b64  	%rd2178, %rd2177, %rd2176;
	shf.l.wrap.b32 	%r11069, %r11063, %r11064, 23;
	shf.l.wrap.b32 	%r11070, %r11064, %r11063, 23;
	mov.b64 	%rd2179, {%r11070, %r11069};
	xor.b64  	%rd2180, %rd2178, %rd2179;
	xor.b64  	%rd2181, %rd2140, %rd2116;
	and.b64  	%rd2182, %rd2164, %rd2181;
	xor.b64  	%rd2183, %rd2182, %rd2116;
	add.s64 	%rd2184, %rd2092, %rd1848;
	add.s64 	%rd2185, %rd2184, %rd2840;
	add.s64 	%rd2186, %rd2185, %rd2183;
	add.s64 	%rd2187, %rd2186, %rd2180;
	add.s64 	%rd2188, %rd2187, %rd2103;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11071,%dummy}, %rd2175;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11072}, %rd2175;
	}
	shf.r.wrap.b32 	%r11073, %r11072, %r11071, 28;
	shf.r.wrap.b32 	%r11074, %r11071, %r11072, 28;
	mov.b64 	%rd2189, {%r11074, %r11073};
	shf.l.wrap.b32 	%r11075, %r11071, %r11072, 30;
	shf.l.wrap.b32 	%r11076, %r11072, %r11071, 30;
	mov.b64 	%rd2190, {%r11076, %r11075};
	xor.b64  	%rd2191, %rd2190, %rd2189;
	shf.l.wrap.b32 	%r11077, %r11071, %r11072, 25;
	shf.l.wrap.b32 	%r11078, %r11072, %r11071, 25;
	mov.b64 	%rd2192, {%r11078, %r11077};
	xor.b64  	%rd2193, %rd2191, %rd2192;
	xor.b64  	%rd2194, %rd2175, %rd2127;
	xor.b64  	%rd2195, %rd2175, %rd2151;
	and.b64  	%rd2196, %rd2195, %rd2194;
	xor.b64  	%rd2197, %rd2196, %rd2175;
	add.s64 	%rd2198, %rd2187, %rd2197;
	add.s64 	%rd2199, %rd2198, %rd2193;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11079,%dummy}, %rd2188;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11080}, %rd2188;
	}
	shf.r.wrap.b32 	%r11081, %r11080, %r11079, 14;
	shf.r.wrap.b32 	%r11082, %r11079, %r11080, 14;
	mov.b64 	%rd2200, {%r11082, %r11081};
	shf.r.wrap.b32 	%r11083, %r11080, %r11079, 18;
	shf.r.wrap.b32 	%r11084, %r11079, %r11080, 18;
	mov.b64 	%rd2201, {%r11084, %r11083};
	xor.b64  	%rd2202, %rd2201, %rd2200;
	shf.l.wrap.b32 	%r11085, %r11079, %r11080, 23;
	shf.l.wrap.b32 	%r11086, %r11080, %r11079, 23;
	mov.b64 	%rd2203, {%r11086, %r11085};
	xor.b64  	%rd2204, %rd2202, %rd2203;
	xor.b64  	%rd2205, %rd2164, %rd2140;
	and.b64  	%rd2206, %rd2188, %rd2205;
	xor.b64  	%rd2207, %rd2206, %rd2140;
	add.s64 	%rd2208, %rd2116, %rd1861;
	add.s64 	%rd2209, %rd2208, %rd2841;
	add.s64 	%rd2210, %rd2209, %rd2207;
	add.s64 	%rd2211, %rd2210, %rd2204;
	add.s64 	%rd2212, %rd2211, %rd2127;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11087,%dummy}, %rd2199;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11088}, %rd2199;
	}
	shf.r.wrap.b32 	%r11089, %r11088, %r11087, 28;
	shf.r.wrap.b32 	%r11090, %r11087, %r11088, 28;
	mov.b64 	%rd2213, {%r11090, %r11089};
	shf.l.wrap.b32 	%r11091, %r11087, %r11088, 30;
	shf.l.wrap.b32 	%r11092, %r11088, %r11087, 30;
	mov.b64 	%rd2214, {%r11092, %r11091};
	xor.b64  	%rd2215, %rd2214, %rd2213;
	shf.l.wrap.b32 	%r11093, %r11087, %r11088, 25;
	shf.l.wrap.b32 	%r11094, %r11088, %r11087, 25;
	mov.b64 	%rd2216, {%r11094, %r11093};
	xor.b64  	%rd2217, %rd2215, %rd2216;
	xor.b64  	%rd2218, %rd2199, %rd2151;
	xor.b64  	%rd2219, %rd2199, %rd2175;
	and.b64  	%rd2220, %rd2219, %rd2218;
	xor.b64  	%rd2221, %rd2220, %rd2199;
	add.s64 	%rd2222, %rd2211, %rd2221;
	add.s64 	%rd2223, %rd2222, %rd2217;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11095,%dummy}, %rd2212;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11096}, %rd2212;
	}
	shf.r.wrap.b32 	%r11097, %r11096, %r11095, 14;
	shf.r.wrap.b32 	%r11098, %r11095, %r11096, 14;
	mov.b64 	%rd2224, {%r11098, %r11097};
	shf.r.wrap.b32 	%r11099, %r11096, %r11095, 18;
	shf.r.wrap.b32 	%r11100, %r11095, %r11096, 18;
	mov.b64 	%rd2225, {%r11100, %r11099};
	xor.b64  	%rd2226, %rd2225, %rd2224;
	shf.l.wrap.b32 	%r11101, %r11095, %r11096, 23;
	shf.l.wrap.b32 	%r11102, %r11096, %r11095, 23;
	mov.b64 	%rd2227, {%r11102, %r11101};
	xor.b64  	%rd2228, %rd2226, %rd2227;
	xor.b64  	%rd2229, %rd2188, %rd2164;
	and.b64  	%rd2230, %rd2212, %rd2229;
	xor.b64  	%rd2231, %rd2230, %rd2164;
	add.s64 	%rd2232, %rd2140, %rd1874;
	add.s64 	%rd2233, %rd2232, %rd2842;
	add.s64 	%rd2234, %rd2233, %rd2231;
	add.s64 	%rd2235, %rd2234, %rd2228;
	add.s64 	%rd2236, %rd2235, %rd2151;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11103,%dummy}, %rd2223;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11104}, %rd2223;
	}
	shf.r.wrap.b32 	%r11105, %r11104, %r11103, 28;
	shf.r.wrap.b32 	%r11106, %r11103, %r11104, 28;
	mov.b64 	%rd2237, {%r11106, %r11105};
	shf.l.wrap.b32 	%r11107, %r11103, %r11104, 30;
	shf.l.wrap.b32 	%r11108, %r11104, %r11103, 30;
	mov.b64 	%rd2238, {%r11108, %r11107};
	xor.b64  	%rd2239, %rd2238, %rd2237;
	shf.l.wrap.b32 	%r11109, %r11103, %r11104, 25;
	shf.l.wrap.b32 	%r11110, %r11104, %r11103, 25;
	mov.b64 	%rd2240, {%r11110, %r11109};
	xor.b64  	%rd2241, %rd2239, %rd2240;
	xor.b64  	%rd2242, %rd2223, %rd2175;
	xor.b64  	%rd2243, %rd2223, %rd2199;
	and.b64  	%rd2244, %rd2243, %rd2242;
	xor.b64  	%rd2245, %rd2244, %rd2223;
	add.s64 	%rd2246, %rd2235, %rd2245;
	add.s64 	%rd2247, %rd2246, %rd2241;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11111,%dummy}, %rd2236;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11112}, %rd2236;
	}
	shf.r.wrap.b32 	%r11113, %r11112, %r11111, 14;
	shf.r.wrap.b32 	%r11114, %r11111, %r11112, 14;
	mov.b64 	%rd2248, {%r11114, %r11113};
	shf.r.wrap.b32 	%r11115, %r11112, %r11111, 18;
	shf.r.wrap.b32 	%r11116, %r11111, %r11112, 18;
	mov.b64 	%rd2249, {%r11116, %r11115};
	xor.b64  	%rd2250, %rd2249, %rd2248;
	shf.l.wrap.b32 	%r11117, %r11111, %r11112, 23;
	shf.l.wrap.b32 	%r11118, %r11112, %r11111, 23;
	mov.b64 	%rd2251, {%r11118, %r11117};
	xor.b64  	%rd2252, %rd2250, %rd2251;
	xor.b64  	%rd2253, %rd2212, %rd2188;
	and.b64  	%rd2254, %rd2236, %rd2253;
	xor.b64  	%rd2255, %rd2254, %rd2188;
	add.s64 	%rd2256, %rd2164, %rd1887;
	add.s64 	%rd2257, %rd2256, %rd2843;
	add.s64 	%rd2258, %rd2257, %rd2255;
	add.s64 	%rd2259, %rd2258, %rd2252;
	add.s64 	%rd2260, %rd2259, %rd2175;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11119,%dummy}, %rd2247;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11120}, %rd2247;
	}
	shf.r.wrap.b32 	%r11121, %r11120, %r11119, 28;
	shf.r.wrap.b32 	%r11122, %r11119, %r11120, 28;
	mov.b64 	%rd2261, {%r11122, %r11121};
	shf.l.wrap.b32 	%r11123, %r11119, %r11120, 30;
	shf.l.wrap.b32 	%r11124, %r11120, %r11119, 30;
	mov.b64 	%rd2262, {%r11124, %r11123};
	xor.b64  	%rd2263, %rd2262, %rd2261;
	shf.l.wrap.b32 	%r11125, %r11119, %r11120, 25;
	shf.l.wrap.b32 	%r11126, %r11120, %r11119, 25;
	mov.b64 	%rd2264, {%r11126, %r11125};
	xor.b64  	%rd2265, %rd2263, %rd2264;
	xor.b64  	%rd2266, %rd2247, %rd2199;
	xor.b64  	%rd2267, %rd2247, %rd2223;
	and.b64  	%rd2268, %rd2267, %rd2266;
	xor.b64  	%rd2269, %rd2268, %rd2247;
	add.s64 	%rd2270, %rd2259, %rd2269;
	add.s64 	%rd2271, %rd2270, %rd2265;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11127,%dummy}, %rd1874;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11128}, %rd1874;
	}
	shf.r.wrap.b32 	%r11129, %r11128, %r11127, 19;
	shf.r.wrap.b32 	%r11130, %r11127, %r11128, 19;
	mov.b64 	%rd2272, {%r11130, %r11129};
	shf.l.wrap.b32 	%r11131, %r11127, %r11128, 3;
	shf.l.wrap.b32 	%r11132, %r11128, %r11127, 3;
	mov.b64 	%rd2273, {%r11132, %r11131};
	shr.u64 	%rd2274, %rd1874, 6;
	xor.b64  	%rd2275, %rd2272, %rd2274;
	xor.b64  	%rd2276, %rd2275, %rd2273;
	shf.r.wrap.b32 	%r11133, %r10742, %r10741, 1;
	shf.r.wrap.b32 	%r11134, %r10741, %r10742, 1;
	mov.b64 	%rd2277, {%r11134, %r11133};
	shf.r.wrap.b32 	%r11135, %r10742, %r10741, 8;
	shf.r.wrap.b32 	%r11136, %r10741, %r10742, 8;
	mov.b64 	%rd2278, {%r11136, %r11135};
	shr.u64 	%rd2279, %rd1705, 7;
	xor.b64  	%rd2280, %rd2277, %rd2279;
	xor.b64  	%rd2281, %rd2280, %rd2278;
	add.s64 	%rd2282, %rd1692, %rd1809;
	add.s64 	%rd2283, %rd2282, %rd2276;
	add.s64 	%rd2284, %rd2283, %rd2281;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11137,%dummy}, %rd1887;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11138}, %rd1887;
	}
	shf.r.wrap.b32 	%r11139, %r11138, %r11137, 19;
	shf.r.wrap.b32 	%r11140, %r11137, %r11138, 19;
	mov.b64 	%rd2285, {%r11140, %r11139};
	shf.l.wrap.b32 	%r11141, %r11137, %r11138, 3;
	shf.l.wrap.b32 	%r11142, %r11138, %r11137, 3;
	mov.b64 	%rd2286, {%r11142, %r11141};
	shr.u64 	%rd2287, %rd1887, 6;
	xor.b64  	%rd2288, %rd2285, %rd2287;
	xor.b64  	%rd2289, %rd2288, %rd2286;
	shf.r.wrap.b32 	%r11143, %r10752, %r10751, 1;
	shf.r.wrap.b32 	%r11144, %r10751, %r10752, 1;
	mov.b64 	%rd2290, {%r11144, %r11143};
	shf.r.wrap.b32 	%r11145, %r10752, %r10751, 8;
	shf.r.wrap.b32 	%r11146, %r10751, %r10752, 8;
	mov.b64 	%rd2291, {%r11146, %r11145};
	shr.u64 	%rd2292, %rd1718, 7;
	xor.b64  	%rd2293, %rd2290, %rd2292;
	xor.b64  	%rd2294, %rd2293, %rd2291;
	add.s64 	%rd2295, %rd1705, %rd1822;
	add.s64 	%rd2296, %rd2295, %rd2289;
	add.s64 	%rd2297, %rd2296, %rd2294;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11147,%dummy}, %rd2284;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11148}, %rd2284;
	}
	shf.r.wrap.b32 	%r11149, %r11148, %r11147, 19;
	shf.r.wrap.b32 	%r11150, %r11147, %r11148, 19;
	mov.b64 	%rd2298, {%r11150, %r11149};
	shf.l.wrap.b32 	%r11151, %r11147, %r11148, 3;
	shf.l.wrap.b32 	%r11152, %r11148, %r11147, 3;
	mov.b64 	%rd2299, {%r11152, %r11151};
	shr.u64 	%rd2300, %rd2284, 6;
	xor.b64  	%rd2301, %rd2298, %rd2300;
	xor.b64  	%rd2302, %rd2301, %rd2299;
	shf.r.wrap.b32 	%r11153, %r10762, %r10761, 1;
	shf.r.wrap.b32 	%r11154, %r10761, %r10762, 1;
	mov.b64 	%rd2303, {%r11154, %r11153};
	shf.r.wrap.b32 	%r11155, %r10762, %r10761, 8;
	shf.r.wrap.b32 	%r11156, %r10761, %r10762, 8;
	mov.b64 	%rd2304, {%r11156, %r11155};
	shr.u64 	%rd2305, %rd1731, 7;
	xor.b64  	%rd2306, %rd2303, %rd2305;
	xor.b64  	%rd2307, %rd2306, %rd2304;
	add.s64 	%rd2308, %rd1718, %rd1835;
	add.s64 	%rd2309, %rd2308, %rd2302;
	add.s64 	%rd2310, %rd2309, %rd2307;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11157,%dummy}, %rd2297;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11158}, %rd2297;
	}
	shf.r.wrap.b32 	%r11159, %r11158, %r11157, 19;
	shf.r.wrap.b32 	%r11160, %r11157, %r11158, 19;
	mov.b64 	%rd2311, {%r11160, %r11159};
	shf.l.wrap.b32 	%r11161, %r11157, %r11158, 3;
	shf.l.wrap.b32 	%r11162, %r11158, %r11157, 3;
	mov.b64 	%rd2312, {%r11162, %r11161};
	shr.u64 	%rd2313, %rd2297, 6;
	xor.b64  	%rd2314, %rd2311, %rd2313;
	xor.b64  	%rd2315, %rd2314, %rd2312;
	shf.r.wrap.b32 	%r11163, %r10772, %r10771, 1;
	shf.r.wrap.b32 	%r11164, %r10771, %r10772, 1;
	mov.b64 	%rd2316, {%r11164, %r11163};
	shf.r.wrap.b32 	%r11165, %r10772, %r10771, 8;
	shf.r.wrap.b32 	%r11166, %r10771, %r10772, 8;
	mov.b64 	%rd2317, {%r11166, %r11165};
	shr.u64 	%rd2318, %rd1744, 7;
	xor.b64  	%rd2319, %rd2316, %rd2318;
	xor.b64  	%rd2320, %rd2319, %rd2317;
	add.s64 	%rd2321, %rd1731, %rd1848;
	add.s64 	%rd2322, %rd2321, %rd2315;
	add.s64 	%rd2323, %rd2322, %rd2320;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11167,%dummy}, %rd2310;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11168}, %rd2310;
	}
	shf.r.wrap.b32 	%r11169, %r11168, %r11167, 19;
	shf.r.wrap.b32 	%r11170, %r11167, %r11168, 19;
	mov.b64 	%rd2324, {%r11170, %r11169};
	shf.l.wrap.b32 	%r11171, %r11167, %r11168, 3;
	shf.l.wrap.b32 	%r11172, %r11168, %r11167, 3;
	mov.b64 	%rd2325, {%r11172, %r11171};
	shr.u64 	%rd2326, %rd2310, 6;
	xor.b64  	%rd2327, %rd2324, %rd2326;
	xor.b64  	%rd2328, %rd2327, %rd2325;
	shf.r.wrap.b32 	%r11173, %r10782, %r10781, 1;
	shf.r.wrap.b32 	%r11174, %r10781, %r10782, 1;
	mov.b64 	%rd2329, {%r11174, %r11173};
	shf.r.wrap.b32 	%r11175, %r10782, %r10781, 8;
	shf.r.wrap.b32 	%r11176, %r10781, %r10782, 8;
	mov.b64 	%rd2330, {%r11176, %r11175};
	shr.u64 	%rd2331, %rd1757, 7;
	xor.b64  	%rd2332, %rd2329, %rd2331;
	xor.b64  	%rd2333, %rd2332, %rd2330;
	add.s64 	%rd2334, %rd1744, %rd1861;
	add.s64 	%rd2335, %rd2334, %rd2328;
	add.s64 	%rd2336, %rd2335, %rd2333;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11177,%dummy}, %rd2323;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11178}, %rd2323;
	}
	shf.r.wrap.b32 	%r11179, %r11178, %r11177, 19;
	shf.r.wrap.b32 	%r11180, %r11177, %r11178, 19;
	mov.b64 	%rd2337, {%r11180, %r11179};
	shf.l.wrap.b32 	%r11181, %r11177, %r11178, 3;
	shf.l.wrap.b32 	%r11182, %r11178, %r11177, 3;
	mov.b64 	%rd2338, {%r11182, %r11181};
	shr.u64 	%rd2339, %rd2323, 6;
	xor.b64  	%rd2340, %rd2337, %rd2339;
	xor.b64  	%rd2341, %rd2340, %rd2338;
	shf.r.wrap.b32 	%r11183, %r10792, %r10791, 1;
	shf.r.wrap.b32 	%r11184, %r10791, %r10792, 1;
	mov.b64 	%rd2342, {%r11184, %r11183};
	shf.r.wrap.b32 	%r11185, %r10792, %r10791, 8;
	shf.r.wrap.b32 	%r11186, %r10791, %r10792, 8;
	mov.b64 	%rd2343, {%r11186, %r11185};
	shr.u64 	%rd2344, %rd1770, 7;
	xor.b64  	%rd2345, %rd2342, %rd2344;
	xor.b64  	%rd2346, %rd2345, %rd2343;
	add.s64 	%rd2347, %rd1757, %rd1874;
	add.s64 	%rd2348, %rd2347, %rd2341;
	add.s64 	%rd2349, %rd2348, %rd2346;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11187,%dummy}, %rd2336;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11188}, %rd2336;
	}
	shf.r.wrap.b32 	%r11189, %r11188, %r11187, 19;
	shf.r.wrap.b32 	%r11190, %r11187, %r11188, 19;
	mov.b64 	%rd2350, {%r11190, %r11189};
	shf.l.wrap.b32 	%r11191, %r11187, %r11188, 3;
	shf.l.wrap.b32 	%r11192, %r11188, %r11187, 3;
	mov.b64 	%rd2351, {%r11192, %r11191};
	shr.u64 	%rd2352, %rd2336, 6;
	xor.b64  	%rd2353, %rd2350, %rd2352;
	xor.b64  	%rd2354, %rd2353, %rd2351;
	shf.r.wrap.b32 	%r11193, %r10802, %r10801, 1;
	shf.r.wrap.b32 	%r11194, %r10801, %r10802, 1;
	mov.b64 	%rd2355, {%r11194, %r11193};
	shf.r.wrap.b32 	%r11195, %r10802, %r10801, 8;
	shf.r.wrap.b32 	%r11196, %r10801, %r10802, 8;
	mov.b64 	%rd2356, {%r11196, %r11195};
	shr.u64 	%rd2357, %rd1783, 7;
	xor.b64  	%rd2358, %rd2355, %rd2357;
	xor.b64  	%rd2359, %rd2358, %rd2356;
	add.s64 	%rd2360, %rd1770, %rd1887;
	add.s64 	%rd2361, %rd2360, %rd2354;
	add.s64 	%rd2362, %rd2361, %rd2359;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11197,%dummy}, %rd2349;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11198}, %rd2349;
	}
	shf.r.wrap.b32 	%r11199, %r11198, %r11197, 19;
	shf.r.wrap.b32 	%r11200, %r11197, %r11198, 19;
	mov.b64 	%rd2363, {%r11200, %r11199};
	shf.l.wrap.b32 	%r11201, %r11197, %r11198, 3;
	shf.l.wrap.b32 	%r11202, %r11198, %r11197, 3;
	mov.b64 	%rd2364, {%r11202, %r11201};
	shr.u64 	%rd2365, %rd2349, 6;
	xor.b64  	%rd2366, %rd2363, %rd2365;
	xor.b64  	%rd2367, %rd2366, %rd2364;
	shf.r.wrap.b32 	%r11203, %r10812, %r10811, 1;
	shf.r.wrap.b32 	%r11204, %r10811, %r10812, 1;
	mov.b64 	%rd2368, {%r11204, %r11203};
	shf.r.wrap.b32 	%r11205, %r10812, %r10811, 8;
	shf.r.wrap.b32 	%r11206, %r10811, %r10812, 8;
	mov.b64 	%rd2369, {%r11206, %r11205};
	shr.u64 	%rd2370, %rd1796, 7;
	xor.b64  	%rd2371, %rd2368, %rd2370;
	xor.b64  	%rd2372, %rd2371, %rd2369;
	add.s64 	%rd2373, %rd2284, %rd1783;
	add.s64 	%rd2374, %rd2373, %rd2367;
	add.s64 	%rd2375, %rd2374, %rd2372;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11207,%dummy}, %rd2362;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11208}, %rd2362;
	}
	shf.r.wrap.b32 	%r11209, %r11208, %r11207, 19;
	shf.r.wrap.b32 	%r11210, %r11207, %r11208, 19;
	mov.b64 	%rd2376, {%r11210, %r11209};
	shf.l.wrap.b32 	%r11211, %r11207, %r11208, 3;
	shf.l.wrap.b32 	%r11212, %r11208, %r11207, 3;
	mov.b64 	%rd2377, {%r11212, %r11211};
	shr.u64 	%rd2378, %rd2362, 6;
	xor.b64  	%rd2379, %rd2376, %rd2378;
	xor.b64  	%rd2380, %rd2379, %rd2377;
	shf.r.wrap.b32 	%r11213, %r10822, %r10821, 1;
	shf.r.wrap.b32 	%r11214, %r10821, %r10822, 1;
	mov.b64 	%rd2381, {%r11214, %r11213};
	shf.r.wrap.b32 	%r11215, %r10822, %r10821, 8;
	shf.r.wrap.b32 	%r11216, %r10821, %r10822, 8;
	mov.b64 	%rd2382, {%r11216, %r11215};
	shr.u64 	%rd2383, %rd1809, 7;
	xor.b64  	%rd2384, %rd2381, %rd2383;
	xor.b64  	%rd2385, %rd2384, %rd2382;
	add.s64 	%rd2386, %rd2297, %rd1796;
	add.s64 	%rd2387, %rd2386, %rd2380;
	add.s64 	%rd2388, %rd2387, %rd2385;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11217,%dummy}, %rd2375;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11218}, %rd2375;
	}
	shf.r.wrap.b32 	%r11219, %r11218, %r11217, 19;
	shf.r.wrap.b32 	%r11220, %r11217, %r11218, 19;
	mov.b64 	%rd2389, {%r11220, %r11219};
	shf.l.wrap.b32 	%r11221, %r11217, %r11218, 3;
	shf.l.wrap.b32 	%r11222, %r11218, %r11217, 3;
	mov.b64 	%rd2390, {%r11222, %r11221};
	shr.u64 	%rd2391, %rd2375, 6;
	xor.b64  	%rd2392, %rd2389, %rd2391;
	xor.b64  	%rd2393, %rd2392, %rd2390;
	shf.r.wrap.b32 	%r11223, %r10832, %r10831, 1;
	shf.r.wrap.b32 	%r11224, %r10831, %r10832, 1;
	mov.b64 	%rd2394, {%r11224, %r11223};
	shf.r.wrap.b32 	%r11225, %r10832, %r10831, 8;
	shf.r.wrap.b32 	%r11226, %r10831, %r10832, 8;
	mov.b64 	%rd2395, {%r11226, %r11225};
	shr.u64 	%rd2396, %rd1822, 7;
	xor.b64  	%rd2397, %rd2394, %rd2396;
	xor.b64  	%rd2398, %rd2397, %rd2395;
	add.s64 	%rd2399, %rd2310, %rd1809;
	add.s64 	%rd2400, %rd2399, %rd2393;
	add.s64 	%rd2401, %rd2400, %rd2398;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11227,%dummy}, %rd2388;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11228}, %rd2388;
	}
	shf.r.wrap.b32 	%r11229, %r11228, %r11227, 19;
	shf.r.wrap.b32 	%r11230, %r11227, %r11228, 19;
	mov.b64 	%rd2402, {%r11230, %r11229};
	shf.l.wrap.b32 	%r11231, %r11227, %r11228, 3;
	shf.l.wrap.b32 	%r11232, %r11228, %r11227, 3;
	mov.b64 	%rd2403, {%r11232, %r11231};
	shr.u64 	%rd2404, %rd2388, 6;
	xor.b64  	%rd2405, %rd2402, %rd2404;
	xor.b64  	%rd2406, %rd2405, %rd2403;
	shf.r.wrap.b32 	%r11233, %r10842, %r10841, 1;
	shf.r.wrap.b32 	%r11234, %r10841, %r10842, 1;
	mov.b64 	%rd2407, {%r11234, %r11233};
	shf.r.wrap.b32 	%r11235, %r10842, %r10841, 8;
	shf.r.wrap.b32 	%r11236, %r10841, %r10842, 8;
	mov.b64 	%rd2408, {%r11236, %r11235};
	shr.u64 	%rd2409, %rd1835, 7;
	xor.b64  	%rd2410, %rd2407, %rd2409;
	xor.b64  	%rd2411, %rd2410, %rd2408;
	add.s64 	%rd2412, %rd2323, %rd1822;
	add.s64 	%rd2413, %rd2412, %rd2406;
	add.s64 	%rd2414, %rd2413, %rd2411;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11237,%dummy}, %rd2401;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11238}, %rd2401;
	}
	shf.r.wrap.b32 	%r11239, %r11238, %r11237, 19;
	shf.r.wrap.b32 	%r11240, %r11237, %r11238, 19;
	mov.b64 	%rd2415, {%r11240, %r11239};
	shf.l.wrap.b32 	%r11241, %r11237, %r11238, 3;
	shf.l.wrap.b32 	%r11242, %r11238, %r11237, 3;
	mov.b64 	%rd2416, {%r11242, %r11241};
	shr.u64 	%rd2417, %rd2401, 6;
	xor.b64  	%rd2418, %rd2415, %rd2417;
	xor.b64  	%rd2419, %rd2418, %rd2416;
	shf.r.wrap.b32 	%r11243, %r10852, %r10851, 1;
	shf.r.wrap.b32 	%r11244, %r10851, %r10852, 1;
	mov.b64 	%rd2420, {%r11244, %r11243};
	shf.r.wrap.b32 	%r11245, %r10852, %r10851, 8;
	shf.r.wrap.b32 	%r11246, %r10851, %r10852, 8;
	mov.b64 	%rd2421, {%r11246, %r11245};
	shr.u64 	%rd2422, %rd1848, 7;
	xor.b64  	%rd2423, %rd2420, %rd2422;
	xor.b64  	%rd2424, %rd2423, %rd2421;
	add.s64 	%rd2425, %rd2336, %rd1835;
	add.s64 	%rd2426, %rd2425, %rd2419;
	add.s64 	%rd2427, %rd2426, %rd2424;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11247,%dummy}, %rd2414;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11248}, %rd2414;
	}
	shf.r.wrap.b32 	%r11249, %r11248, %r11247, 19;
	shf.r.wrap.b32 	%r11250, %r11247, %r11248, 19;
	mov.b64 	%rd2428, {%r11250, %r11249};
	shf.l.wrap.b32 	%r11251, %r11247, %r11248, 3;
	shf.l.wrap.b32 	%r11252, %r11248, %r11247, 3;
	mov.b64 	%rd2429, {%r11252, %r11251};
	shr.u64 	%rd2430, %rd2414, 6;
	xor.b64  	%rd2431, %rd2428, %rd2430;
	xor.b64  	%rd2432, %rd2431, %rd2429;
	shf.r.wrap.b32 	%r11253, %r10862, %r10861, 1;
	shf.r.wrap.b32 	%r11254, %r10861, %r10862, 1;
	mov.b64 	%rd2433, {%r11254, %r11253};
	shf.r.wrap.b32 	%r11255, %r10862, %r10861, 8;
	shf.r.wrap.b32 	%r11256, %r10861, %r10862, 8;
	mov.b64 	%rd2434, {%r11256, %r11255};
	shr.u64 	%rd2435, %rd1861, 7;
	xor.b64  	%rd2436, %rd2433, %rd2435;
	xor.b64  	%rd2437, %rd2436, %rd2434;
	add.s64 	%rd2438, %rd2349, %rd1848;
	add.s64 	%rd2439, %rd2438, %rd2432;
	add.s64 	%rd2440, %rd2439, %rd2437;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11257,%dummy}, %rd2260;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11258}, %rd2260;
	}
	shf.r.wrap.b32 	%r11259, %r11258, %r11257, 14;
	shf.r.wrap.b32 	%r11260, %r11257, %r11258, 14;
	mov.b64 	%rd2441, {%r11260, %r11259};
	shf.r.wrap.b32 	%r11261, %r11258, %r11257, 18;
	shf.r.wrap.b32 	%r11262, %r11257, %r11258, 18;
	mov.b64 	%rd2442, {%r11262, %r11261};
	xor.b64  	%rd2443, %rd2442, %rd2441;
	shf.l.wrap.b32 	%r11263, %r11257, %r11258, 23;
	shf.l.wrap.b32 	%r11264, %r11258, %r11257, 23;
	mov.b64 	%rd2444, {%r11264, %r11263};
	xor.b64  	%rd2445, %rd2443, %rd2444;
	xor.b64  	%rd2446, %rd2212, %rd2236;
	and.b64  	%rd2447, %rd2446, %rd2260;
	xor.b64  	%rd2448, %rd2447, %rd2212;
	add.s64 	%rd2449, %rd2448, %rd2188;
	add.s64 	%rd2450, %rd2449, %rd2284;
	add.s64 	%rd2451, %rd2450, %rd2844;
	add.s64 	%rd2452, %rd2451, %rd2445;
	add.s64 	%rd2453, %rd2452, %rd2199;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11265,%dummy}, %rd2271;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11266}, %rd2271;
	}
	shf.r.wrap.b32 	%r11267, %r11266, %r11265, 28;
	shf.r.wrap.b32 	%r11268, %r11265, %r11266, 28;
	mov.b64 	%rd2454, {%r11268, %r11267};
	shf.l.wrap.b32 	%r11269, %r11265, %r11266, 30;
	shf.l.wrap.b32 	%r11270, %r11266, %r11265, 30;
	mov.b64 	%rd2455, {%r11270, %r11269};
	xor.b64  	%rd2456, %rd2455, %rd2454;
	shf.l.wrap.b32 	%r11271, %r11265, %r11266, 25;
	shf.l.wrap.b32 	%r11272, %r11266, %r11265, 25;
	mov.b64 	%rd2457, {%r11272, %r11271};
	xor.b64  	%rd2458, %rd2456, %rd2457;
	xor.b64  	%rd2459, %rd2271, %rd2223;
	xor.b64  	%rd2460, %rd2271, %rd2247;
	and.b64  	%rd2461, %rd2460, %rd2459;
	xor.b64  	%rd2462, %rd2461, %rd2271;
	add.s64 	%rd2463, %rd2452, %rd2462;
	add.s64 	%rd2464, %rd2463, %rd2458;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11273,%dummy}, %rd2453;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11274}, %rd2453;
	}
	shf.r.wrap.b32 	%r11275, %r11274, %r11273, 14;
	shf.r.wrap.b32 	%r11276, %r11273, %r11274, 14;
	mov.b64 	%rd2465, {%r11276, %r11275};
	shf.r.wrap.b32 	%r11277, %r11274, %r11273, 18;
	shf.r.wrap.b32 	%r11278, %r11273, %r11274, 18;
	mov.b64 	%rd2466, {%r11278, %r11277};
	xor.b64  	%rd2467, %rd2466, %rd2465;
	shf.l.wrap.b32 	%r11279, %r11273, %r11274, 23;
	shf.l.wrap.b32 	%r11280, %r11274, %r11273, 23;
	mov.b64 	%rd2468, {%r11280, %r11279};
	xor.b64  	%rd2469, %rd2467, %rd2468;
	xor.b64  	%rd2470, %rd2236, %rd2260;
	and.b64  	%rd2471, %rd2453, %rd2470;
	xor.b64  	%rd2472, %rd2471, %rd2236;
	add.s64 	%rd2473, %rd2297, %rd2212;
	add.s64 	%rd2474, %rd2473, %rd2845;
	add.s64 	%rd2475, %rd2474, %rd2472;
	add.s64 	%rd2476, %rd2475, %rd2469;
	add.s64 	%rd2477, %rd2476, %rd2223;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11281,%dummy}, %rd2464;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11282}, %rd2464;
	}
	shf.r.wrap.b32 	%r11283, %r11282, %r11281, 28;
	shf.r.wrap.b32 	%r11284, %r11281, %r11282, 28;
	mov.b64 	%rd2478, {%r11284, %r11283};
	shf.l.wrap.b32 	%r11285, %r11281, %r11282, 30;
	shf.l.wrap.b32 	%r11286, %r11282, %r11281, 30;
	mov.b64 	%rd2479, {%r11286, %r11285};
	xor.b64  	%rd2480, %rd2479, %rd2478;
	shf.l.wrap.b32 	%r11287, %r11281, %r11282, 25;
	shf.l.wrap.b32 	%r11288, %r11282, %r11281, 25;
	mov.b64 	%rd2481, {%r11288, %r11287};
	xor.b64  	%rd2482, %rd2480, %rd2481;
	xor.b64  	%rd2483, %rd2464, %rd2247;
	xor.b64  	%rd2484, %rd2464, %rd2271;
	and.b64  	%rd2485, %rd2484, %rd2483;
	xor.b64  	%rd2486, %rd2485, %rd2464;
	add.s64 	%rd2487, %rd2476, %rd2486;
	add.s64 	%rd2488, %rd2487, %rd2482;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11289,%dummy}, %rd2477;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11290}, %rd2477;
	}
	shf.r.wrap.b32 	%r11291, %r11290, %r11289, 14;
	shf.r.wrap.b32 	%r11292, %r11289, %r11290, 14;
	mov.b64 	%rd2489, {%r11292, %r11291};
	shf.r.wrap.b32 	%r11293, %r11290, %r11289, 18;
	shf.r.wrap.b32 	%r11294, %r11289, %r11290, 18;
	mov.b64 	%rd2490, {%r11294, %r11293};
	xor.b64  	%rd2491, %rd2490, %rd2489;
	shf.l.wrap.b32 	%r11295, %r11289, %r11290, 23;
	shf.l.wrap.b32 	%r11296, %r11290, %r11289, 23;
	mov.b64 	%rd2492, {%r11296, %r11295};
	xor.b64  	%rd2493, %rd2491, %rd2492;
	xor.b64  	%rd2494, %rd2453, %rd2260;
	and.b64  	%rd2495, %rd2477, %rd2494;
	xor.b64  	%rd2496, %rd2495, %rd2260;
	add.s64 	%rd2497, %rd2310, %rd2236;
	add.s64 	%rd2498, %rd2497, %rd2846;
	add.s64 	%rd2499, %rd2498, %rd2496;
	add.s64 	%rd2500, %rd2499, %rd2493;
	add.s64 	%rd2501, %rd2500, %rd2247;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11297,%dummy}, %rd2488;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11298}, %rd2488;
	}
	shf.r.wrap.b32 	%r11299, %r11298, %r11297, 28;
	shf.r.wrap.b32 	%r11300, %r11297, %r11298, 28;
	mov.b64 	%rd2502, {%r11300, %r11299};
	shf.l.wrap.b32 	%r11301, %r11297, %r11298, 30;
	shf.l.wrap.b32 	%r11302, %r11298, %r11297, 30;
	mov.b64 	%rd2503, {%r11302, %r11301};
	xor.b64  	%rd2504, %rd2503, %rd2502;
	shf.l.wrap.b32 	%r11303, %r11297, %r11298, 25;
	shf.l.wrap.b32 	%r11304, %r11298, %r11297, 25;
	mov.b64 	%rd2505, {%r11304, %r11303};
	xor.b64  	%rd2506, %rd2504, %rd2505;
	xor.b64  	%rd2507, %rd2488, %rd2271;
	xor.b64  	%rd2508, %rd2488, %rd2464;
	and.b64  	%rd2509, %rd2508, %rd2507;
	xor.b64  	%rd2510, %rd2509, %rd2488;
	add.s64 	%rd2511, %rd2500, %rd2510;
	add.s64 	%rd2512, %rd2511, %rd2506;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11305,%dummy}, %rd2501;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11306}, %rd2501;
	}
	shf.r.wrap.b32 	%r11307, %r11306, %r11305, 14;
	shf.r.wrap.b32 	%r11308, %r11305, %r11306, 14;
	mov.b64 	%rd2513, {%r11308, %r11307};
	shf.r.wrap.b32 	%r11309, %r11306, %r11305, 18;
	shf.r.wrap.b32 	%r11310, %r11305, %r11306, 18;
	mov.b64 	%rd2514, {%r11310, %r11309};
	xor.b64  	%rd2515, %rd2514, %rd2513;
	shf.l.wrap.b32 	%r11311, %r11305, %r11306, 23;
	shf.l.wrap.b32 	%r11312, %r11306, %r11305, 23;
	mov.b64 	%rd2516, {%r11312, %r11311};
	xor.b64  	%rd2517, %rd2515, %rd2516;
	xor.b64  	%rd2518, %rd2477, %rd2453;
	and.b64  	%rd2519, %rd2501, %rd2518;
	xor.b64  	%rd2520, %rd2519, %rd2453;
	add.s64 	%rd2521, %rd2323, %rd2260;
	add.s64 	%rd2522, %rd2521, %rd2847;
	add.s64 	%rd2523, %rd2522, %rd2520;
	add.s64 	%rd2524, %rd2523, %rd2517;
	add.s64 	%rd2525, %rd2524, %rd2271;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11313,%dummy}, %rd2512;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11314}, %rd2512;
	}
	shf.r.wrap.b32 	%r11315, %r11314, %r11313, 28;
	shf.r.wrap.b32 	%r11316, %r11313, %r11314, 28;
	mov.b64 	%rd2526, {%r11316, %r11315};
	shf.l.wrap.b32 	%r11317, %r11313, %r11314, 30;
	shf.l.wrap.b32 	%r11318, %r11314, %r11313, 30;
	mov.b64 	%rd2527, {%r11318, %r11317};
	xor.b64  	%rd2528, %rd2527, %rd2526;
	shf.l.wrap.b32 	%r11319, %r11313, %r11314, 25;
	shf.l.wrap.b32 	%r11320, %r11314, %r11313, 25;
	mov.b64 	%rd2529, {%r11320, %r11319};
	xor.b64  	%rd2530, %rd2528, %rd2529;
	xor.b64  	%rd2531, %rd2512, %rd2464;
	xor.b64  	%rd2532, %rd2512, %rd2488;
	and.b64  	%rd2533, %rd2532, %rd2531;
	xor.b64  	%rd2534, %rd2533, %rd2512;
	add.s64 	%rd2535, %rd2524, %rd2534;
	add.s64 	%rd2536, %rd2535, %rd2530;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11321,%dummy}, %rd2525;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11322}, %rd2525;
	}
	shf.r.wrap.b32 	%r11323, %r11322, %r11321, 14;
	shf.r.wrap.b32 	%r11324, %r11321, %r11322, 14;
	mov.b64 	%rd2537, {%r11324, %r11323};
	shf.r.wrap.b32 	%r11325, %r11322, %r11321, 18;
	shf.r.wrap.b32 	%r11326, %r11321, %r11322, 18;
	mov.b64 	%rd2538, {%r11326, %r11325};
	xor.b64  	%rd2539, %rd2538, %rd2537;
	shf.l.wrap.b32 	%r11327, %r11321, %r11322, 23;
	shf.l.wrap.b32 	%r11328, %r11322, %r11321, 23;
	mov.b64 	%rd2540, {%r11328, %r11327};
	xor.b64  	%rd2541, %rd2539, %rd2540;
	xor.b64  	%rd2542, %rd2501, %rd2477;
	and.b64  	%rd2543, %rd2525, %rd2542;
	xor.b64  	%rd2544, %rd2543, %rd2477;
	add.s64 	%rd2545, %rd2453, %rd2336;
	add.s64 	%rd2546, %rd2545, %rd2848;
	add.s64 	%rd2547, %rd2546, %rd2544;
	add.s64 	%rd2548, %rd2547, %rd2541;
	add.s64 	%rd2549, %rd2548, %rd2464;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11329,%dummy}, %rd2536;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11330}, %rd2536;
	}
	shf.r.wrap.b32 	%r11331, %r11330, %r11329, 28;
	shf.r.wrap.b32 	%r11332, %r11329, %r11330, 28;
	mov.b64 	%rd2550, {%r11332, %r11331};
	shf.l.wrap.b32 	%r11333, %r11329, %r11330, 30;
	shf.l.wrap.b32 	%r11334, %r11330, %r11329, 30;
	mov.b64 	%rd2551, {%r11334, %r11333};
	xor.b64  	%rd2552, %rd2551, %rd2550;
	shf.l.wrap.b32 	%r11335, %r11329, %r11330, 25;
	shf.l.wrap.b32 	%r11336, %r11330, %r11329, 25;
	mov.b64 	%rd2553, {%r11336, %r11335};
	xor.b64  	%rd2554, %rd2552, %rd2553;
	xor.b64  	%rd2555, %rd2536, %rd2488;
	xor.b64  	%rd2556, %rd2536, %rd2512;
	and.b64  	%rd2557, %rd2556, %rd2555;
	xor.b64  	%rd2558, %rd2557, %rd2536;
	add.s64 	%rd2559, %rd2548, %rd2558;
	add.s64 	%rd2560, %rd2559, %rd2554;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11337,%dummy}, %rd2549;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11338}, %rd2549;
	}
	shf.r.wrap.b32 	%r11339, %r11338, %r11337, 14;
	shf.r.wrap.b32 	%r11340, %r11337, %r11338, 14;
	mov.b64 	%rd2561, {%r11340, %r11339};
	shf.r.wrap.b32 	%r11341, %r11338, %r11337, 18;
	shf.r.wrap.b32 	%r11342, %r11337, %r11338, 18;
	mov.b64 	%rd2562, {%r11342, %r11341};
	xor.b64  	%rd2563, %rd2562, %rd2561;
	shf.l.wrap.b32 	%r11343, %r11337, %r11338, 23;
	shf.l.wrap.b32 	%r11344, %r11338, %r11337, 23;
	mov.b64 	%rd2564, {%r11344, %r11343};
	xor.b64  	%rd2565, %rd2563, %rd2564;
	xor.b64  	%rd2566, %rd2525, %rd2501;
	and.b64  	%rd2567, %rd2549, %rd2566;
	xor.b64  	%rd2568, %rd2567, %rd2501;
	add.s64 	%rd2569, %rd2477, %rd2349;
	add.s64 	%rd2570, %rd2569, %rd2849;
	add.s64 	%rd2571, %rd2570, %rd2568;
	add.s64 	%rd2572, %rd2571, %rd2565;
	add.s64 	%rd2573, %rd2572, %rd2488;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11345,%dummy}, %rd2560;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11346}, %rd2560;
	}
	shf.r.wrap.b32 	%r11347, %r11346, %r11345, 28;
	shf.r.wrap.b32 	%r11348, %r11345, %r11346, 28;
	mov.b64 	%rd2574, {%r11348, %r11347};
	shf.l.wrap.b32 	%r11349, %r11345, %r11346, 30;
	shf.l.wrap.b32 	%r11350, %r11346, %r11345, 30;
	mov.b64 	%rd2575, {%r11350, %r11349};
	xor.b64  	%rd2576, %rd2575, %rd2574;
	shf.l.wrap.b32 	%r11351, %r11345, %r11346, 25;
	shf.l.wrap.b32 	%r11352, %r11346, %r11345, 25;
	mov.b64 	%rd2577, {%r11352, %r11351};
	xor.b64  	%rd2578, %rd2576, %rd2577;
	xor.b64  	%rd2579, %rd2560, %rd2512;
	xor.b64  	%rd2580, %rd2560, %rd2536;
	and.b64  	%rd2581, %rd2580, %rd2579;
	xor.b64  	%rd2582, %rd2581, %rd2560;
	add.s64 	%rd2583, %rd2572, %rd2582;
	add.s64 	%rd2584, %rd2583, %rd2578;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11353,%dummy}, %rd2573;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11354}, %rd2573;
	}
	shf.r.wrap.b32 	%r11355, %r11354, %r11353, 14;
	shf.r.wrap.b32 	%r11356, %r11353, %r11354, 14;
	mov.b64 	%rd2585, {%r11356, %r11355};
	shf.r.wrap.b32 	%r11357, %r11354, %r11353, 18;
	shf.r.wrap.b32 	%r11358, %r11353, %r11354, 18;
	mov.b64 	%rd2586, {%r11358, %r11357};
	xor.b64  	%rd2587, %rd2586, %rd2585;
	shf.l.wrap.b32 	%r11359, %r11353, %r11354, 23;
	shf.l.wrap.b32 	%r11360, %r11354, %r11353, 23;
	mov.b64 	%rd2588, {%r11360, %r11359};
	xor.b64  	%rd2589, %rd2587, %rd2588;
	xor.b64  	%rd2590, %rd2549, %rd2525;
	and.b64  	%rd2591, %rd2573, %rd2590;
	xor.b64  	%rd2592, %rd2591, %rd2525;
	add.s64 	%rd2593, %rd2501, %rd2362;
	add.s64 	%rd2594, %rd2593, %rd2850;
	add.s64 	%rd2595, %rd2594, %rd2592;
	add.s64 	%rd2596, %rd2595, %rd2589;
	add.s64 	%rd2597, %rd2596, %rd2512;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11361,%dummy}, %rd2584;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11362}, %rd2584;
	}
	shf.r.wrap.b32 	%r11363, %r11362, %r11361, 28;
	shf.r.wrap.b32 	%r11364, %r11361, %r11362, 28;
	mov.b64 	%rd2598, {%r11364, %r11363};
	shf.l.wrap.b32 	%r11365, %r11361, %r11362, 30;
	shf.l.wrap.b32 	%r11366, %r11362, %r11361, 30;
	mov.b64 	%rd2599, {%r11366, %r11365};
	xor.b64  	%rd2600, %rd2599, %rd2598;
	shf.l.wrap.b32 	%r11367, %r11361, %r11362, 25;
	shf.l.wrap.b32 	%r11368, %r11362, %r11361, 25;
	mov.b64 	%rd2601, {%r11368, %r11367};
	xor.b64  	%rd2602, %rd2600, %rd2601;
	xor.b64  	%rd2603, %rd2584, %rd2536;
	xor.b64  	%rd2604, %rd2584, %rd2560;
	and.b64  	%rd2605, %rd2604, %rd2603;
	xor.b64  	%rd2606, %rd2605, %rd2584;
	add.s64 	%rd2607, %rd2596, %rd2606;
	add.s64 	%rd2608, %rd2607, %rd2602;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11369,%dummy}, %rd2597;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11370}, %rd2597;
	}
	shf.r.wrap.b32 	%r11371, %r11370, %r11369, 14;
	shf.r.wrap.b32 	%r11372, %r11369, %r11370, 14;
	mov.b64 	%rd2609, {%r11372, %r11371};
	shf.r.wrap.b32 	%r11373, %r11370, %r11369, 18;
	shf.r.wrap.b32 	%r11374, %r11369, %r11370, 18;
	mov.b64 	%rd2610, {%r11374, %r11373};
	xor.b64  	%rd2611, %rd2610, %rd2609;
	shf.l.wrap.b32 	%r11375, %r11369, %r11370, 23;
	shf.l.wrap.b32 	%r11376, %r11370, %r11369, 23;
	mov.b64 	%rd2612, {%r11376, %r11375};
	xor.b64  	%rd2613, %rd2611, %rd2612;
	xor.b64  	%rd2614, %rd2573, %rd2549;
	and.b64  	%rd2615, %rd2597, %rd2614;
	xor.b64  	%rd2616, %rd2615, %rd2549;
	add.s64 	%rd2617, %rd2525, %rd2375;
	add.s64 	%rd2618, %rd2617, %rd2851;
	add.s64 	%rd2619, %rd2618, %rd2616;
	add.s64 	%rd2620, %rd2619, %rd2613;
	add.s64 	%rd2621, %rd2620, %rd2536;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11377,%dummy}, %rd2608;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11378}, %rd2608;
	}
	shf.r.wrap.b32 	%r11379, %r11378, %r11377, 28;
	shf.r.wrap.b32 	%r11380, %r11377, %r11378, 28;
	mov.b64 	%rd2622, {%r11380, %r11379};
	shf.l.wrap.b32 	%r11381, %r11377, %r11378, 30;
	shf.l.wrap.b32 	%r11382, %r11378, %r11377, 30;
	mov.b64 	%rd2623, {%r11382, %r11381};
	xor.b64  	%rd2624, %rd2623, %rd2622;
	shf.l.wrap.b32 	%r11383, %r11377, %r11378, 25;
	shf.l.wrap.b32 	%r11384, %r11378, %r11377, 25;
	mov.b64 	%rd2625, {%r11384, %r11383};
	xor.b64  	%rd2626, %rd2624, %rd2625;
	xor.b64  	%rd2627, %rd2608, %rd2560;
	xor.b64  	%rd2628, %rd2608, %rd2584;
	and.b64  	%rd2629, %rd2628, %rd2627;
	xor.b64  	%rd2630, %rd2629, %rd2608;
	add.s64 	%rd2631, %rd2620, %rd2630;
	add.s64 	%rd2632, %rd2631, %rd2626;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11385,%dummy}, %rd2621;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11386}, %rd2621;
	}
	shf.r.wrap.b32 	%r11387, %r11386, %r11385, 14;
	shf.r.wrap.b32 	%r11388, %r11385, %r11386, 14;
	mov.b64 	%rd2633, {%r11388, %r11387};
	shf.r.wrap.b32 	%r11389, %r11386, %r11385, 18;
	shf.r.wrap.b32 	%r11390, %r11385, %r11386, 18;
	mov.b64 	%rd2634, {%r11390, %r11389};
	xor.b64  	%rd2635, %rd2634, %rd2633;
	shf.l.wrap.b32 	%r11391, %r11385, %r11386, 23;
	shf.l.wrap.b32 	%r11392, %r11386, %r11385, 23;
	mov.b64 	%rd2636, {%r11392, %r11391};
	xor.b64  	%rd2637, %rd2635, %rd2636;
	xor.b64  	%rd2638, %rd2597, %rd2573;
	and.b64  	%rd2639, %rd2621, %rd2638;
	xor.b64  	%rd2640, %rd2639, %rd2573;
	add.s64 	%rd2641, %rd2549, %rd2388;
	add.s64 	%rd2642, %rd2641, %rd2852;
	add.s64 	%rd2643, %rd2642, %rd2640;
	add.s64 	%rd2644, %rd2643, %rd2637;
	add.s64 	%rd2645, %rd2644, %rd2560;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11393,%dummy}, %rd2632;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11394}, %rd2632;
	}
	shf.r.wrap.b32 	%r11395, %r11394, %r11393, 28;
	shf.r.wrap.b32 	%r11396, %r11393, %r11394, 28;
	mov.b64 	%rd2646, {%r11396, %r11395};
	shf.l.wrap.b32 	%r11397, %r11393, %r11394, 30;
	shf.l.wrap.b32 	%r11398, %r11394, %r11393, 30;
	mov.b64 	%rd2647, {%r11398, %r11397};
	xor.b64  	%rd2648, %rd2647, %rd2646;
	shf.l.wrap.b32 	%r11399, %r11393, %r11394, 25;
	shf.l.wrap.b32 	%r11400, %r11394, %r11393, 25;
	mov.b64 	%rd2649, {%r11400, %r11399};
	xor.b64  	%rd2650, %rd2648, %rd2649;
	xor.b64  	%rd2651, %rd2632, %rd2584;
	xor.b64  	%rd2652, %rd2632, %rd2608;
	and.b64  	%rd2653, %rd2652, %rd2651;
	xor.b64  	%rd2654, %rd2653, %rd2632;
	add.s64 	%rd2655, %rd2644, %rd2654;
	add.s64 	%rd2656, %rd2655, %rd2650;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11401,%dummy}, %rd2645;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11402}, %rd2645;
	}
	shf.r.wrap.b32 	%r11403, %r11402, %r11401, 14;
	shf.r.wrap.b32 	%r11404, %r11401, %r11402, 14;
	mov.b64 	%rd2657, {%r11404, %r11403};
	shf.r.wrap.b32 	%r11405, %r11402, %r11401, 18;
	shf.r.wrap.b32 	%r11406, %r11401, %r11402, 18;
	mov.b64 	%rd2658, {%r11406, %r11405};
	xor.b64  	%rd2659, %rd2658, %rd2657;
	shf.l.wrap.b32 	%r11407, %r11401, %r11402, 23;
	shf.l.wrap.b32 	%r11408, %r11402, %r11401, 23;
	mov.b64 	%rd2660, {%r11408, %r11407};
	xor.b64  	%rd2661, %rd2659, %rd2660;
	xor.b64  	%rd2662, %rd2621, %rd2597;
	and.b64  	%rd2663, %rd2645, %rd2662;
	xor.b64  	%rd2664, %rd2663, %rd2597;
	add.s64 	%rd2665, %rd2573, %rd2401;
	add.s64 	%rd2666, %rd2665, %rd2853;
	add.s64 	%rd2667, %rd2666, %rd2664;
	add.s64 	%rd2668, %rd2667, %rd2661;
	add.s64 	%rd2669, %rd2668, %rd2584;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11409,%dummy}, %rd2656;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11410}, %rd2656;
	}
	shf.r.wrap.b32 	%r11411, %r11410, %r11409, 28;
	shf.r.wrap.b32 	%r11412, %r11409, %r11410, 28;
	mov.b64 	%rd2670, {%r11412, %r11411};
	shf.l.wrap.b32 	%r11413, %r11409, %r11410, 30;
	shf.l.wrap.b32 	%r11414, %r11410, %r11409, 30;
	mov.b64 	%rd2671, {%r11414, %r11413};
	xor.b64  	%rd2672, %rd2671, %rd2670;
	shf.l.wrap.b32 	%r11415, %r11409, %r11410, 25;
	shf.l.wrap.b32 	%r11416, %r11410, %r11409, 25;
	mov.b64 	%rd2673, {%r11416, %r11415};
	xor.b64  	%rd2674, %rd2672, %rd2673;
	xor.b64  	%rd2675, %rd2656, %rd2608;
	xor.b64  	%rd2676, %rd2656, %rd2632;
	and.b64  	%rd2677, %rd2676, %rd2675;
	xor.b64  	%rd2678, %rd2677, %rd2656;
	add.s64 	%rd2679, %rd2668, %rd2678;
	add.s64 	%rd2680, %rd2679, %rd2674;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11417,%dummy}, %rd2669;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11418}, %rd2669;
	}
	shf.r.wrap.b32 	%r11419, %r11418, %r11417, 14;
	shf.r.wrap.b32 	%r11420, %r11417, %r11418, 14;
	mov.b64 	%rd2681, {%r11420, %r11419};
	shf.r.wrap.b32 	%r11421, %r11418, %r11417, 18;
	shf.r.wrap.b32 	%r11422, %r11417, %r11418, 18;
	mov.b64 	%rd2682, {%r11422, %r11421};
	xor.b64  	%rd2683, %rd2682, %rd2681;
	shf.l.wrap.b32 	%r11423, %r11417, %r11418, 23;
	shf.l.wrap.b32 	%r11424, %r11418, %r11417, 23;
	mov.b64 	%rd2684, {%r11424, %r11423};
	xor.b64  	%rd2685, %rd2683, %rd2684;
	xor.b64  	%rd2686, %rd2645, %rd2621;
	and.b64  	%rd2687, %rd2669, %rd2686;
	xor.b64  	%rd2688, %rd2687, %rd2621;
	add.s64 	%rd2689, %rd2597, %rd2414;
	add.s64 	%rd2690, %rd2689, %rd2861;
	add.s64 	%rd2691, %rd2690, %rd2688;
	add.s64 	%rd2692, %rd2691, %rd2685;
	add.s64 	%rd2693, %rd2692, %rd2608;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11425,%dummy}, %rd2680;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11426}, %rd2680;
	}
	shf.r.wrap.b32 	%r11427, %r11426, %r11425, 28;
	shf.r.wrap.b32 	%r11428, %r11425, %r11426, 28;
	mov.b64 	%rd2694, {%r11428, %r11427};
	shf.l.wrap.b32 	%r11429, %r11425, %r11426, 30;
	shf.l.wrap.b32 	%r11430, %r11426, %r11425, 30;
	mov.b64 	%rd2695, {%r11430, %r11429};
	xor.b64  	%rd2696, %rd2695, %rd2694;
	shf.l.wrap.b32 	%r11431, %r11425, %r11426, 25;
	shf.l.wrap.b32 	%r11432, %r11426, %r11425, 25;
	mov.b64 	%rd2697, {%r11432, %r11431};
	xor.b64  	%rd2698, %rd2696, %rd2697;
	xor.b64  	%rd2699, %rd2680, %rd2632;
	xor.b64  	%rd2700, %rd2680, %rd2656;
	and.b64  	%rd2701, %rd2700, %rd2699;
	xor.b64  	%rd2702, %rd2701, %rd2680;
	add.s64 	%rd2703, %rd2692, %rd2702;
	add.s64 	%rd2704, %rd2703, %rd2698;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11433,%dummy}, %rd2693;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11434}, %rd2693;
	}
	shf.r.wrap.b32 	%r11435, %r11434, %r11433, 14;
	shf.r.wrap.b32 	%r11436, %r11433, %r11434, 14;
	mov.b64 	%rd2705, {%r11436, %r11435};
	shf.r.wrap.b32 	%r11437, %r11434, %r11433, 18;
	shf.r.wrap.b32 	%r11438, %r11433, %r11434, 18;
	mov.b64 	%rd2706, {%r11438, %r11437};
	xor.b64  	%rd2707, %rd2706, %rd2705;
	shf.l.wrap.b32 	%r11439, %r11433, %r11434, 23;
	shf.l.wrap.b32 	%r11440, %r11434, %r11433, 23;
	mov.b64 	%rd2708, {%r11440, %r11439};
	xor.b64  	%rd2709, %rd2707, %rd2708;
	xor.b64  	%rd2710, %rd2669, %rd2645;
	and.b64  	%rd2711, %rd2693, %rd2710;
	xor.b64  	%rd2712, %rd2711, %rd2645;
	add.s64 	%rd2713, %rd2621, %rd2427;
	add.s64 	%rd2714, %rd2713, %rd2862;
	add.s64 	%rd2715, %rd2714, %rd2712;
	add.s64 	%rd2716, %rd2715, %rd2709;
	add.s64 	%rd2717, %rd2716, %rd2632;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11441,%dummy}, %rd2704;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11442}, %rd2704;
	}
	shf.r.wrap.b32 	%r11443, %r11442, %r11441, 28;
	shf.r.wrap.b32 	%r11444, %r11441, %r11442, 28;
	mov.b64 	%rd2718, {%r11444, %r11443};
	shf.l.wrap.b32 	%r11445, %r11441, %r11442, 30;
	shf.l.wrap.b32 	%r11446, %r11442, %r11441, 30;
	mov.b64 	%rd2719, {%r11446, %r11445};
	xor.b64  	%rd2720, %rd2719, %rd2718;
	shf.l.wrap.b32 	%r11447, %r11441, %r11442, 25;
	shf.l.wrap.b32 	%r11448, %r11442, %r11441, 25;
	mov.b64 	%rd2721, {%r11448, %r11447};
	xor.b64  	%rd2722, %rd2720, %rd2721;
	xor.b64  	%rd2723, %rd2704, %rd2656;
	xor.b64  	%rd2724, %rd2704, %rd2680;
	and.b64  	%rd2725, %rd2724, %rd2723;
	xor.b64  	%rd2726, %rd2725, %rd2704;
	add.s64 	%rd2727, %rd2716, %rd2726;
	add.s64 	%rd2728, %rd2727, %rd2722;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11449,%dummy}, %rd2717;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11450}, %rd2717;
	}
	shf.r.wrap.b32 	%r11451, %r11450, %r11449, 14;
	shf.r.wrap.b32 	%r11452, %r11449, %r11450, 14;
	mov.b64 	%rd2729, {%r11452, %r11451};
	shf.r.wrap.b32 	%r11453, %r11450, %r11449, 18;
	shf.r.wrap.b32 	%r11454, %r11449, %r11450, 18;
	mov.b64 	%rd2730, {%r11454, %r11453};
	xor.b64  	%rd2731, %rd2730, %rd2729;
	shf.l.wrap.b32 	%r11455, %r11449, %r11450, 23;
	shf.l.wrap.b32 	%r11456, %r11450, %r11449, 23;
	mov.b64 	%rd2732, {%r11456, %r11455};
	xor.b64  	%rd2733, %rd2731, %rd2732;
	xor.b64  	%rd2734, %rd2693, %rd2669;
	and.b64  	%rd2735, %rd2717, %rd2734;
	xor.b64  	%rd2736, %rd2735, %rd2669;
	add.s64 	%rd2737, %rd2645, %rd2440;
	add.s64 	%rd2738, %rd2737, %rd2863;
	add.s64 	%rd2739, %rd2738, %rd2736;
	add.s64 	%rd2740, %rd2739, %rd2733;
	add.s64 	%rd2741, %rd2740, %rd2656;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11457,%dummy}, %rd2728;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11458}, %rd2728;
	}
	shf.r.wrap.b32 	%r11459, %r11458, %r11457, 28;
	shf.r.wrap.b32 	%r11460, %r11457, %r11458, 28;
	mov.b64 	%rd2742, {%r11460, %r11459};
	shf.l.wrap.b32 	%r11461, %r11457, %r11458, 30;
	shf.l.wrap.b32 	%r11462, %r11458, %r11457, 30;
	mov.b64 	%rd2743, {%r11462, %r11461};
	xor.b64  	%rd2744, %rd2743, %rd2742;
	shf.l.wrap.b32 	%r11463, %r11457, %r11458, 25;
	shf.l.wrap.b32 	%r11464, %r11458, %r11457, 25;
	mov.b64 	%rd2745, {%r11464, %r11463};
	xor.b64  	%rd2746, %rd2744, %rd2745;
	xor.b64  	%rd2747, %rd2728, %rd2680;
	xor.b64  	%rd2748, %rd2728, %rd2704;
	and.b64  	%rd2749, %rd2748, %rd2747;
	xor.b64  	%rd2750, %rd2749, %rd2728;
	add.s64 	%rd2751, %rd2740, %rd2750;
	add.s64 	%rd2752, %rd2751, %rd2746;
	cvt.u32.u64	%r1721, %rd2741;
	shr.u64 	%rd2753, %rd2741, 32;
	cvt.u32.u64	%r1722, %rd2753;
	cvt.u32.u64	%r1723, %rd2752;
	shr.u64 	%rd2754, %rd2752, 32;
	cvt.u32.u64	%r1724, %rd2754;
	and.b32  	%r11465, %r11534, 31;
	shr.u32 	%r11466, %r1721, %r11465;
	and.b32  	%r11467, %r11466, %r1745;
	mul.wide.u32 	%rd2755, %r11467, 4;
	add.s64 	%rd2756, %rd2864, %rd2755;
	and.b32  	%r11468, %r1721, 31;
	mov.u32 	%r11469, 1;
	shl.b32 	%r1725, %r11469, %r11468;
	ld.global.u32 	%r11470, [%rd2756];
	and.b32  	%r11471, %r11470, %r1725;
	setp.eq.s32	%p815, %r11471, 0;
	@%p815 bra 	BB3_1097;

	mov.u32 	%r11537, 1;
	ld.param.u32 	%r11536, [m01710_m04_param_25];
	and.b32  	%r11535, %r11536, 31;
	ld.param.u64 	%rd2854, [m01710_m04_param_7];
	shr.u32 	%r11473, %r1722, %r11535;
	and.b32  	%r11474, %r11473, %r1745;
	mul.wide.u32 	%rd2757, %r11474, 4;
	add.s64 	%rd2758, %rd2854, %rd2757;
	and.b32  	%r11475, %r1722, 31;
	shl.b32 	%r1726, %r11537, %r11475;
	ld.global.u32 	%r11477, [%rd2758];
	and.b32  	%r11478, %r11477, %r1726;
	setp.eq.s32	%p816, %r11478, 0;
	@%p816 bra 	BB3_1097;

	mov.u32 	%r11540, 1;
	ld.param.u32 	%r11539, [m01710_m04_param_25];
	and.b32  	%r11538, %r11539, 31;
	ld.param.u64 	%rd2855, [m01710_m04_param_8];
	shr.u32 	%r11480, %r1723, %r11538;
	and.b32  	%r11481, %r11480, %r1745;
	mul.wide.u32 	%rd2759, %r11481, 4;
	add.s64 	%rd2760, %rd2855, %rd2759;
	and.b32  	%r11482, %r1723, 31;
	shl.b32 	%r1727, %r11540, %r11482;
	ld.global.u32 	%r11484, [%rd2760];
	and.b32  	%r11485, %r11484, %r1727;
	setp.eq.s32	%p817, %r11485, 0;
	@%p817 bra 	BB3_1097;

	mov.u32 	%r11543, 1;
	ld.param.u32 	%r11542, [m01710_m04_param_25];
	and.b32  	%r11541, %r11542, 31;
	ld.param.u64 	%rd2856, [m01710_m04_param_9];
	shr.u32 	%r11487, %r1724, %r11541;
	and.b32  	%r11488, %r11487, %r1745;
	mul.wide.u32 	%rd2761, %r11488, 4;
	add.s64 	%rd2762, %rd2856, %rd2761;
	and.b32  	%r11489, %r1724, 31;
	shl.b32 	%r1728, %r11543, %r11489;
	ld.global.u32 	%r11491, [%rd2762];
	and.b32  	%r11492, %r11491, %r1728;
	setp.eq.s32	%p818, %r11492, 0;
	@%p818 bra 	BB3_1097;

	and.b32  	%r11546, %r1721, 31;
	mov.u32 	%r11545, 1;
	shl.b32 	%r11544, %r11545, %r11546;
	ld.param.u64 	%rd2857, [m01710_m04_param_10];
	ld.param.u32 	%r11533, [m01710_m04_param_26];
	and.b32  	%r11493, %r11533, 31;
	shr.u32 	%r11494, %r1721, %r11493;
	and.b32  	%r11495, %r11494, %r1745;
	mul.wide.u32 	%rd2763, %r11495, 4;
	add.s64 	%rd2764, %rd2857, %rd2763;
	ld.global.u32 	%r11496, [%rd2764];
	and.b32  	%r11497, %r11496, %r11544;
	setp.eq.s32	%p819, %r11497, 0;
	@%p819 bra 	BB3_1097;

	ld.param.u64 	%rd2858, [m01710_m04_param_11];
	shr.u32 	%r11499, %r1722, %r11493;
	and.b32  	%r11500, %r11499, %r1745;
	mul.wide.u32 	%rd2765, %r11500, 4;
	add.s64 	%rd2766, %rd2858, %rd2765;
	ld.global.u32 	%r11501, [%rd2766];
	and.b32  	%r11502, %r11501, %r1726;
	setp.eq.s32	%p820, %r11502, 0;
	@%p820 bra 	BB3_1097;

	ld.param.u64 	%rd2859, [m01710_m04_param_12];
	shr.u32 	%r11504, %r1723, %r11493;
	and.b32  	%r11505, %r11504, %r1745;
	mul.wide.u32 	%rd2767, %r11505, 4;
	add.s64 	%rd2768, %rd2859, %rd2767;
	ld.global.u32 	%r11506, [%rd2768];
	and.b32  	%r11507, %r11506, %r1727;
	setp.eq.s32	%p821, %r11507, 0;
	@%p821 bra 	BB3_1097;

	ld.param.u64 	%rd2860, [m01710_m04_param_13];
	shr.u32 	%r11509, %r1724, %r11493;
	and.b32  	%r11510, %r11509, %r1745;
	mul.wide.u32 	%rd2769, %r11510, 4;
	add.s64 	%rd2770, %rd2860, %rd2769;
	ld.global.u32 	%r11511, [%rd2770];
	and.b32  	%r11512, %r11511, %r1728;
	setp.eq.s32	%p822, %r11512, 0;
	@%p822 bra 	BB3_1097;

	mov.u32 	%r11768, 0;
	setp.eq.s32	%p823, %r1750, 0;
	mov.u32 	%r11513, -1;
	mov.u32 	%r11767, %r1750;
	@%p823 bra 	BB3_1091;

BB3_1079:
	ld.param.u64 	%rd2865, [m01710_m04_param_15];
	mov.u32 	%r11769, 1;
	shr.u32 	%r1731, %r11767, 1;
	add.s32 	%r11770, %r1731, %r11768;
	cvt.u64.u32	%rd2771, %r11770;
	cvt.u64.u32	%rd2772, %r1751;
	add.s64 	%rd2773, %rd2771, %rd2772;
	shl.b64 	%rd2774, %rd2773, 6;
	add.s64 	%rd91, %rd2865, %rd2774;
	ld.global.u32 	%r1733, [%rd91+28];
	setp.gt.u32	%p824, %r1724, %r1733;
	@%p824 bra 	BB3_1089;

	setp.lt.u32	%p825, %r1724, %r1733;
	mov.u32 	%r11516, -1;
	@%p825 bra 	BB3_1081;
	bra.uni 	BB3_1082;

BB3_1081:
	mov.u32 	%r11769, %r11516;
	bra.uni 	BB3_1089;

BB3_1082:
	mov.u32 	%r11769, 1;
	ld.global.u32 	%r1734, [%rd91+24];
	setp.gt.u32	%p826, %r1723, %r1734;
	@%p826 bra 	BB3_1089;

	setp.lt.u32	%p827, %r1723, %r1734;
	@%p827 bra 	BB3_1084;
	bra.uni 	BB3_1085;

BB3_1084:
	mov.u32 	%r11769, %r11516;
	bra.uni 	BB3_1089;

BB3_1085:
	mov.u32 	%r11769, 1;
	ld.global.u32 	%r1735, [%rd91+60];
	setp.gt.u32	%p828, %r1722, %r1735;
	@%p828 bra 	BB3_1089;

	setp.lt.u32	%p829, %r1722, %r1735;
	mov.u32 	%r11769, %r11516;
	@%p829 bra 	BB3_1089;

	mov.u32 	%r11769, 1;
	ld.global.u32 	%r1736, [%rd91+56];
	setp.gt.u32	%p830, %r1721, %r1736;
	@%p830 bra 	BB3_1089;

	setp.lt.u32	%p831, %r1721, %r1736;
	selp.b32	%r11769, -1, 0, %p831;

BB3_1089:
	add.s32 	%r11522, %r1731, 1;
	setp.gt.s32	%p832, %r11769, 0;
	selp.b32	%r11523, %r11522, 0, %p832;
	add.s32 	%r11768, %r11523, %r11768;
	selp.b32	%r11524, -1, 0, %p832;
	add.s32 	%r11525, %r11524, %r11767;
	shr.u32 	%r11767, %r11525, 1;
	setp.eq.s32	%p833, %r11769, 0;
	@%p833 bra 	BB3_1092;

	setp.ne.s32	%p834, %r11767, 0;
	@%p834 bra 	BB3_1079;

BB3_1091:
	mov.u32 	%r11770, %r11513;

BB3_1092:
	setp.eq.s32	%p835, %r11770, -1;
	@%p835 bra 	BB3_1097;

	ld.param.u64 	%rd2866, [m01710_m04_param_16];
	add.s32 	%r1742, %r11770, %r1751;
	mul.wide.u32 	%rd2775, %r1742, 4;
	add.s64 	%rd2776, %rd2866, %rd2775;
	atom.global.add.u32 	%r11527, [%rd2776], 1;
	setp.ne.s32	%p836, %r11527, 0;
	@%p836 bra 	BB3_1097;

	atom.global.add.u32 	%r1743, [%rd106], 1;
	setp.lt.u32	%p837, %r1743, %r1750;
	@%p837 bra 	BB3_1096;
	bra.uni 	BB3_1095;

BB3_1096:
	ld.param.u64 	%rd2867, [m01710_m04_param_14];
	ld.param.u32 	%r11532, [m01710_m04_param_27];
	mul.wide.u32 	%rd2777, %r1743, 32;
	add.s64 	%rd2778, %rd2867, %rd2777;
	st.global.v2.u32 	[%rd2778], {%r11532, %r11770};
	st.global.u32 	[%rd2778+8], %r1742;
	st.global.u32 	[%rd2778+24], %r11551;
	st.global.u64 	[%rd2778+16], %rd21;
	bra.uni 	BB3_1097;

BB3_1095:
	atom.global.add.u32 	%r11528, [%rd106], -1;

BB3_1097:
	ld.param.u32 	%r11530, [m01710_m04_param_30];
	add.s32 	%r11551, %r11551, 1;
	setp.lt.u32	%p838, %r11551, %r11530;
	@%p838 bra 	BB3_3;

BB3_1098:
	ret;
}

	// .globl	m01710_m08
.entry m01710_m08(
	.param .u64 .ptr .global .align 4 m01710_m08_param_0,
	.param .u64 .ptr .const .align 4 m01710_m08_param_1,
	.param .u64 .ptr .global .align 4 m01710_m08_param_2,
	.param .u64 .ptr .global .align 4 m01710_m08_param_3,
	.param .u64 .ptr .global .align 1 m01710_m08_param_4,
	.param .u64 .ptr .global .align 1 m01710_m08_param_5,
	.param .u64 .ptr .global .align 4 m01710_m08_param_6,
	.param .u64 .ptr .global .align 4 m01710_m08_param_7,
	.param .u64 .ptr .global .align 4 m01710_m08_param_8,
	.param .u64 .ptr .global .align 4 m01710_m08_param_9,
	.param .u64 .ptr .global .align 4 m01710_m08_param_10,
	.param .u64 .ptr .global .align 4 m01710_m08_param_11,
	.param .u64 .ptr .global .align 4 m01710_m08_param_12,
	.param .u64 .ptr .global .align 4 m01710_m08_param_13,
	.param .u64 .ptr .global .align 8 m01710_m08_param_14,
	.param .u64 .ptr .global .align 4 m01710_m08_param_15,
	.param .u64 .ptr .global .align 4 m01710_m08_param_16,
	.param .u64 .ptr .global .align 4 m01710_m08_param_17,
	.param .u64 .ptr .global .align 1 m01710_m08_param_18,
	.param .u64 .ptr .global .align 4 m01710_m08_param_19,
	.param .u64 .ptr .global .align 4 m01710_m08_param_20,
	.param .u64 .ptr .global .align 4 m01710_m08_param_21,
	.param .u64 .ptr .global .align 4 m01710_m08_param_22,
	.param .u64 .ptr .global .align 4 m01710_m08_param_23,
	.param .u32 m01710_m08_param_24,
	.param .u32 m01710_m08_param_25,
	.param .u32 m01710_m08_param_26,
	.param .u32 m01710_m08_param_27,
	.param .u32 m01710_m08_param_28,
	.param .u32 m01710_m08_param_29,
	.param .u32 m01710_m08_param_30,
	.param .u32 m01710_m08_param_31,
	.param .u32 m01710_m08_param_32,
	.param .u32 m01710_m08_param_33,
	.param .u64 m01710_m08_param_34
)
{



	ret;
}

	// .globl	m01710_m16
.entry m01710_m16(
	.param .u64 .ptr .global .align 4 m01710_m16_param_0,
	.param .u64 .ptr .const .align 4 m01710_m16_param_1,
	.param .u64 .ptr .global .align 4 m01710_m16_param_2,
	.param .u64 .ptr .global .align 4 m01710_m16_param_3,
	.param .u64 .ptr .global .align 1 m01710_m16_param_4,
	.param .u64 .ptr .global .align 1 m01710_m16_param_5,
	.param .u64 .ptr .global .align 4 m01710_m16_param_6,
	.param .u64 .ptr .global .align 4 m01710_m16_param_7,
	.param .u64 .ptr .global .align 4 m01710_m16_param_8,
	.param .u64 .ptr .global .align 4 m01710_m16_param_9,
	.param .u64 .ptr .global .align 4 m01710_m16_param_10,
	.param .u64 .ptr .global .align 4 m01710_m16_param_11,
	.param .u64 .ptr .global .align 4 m01710_m16_param_12,
	.param .u64 .ptr .global .align 4 m01710_m16_param_13,
	.param .u64 .ptr .global .align 8 m01710_m16_param_14,
	.param .u64 .ptr .global .align 4 m01710_m16_param_15,
	.param .u64 .ptr .global .align 4 m01710_m16_param_16,
	.param .u64 .ptr .global .align 4 m01710_m16_param_17,
	.param .u64 .ptr .global .align 1 m01710_m16_param_18,
	.param .u64 .ptr .global .align 4 m01710_m16_param_19,
	.param .u64 .ptr .global .align 4 m01710_m16_param_20,
	.param .u64 .ptr .global .align 4 m01710_m16_param_21,
	.param .u64 .ptr .global .align 4 m01710_m16_param_22,
	.param .u64 .ptr .global .align 4 m01710_m16_param_23,
	.param .u32 m01710_m16_param_24,
	.param .u32 m01710_m16_param_25,
	.param .u32 m01710_m16_param_26,
	.param .u32 m01710_m16_param_27,
	.param .u32 m01710_m16_param_28,
	.param .u32 m01710_m16_param_29,
	.param .u32 m01710_m16_param_30,
	.param .u32 m01710_m16_param_31,
	.param .u32 m01710_m16_param_32,
	.param .u32 m01710_m16_param_33,
	.param .u64 m01710_m16_param_34
)
{



	ret;
}

	// .globl	m01710_s04
.entry m01710_s04(
	.param .u64 .ptr .global .align 4 m01710_s04_param_0,
	.param .u64 .ptr .const .align 4 m01710_s04_param_1,
	.param .u64 .ptr .global .align 4 m01710_s04_param_2,
	.param .u64 .ptr .global .align 4 m01710_s04_param_3,
	.param .u64 .ptr .global .align 1 m01710_s04_param_4,
	.param .u64 .ptr .global .align 1 m01710_s04_param_5,
	.param .u64 .ptr .global .align 4 m01710_s04_param_6,
	.param .u64 .ptr .global .align 4 m01710_s04_param_7,
	.param .u64 .ptr .global .align 4 m01710_s04_param_8,
	.param .u64 .ptr .global .align 4 m01710_s04_param_9,
	.param .u64 .ptr .global .align 4 m01710_s04_param_10,
	.param .u64 .ptr .global .align 4 m01710_s04_param_11,
	.param .u64 .ptr .global .align 4 m01710_s04_param_12,
	.param .u64 .ptr .global .align 4 m01710_s04_param_13,
	.param .u64 .ptr .global .align 8 m01710_s04_param_14,
	.param .u64 .ptr .global .align 4 m01710_s04_param_15,
	.param .u64 .ptr .global .align 4 m01710_s04_param_16,
	.param .u64 .ptr .global .align 4 m01710_s04_param_17,
	.param .u64 .ptr .global .align 1 m01710_s04_param_18,
	.param .u64 .ptr .global .align 4 m01710_s04_param_19,
	.param .u64 .ptr .global .align 4 m01710_s04_param_20,
	.param .u64 .ptr .global .align 4 m01710_s04_param_21,
	.param .u64 .ptr .global .align 4 m01710_s04_param_22,
	.param .u64 .ptr .global .align 4 m01710_s04_param_23,
	.param .u32 m01710_s04_param_24,
	.param .u32 m01710_s04_param_25,
	.param .u32 m01710_s04_param_26,
	.param .u32 m01710_s04_param_27,
	.param .u32 m01710_s04_param_28,
	.param .u32 m01710_s04_param_29,
	.param .u32 m01710_s04_param_30,
	.param .u32 m01710_s04_param_31,
	.param .u32 m01710_s04_param_32,
	.param .u32 m01710_s04_param_33,
	.param .u64 m01710_s04_param_34
)
{
	.local .align 16 .b8 	__local_depot6[64];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<824>;
	.reg .b16 	%rs<24>;
	.reg .b32 	%r<11672>;
	.reg .b64 	%rd<2832>;


	mov.u64 	%SPL, __local_depot6;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd93, [m01710_s04_param_0];
	ld.param.u64 	%rd94, [m01710_s04_param_1];
	ld.param.u64 	%rd96, [m01710_s04_param_15];
	ld.param.u64 	%rd98, [m01710_s04_param_17];
	ld.param.u64 	%rd99, [m01710_s04_param_19];
	ld.param.u32 	%r1727, [m01710_s04_param_27];
	ld.param.u32 	%r1728, [m01710_s04_param_30];
	ld.param.u32 	%r1730, [m01710_s04_param_32];
	ld.param.u64 	%rd100, [m01710_s04_param_34];
	mov.u32 	%r1731, %ctaid.x;
	mov.u32 	%r1732, %ntid.x;
	mov.b32	%r1733, %envreg3;
	mad.lo.s32 	%r1734, %r1731, %r1732, %r1733;
	mov.u32 	%r1735, %tid.x;
	add.s32 	%r1, %r1734, %r1735;
	cvt.s64.s32	%rd1, %r1;
	setp.ge.u64	%p1, %rd1, %rd100;
	@%p1 bra 	BB6_1080;

	mul.wide.s32 	%rd101, %r1, 260;
	add.s64 	%rd102, %rd93, %rd101;
	ld.global.u32 	%r2, [%rd102];
	ld.global.u32 	%r3, [%rd102+4];
	ld.global.u32 	%r4, [%rd102+8];
	ld.global.u32 	%r5, [%rd102+12];
	ld.global.u32 	%r6, [%rd102+16];
	ld.global.u32 	%r7, [%rd102+20];
	ld.global.u32 	%r8, [%rd102+24];
	ld.global.u32 	%r9, [%rd102+28];
	ld.global.u32 	%r10, [%rd102+256];
	mul.wide.u32 	%rd103, %r1727, 564;
	add.s64 	%rd2, %rd98, %rd103;
	ld.global.u32 	%r11, [%rd2];
	ld.global.u32 	%r12, [%rd2+4];
	ld.global.u32 	%r13, [%rd2+8];
	ld.global.u32 	%r14, [%rd2+12];
	ld.global.u32 	%r15, [%rd2+16];
	ld.global.u32 	%r16, [%rd2+20];
	ld.global.u32 	%r17, [%rd2+24];
	ld.global.u32 	%r18, [%rd2+28];
	ld.global.u32 	%r19, [%rd2+32];
	ld.global.u32 	%r20, [%rd2+36];
	ld.global.u32 	%r21, [%rd2+40];
	ld.global.u32 	%r22, [%rd2+44];
	ld.global.u32 	%r23, [%rd2+48];
	ld.global.u32 	%r24, [%rd2+52];
	setp.eq.s32	%p2, %r1728, 0;
	@%p2 bra 	BB6_1080;

	ld.global.u32 	%r25, [%rd2+512];
	mul.wide.u32 	%rd104, %r1730, 64;
	add.s64 	%rd105, %rd96, %rd104;
	ld.global.u32 	%r26, [%rd105+56];
	ld.global.u32 	%r27, [%rd105+60];
	ld.global.u32 	%r28, [%rd105+24];
	ld.global.u32 	%r29, [%rd105+28];
	mov.u64 	%rd106, 5840696475078001361;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1738,%dummy}, %rd106;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1739}, %rd106;
	}
	shf.r.wrap.b32 	%r1740, %r1739, %r1738, 18;
	shf.r.wrap.b32 	%r1741, %r1738, %r1739, 18;
	mov.b64 	%rd107, {%r1741, %r1740};
	shf.r.wrap.b32 	%r1742, %r1739, %r1738, 14;
	shf.r.wrap.b32 	%r1743, %r1738, %r1739, 14;
	mov.b64 	%rd108, {%r1743, %r1742};
	xor.b64  	%rd109, %rd107, %rd108;
	shf.l.wrap.b32 	%r1744, %r1738, %r1739, 23;
	shf.l.wrap.b32 	%r1745, %r1739, %r1738, 23;
	mov.b64 	%rd110, {%r1745, %r1744};
	xor.b64  	%rd4, %rd109, %rd110;
	mov.u64 	%rd111, 7640891576956012808;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1746}, %rd111;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1747,%dummy}, %rd111;
	}
	shf.l.wrap.b32 	%r1748, %r1747, %r1746, 30;
	shf.l.wrap.b32 	%r1749, %r1746, %r1747, 30;
	mov.b64 	%rd112, {%r1749, %r1748};
	shf.r.wrap.b32 	%r1750, %r1746, %r1747, 28;
	shf.r.wrap.b32 	%r1751, %r1747, %r1746, 28;
	mov.b64 	%rd113, {%r1751, %r1750};
	xor.b64  	%rd114, %rd112, %rd113;
	shf.l.wrap.b32 	%r1752, %r1747, %r1746, 25;
	shf.l.wrap.b32 	%r1753, %r1746, %r1747, 25;
	mov.b64 	%rd115, {%r1753, %r1752};
	xor.b64  	%rd5, %rd114, %rd115;
	and.b64  	%rd21, %rd1, 4294967295;
	mov.u64 	%rd116, 0;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r1754}, %rd116;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r1755,%dummy}, %rd116;
	}
	shf.l.wrap.b32 	%r1756, %r1755, %r1754, 3;
	shf.l.wrap.b32 	%r1757, %r1754, %r1755, 3;
	mov.b64 	%rd117, {%r1757, %r1756};
	shf.r.wrap.b32 	%r1758, %r1754, %r1755, 19;
	shf.r.wrap.b32 	%r1759, %r1755, %r1754, 19;
	mov.b64 	%rd118, {%r1759, %r1758};
	xor.b64  	%rd22, %rd118, %rd117;
	shf.r.wrap.b32 	%r1760, %r1754, %r1755, 8;
	shf.r.wrap.b32 	%r1761, %r1755, %r1754, 8;
	mov.b64 	%rd119, {%r1761, %r1760};
	shf.r.wrap.b32 	%r1762, %r1754, %r1755, 1;
	shf.r.wrap.b32 	%r1763, %r1755, %r1754, 1;
	mov.b64 	%rd120, {%r1763, %r1762};
	xor.b64  	%rd23, %rd120, %rd119;
	mov.u32 	%r11456, 0;

BB6_3:
	mov.u32 	%r11466, 0;
	cvt.u64.u32	%rd85, %r11456;
	mul.wide.u32 	%rd121, %r11456, 128;
	add.s64 	%rd122, %rd94, %rd121;
	ld.const.u32 	%r11465, [%rd122];
	setp.eq.s32	%p3, %r11465, 0;
	mov.u32 	%r11657, %r10;
	mov.u32 	%r11648, %r6;
	mov.u32 	%r11647, %r7;
	mov.u32 	%r11646, %r8;
	mov.u32 	%r11645, %r9;
	mov.u32 	%r11644, %r2;
	mov.u32 	%r11643, %r3;
	mov.u32 	%r11642, %r4;
	mov.u32 	%r11641, %r5;
	@%p3 bra 	BB6_1031;
	bra.uni 	BB6_4;

BB6_374:
	setp.gt.s32	%p284, %r60, 23;
	@%p284 bra 	BB6_391;

	setp.gt.s32	%p296, %r60, 19;
	@%p296 bra 	BB6_383;

	setp.gt.s32	%p302, %r60, 17;
	@%p302 bra 	BB6_380;

	setp.eq.s32	%p305, %r60, 16;
	@%p305 bra 	BB6_415;
	bra.uni 	BB6_378;

BB6_415:
	mov.u32 	%r11527, 0;
	mov.u32 	%r11528, %r11527;
	mov.u32 	%r11529, %r11527;
	mov.u32 	%r11530, %r11527;
	mov.u32 	%r54, %r58;
	mov.u32 	%r53, %r57;
	mov.u32 	%r52, %r56;
	bra.uni 	BB6_429;

BB6_510:
	setp.eq.s32	%p481, %r5336, 2;
	@%p481 bra 	BB6_525;
	bra.uni 	BB6_511;

BB6_525:
	and.b32  	%r5357, %r728, %r57;
	or.b32  	%r5358, %r5357, %r727;
	and.b32  	%r5359, %r11655, %r729;
	or.b32  	%r11642, %r5358, %r5359;
	mov.u32 	%r11641, %r11656;
	bra.uni 	BB6_513;

BB6_655:
	setp.gt.s32	%p574, %r60, 11;
	@%p574 bra 	BB6_663;

	setp.gt.s32	%p580, %r60, 9;
	@%p580 bra 	BB6_660;

	setp.eq.s32	%p583, %r60, 8;
	@%p583 bra 	BB6_715;
	bra.uni 	BB6_658;

BB6_715:
	mov.u32 	%r11645, 0;
	mov.u32 	%r11641, %r52;
	mov.u32 	%r11642, %r51;
	mov.u32 	%r11643, %r58;
	mov.u32 	%r58, %r57;
	mov.u32 	%r11646, %r11645;
	mov.u32 	%r11647, %r54;
	mov.u32 	%r54, %r53;
	bra.uni 	BB6_723;

BB6_486:
	setp.gt.s32	%p460, %r5254, 5;
	@%p460 bra 	BB6_490;

	setp.eq.s32	%p463, %r5254, 4;
	@%p463 bra 	BB6_498;
	bra.uni 	BB6_488;

BB6_498:
	and.b32  	%r11648, %r701, %r51;
	mov.u32 	%r11645, 0;
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11646, %r11645;
	mov.u32 	%r11647, %r11645;
	mov.u32 	%r11657, %r60;
	bra.uni 	BB6_1030;

BB6_953:
	setp.gt.s32	%p770, %r7992, 11;
	@%p770 bra 	BB6_961;

	setp.gt.s32	%p776, %r7992, 9;
	@%p776 bra 	BB6_958;

	setp.eq.s32	%p779, %r7992, 8;
	@%p779 bra 	BB6_1014;
	bra.uni 	BB6_956;

BB6_1014:
	mov.u32 	%r11651, 0;
	mov.u32 	%r11649, %r56;
	mov.u32 	%r11650, %r55;
	mov.u32 	%r11652, %r11651;
	mov.u32 	%r11653, %r52;
	mov.u32 	%r11654, %r51;
	mov.u32 	%r11655, %r58;
	mov.u32 	%r11656, %r57;
	bra.uni 	BB6_1023;

BB6_149:
	setp.gt.s32	%p131, %r60, 23;
	@%p131 bra 	BB6_165;

	setp.gt.s32	%p143, %r60, 19;
	@%p143 bra 	BB6_158;

	setp.gt.s32	%p149, %r60, 17;
	@%p149 bra 	BB6_155;

	setp.eq.s32	%p152, %r60, 16;
	@%p152 bra 	BB6_190;
	bra.uni 	BB6_153;

BB6_190:
	mov.u32 	%r11653, 0;
	mov.u32 	%r11649, %r55;
	mov.u32 	%r11650, %r56;
	mov.u32 	%r11651, %r57;
	mov.u32 	%r11652, %r58;
	bra.uni 	BB6_191;

BB6_226:
	setp.gt.s32	%p192, %r3305, 5;
	@%p192 bra 	BB6_230;

	setp.eq.s32	%p195, %r3305, 4;
	@%p195 bra 	BB6_235;
	bra.uni 	BB6_228;

BB6_235:
	and.b32  	%r11649, %r252, %r51;
	mov.u32 	%r11650, 0;
	mov.u32 	%r11651, %r11650;
	mov.u32 	%r11652, %r11650;
	bra.uni 	BB6_233;

BB6_520:
	setp.eq.s32	%p476, %r5336, 6;
	@%p476 bra 	BB6_523;
	bra.uni 	BB6_521;

BB6_523:
	and.b32  	%r5345, %r728, %r53;
	or.b32  	%r5346, %r5345, %r727;
	and.b32  	%r5347, %r11651, %r729;
	or.b32  	%r11646, %r5346, %r5347;
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11645, %r11652;
	bra.uni 	BB6_115;

BB6_686:
	setp.gt.s32	%p551, %r60, 27;
	@%p551 bra 	BB6_695;

	setp.gt.s32	%p557, %r60, 25;
	@%p557 bra 	BB6_691;

	setp.eq.s32	%p560, %r60, 24;
	@%p560 bra 	BB6_704;
	bra.uni 	BB6_689;

BB6_704:
	mov.u32 	%r11641, 0;
	mov.u32 	%r11642, %r11641;
	mov.u32 	%r11643, %r54;
	mov.u32 	%r58, %r53;
	bra.uni 	BB6_722;

BB6_986:
	setp.gt.s32	%p747, %r7992, 27;
	@%p747 bra 	BB6_995;

	setp.gt.s32	%p753, %r7992, 25;
	@%p753 bra 	BB6_991;

	setp.eq.s32	%p756, %r7992, 24;
	@%p756 bra 	BB6_1004;
	bra.uni 	BB6_989;

BB6_1004:
	mov.u32 	%r11649, 0;
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;
	mov.u32 	%r11653, %r56;
	mov.u32 	%r11654, %r55;
	bra.uni 	BB6_1021;

BB6_545:
	setp.gt.s32	%p513, %r62, 11;
	@%p513 bra 	BB6_553;

	setp.gt.s32	%p519, %r62, 9;
	@%p519 bra 	BB6_550;

	setp.eq.s32	%p522, %r62, 8;
	@%p522 bra 	BB6_609;
	bra.uni 	BB6_548;

BB6_609:
	mov.u32 	%r11649, %r53;
	mov.u32 	%r11650, %r54;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r57;
	mov.u32 	%r11654, %r58;
	mov.u32 	%r11655, %r51;
	mov.u32 	%r11656, %r52;
	bra.uni 	BB6_614;

BB6_929:
	setp.eq.s32	%p741, %r7813, 2;
	@%p741 bra 	BB6_941;
	bra.uni 	BB6_930;

BB6_941:
	// inline asm
	prmt.b32 %r11632, %r51, %r52, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11631, %r58, %r51, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11630, %r57, %r58, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11629, %r56, %r57, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11628, %r55, %r56, %r1344;
	// inline asm
	mov.u32 	%r11625, 0;
	// inline asm
	prmt.b32 %r11627, %r11625, %r55, %r1344;
	// inline asm
	mov.u32 	%r11626, %r11625;
	bra.uni 	BB6_943;

BB6_829:
	setp.gt.s32	%p662, %r6825, 23;
	@%p662 bra 	BB6_846;

	setp.gt.s32	%p674, %r6825, 19;
	@%p674 bra 	BB6_839;

	setp.gt.s32	%p680, %r6825, 17;
	@%p680 bra 	BB6_835;

	setp.eq.s32	%p683, %r6825, 16;
	@%p683 bra 	BB6_872;
	bra.uni 	BB6_833;

BB6_872:
	mov.u32 	%r11656, %r55;
	mov.u32 	%r11655, %r56;
	mov.u32 	%r11654, %r57;
	mov.u32 	%r11653, %r58;
	bra.uni 	BB6_866;

BB6_753:
	setp.gt.s32	%p607, %r6550, 5;
	@%p607 bra 	BB6_759;

	setp.eq.s32	%p610, %r6550, 4;
	@%p610 bra 	BB6_766;
	bra.uni 	BB6_755;

BB6_766:
	and.b32  	%r6561, %r1006, %r51;
	and.b32  	%r6562, %r11649, %r1007;
	or.b32  	%r11648, %r6562, %r6561;
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11645, %r11652;
	mov.u32 	%r11646, %r11651;
	mov.u32 	%r11647, %r11650;
	bra.uni 	BB6_758;

BB6_359:
	setp.gt.s32	%p308, %r60, 11;
	@%p308 bra 	BB6_367;

	setp.gt.s32	%p314, %r60, 9;
	@%p314 bra 	BB6_364;

	setp.eq.s32	%p317, %r60, 8;
	@%p317 bra 	BB6_419;
	bra.uni 	BB6_362;

BB6_419:
	mov.u32 	%r11529, 0;
	mov.u32 	%r11527, %r56;
	mov.u32 	%r11528, %r55;
	mov.u32 	%r11530, %r11529;
	mov.u32 	%r54, %r52;
	mov.u32 	%r53, %r51;
	mov.u32 	%r52, %r58;
	mov.u32 	%r55, %r57;
	bra.uni 	BB6_429;

BB6_772:
	setp.ne.s32	%p633, %r6621, 1;
	mov.u32 	%r11590, %r6620;
	@%p633 bra 	BB6_775;

	and.b32  	%r6627, %r11653, %r51;
	and.b32  	%r6628, %r11654, %r52;
	or.b32  	%r6629, %r6627, %r6628;
	and.b32  	%r6630, %r11655, %r53;
	or.b32  	%r6631, %r6629, %r6630;
	and.b32  	%r6632, %r11656, %r54;
	or.b32  	%r11590, %r6631, %r6632;

BB6_775:
	shr.u32 	%r6672, %r11590, %r1043;
	and.b32  	%r6673, %r6672, 255;
	mov.u32 	%r6670, 24;
	// inline asm
	shf.r.wrap.b32 %r11645, %r53, %r54, %r6670;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11646, %r52, %r53, %r6670;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11647, %r51, %r52, %r6670;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11648, %r58, %r51, %r6670;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11641, %r57, %r58, %r6670;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11642, %r56, %r57, %r6670;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r55, %r56, %r6670;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6667, %r6620, %r55, %r6670;
	// inline asm
	or.b32  	%r11644, %r6667, %r6673;
	and.b32  	%r6674, %r42, 3;
	shl.b32 	%r6675, %r6674, 3;
	mov.u32 	%r6676, 1;
	shl.b32 	%r6677, %r6676, %r6675;
	add.s32 	%r1059, %r6677, -1;
	shr.u32 	%r6671, %r42, 2;
	setp.gt.s32	%p634, %r6671, 3;
	@%p634 bra 	BB6_783;

	setp.gt.s32	%p640, %r6671, 1;
	@%p640 bra 	BB6_780;

	setp.eq.s32	%p643, %r6671, 0;
	@%p643 bra 	BB6_794;
	bra.uni 	BB6_778;

BB6_794:
	and.b32  	%r11644, %r11644, %r1059;
	mov.u32 	%r11641, 0;
	mov.u32 	%r11642, %r11641;
	mov.u32 	%r11643, %r11641;
	bra.uni 	BB6_793;

BB6_579:
	setp.gt.s32	%p490, %r62, 27;
	@%p490 bra 	BB6_589;

	setp.gt.s32	%p496, %r62, 25;
	@%p496 bra 	BB6_584;

	setp.eq.s32	%p499, %r62, 24;
	@%p499 bra 	BB6_601;
	bra.uni 	BB6_582;

BB6_601:
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r53;
	mov.u32 	%r11654, %r54;
	bra.uni 	BB6_599;

BB6_936:
	setp.eq.s32	%p736, %r7813, 6;
	@%p736 bra 	BB6_939;
	bra.uni 	BB6_937;

BB6_939:
	// inline asm
	prmt.b32 %r11632, %r55, %r56, %r1344;
	// inline asm
	mov.u32 	%r11625, 0;
	// inline asm
	prmt.b32 %r11631, %r11625, %r55, %r1344;
	// inline asm
	mov.u32 	%r11626, %r11625;
	mov.u32 	%r11627, %r11625;
	mov.u32 	%r11628, %r11625;
	mov.u32 	%r11629, %r11625;
	mov.u32 	%r11630, %r11625;
	bra.uni 	BB6_943;

BB6_391:
	setp.gt.s32	%p285, %r60, 27;
	@%p285 bra 	BB6_400;

	setp.gt.s32	%p291, %r60, 25;
	@%p291 bra 	BB6_396;

	setp.eq.s32	%p294, %r60, 24;
	@%p294 bra 	BB6_409;
	bra.uni 	BB6_394;

BB6_409:
	mov.u32 	%r11527, 0;
	mov.u32 	%r11528, %r11527;
	mov.u32 	%r11529, %r11527;
	mov.u32 	%r11530, %r11527;
	mov.u32 	%r54, %r56;
	mov.u32 	%r53, %r55;
	bra.uni 	BB6_427;

BB6_783:
	setp.gt.s32	%p635, %r6671, 5;
	@%p635 bra 	BB6_787;

	setp.eq.s32	%p638, %r6671, 4;
	@%p638 bra 	BB6_791;
	bra.uni 	BB6_785;

BB6_791:
	and.b32  	%r11648, %r11648, %r1059;
	mov.u32 	%r11645, 0;
	mov.u32 	%r11646, %r11645;
	mov.u32 	%r11647, %r11645;
	bra.uni 	BB6_1029;

BB6_508:
	setp.eq.s32	%p484, %r5336, 1;
	@%p484 bra 	BB6_509;
	bra.uni 	BB6_113;

BB6_509:
	and.b32  	%r5360, %r728, %r56;
	or.b32  	%r5361, %r5360, %r727;
	and.b32  	%r5362, %r11654, %r729;
	or.b32  	%r11643, %r5361, %r5362;
	mov.u32 	%r11641, %r11656;
	mov.u32 	%r11642, %r11655;
	bra.uni 	BB6_514;

BB6_648:
	setp.gt.s32	%p586, %r60, 5;
	@%p586 bra 	BB6_652;

	setp.eq.s32	%p589, %r60, 4;
	@%p589 bra 	BB6_717;
	bra.uni 	BB6_650;

BB6_717:
	mov.u32 	%r11645, 0;
	mov.u32 	%r11641, %r51;
	mov.u32 	%r11642, %r58;
	mov.u32 	%r11643, %r57;
	mov.u32 	%r58, %r56;
	mov.u32 	%r11646, %r54;
	mov.u32 	%r11647, %r53;
	mov.u32 	%r54, %r52;
	bra.uni 	BB6_723;

BB6_483:
	setp.eq.s32	%p466, %r5254, 2;
	@%p466 bra 	BB6_499;
	bra.uni 	BB6_484;

BB6_499:
	and.b32  	%r11642, %r701, %r57;
	mov.u32 	%r11641, 0;
	mov.u32 	%r11643, %r56;
	bra.uni 	BB6_500;

BB6_946:
	setp.gt.s32	%p782, %r7992, 5;
	@%p782 bra 	BB6_950;

	setp.eq.s32	%p785, %r7992, 4;
	@%p785 bra 	BB6_1016;
	bra.uni 	BB6_948;

BB6_1016:
	mov.u32 	%r11652, 0;
	mov.u32 	%r11649, %r57;
	mov.u32 	%r11650, %r56;
	mov.u32 	%r11651, %r55;
	mov.u32 	%r11653, %r53;
	mov.u32 	%r11654, %r52;
	mov.u32 	%r11655, %r51;
	mov.u32 	%r11656, %r58;
	bra.uni 	BB6_1023;

BB6_133:
	setp.gt.s32	%p155, %r60, 11;
	@%p155 bra 	BB6_141;

	setp.gt.s32	%p161, %r60, 9;
	@%p161 bra 	BB6_138;

	setp.eq.s32	%p164, %r60, 8;
	@%p164 bra 	BB6_195;
	bra.uni 	BB6_136;

BB6_195:
	mov.u32 	%r11653, 0;
	mov.u32 	%r11649, %r57;
	mov.u32 	%r11650, %r58;
	mov.u32 	%r11651, %r51;
	mov.u32 	%r11652, %r52;
	mov.u32 	%r11654, %r11653;
	mov.u32 	%r11655, %r55;
	mov.u32 	%r11656, %r56;
	bra.uni 	BB6_199;

BB6_223:
	setp.eq.s32	%p198, %r3305, 2;
	@%p198 bra 	BB6_236;
	bra.uni 	BB6_224;

BB6_236:
	and.b32  	%r11655, %r252, %r57;
	mov.u32 	%r11649, 0;
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;
	mov.u32 	%r11653, %r55;
	mov.u32 	%r11654, %r56;
	bra.uni 	BB6_237;

BB6_518:
	setp.eq.s32	%p479, %r5336, 5;
	@%p479 bra 	BB6_519;
	bra.uni 	BB6_113;

BB6_519:
	and.b32  	%r5348, %r728, %r52;
	or.b32  	%r5349, %r5348, %r727;
	and.b32  	%r5350, %r11650, %r729;
	or.b32  	%r11647, %r5349, %r5350;
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11645, %r11652;
	mov.u32 	%r11646, %r11651;
	mov.u32 	%r11648, %r51;
	bra.uni 	BB6_1030;

BB6_679:
	setp.gt.s32	%p563, %r60, 21;
	@%p563 bra 	BB6_683;

	setp.eq.s32	%p566, %r60, 20;
	@%p566 bra 	BB6_706;
	bra.uni 	BB6_681;

BB6_706:
	mov.u32 	%r11641, 0;
	mov.u32 	%r11642, %r54;
	mov.u32 	%r11643, %r53;
	mov.u32 	%r58, %r52;
	bra.uni 	BB6_722;

BB6_978:
	setp.gt.s32	%p759, %r7992, 21;
	@%p759 bra 	BB6_982;

	setp.eq.s32	%p762, %r7992, 20;
	@%p762 bra 	BB6_1006;
	bra.uni 	BB6_980;

BB6_1006:
	mov.u32 	%r11649, 0;
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;
	mov.u32 	%r11653, %r57;
	mov.u32 	%r11654, %r56;
	mov.u32 	%r11655, %r55;
	bra.uni 	BB6_1022;

BB6_511:
	setp.eq.s32	%p482, %r5336, 3;
	@%p482 bra 	BB6_512;
	bra.uni 	BB6_113;

BB6_512:
	and.b32  	%r5354, %r728, %r58;
	or.b32  	%r5355, %r5354, %r727;
	and.b32  	%r5356, %r11656, %r729;
	or.b32  	%r11641, %r5355, %r5356;
	mov.u32 	%r11642, %r57;

BB6_513:
	mov.u32 	%r11643, %r56;

BB6_514:
	mov.u32 	%r11644, %r55;

BB6_515:
	mov.u32 	%r11645, %r11652;
	mov.u32 	%r11646, %r11651;
	mov.u32 	%r11647, %r11650;
	mov.u32 	%r11648, %r11649;
	bra.uni 	BB6_1030;

BB6_663:
	setp.gt.s32	%p575, %r60, 13;
	@%p575 bra 	BB6_667;

	setp.eq.s32	%p578, %r60, 12;
	@%p578 bra 	BB6_712;
	bra.uni 	BB6_665;

BB6_712:
	mov.u32 	%r11645, 0;
	mov.u32 	%r11641, %r53;
	mov.u32 	%r11642, %r52;
	mov.u32 	%r11643, %r51;
	bra.uni 	BB6_713;

BB6_490:
	setp.eq.s32	%p461, %r5254, 6;
	@%p461 bra 	BB6_497;
	bra.uni 	BB6_491;

BB6_497:
	and.b32  	%r11646, %r701, %r53;
	mov.u32 	%r11645, 0;
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	bra.uni 	BB6_495;

BB6_961:
	setp.gt.s32	%p771, %r7992, 13;
	@%p771 bra 	BB6_965;

	setp.eq.s32	%p774, %r7992, 12;
	@%p774 bra 	BB6_1012;
	bra.uni 	BB6_963;

BB6_1012:
	mov.u32 	%r11650, 0;
	mov.u32 	%r11649, %r55;
	mov.u32 	%r11651, %r11650;
	mov.u32 	%r11652, %r11650;
	mov.u32 	%r11653, %r51;
	mov.u32 	%r11654, %r58;
	mov.u32 	%r11655, %r57;
	mov.u32 	%r11656, %r56;
	bra.uni 	BB6_1023;

BB6_165:
	setp.gt.s32	%p132, %r60, 27;
	@%p132 bra 	BB6_174;

	setp.gt.s32	%p138, %r60, 25;
	@%p138 bra 	BB6_170;

	setp.eq.s32	%p141, %r60, 24;
	@%p141 bra 	BB6_186;
	bra.uni 	BB6_168;

BB6_186:
	mov.u32 	%r11649, 0;
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r55;
	mov.u32 	%r11652, %r56;
	bra.uni 	BB6_184;

BB6_230:
	setp.eq.s32	%p193, %r3305, 6;
	@%p193 bra 	BB6_234;
	bra.uni 	BB6_231;

BB6_234:
	and.b32  	%r11651, %r252, %r53;
	mov.u32 	%r11652, 0;
	mov.u32 	%r11649, %r51;
	mov.u32 	%r11650, %r52;
	bra.uni 	BB6_233;

BB6_521:
	setp.ne.s32	%p477, %r5336, 7;
	@%p477 bra 	BB6_113;

	and.b32  	%r5342, %r728, %r54;
	or.b32  	%r5343, %r5342, %r727;
	and.b32  	%r5344, %r11652, %r729;
	or.b32  	%r11645, %r5343, %r5344;
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	bra.uni 	BB6_114;

BB6_113:
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11645, %r54;

BB6_114:
	mov.u32 	%r11646, %r53;

BB6_115:
	mov.u32 	%r11647, %r52;
	mov.u32 	%r11648, %r51;
	bra.uni 	BB6_1030;

BB6_695:
	setp.gt.s32	%p552, %r60, 29;
	@%p552 bra 	BB6_699;

	setp.eq.s32	%p555, %r60, 28;
	@%p555 bra 	BB6_702;
	bra.uni 	BB6_697;

BB6_702:
	mov.u32 	%r11641, 0;
	mov.u32 	%r11642, %r11641;
	mov.u32 	%r11643, %r11641;
	mov.u32 	%r58, %r54;
	bra.uni 	BB6_722;

BB6_995:
	setp.gt.s32	%p748, %r7992, 29;
	@%p748 bra 	BB6_999;

	setp.eq.s32	%p751, %r7992, 28;
	@%p751 bra 	BB6_1002;
	bra.uni 	BB6_997;

BB6_1002:
	mov.u32 	%r11649, 0;
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;
	mov.u32 	%r11653, %r55;
	bra.uni 	BB6_1020;

BB6_538:
	setp.gt.s32	%p525, %r62, 5;
	@%p525 bra 	BB6_542;

	setp.eq.s32	%p528, %r62, 4;
	@%p528 bra 	BB6_611;
	bra.uni 	BB6_540;

BB6_611:
	mov.u32 	%r11649, %r52;
	mov.u32 	%r11650, %r53;
	mov.u32 	%r11651, %r54;
	mov.u32 	%r11653, %r56;
	mov.u32 	%r11654, %r57;
	mov.u32 	%r11655, %r58;
	mov.u32 	%r11656, %r51;
	bra.uni 	BB6_614;

BB6_927:
	setp.eq.s32	%p744, %r7813, 1;
	mov.u32 	%r11626, %r11625;
	mov.u32 	%r11627, %r11625;
	mov.u32 	%r11628, %r11625;
	mov.u32 	%r11629, %r11625;
	mov.u32 	%r11630, %r11625;
	mov.u32 	%r11631, %r11625;
	mov.u32 	%r11632, %r11625;
	@%p744 bra 	BB6_928;
	bra.uni 	BB6_943;

BB6_928:
	// inline asm
	prmt.b32 %r11632, %r52, %r53, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11631, %r51, %r52, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11630, %r58, %r51, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11629, %r57, %r58, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11628, %r56, %r57, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11627, %r55, %r56, %r1344;
	// inline asm
	mov.u32 	%r11625, 0;
	// inline asm
	prmt.b32 %r11626, %r11625, %r55, %r1344;
	// inline asm
	bra.uni 	BB6_943;

BB6_813:
	setp.gt.s32	%p686, %r6825, 11;
	@%p686 bra 	BB6_821;

	setp.gt.s32	%p692, %r6825, 9;
	@%p692 bra 	BB6_818;

	setp.eq.s32	%p695, %r6825, 8;
	@%p695 bra 	BB6_876;
	bra.uni 	BB6_816;

BB6_876:
	mov.u32 	%r11656, %r57;
	mov.u32 	%r11655, %r58;
	mov.u32 	%r11654, %r51;
	mov.u32 	%r11653, %r52;
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r11599;
	mov.u32 	%r11650, %r55;
	mov.u32 	%r11649, %r56;
	bra.uni 	BB6_880;

BB6_458:
	setp.eq.s32	%p375, %r652, 2;
	mov.u32 	%r11657, 0;
	@%p375 bra 	BB6_459;
	bra.uni 	BB6_460;

BB6_459:
	mov.u32 	%r11543, %r11657;
	bra.uni 	BB6_462;

BB6_748:
	setp.eq.s32	%p613, %r6550, 2;
	@%p613 bra 	BB6_767;
	bra.uni 	BB6_749;

BB6_767:
	and.b32  	%r6565, %r1006, %r57;
	and.b32  	%r6566, %r11655, %r1007;
	or.b32  	%r11642, %r6566, %r6565;
	mov.u32 	%r11641, %r11656;
	bra.uni 	BB6_751;

BB6_352:
	setp.gt.s32	%p320, %r60, 5;
	@%p320 bra 	BB6_356;

	setp.eq.s32	%p323, %r60, 4;
	@%p323 bra 	BB6_421;
	bra.uni 	BB6_354;

BB6_421:
	mov.u32 	%r11530, 0;
	mov.u32 	%r11527, %r57;
	mov.u32 	%r11528, %r56;
	mov.u32 	%r11529, %r55;
	mov.u32 	%r54, %r53;
	mov.u32 	%r53, %r52;
	mov.u32 	%r52, %r51;
	mov.u32 	%r55, %r58;
	bra.uni 	BB6_429;

BB6_571:
	setp.gt.s32	%p502, %r62, 21;
	@%p502 bra 	BB6_575;

	setp.eq.s32	%p505, %r62, 20;
	@%p505 bra 	BB6_603;
	bra.uni 	BB6_573;

BB6_603:
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r52;
	mov.u32 	%r11654, %r53;
	mov.u32 	%r11655, %r54;
	mov.u32 	%r11656, %r11652;
	bra.uni 	BB6_614;

BB6_934:
	setp.eq.s32	%p739, %r7813, 5;
	mov.u32 	%r11626, %r11625;
	mov.u32 	%r11627, %r11625;
	mov.u32 	%r11628, %r11625;
	mov.u32 	%r11629, %r11625;
	mov.u32 	%r11630, %r11625;
	mov.u32 	%r11631, %r11625;
	mov.u32 	%r11632, %r11625;
	@%p739 bra 	BB6_935;
	bra.uni 	BB6_943;

BB6_935:
	// inline asm
	prmt.b32 %r11632, %r56, %r57, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11631, %r55, %r56, %r1344;
	// inline asm
	mov.u32 	%r11625, 0;
	// inline asm
	prmt.b32 %r11630, %r11625, %r55, %r1344;
	// inline asm
	mov.u32 	%r11626, %r11625;
	mov.u32 	%r11627, %r11625;
	mov.u32 	%r11628, %r11625;
	mov.u32 	%r11629, %r11625;
	bra.uni 	BB6_943;

BB6_383:
	setp.gt.s32	%p297, %r60, 21;
	@%p297 bra 	BB6_387;

	setp.eq.s32	%p300, %r60, 20;
	@%p300 bra 	BB6_411;
	bra.uni 	BB6_385;

BB6_411:
	mov.u32 	%r11527, 0;
	mov.u32 	%r11528, %r11527;
	mov.u32 	%r11529, %r11527;
	mov.u32 	%r11530, %r11527;
	mov.u32 	%r54, %r57;
	mov.u32 	%r53, %r56;
	mov.u32 	%r52, %r55;
	bra.uni 	BB6_428;

BB6_553:
	setp.gt.s32	%p514, %r62, 13;
	@%p514 bra 	BB6_557;

	setp.eq.s32	%p517, %r62, 12;
	@%p517 bra 	BB6_607;
	bra.uni 	BB6_555;

BB6_607:
	mov.u32 	%r11649, %r54;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r58;
	mov.u32 	%r11654, %r51;
	mov.u32 	%r11655, %r52;
	mov.u32 	%r11656, %r53;
	bra.uni 	BB6_614;

BB6_930:
	setp.eq.s32	%p742, %r7813, 3;
	mov.u32 	%r11626, %r11625;
	mov.u32 	%r11627, %r11625;
	mov.u32 	%r11628, %r11625;
	mov.u32 	%r11629, %r11625;
	mov.u32 	%r11630, %r11625;
	mov.u32 	%r11631, %r11625;
	mov.u32 	%r11632, %r11625;
	@%p742 bra 	BB6_931;
	bra.uni 	BB6_943;

BB6_931:
	// inline asm
	prmt.b32 %r11632, %r58, %r51, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11631, %r57, %r58, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11630, %r56, %r57, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11629, %r55, %r56, %r1344;
	// inline asm
	mov.u32 	%r11625, 0;
	// inline asm
	prmt.b32 %r11628, %r11625, %r55, %r1344;
	// inline asm
	mov.u32 	%r11626, %r11625;
	mov.u32 	%r11627, %r11625;
	bra.uni 	BB6_943;

BB6_846:
	setp.gt.s32	%p663, %r6825, 27;
	@%p663 bra 	BB6_856;

	setp.gt.s32	%p669, %r6825, 25;
	@%p669 bra 	BB6_851;

	setp.eq.s32	%p672, %r6825, 24;
	@%p672 bra 	BB6_868;
	bra.uni 	BB6_849;

BB6_868:
	mov.u32 	%r11656, %r11599;
	mov.u32 	%r11655, %r11599;
	mov.u32 	%r11654, %r55;
	mov.u32 	%r11653, %r56;
	bra.uni 	BB6_866;

BB6_759:
	setp.eq.s32	%p608, %r6550, 6;
	@%p608 bra 	BB6_765;
	bra.uni 	BB6_760;

BB6_765:
	and.b32  	%r6557, %r1006, %r53;
	and.b32  	%r6558, %r11651, %r1007;
	or.b32  	%r11646, %r6558, %r6557;
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11645, %r11652;
	bra.uni 	BB6_763;

BB6_367:
	setp.gt.s32	%p309, %r60, 13;
	@%p309 bra 	BB6_371;

	setp.eq.s32	%p312, %r60, 12;
	@%p312 bra 	BB6_417;
	bra.uni 	BB6_369;

BB6_417:
	mov.u32 	%r11528, 0;
	mov.u32 	%r11527, %r55;
	mov.u32 	%r11529, %r11528;
	mov.u32 	%r11530, %r11528;
	mov.u32 	%r54, %r51;
	mov.u32 	%r53, %r58;
	mov.u32 	%r52, %r57;
	mov.u32 	%r55, %r56;
	bra.uni 	BB6_429;

BB6_780:
	setp.eq.s32	%p641, %r6671, 2;
	@%p641 bra 	BB6_792;
	bra.uni 	BB6_781;

BB6_792:
	and.b32  	%r11642, %r11642, %r1059;
	mov.u32 	%r11641, 0;
	bra.uni 	BB6_793;

BB6_589:
	setp.gt.s32	%p491, %r62, 29;
	@%p491 bra 	BB6_593;

	setp.eq.s32	%p494, %r62, 28;
	@%p494 bra 	BB6_598;
	bra.uni 	BB6_591;

BB6_598:
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r54;
	mov.u32 	%r11654, %r11652;

BB6_599:
	mov.u32 	%r11655, %r11652;
	mov.u32 	%r11656, %r11652;
	bra.uni 	BB6_614;

BB6_937:
	setp.ne.s32	%p737, %r7813, 7;
	mov.u32 	%r11626, %r11625;
	mov.u32 	%r11627, %r11625;
	mov.u32 	%r11628, %r11625;
	mov.u32 	%r11629, %r11625;
	mov.u32 	%r11630, %r11625;
	mov.u32 	%r11631, %r11625;
	mov.u32 	%r11632, %r11625;
	@%p737 bra 	BB6_943;

	mov.u32 	%r11625, 0;
	// inline asm
	prmt.b32 %r11632, %r11625, %r55, %r1344;
	// inline asm
	mov.u32 	%r11626, %r11625;
	mov.u32 	%r11627, %r11625;
	mov.u32 	%r11628, %r11625;
	mov.u32 	%r11629, %r11625;
	mov.u32 	%r11630, %r11625;
	mov.u32 	%r11631, %r11625;

BB6_943:
	or.b32  	%r11644, %r11625, %r55;
	or.b32  	%r11643, %r11626, %r56;
	or.b32  	%r11642, %r11627, %r57;
	or.b32  	%r11641, %r11628, %r58;
	or.b32  	%r11648, %r11629, %r51;
	or.b32  	%r11647, %r11630, %r52;
	or.b32  	%r11646, %r11631, %r53;
	or.b32  	%r11645, %r11632, %r54;
	bra.uni 	BB6_1030;

BB6_400:
	setp.gt.s32	%p286, %r60, 29;
	@%p286 bra 	BB6_404;

	setp.eq.s32	%p289, %r60, 28;
	@%p289 bra 	BB6_407;
	bra.uni 	BB6_402;

BB6_407:
	mov.u32 	%r11527, 0;
	mov.u32 	%r11528, %r11527;
	mov.u32 	%r11529, %r11527;
	mov.u32 	%r11530, %r11527;
	mov.u32 	%r54, %r55;
	bra.uni 	BB6_426;

BB6_787:
	setp.eq.s32	%p636, %r6671, 6;
	@%p636 bra 	BB6_790;
	bra.uni 	BB6_788;

BB6_790:
	and.b32  	%r11646, %r11646, %r1059;
	mov.u32 	%r11645, 0;
	bra.uni 	BB6_1029;

BB6_481:
	setp.eq.s32	%p469, %r5254, 1;
	@%p469 bra 	BB6_482;
	bra.uni 	BB6_492;

BB6_482:
	and.b32  	%r11643, %r701, %r56;
	mov.u32 	%r11641, 0;
	mov.u32 	%r11642, %r11641;

BB6_500:
	mov.u32 	%r11644, %r55;

BB6_501:
	mov.u32 	%r11645, %r11641;
	mov.u32 	%r11646, %r11641;
	mov.u32 	%r11647, %r11641;
	mov.u32 	%r11648, %r11641;
	mov.u32 	%r11657, %r60;
	bra.uni 	BB6_1030;

BB6_126:
	setp.gt.s32	%p167, %r60, 5;
	@%p167 bra 	BB6_130;

	setp.eq.s32	%p170, %r60, 4;
	@%p170 bra 	BB6_197;
	bra.uni 	BB6_128;

BB6_197:
	mov.u32 	%r11653, 0;
	mov.u32 	%r11649, %r58;
	mov.u32 	%r11650, %r51;
	mov.u32 	%r11651, %r52;
	mov.u32 	%r11652, %r53;
	mov.u32 	%r11654, %r55;
	mov.u32 	%r11655, %r56;
	mov.u32 	%r11656, %r57;
	bra.uni 	BB6_199;

BB6_221:
	setp.eq.s32	%p201, %r3305, 1;
	mov.u32 	%r11649, %r51;
	mov.u32 	%r11650, %r52;
	mov.u32 	%r11651, %r53;
	mov.u32 	%r11652, %r54;
	mov.u32 	%r11653, %r55;
	mov.u32 	%r11654, %r56;
	mov.u32 	%r11655, %r57;
	mov.u32 	%r11656, %r58;
	@%p201 bra 	BB6_222;
	bra.uni 	BB6_238;

BB6_222:
	and.b32  	%r11654, %r252, %r56;
	mov.u32 	%r11649, 0;
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;
	mov.u32 	%r11653, %r55;

BB6_106:
	mov.u32 	%r11655, %r11649;

BB6_237:
	mov.u32 	%r11656, %r11649;
	bra.uni 	BB6_238;

BB6_676:
	setp.eq.s32	%p569, %r60, 18;
	@%p569 bra 	BB6_707;
	bra.uni 	BB6_677;

BB6_707:
	mov.u32 	%r6109, 16;
	// inline asm
	shf.r.wrap.b32 %r58, %r51, %r52, %r6109;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r52, %r53, %r6109;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11642, %r53, %r54, %r6109;
	// inline asm
	mov.u32 	%r11645, 0;
	// inline asm
	shf.r.wrap.b32 %r11641, %r54, %r11645, %r6109;
	// inline asm
	bra.uni 	BB6_709;

BB6_975:
	setp.eq.s32	%p765, %r7992, 18;
	@%p765 bra 	BB6_1009;
	bra.uni 	BB6_976;

BB6_1009:
	mov.u32 	%r8162, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r57, %r58, %r8162;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r56, %r57, %r8162;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r55, %r56, %r8162;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11656, %r11649, %r55, %r8162;
	// inline asm
	bra.uni 	BB6_1008;

BB6_660:
	setp.eq.s32	%p581, %r60, 10;
	@%p581 bra 	BB6_714;
	bra.uni 	BB6_661;

BB6_714:
	mov.u32 	%r6259, 16;
	// inline asm
	shf.r.wrap.b32 %r6236, %r57, %r58, %r6259;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r58, %r51, %r6259;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11642, %r51, %r52, %r6259;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11641, %r52, %r53, %r6259;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6252, %r53, %r54, %r6259;
	// inline asm
	mov.u32 	%r11645, 0;
	// inline asm
	shf.r.wrap.b32 %r11647, %r54, %r11645, %r6259;
	// inline asm
	mov.u32 	%r58, %r6236;
	mov.u32 	%r11646, %r11645;
	mov.u32 	%r54, %r6252;
	bra.uni 	BB6_723;

BB6_488:
	setp.eq.s32	%p464, %r5254, 5;
	@%p464 bra 	BB6_489;
	bra.uni 	BB6_492;

BB6_489:
	and.b32  	%r11647, %r701, %r52;
	mov.u32 	%r11645, 0;
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11646, %r11645;
	bra.uni 	BB6_496;

BB6_958:
	setp.eq.s32	%p777, %r7992, 10;
	@%p777 bra 	BB6_1013;
	bra.uni 	BB6_959;

BB6_1013:
	mov.u32 	%r8312, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r51, %r52, %r8312;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r58, %r51, %r8312;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r57, %r58, %r8312;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r56, %r57, %r8312;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r55, %r56, %r8312;
	// inline asm
	mov.u32 	%r11651, 0;
	// inline asm
	shf.r.wrap.b32 %r11650, %r11651, %r55, %r8312;
	// inline asm
	mov.u32 	%r11652, %r11651;
	bra.uni 	BB6_1023;

BB6_158:
	setp.gt.s32	%p144, %r60, 21;
	@%p144 bra 	BB6_162;

	setp.eq.s32	%p147, %r60, 20;
	@%p147 bra 	BB6_188;
	bra.uni 	BB6_160;

BB6_188:
	mov.u32 	%r11649, 0;
	mov.u32 	%r11650, %r55;
	mov.u32 	%r11651, %r56;
	mov.u32 	%r11652, %r57;
	bra.uni 	BB6_184;

BB6_228:
	setp.eq.s32	%p196, %r3305, 5;
	mov.u32 	%r11649, %r51;
	mov.u32 	%r11650, %r52;
	mov.u32 	%r11651, %r53;
	mov.u32 	%r11652, %r54;
	mov.u32 	%r11653, %r55;
	mov.u32 	%r11654, %r56;
	mov.u32 	%r11655, %r57;
	mov.u32 	%r11656, %r58;
	@%p196 bra 	BB6_229;
	bra.uni 	BB6_238;

BB6_229:
	and.b32  	%r11650, %r252, %r52;
	mov.u32 	%r11651, 0;
	mov.u32 	%r11649, %r51;
	mov.u32 	%r11652, %r11651;
	bra.uni 	BB6_233;

BB6_691:
	setp.eq.s32	%p558, %r60, 26;
	@%p558 bra 	BB6_703;
	bra.uni 	BB6_692;

BB6_703:
	mov.u32 	%r5991, 16;
	// inline asm
	shf.r.wrap.b32 %r58, %r53, %r54, %r5991;
	// inline asm
	mov.u32 	%r11641, 0;
	// inline asm
	shf.r.wrap.b32 %r11643, %r54, %r11641, %r5991;
	// inline asm
	bra.uni 	BB6_694;

BB6_991:
	setp.eq.s32	%p754, %r7992, 26;
	@%p754 bra 	BB6_1003;
	bra.uni 	BB6_992;

BB6_1003:
	mov.u32 	%r8044, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r55, %r56, %r8044;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11654, %r11649, %r55, %r8044;
	// inline asm
	bra.uni 	BB6_994;

BB6_652:
	setp.eq.s32	%p587, %r60, 6;
	@%p587 bra 	BB6_716;
	bra.uni 	BB6_653;

BB6_716:
	mov.u32 	%r6346, 16;
	// inline asm
	shf.r.wrap.b32 %r6319, %r56, %r57, %r6346;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r57, %r58, %r6346;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11642, %r58, %r51, %r6346;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11641, %r51, %r52, %r6346;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6335, %r52, %r53, %r6346;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11647, %r53, %r54, %r6346;
	// inline asm
	mov.u32 	%r11645, 0;
	// inline asm
	shf.r.wrap.b32 %r11646, %r54, %r11645, %r6346;
	// inline asm
	mov.u32 	%r58, %r6319;
	mov.u32 	%r54, %r6335;
	bra.uni 	BB6_723;

BB6_484:
	setp.eq.s32	%p467, %r5254, 3;
	@%p467 bra 	BB6_485;
	bra.uni 	BB6_492;

BB6_485:
	and.b32  	%r11641, %r701, %r58;
	mov.u32 	%r11645, 0;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11646, %r11645;
	mov.u32 	%r11647, %r11645;
	mov.u32 	%r11648, %r11645;
	mov.u32 	%r11657, %r60;
	bra.uni 	BB6_1030;

BB6_950:
	setp.eq.s32	%p783, %r7992, 6;
	@%p783 bra 	BB6_1015;
	bra.uni 	BB6_951;

BB6_1015:
	mov.u32 	%r8399, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r52, %r53, %r8399;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r51, %r52, %r8399;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r58, %r51, %r8399;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r57, %r58, %r8399;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r56, %r57, %r8399;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r55, %r56, %r8399;
	// inline asm
	mov.u32 	%r11652, 0;
	// inline asm
	shf.r.wrap.b32 %r11651, %r11652, %r55, %r8399;
	// inline asm
	bra.uni 	BB6_1023;

BB6_141:
	setp.gt.s32	%p156, %r60, 13;
	@%p156 bra 	BB6_145;

	setp.eq.s32	%p159, %r60, 12;
	@%p159 bra 	BB6_193;
	bra.uni 	BB6_143;

BB6_193:
	mov.u32 	%r11653, 0;
	mov.u32 	%r11649, %r56;
	mov.u32 	%r11650, %r57;
	mov.u32 	%r11651, %r58;
	mov.u32 	%r11652, %r51;
	mov.u32 	%r11654, %r11653;
	mov.u32 	%r11655, %r11653;
	mov.u32 	%r11656, %r55;
	bra.uni 	BB6_199;

BB6_224:
	setp.eq.s32	%p199, %r3305, 3;
	mov.u32 	%r11649, %r51;
	mov.u32 	%r11650, %r52;
	mov.u32 	%r11651, %r53;
	mov.u32 	%r11652, %r54;
	mov.u32 	%r11653, %r55;
	mov.u32 	%r11654, %r56;
	mov.u32 	%r11655, %r57;
	mov.u32 	%r11656, %r58;
	@%p199 bra 	BB6_225;
	bra.uni 	BB6_238;

BB6_225:
	and.b32  	%r11656, %r252, %r58;
	mov.u32 	%r11649, 0;
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;
	mov.u32 	%r11653, %r55;
	mov.u32 	%r11654, %r56;
	mov.u32 	%r11655, %r57;
	bra.uni 	BB6_238;

BB6_683:
	setp.eq.s32	%p564, %r60, 22;
	@%p564 bra 	BB6_705;
	bra.uni 	BB6_684;

BB6_705:
	mov.u32 	%r6046, 16;
	// inline asm
	shf.r.wrap.b32 %r58, %r52, %r53, %r6046;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r53, %r54, %r6046;
	// inline asm
	mov.u32 	%r11641, 0;
	// inline asm
	shf.r.wrap.b32 %r11642, %r54, %r11641, %r6046;
	// inline asm
	bra.uni 	BB6_722;

BB6_982:
	setp.eq.s32	%p760, %r7992, 22;
	@%p760 bra 	BB6_1005;
	bra.uni 	BB6_983;

BB6_1005:
	mov.u32 	%r8099, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r56, %r57, %r8099;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r55, %r56, %r8099;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11655, %r11649, %r55, %r8099;
	// inline asm
	bra.uni 	BB6_985;

BB6_667:
	setp.eq.s32	%p576, %r60, 14;
	@%p576 bra 	BB6_711;
	bra.uni 	BB6_668;

BB6_711:
	mov.u32 	%r6180, 16;
	// inline asm
	shf.r.wrap.b32 %r58, %r58, %r51, %r6180;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r51, %r52, %r6180;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11642, %r52, %r53, %r6180;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11641, %r53, %r54, %r6180;
	// inline asm
	mov.u32 	%r11645, 0;
	// inline asm
	shf.r.wrap.b32 %r54, %r54, %r11645, %r6180;
	// inline asm
	bra.uni 	BB6_713;

BB6_491:
	setp.ne.s32	%p462, %r5254, 7;
	@%p462 bra 	BB6_492;

	and.b32  	%r11645, %r701, %r54;
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	bra.uni 	BB6_494;

BB6_492:
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11645, %r54;

BB6_494:
	mov.u32 	%r11646, %r53;

BB6_495:
	mov.u32 	%r11647, %r52;

BB6_496:
	mov.u32 	%r11648, %r51;
	mov.u32 	%r11657, %r60;
	bra.uni 	BB6_1030;

BB6_965:
	setp.eq.s32	%p772, %r7992, 14;
	@%p772 bra 	BB6_1011;
	bra.uni 	BB6_966;

BB6_1011:
	mov.u32 	%r8233, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r58, %r51, %r8233;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r57, %r58, %r8233;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r56, %r57, %r8233;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r55, %r56, %r8233;
	// inline asm
	mov.u32 	%r11650, 0;
	// inline asm
	shf.r.wrap.b32 %r11649, %r11650, %r55, %r8233;
	// inline asm
	bra.uni 	BB6_968;

BB6_174:
	setp.gt.s32	%p133, %r60, 29;
	@%p133 bra 	BB6_178;

	setp.eq.s32	%p136, %r60, 28;
	@%p136 bra 	BB6_183;
	bra.uni 	BB6_176;

BB6_183:
	mov.u32 	%r11649, 0;
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r55;
	bra.uni 	BB6_184;

BB6_231:
	setp.ne.s32	%p194, %r3305, 7;
	mov.u32 	%r11649, %r51;
	mov.u32 	%r11650, %r52;
	mov.u32 	%r11651, %r53;
	mov.u32 	%r11652, %r54;
	mov.u32 	%r11653, %r55;
	mov.u32 	%r11654, %r56;
	mov.u32 	%r11655, %r57;
	mov.u32 	%r11656, %r58;
	@%p194 bra 	BB6_238;

	and.b32  	%r11652, %r252, %r54;
	mov.u32 	%r11649, %r51;
	mov.u32 	%r11650, %r52;
	mov.u32 	%r11651, %r53;

BB6_233:
	mov.u32 	%r11653, %r55;
	mov.u32 	%r11654, %r56;
	mov.u32 	%r11655, %r57;
	mov.u32 	%r11656, %r58;

BB6_238:
	setp.gt.s32	%p202, %r60, 15;
	@%p202 bra 	BB6_267;

	setp.gt.s32	%p226, %r60, 7;
	@%p226 bra 	BB6_252;

	setp.gt.s32	%p238, %r60, 3;
	@%p238 bra 	BB6_245;

	setp.eq.s32	%p244, %r60, 1;
	@%p244 bra 	BB6_316;

	setp.eq.s32	%p245, %r60, 2;
	@%p245 bra 	BB6_315;
	bra.uni 	BB6_243;

BB6_315:
	mov.u32 	%r3838, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r53, %r54, %r3838;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r52, %r53, %r3838;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r51, %r52, %r3838;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3819, %r58, %r51, %r3838;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11508, %r57, %r58, %r3838;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11509, %r56, %r57, %r3838;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11510, %r55, %r56, %r3838;
	// inline asm
	mov.u32 	%r3836, 0;
	// inline asm
	shf.r.wrap.b32 %r11511, %r3836, %r55, %r3838;
	// inline asm
	mov.u32 	%r55, %r3819;
	bra.uni 	BB6_322;

BB6_267:
	setp.gt.s32	%p203, %r60, 23;
	@%p203 bra 	BB6_284;

	setp.gt.s32	%p215, %r60, 19;
	@%p215 bra 	BB6_276;

	setp.gt.s32	%p221, %r60, 17;
	@%p221 bra 	BB6_273;

	setp.eq.s32	%p224, %r60, 16;
	@%p224 bra 	BB6_308;
	bra.uni 	BB6_271;

BB6_308:
	mov.u32 	%r11508, 0;
	mov.u32 	%r11509, %r11508;
	mov.u32 	%r11510, %r11508;
	mov.u32 	%r11511, %r11508;
	mov.u32 	%r54, %r58;
	mov.u32 	%r53, %r57;
	mov.u32 	%r52, %r56;
	bra.uni 	BB6_322;

BB6_252:
	setp.gt.s32	%p227, %r60, 11;
	@%p227 bra 	BB6_260;

	setp.gt.s32	%p233, %r60, 9;
	@%p233 bra 	BB6_257;

	setp.eq.s32	%p236, %r60, 8;
	@%p236 bra 	BB6_312;
	bra.uni 	BB6_255;

BB6_312:
	mov.u32 	%r11510, 0;
	mov.u32 	%r11508, %r56;
	mov.u32 	%r11509, %r55;
	mov.u32 	%r11511, %r11510;
	mov.u32 	%r54, %r52;
	mov.u32 	%r53, %r51;
	mov.u32 	%r52, %r58;
	mov.u32 	%r55, %r57;
	bra.uni 	BB6_322;

BB6_284:
	setp.gt.s32	%p204, %r60, 27;
	@%p204 bra 	BB6_293;

	setp.gt.s32	%p210, %r60, 25;
	@%p210 bra 	BB6_289;

	setp.eq.s32	%p213, %r60, 24;
	@%p213 bra 	BB6_302;
	bra.uni 	BB6_287;

BB6_302:
	mov.u32 	%r11508, 0;
	mov.u32 	%r11509, %r11508;
	mov.u32 	%r11510, %r11508;
	mov.u32 	%r11511, %r11508;
	mov.u32 	%r54, %r56;
	mov.u32 	%r53, %r55;
	bra.uni 	BB6_320;

BB6_699:
	setp.eq.s32	%p553, %r60, 31;
	@%p553 bra 	BB6_720;
	bra.uni 	BB6_700;

BB6_720:
	mov.u32 	%r11641, 0;
	mov.u32 	%r6477, 24;
	// inline asm
	shf.r.wrap.b32 %r58, %r54, %r11641, %r6477;
	// inline asm
	bra.uni 	BB6_721;

BB6_999:
	setp.eq.s32	%p749, %r7992, 31;
	@%p749 bra 	BB6_1018;
	bra.uni 	BB6_1000;

BB6_1018:
	mov.u32 	%r11649, 0;
	mov.u32 	%r8530, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r11649, %r55, %r8530;
	// inline asm
	bra.uni 	BB6_1019;

BB6_245:
	setp.gt.s32	%p239, %r60, 5;
	@%p239 bra 	BB6_249;

	setp.eq.s32	%p242, %r60, 4;
	@%p242 bra 	BB6_314;
	bra.uni 	BB6_247;

BB6_314:
	mov.u32 	%r11511, 0;
	mov.u32 	%r11508, %r57;
	mov.u32 	%r11509, %r56;
	mov.u32 	%r11510, %r55;
	mov.u32 	%r54, %r53;
	mov.u32 	%r53, %r52;
	mov.u32 	%r52, %r51;
	mov.u32 	%r55, %r58;
	bra.uni 	BB6_322;

BB6_276:
	setp.gt.s32	%p216, %r60, 21;
	@%p216 bra 	BB6_280;

	setp.eq.s32	%p219, %r60, 20;
	@%p219 bra 	BB6_304;
	bra.uni 	BB6_278;

BB6_304:
	mov.u32 	%r11508, 0;
	mov.u32 	%r11509, %r11508;
	mov.u32 	%r11510, %r11508;
	mov.u32 	%r11511, %r11508;
	mov.u32 	%r54, %r57;
	mov.u32 	%r53, %r56;
	mov.u32 	%r52, %r55;
	bra.uni 	BB6_321;

BB6_260:
	setp.gt.s32	%p228, %r60, 13;
	@%p228 bra 	BB6_264;

	setp.eq.s32	%p231, %r60, 12;
	@%p231 bra 	BB6_310;
	bra.uni 	BB6_262;

BB6_310:
	mov.u32 	%r11509, 0;
	mov.u32 	%r11508, %r55;
	mov.u32 	%r11510, %r11509;
	mov.u32 	%r11511, %r11509;
	mov.u32 	%r54, %r51;
	mov.u32 	%r53, %r58;
	mov.u32 	%r52, %r57;
	mov.u32 	%r55, %r56;
	bra.uni 	BB6_322;

BB6_293:
	setp.gt.s32	%p205, %r60, 29;
	@%p205 bra 	BB6_297;

	setp.eq.s32	%p208, %r60, 28;
	@%p208 bra 	BB6_300;
	bra.uni 	BB6_295;

BB6_300:
	mov.u32 	%r11508, 0;
	mov.u32 	%r11509, %r11508;
	mov.u32 	%r11510, %r11508;
	mov.u32 	%r11511, %r11508;
	mov.u32 	%r54, %r55;
	bra.uni 	BB6_319;

BB6_719:
	mov.u32 	%r6473, 8;
	// inline asm
	shf.r.wrap.b32 %r6442, %r55, %r56, %r6473;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r56, %r57, %r6473;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11642, %r57, %r58, %r6473;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11641, %r58, %r51, %r6473;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6458, %r51, %r52, %r6473;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11647, %r52, %r53, %r6473;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11646, %r53, %r54, %r6473;
	// inline asm
	mov.u32 	%r6472, 0;
	// inline asm
	shf.r.wrap.b32 %r11645, %r54, %r6472, %r6473;
	// inline asm
	mov.u32 	%r58, %r6442;
	mov.u32 	%r54, %r6458;
	bra.uni 	BB6_723;

BB6_646:
	setp.eq.s32	%p593, %r60, 3;
	@%p593 bra 	BB6_647;
	bra.uni 	BB6_669;

BB6_647:
	mov.u32 	%r6409, 24;
	// inline asm
	shf.r.wrap.b32 %r6378, %r55, %r56, %r6409;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r56, %r57, %r6409;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11642, %r57, %r58, %r6409;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11641, %r58, %r51, %r6409;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6394, %r51, %r52, %r6409;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11647, %r52, %r53, %r6409;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11646, %r53, %r54, %r6409;
	// inline asm
	mov.u32 	%r6408, 0;
	// inline asm
	shf.r.wrap.b32 %r11645, %r54, %r6408, %r6409;
	// inline asm
	mov.u32 	%r58, %r6378;
	mov.u32 	%r54, %r6394;
	bra.uni 	BB6_723;

BB6_535:
	setp.eq.s32	%p531, %r62, 2;
	@%p531 bra 	BB6_612;
	bra.uni 	BB6_536;

BB6_612:
	mov.u32 	%r5886, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r55, %r56, %r5886;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r56, %r57, %r5886;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r57, %r58, %r5886;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r58, %r51, %r5886;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r51, %r52, %r5886;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r52, %r53, %r5886;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r53, %r54, %r5886;
	// inline asm
	mov.u32 	%r5885, 0;
	// inline asm
	shf.r.wrap.b32 %r11652, %r54, %r5885, %r5886;
	// inline asm
	bra.uni 	BB6_614;

BB6_1017:
	mov.u32 	%r8526, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r53, %r54, %r8526;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r52, %r53, %r8526;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r51, %r52, %r8526;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r58, %r51, %r8526;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r57, %r58, %r8526;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r56, %r57, %r8526;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r55, %r56, %r8526;
	// inline asm
	mov.u32 	%r8524, 0;
	// inline asm
	shf.r.wrap.b32 %r11652, %r8524, %r55, %r8526;
	// inline asm
	bra.uni 	BB6_1023;

BB6_944:
	setp.eq.s32	%p789, %r7992, 3;
	@%p789 bra 	BB6_945;
	bra.uni 	BB6_977;

BB6_945:
	mov.u32 	%r8462, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r53, %r54, %r8462;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r52, %r53, %r8462;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r51, %r52, %r8462;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r58, %r51, %r8462;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r57, %r58, %r8462;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r56, %r57, %r8462;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r55, %r56, %r8462;
	// inline asm
	mov.u32 	%r8460, 0;
	// inline asm
	shf.r.wrap.b32 %r11652, %r8460, %r55, %r8462;
	// inline asm
	bra.uni 	BB6_1023;

BB6_806:
	setp.gt.s32	%p698, %r6825, 5;
	@%p698 bra 	BB6_810;

	setp.eq.s32	%p701, %r6825, 4;
	@%p701 bra 	BB6_878;
	bra.uni 	BB6_808;

BB6_878:
	mov.u32 	%r11656, %r58;
	mov.u32 	%r11655, %r51;
	mov.u32 	%r11654, %r52;
	mov.u32 	%r11653, %r53;
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r55;
	mov.u32 	%r11650, %r56;
	mov.u32 	%r11649, %r57;
	bra.uni 	BB6_880;

BB6_746:
	setp.eq.s32	%p616, %r6550, 1;
	@%p616 bra 	BB6_747;
	bra.uni 	BB6_761;

BB6_747:
	and.b32  	%r6567, %r1006, %r56;
	and.b32  	%r6568, %r11654, %r1007;
	or.b32  	%r11643, %r6568, %r6567;
	mov.u32 	%r11641, %r11656;
	mov.u32 	%r11642, %r11655;
	bra.uni 	BB6_752;

BB6_567:
	setp.eq.s32	%p508, %r62, 18;
	@%p508 bra 	BB6_604;
	bra.uni 	BB6_568;

BB6_604:
	mov.u32 	%r5554, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r51, %r52, %r5554;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r52, %r53, %r5554;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r53, %r54, %r5554;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11656, %r54, %r11649, %r5554;
	// inline asm
	bra.uni 	BB6_570;

BB6_380:
	setp.eq.s32	%p303, %r60, 18;
	@%p303 bra 	BB6_414;
	bra.uni 	BB6_381;

BB6_414:
	mov.u32 	%r4327, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r57, %r58, %r4327;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r56, %r57, %r4327;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r55, %r56, %r4327;
	// inline asm
	mov.u32 	%r11527, 0;
	// inline asm
	shf.r.wrap.b32 %r55, %r11527, %r55, %r4327;
	// inline asm
	bra.uni 	BB6_413;

BB6_550:
	setp.eq.s32	%p520, %r62, 10;
	@%p520 bra 	BB6_608;
	bra.uni 	BB6_551;

BB6_608:
	mov.u32 	%r5704, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r57, %r58, %r5704;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r58, %r51, %r5704;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r51, %r52, %r5704;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r52, %r53, %r5704;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r53, %r54, %r5704;
	// inline asm
	mov.u32 	%r11651, 0;
	// inline asm
	shf.r.wrap.b32 %r11650, %r54, %r11651, %r5704;
	// inline asm
	mov.u32 	%r11652, %r11651;
	bra.uni 	BB6_614;

BB6_839:
	setp.gt.s32	%p675, %r6825, 21;
	@%p675 bra 	BB6_843;

	setp.eq.s32	%p678, %r6825, 20;
	@%p678 bra 	BB6_870;
	bra.uni 	BB6_841;

BB6_870:
	mov.u32 	%r11656, %r11599;
	mov.u32 	%r11655, %r55;
	mov.u32 	%r11654, %r56;
	mov.u32 	%r11653, %r57;
	bra.uni 	BB6_866;

BB6_755:
	setp.eq.s32	%p611, %r6550, 5;
	@%p611 bra 	BB6_756;
	bra.uni 	BB6_761;

BB6_756:
	and.b32  	%r6559, %r1006, %r52;
	and.b32  	%r6560, %r11650, %r1007;
	or.b32  	%r11647, %r6560, %r6559;
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11645, %r11652;
	mov.u32 	%r11646, %r11651;
	bra.uni 	BB6_757;

BB6_364:
	setp.eq.s32	%p315, %r60, 10;
	@%p315 bra 	BB6_418;
	bra.uni 	BB6_365;

BB6_418:
	mov.u32 	%r4477, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r51, %r52, %r4477;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r58, %r51, %r4477;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r57, %r58, %r4477;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4466, %r56, %r57, %r4477;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11527, %r55, %r56, %r4477;
	// inline asm
	mov.u32 	%r11529, 0;
	// inline asm
	shf.r.wrap.b32 %r11528, %r11529, %r55, %r4477;
	// inline asm
	mov.u32 	%r11530, %r11529;
	mov.u32 	%r55, %r4466;
	bra.uni 	BB6_429;

BB6_778:
	setp.eq.s32	%p644, %r6671, 1;
	@%p644 bra 	BB6_779;
	bra.uni 	BB6_1029;

BB6_779:
	and.b32  	%r11643, %r11643, %r1059;
	mov.u32 	%r11641, 0;
	mov.u32 	%r11642, %r11641;

BB6_793:
	mov.u32 	%r11645, %r11641;
	mov.u32 	%r11646, %r11641;
	mov.u32 	%r11647, %r11641;
	mov.u32 	%r11648, %r11641;
	bra.uni 	BB6_1029;

BB6_584:
	setp.eq.s32	%p497, %r62, 26;
	@%p497 bra 	BB6_600;
	bra.uni 	BB6_585;

BB6_600:
	mov.u32 	%r5436, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r53, %r54, %r5436;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11654, %r54, %r11649, %r5436;
	// inline asm
	bra.uni 	BB6_587;

BB6_396:
	setp.eq.s32	%p292, %r60, 26;
	@%p292 bra 	BB6_408;
	bra.uni 	BB6_397;

BB6_408:
	mov.u32 	%r4209, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r55, %r56, %r4209;
	// inline asm
	mov.u32 	%r11527, 0;
	// inline asm
	shf.r.wrap.b32 %r53, %r11527, %r55, %r4209;
	// inline asm
	bra.uni 	BB6_399;

BB6_785:
	setp.eq.s32	%p639, %r6671, 5;
	@%p639 bra 	BB6_786;
	bra.uni 	BB6_1029;

BB6_786:
	and.b32  	%r11647, %r11647, %r1059;
	mov.u32 	%r11645, 0;
	mov.u32 	%r11646, %r11645;
	bra.uni 	BB6_1029;

BB6_542:
	setp.eq.s32	%p526, %r62, 6;
	@%p526 bra 	BB6_610;
	bra.uni 	BB6_543;

BB6_610:
	mov.u32 	%r5791, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r56, %r57, %r5791;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r57, %r58, %r5791;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r58, %r51, %r5791;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r51, %r52, %r5791;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r52, %r53, %r5791;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r53, %r54, %r5791;
	// inline asm
	mov.u32 	%r11652, 0;
	// inline asm
	shf.r.wrap.b32 %r11651, %r54, %r11652, %r5791;
	// inline asm
	bra.uni 	BB6_614;

BB6_821:
	setp.gt.s32	%p687, %r6825, 13;
	@%p687 bra 	BB6_825;

	setp.eq.s32	%p690, %r6825, 12;
	@%p690 bra 	BB6_874;
	bra.uni 	BB6_823;

BB6_874:
	mov.u32 	%r11656, %r56;
	mov.u32 	%r11655, %r57;
	mov.u32 	%r11654, %r58;
	mov.u32 	%r11653, %r51;
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r11599;
	mov.u32 	%r11650, %r11599;
	mov.u32 	%r11649, %r55;
	bra.uni 	BB6_880;

BB6_460:
	and.b16  	%rs14, %rs2, 255;
	and.b16  	%rs15, %rs23, 255;
	setp.eq.s16	%p376, %rs15, %rs14;
	mov.u32 	%r11543, 1;
	@%p376 bra 	BB6_462;

	st.local.u8 	[%rd87], %rs23;
	mov.u32 	%r11543, 1;
	mov.u32 	%r11657, %r11543;

BB6_462:
	cvt.u64.u32	%rd143, %r11543;
	add.s64 	%rd144, %rd86, %rd143;
	ld.local.u8 	%rs3, [%rd144];
	and.b16  	%rs16, %rs2, 255;
	setp.eq.s16	%p377, %rs3, %rs16;
	@%p377 bra 	BB6_464;

	cvt.u64.u32	%rd145, %r11657;
	add.s64 	%rd148, %rd87, %rd145;
	st.local.u8 	[%rd148], %rs3;
	add.s32 	%r11657, %r11657, 1;

BB6_464:
	add.s32 	%r11546, %r11543, 1;
	cvt.u64.u32	%rd149, %r11546;
	add.s64 	%rd152, %rd86, %rd149;
	ld.local.u8 	%rs23, [%rd152];

BB6_465:
	and.b16  	%rs17, %rs2, 255;
	and.b16  	%rs18, %rs23, 255;
	setp.eq.s16	%p378, %rs18, %rs17;
	@%p378 bra 	BB6_467;

	cvt.u64.u32	%rd153, %r11657;
	add.s64 	%rd156, %rd87, %rd153;
	st.local.u8 	[%rd156], %rs23;
	add.s32 	%r11657, %r11657, 1;

BB6_467:
	add.s32 	%r11551, %r11546, 1;

BB6_468:
	setp.lt.u32	%p379, %r42, 4;
	@%p379 bra 	BB6_478;

BB6_469:
	cvt.u64.u32	%rd158, %r11551;
	add.s64 	%rd159, %rd86, %rd158;
	ld.local.u8 	%rs6, [%rd159];
	and.b16  	%rs19, %rs2, 255;
	setp.eq.s16	%p380, %rs6, %rs19;
	@%p380 bra 	BB6_471;

	cvt.u64.u32	%rd160, %r11657;
	add.s64 	%rd163, %rd87, %rd160;
	st.local.u8 	[%rd163], %rs6;
	add.s32 	%r11657, %r11657, 1;

BB6_471:
	add.s32 	%r4938, %r11551, 1;
	cvt.u64.u32	%rd164, %r4938;
	add.s64 	%rd165, %rd86, %rd164;
	ld.local.u8 	%rs7, [%rd165];
	setp.eq.s16	%p381, %rs7, %rs19;
	@%p381 bra 	BB6_473;

	cvt.u64.u32	%rd166, %r11657;
	add.s64 	%rd169, %rd87, %rd166;
	st.local.u8 	[%rd169], %rs7;
	add.s32 	%r11657, %r11657, 1;

BB6_473:
	add.s32 	%r4939, %r11551, 2;
	cvt.u64.u32	%rd170, %r4939;
	add.s64 	%rd171, %rd86, %rd170;
	ld.local.u8 	%rs8, [%rd171];
	setp.eq.s16	%p382, %rs8, %rs19;
	@%p382 bra 	BB6_475;

	cvt.u64.u32	%rd172, %r11657;
	add.s64 	%rd175, %rd87, %rd172;
	st.local.u8 	[%rd175], %rs8;
	add.s32 	%r11657, %r11657, 1;

BB6_475:
	add.s32 	%r4940, %r11551, 3;
	cvt.u64.u32	%rd176, %r4940;
	add.s64 	%rd177, %rd86, %rd176;
	ld.local.u8 	%rs9, [%rd177];
	setp.eq.s16	%p383, %rs9, %rs19;
	@%p383 bra 	BB6_477;

	cvt.u64.u32	%rd178, %r11657;
	add.s64 	%rd181, %rd87, %rd178;
	st.local.u8 	[%rd181], %rs9;
	add.s32 	%r11657, %r11657, 1;

BB6_477:
	add.s32 	%r11551, %r11551, 4;
	setp.lt.u32	%p384, %r11551, %r42;
	@%p384 bra 	BB6_469;

BB6_478:
	ld.local.v4.u32 	{%r11644, %r11643, %r11642, %r11641}, [%rd87];
	ld.local.v4.u32 	{%r11648, %r11647, %r11646, %r11645}, [%rd87+16];
	bra.uni 	BB6_1030;

BB6_749:
	setp.eq.s32	%p614, %r6550, 3;
	@%p614 bra 	BB6_750;
	bra.uni 	BB6_761;

BB6_750:
	and.b32  	%r6563, %r1006, %r58;
	and.b32  	%r6564, %r11656, %r1007;
	or.b32  	%r11641, %r6564, %r6563;
	mov.u32 	%r11642, %r57;

BB6_751:
	mov.u32 	%r11643, %r56;

BB6_752:
	mov.u32 	%r11644, %r55;

BB6_32:
	mov.u32 	%r11645, %r11652;
	mov.u32 	%r11646, %r11651;
	mov.u32 	%r11647, %r11650;
	mov.u32 	%r11648, %r11649;
	bra.uni 	BB6_758;

BB6_356:
	setp.eq.s32	%p321, %r60, 6;
	@%p321 bra 	BB6_420;
	bra.uni 	BB6_357;

BB6_420:
	mov.u32 	%r4564, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r52, %r53, %r4564;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r51, %r52, %r4564;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r58, %r51, %r4564;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4549, %r57, %r58, %r4564;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11527, %r56, %r57, %r4564;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11528, %r55, %r56, %r4564;
	// inline asm
	mov.u32 	%r11530, 0;
	// inline asm
	shf.r.wrap.b32 %r11529, %r11530, %r55, %r4564;
	// inline asm
	mov.u32 	%r55, %r4549;
	bra.uni 	BB6_429;

BB6_575:
	setp.eq.s32	%p503, %r62, 22;
	@%p503 bra 	BB6_602;
	bra.uni 	BB6_576;

BB6_602:
	mov.u32 	%r5491, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r52, %r53, %r5491;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r53, %r54, %r5491;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11655, %r54, %r11649, %r5491;
	// inline asm
	bra.uni 	BB6_578;

BB6_387:
	setp.eq.s32	%p298, %r60, 22;
	@%p298 bra 	BB6_410;
	bra.uni 	BB6_388;

BB6_410:
	mov.u32 	%r4264, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r56, %r57, %r4264;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r55, %r56, %r4264;
	// inline asm
	mov.u32 	%r11527, 0;
	// inline asm
	shf.r.wrap.b32 %r52, %r11527, %r55, %r4264;
	// inline asm
	bra.uni 	BB6_390;

BB6_557:
	setp.eq.s32	%p515, %r62, 14;
	@%p515 bra 	BB6_606;
	bra.uni 	BB6_558;

BB6_606:
	mov.u32 	%r5625, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r58, %r51, %r5625;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r51, %r52, %r5625;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r52, %r53, %r5625;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r53, %r54, %r5625;
	// inline asm
	mov.u32 	%r11650, 0;
	// inline asm
	shf.r.wrap.b32 %r11649, %r54, %r11650, %r5625;
	// inline asm
	bra.uni 	BB6_560;

BB6_856:
	setp.gt.s32	%p664, %r6825, 29;
	@%p664 bra 	BB6_860;

	setp.eq.s32	%p667, %r6825, 28;
	@%p667 bra 	BB6_865;
	bra.uni 	BB6_858;

BB6_865:
	mov.u32 	%r11656, %r11599;
	mov.u32 	%r11655, %r11599;
	mov.u32 	%r11654, %r11599;
	mov.u32 	%r11653, %r55;

BB6_866:
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r11599;
	mov.u32 	%r11650, %r11599;
	mov.u32 	%r11649, %r11599;
	bra.uni 	BB6_880;

BB6_760:
	setp.ne.s32	%p609, %r6550, 7;
	@%p609 bra 	BB6_761;

	and.b32  	%r6555, %r1006, %r54;
	and.b32  	%r6556, %r11652, %r1007;
	or.b32  	%r11645, %r6556, %r6555;
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	bra.uni 	BB6_762;

BB6_761:
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11645, %r54;

BB6_762:
	mov.u32 	%r11646, %r53;

BB6_763:
	mov.u32 	%r11647, %r52;

BB6_757:
	mov.u32 	%r11648, %r51;

BB6_758:
	add.s32 	%r11657, %r42, -1;
	bra.uni 	BB6_1030;

BB6_371:
	setp.eq.s32	%p310, %r60, 14;
	@%p310 bra 	BB6_416;
	bra.uni 	BB6_372;

BB6_416:
	mov.u32 	%r4398, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r58, %r51, %r4398;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r57, %r58, %r4398;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r56, %r57, %r4398;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4391, %r55, %r56, %r4398;
	// inline asm
	mov.u32 	%r11528, 0;
	// inline asm
	shf.r.wrap.b32 %r11527, %r11528, %r55, %r4398;
	// inline asm
	mov.u32 	%r11529, %r11528;
	mov.u32 	%r11530, %r11528;
	mov.u32 	%r55, %r4391;
	bra.uni 	BB6_429;

BB6_781:
	setp.eq.s32	%p642, %r6671, 3;
	@%p642 bra 	BB6_782;
	bra.uni 	BB6_1029;

BB6_782:
	and.b32  	%r11641, %r11641, %r1059;
	mov.u32 	%r11645, 0;
	mov.u32 	%r11646, %r11645;
	mov.u32 	%r11647, %r11645;
	mov.u32 	%r11648, %r11645;
	bra.uni 	BB6_1029;

BB6_593:
	setp.eq.s32	%p492, %r62, 30;
	@%p492 bra 	BB6_597;
	bra.uni 	BB6_594;

BB6_597:
	mov.u32 	%r11649, 0;
	mov.u32 	%r5389, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r54, %r11649, %r5389;
	// inline asm
	bra.uni 	BB6_596;

BB6_404:
	setp.eq.s32	%p287, %r60, 31;
	@%p287 bra 	BB6_424;
	bra.uni 	BB6_405;

BB6_424:
	mov.u32 	%r11527, 0;
	mov.u32 	%r4695, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r11527, %r55, %r4695;
	// inline asm
	bra.uni 	BB6_425;

BB6_788:
	setp.ne.s32	%p637, %r6671, 7;
	@%p637 bra 	BB6_1029;

	and.b32  	%r11645, %r11645, %r1059;
	bra.uni 	BB6_1029;

BB6_123:
	setp.eq.s32	%p173, %r60, 2;
	@%p173 bra 	BB6_198;
	bra.uni 	BB6_124;

BB6_198:
	mov.u32 	%r3240, 16;
	// inline asm
	shf.r.wrap.b32 %r11652, %r53, %r54, %r3240;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r52, %r53, %r3240;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r51, %r52, %r3240;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r58, %r51, %r3240;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r57, %r58, %r3240;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r56, %r57, %r3240;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r55, %r56, %r3240;
	// inline asm
	mov.u32 	%r3238, 0;
	// inline asm
	shf.r.wrap.b32 %r11653, %r3238, %r55, %r3240;
	// inline asm
	bra.uni 	BB6_199;

BB6_674:
	setp.eq.s32	%p572, %r60, 17;
	@%p572 bra 	BB6_675;
	bra.uni 	BB6_669;

BB6_675:
	mov.u32 	%r6129, 8;
	// inline asm
	shf.r.wrap.b32 %r58, %r51, %r52, %r6129;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r52, %r53, %r6129;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11642, %r53, %r54, %r6129;
	// inline asm
	mov.u32 	%r11645, 0;
	// inline asm
	shf.r.wrap.b32 %r11641, %r54, %r11645, %r6129;
	// inline asm
	bra.uni 	BB6_709;

BB6_973:
	setp.eq.s32	%p768, %r7992, 17;
	@%p768 bra 	BB6_974;
	bra.uni 	BB6_977;

BB6_974:
	mov.u32 	%r8182, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r57, %r58, %r8182;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r56, %r57, %r8182;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r55, %r56, %r8182;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11656, %r11649, %r55, %r8182;
	// inline asm
	bra.uni 	BB6_1008;

BB6_658:
	setp.eq.s32	%p584, %r60, 9;
	@%p584 bra 	BB6_659;
	bra.uni 	BB6_669;

BB6_659:
	mov.u32 	%r6285, 8;
	// inline asm
	shf.r.wrap.b32 %r6262, %r57, %r58, %r6285;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r58, %r51, %r6285;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11642, %r51, %r52, %r6285;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11641, %r52, %r53, %r6285;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6278, %r53, %r54, %r6285;
	// inline asm
	mov.u32 	%r11645, 0;
	// inline asm
	shf.r.wrap.b32 %r11647, %r54, %r11645, %r6285;
	// inline asm
	mov.u32 	%r58, %r6262;
	mov.u32 	%r11646, %r11645;
	mov.u32 	%r54, %r6278;
	bra.uni 	BB6_723;

BB6_956:
	setp.eq.s32	%p780, %r7992, 9;
	@%p780 bra 	BB6_957;
	bra.uni 	BB6_977;

BB6_957:
	mov.u32 	%r8338, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r51, %r52, %r8338;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r58, %r51, %r8338;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r57, %r58, %r8338;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r56, %r57, %r8338;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r55, %r56, %r8338;
	// inline asm
	mov.u32 	%r11651, 0;
	// inline asm
	shf.r.wrap.b32 %r11650, %r11651, %r55, %r8338;
	// inline asm
	mov.u32 	%r11652, %r11651;
	bra.uni 	BB6_1023;

BB6_155:
	setp.eq.s32	%p150, %r60, 18;
	@%p150 bra 	BB6_189;
	bra.uni 	BB6_156;

BB6_189:
	mov.u32 	%r2908, 16;
	// inline asm
	shf.r.wrap.b32 %r11652, %r57, %r58, %r2908;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r56, %r57, %r2908;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r55, %r56, %r2908;
	// inline asm
	mov.u32 	%r11653, 0;
	// inline asm
	shf.r.wrap.b32 %r11649, %r11653, %r55, %r2908;
	// inline asm
	bra.uni 	BB6_191;

BB6_689:
	setp.eq.s32	%p561, %r60, 25;
	@%p561 bra 	BB6_690;
	bra.uni 	BB6_669;

BB6_690:
	mov.u32 	%r6005, 8;
	// inline asm
	shf.r.wrap.b32 %r58, %r53, %r54, %r6005;
	// inline asm
	mov.u32 	%r11641, 0;
	// inline asm
	shf.r.wrap.b32 %r11643, %r54, %r11641, %r6005;
	// inline asm
	bra.uni 	BB6_694;

BB6_989:
	setp.eq.s32	%p757, %r7992, 25;
	@%p757 bra 	BB6_990;
	bra.uni 	BB6_977;

BB6_990:
	mov.u32 	%r8058, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r55, %r56, %r8058;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11654, %r11649, %r55, %r8058;
	// inline asm
	bra.uni 	BB6_994;

BB6_650:
	setp.eq.s32	%p590, %r60, 5;
	@%p590 bra 	BB6_651;
	bra.uni 	BB6_669;

BB6_651:
	mov.u32 	%r6375, 8;
	// inline asm
	shf.r.wrap.b32 %r6348, %r56, %r57, %r6375;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r57, %r58, %r6375;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11642, %r58, %r51, %r6375;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11641, %r51, %r52, %r6375;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6364, %r52, %r53, %r6375;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11647, %r53, %r54, %r6375;
	// inline asm
	mov.u32 	%r11645, 0;
	// inline asm
	shf.r.wrap.b32 %r11646, %r54, %r11645, %r6375;
	// inline asm
	mov.u32 	%r58, %r6348;
	mov.u32 	%r54, %r6364;
	bra.uni 	BB6_723;

BB6_948:
	setp.eq.s32	%p786, %r7992, 5;
	@%p786 bra 	BB6_949;
	bra.uni 	BB6_977;

BB6_949:
	mov.u32 	%r8428, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r52, %r53, %r8428;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r51, %r52, %r8428;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r58, %r51, %r8428;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r57, %r58, %r8428;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r56, %r57, %r8428;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r55, %r56, %r8428;
	// inline asm
	mov.u32 	%r11652, 0;
	// inline asm
	shf.r.wrap.b32 %r11651, %r11652, %r55, %r8428;
	// inline asm
	bra.uni 	BB6_1023;

BB6_138:
	setp.eq.s32	%p162, %r60, 10;
	@%p162 bra 	BB6_194;
	bra.uni 	BB6_139;

BB6_194:
	mov.u32 	%r3058, 16;
	// inline asm
	shf.r.wrap.b32 %r11652, %r51, %r52, %r3058;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r58, %r51, %r3058;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r57, %r58, %r3058;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r56, %r57, %r3058;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r55, %r56, %r3058;
	// inline asm
	mov.u32 	%r11653, 0;
	// inline asm
	shf.r.wrap.b32 %r11655, %r11653, %r55, %r3058;
	// inline asm
	mov.u32 	%r11654, %r11653;
	bra.uni 	BB6_199;

BB6_681:
	setp.eq.s32	%p567, %r60, 21;
	@%p567 bra 	BB6_682;
	bra.uni 	BB6_669;

BB6_682:
	mov.u32 	%r6063, 8;
	// inline asm
	shf.r.wrap.b32 %r58, %r52, %r53, %r6063;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r53, %r54, %r6063;
	// inline asm
	mov.u32 	%r11641, 0;
	// inline asm
	shf.r.wrap.b32 %r11642, %r54, %r11641, %r6063;
	// inline asm
	bra.uni 	BB6_722;

BB6_980:
	setp.eq.s32	%p763, %r7992, 21;
	@%p763 bra 	BB6_981;
	bra.uni 	BB6_977;

BB6_981:
	mov.u32 	%r8116, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r56, %r57, %r8116;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r55, %r56, %r8116;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11655, %r11649, %r55, %r8116;
	// inline asm
	bra.uni 	BB6_985;

BB6_665:
	setp.eq.s32	%p579, %r60, 13;
	@%p579 bra 	BB6_666;
	bra.uni 	BB6_669;

BB6_666:
	mov.u32 	%r6203, 8;
	// inline asm
	shf.r.wrap.b32 %r58, %r58, %r51, %r6203;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r51, %r52, %r6203;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11642, %r52, %r53, %r6203;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11641, %r53, %r54, %r6203;
	// inline asm
	mov.u32 	%r11645, 0;
	// inline asm
	shf.r.wrap.b32 %r54, %r54, %r11645, %r6203;
	// inline asm
	bra.uni 	BB6_713;

BB6_963:
	setp.eq.s32	%p775, %r7992, 13;
	@%p775 bra 	BB6_964;
	bra.uni 	BB6_977;

BB6_964:
	mov.u32 	%r8256, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r58, %r51, %r8256;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r57, %r58, %r8256;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r56, %r57, %r8256;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r55, %r56, %r8256;
	// inline asm
	mov.u32 	%r11650, 0;
	// inline asm
	shf.r.wrap.b32 %r11649, %r11650, %r55, %r8256;
	// inline asm
	bra.uni 	BB6_968;

BB6_170:
	setp.eq.s32	%p139, %r60, 26;
	@%p139 bra 	BB6_185;
	bra.uni 	BB6_171;

BB6_185:
	mov.u32 	%r2790, 16;
	// inline asm
	shf.r.wrap.b32 %r11652, %r55, %r56, %r2790;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11651, %r11649, %r55, %r2790;
	// inline asm
	bra.uni 	BB6_173;

BB6_697:
	setp.eq.s32	%p556, %r60, 29;
	@%p556 bra 	BB6_698;
	bra.uni 	BB6_669;

BB6_698:
	mov.u32 	%r11641, 0;
	mov.u32 	%r5955, 8;
	// inline asm
	shf.r.wrap.b32 %r58, %r54, %r11641, %r5955;
	// inline asm
	bra.uni 	BB6_721;

BB6_997:
	setp.eq.s32	%p752, %r7992, 29;
	@%p752 bra 	BB6_998;
	bra.uni 	BB6_977;

BB6_998:
	mov.u32 	%r11649, 0;
	mov.u32 	%r8008, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r11649, %r55, %r8008;
	// inline asm
	bra.uni 	BB6_1019;

BB6_130:
	setp.eq.s32	%p168, %r60, 6;
	@%p168 bra 	BB6_196;
	bra.uni 	BB6_131;

BB6_196:
	mov.u32 	%r3145, 16;
	// inline asm
	shf.r.wrap.b32 %r11652, %r52, %r53, %r3145;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r51, %r52, %r3145;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r58, %r51, %r3145;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r57, %r58, %r3145;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r56, %r57, %r3145;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r55, %r56, %r3145;
	// inline asm
	mov.u32 	%r11653, 0;
	// inline asm
	shf.r.wrap.b32 %r11654, %r11653, %r55, %r3145;
	// inline asm
	bra.uni 	BB6_199;

BB6_677:
	setp.eq.s32	%p570, %r60, 19;
	@%p570 bra 	BB6_678;
	bra.uni 	BB6_669;

BB6_678:
	mov.u32 	%r6089, 24;
	// inline asm
	shf.r.wrap.b32 %r58, %r51, %r52, %r6089;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r52, %r53, %r6089;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11642, %r53, %r54, %r6089;
	// inline asm
	mov.u32 	%r11645, 0;
	// inline asm
	shf.r.wrap.b32 %r11641, %r54, %r11645, %r6089;
	// inline asm

BB6_709:
	mov.u32 	%r11646, %r11645;
	mov.u32 	%r11647, %r11645;
	mov.u32 	%r54, %r11645;
	bra.uni 	BB6_723;

BB6_976:
	setp.eq.s32	%p766, %r7992, 19;
	@%p766 bra 	BB6_1007;
	bra.uni 	BB6_977;

BB6_1007:
	mov.u32 	%r8142, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r57, %r58, %r8142;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r56, %r57, %r8142;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r55, %r56, %r8142;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11656, %r11649, %r55, %r8142;
	// inline asm

BB6_1008:
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;
	bra.uni 	BB6_1023;

BB6_661:
	setp.eq.s32	%p582, %r60, 11;
	@%p582 bra 	BB6_662;
	bra.uni 	BB6_669;

BB6_662:
	mov.u32 	%r6233, 24;
	// inline asm
	shf.r.wrap.b32 %r6210, %r57, %r58, %r6233;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r58, %r51, %r6233;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11642, %r51, %r52, %r6233;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11641, %r52, %r53, %r6233;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6226, %r53, %r54, %r6233;
	// inline asm
	mov.u32 	%r11645, 0;
	// inline asm
	shf.r.wrap.b32 %r11647, %r54, %r11645, %r6233;
	// inline asm
	mov.u32 	%r58, %r6210;
	mov.u32 	%r11646, %r11645;
	mov.u32 	%r54, %r6226;
	bra.uni 	BB6_723;

BB6_959:
	setp.eq.s32	%p778, %r7992, 11;
	@%p778 bra 	BB6_960;
	bra.uni 	BB6_977;

BB6_960:
	mov.u32 	%r8286, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r51, %r52, %r8286;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r58, %r51, %r8286;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r57, %r58, %r8286;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r56, %r57, %r8286;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r55, %r56, %r8286;
	// inline asm
	mov.u32 	%r11651, 0;
	// inline asm
	shf.r.wrap.b32 %r11650, %r11651, %r55, %r8286;
	// inline asm
	mov.u32 	%r11652, %r11651;
	bra.uni 	BB6_1023;

BB6_162:
	setp.eq.s32	%p145, %r60, 22;
	@%p145 bra 	BB6_187;
	bra.uni 	BB6_163;

BB6_187:
	mov.u32 	%r2845, 16;
	// inline asm
	shf.r.wrap.b32 %r11652, %r56, %r57, %r2845;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r55, %r56, %r2845;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11650, %r11649, %r55, %r2845;
	// inline asm
	bra.uni 	BB6_184;

BB6_692:
	setp.eq.s32	%p559, %r60, 27;
	@%p559 bra 	BB6_693;
	bra.uni 	BB6_669;

BB6_693:
	mov.u32 	%r5977, 24;
	// inline asm
	shf.r.wrap.b32 %r58, %r53, %r54, %r5977;
	// inline asm
	mov.u32 	%r11641, 0;
	// inline asm
	shf.r.wrap.b32 %r11643, %r54, %r11641, %r5977;
	// inline asm

BB6_694:
	mov.u32 	%r11642, %r11641;
	bra.uni 	BB6_722;

BB6_992:
	setp.eq.s32	%p755, %r7992, 27;
	@%p755 bra 	BB6_993;
	bra.uni 	BB6_977;

BB6_993:
	mov.u32 	%r8030, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r55, %r56, %r8030;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11654, %r11649, %r55, %r8030;
	// inline asm

BB6_994:
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;
	bra.uni 	BB6_1021;

BB6_653:
	setp.eq.s32	%p588, %r60, 7;
	@%p588 bra 	BB6_654;
	bra.uni 	BB6_669;

BB6_654:
	mov.u32 	%r6317, 24;
	// inline asm
	shf.r.wrap.b32 %r6290, %r56, %r57, %r6317;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r57, %r58, %r6317;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11642, %r58, %r51, %r6317;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11641, %r51, %r52, %r6317;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6306, %r52, %r53, %r6317;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11647, %r53, %r54, %r6317;
	// inline asm
	mov.u32 	%r11645, 0;
	// inline asm
	shf.r.wrap.b32 %r11646, %r54, %r11645, %r6317;
	// inline asm
	mov.u32 	%r58, %r6290;
	mov.u32 	%r54, %r6306;
	bra.uni 	BB6_723;

BB6_951:
	setp.eq.s32	%p784, %r7992, 7;
	@%p784 bra 	BB6_952;
	bra.uni 	BB6_977;

BB6_952:
	mov.u32 	%r8370, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r52, %r53, %r8370;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r51, %r52, %r8370;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r58, %r51, %r8370;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r57, %r58, %r8370;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r56, %r57, %r8370;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r55, %r56, %r8370;
	// inline asm
	mov.u32 	%r11652, 0;
	// inline asm
	shf.r.wrap.b32 %r11651, %r11652, %r55, %r8370;
	// inline asm
	bra.uni 	BB6_1023;

BB6_145:
	setp.eq.s32	%p157, %r60, 14;
	@%p157 bra 	BB6_192;
	bra.uni 	BB6_146;

BB6_192:
	mov.u32 	%r2979, 16;
	// inline asm
	shf.r.wrap.b32 %r11652, %r58, %r51, %r2979;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r57, %r58, %r2979;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r56, %r57, %r2979;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r55, %r56, %r2979;
	// inline asm
	mov.u32 	%r11653, 0;
	// inline asm
	shf.r.wrap.b32 %r11656, %r11653, %r55, %r2979;
	// inline asm
	bra.uni 	BB6_148;

BB6_684:
	setp.eq.s32	%p565, %r60, 23;
	@%p565 bra 	BB6_685;
	bra.uni 	BB6_669;

BB6_685:
	mov.u32 	%r6029, 24;
	// inline asm
	shf.r.wrap.b32 %r58, %r52, %r53, %r6029;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r53, %r54, %r6029;
	// inline asm
	mov.u32 	%r11641, 0;
	// inline asm
	shf.r.wrap.b32 %r11642, %r54, %r11641, %r6029;
	// inline asm
	bra.uni 	BB6_722;

BB6_983:
	setp.eq.s32	%p761, %r7992, 23;
	@%p761 bra 	BB6_984;
	bra.uni 	BB6_977;

BB6_984:
	mov.u32 	%r8082, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r56, %r57, %r8082;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r55, %r56, %r8082;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11655, %r11649, %r55, %r8082;
	// inline asm

BB6_985:
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;
	bra.uni 	BB6_1022;

BB6_668:
	setp.eq.s32	%p577, %r60, 15;
	@%p577 bra 	BB6_710;
	bra.uni 	BB6_669;

BB6_710:
	mov.u32 	%r6157, 24;
	// inline asm
	shf.r.wrap.b32 %r58, %r58, %r51, %r6157;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r51, %r52, %r6157;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11642, %r52, %r53, %r6157;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11641, %r53, %r54, %r6157;
	// inline asm
	mov.u32 	%r11645, 0;
	// inline asm
	shf.r.wrap.b32 %r54, %r54, %r11645, %r6157;
	// inline asm

BB6_713:
	mov.u32 	%r11646, %r11645;
	mov.u32 	%r11647, %r11645;
	bra.uni 	BB6_723;

BB6_966:
	setp.eq.s32	%p773, %r7992, 15;
	@%p773 bra 	BB6_967;
	bra.uni 	BB6_977;

BB6_967:
	mov.u32 	%r8210, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r58, %r51, %r8210;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r57, %r58, %r8210;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r56, %r57, %r8210;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r55, %r56, %r8210;
	// inline asm
	mov.u32 	%r11650, 0;
	// inline asm
	shf.r.wrap.b32 %r11649, %r11650, %r55, %r8210;
	// inline asm

BB6_968:
	mov.u32 	%r11651, %r11650;
	mov.u32 	%r11652, %r11650;
	bra.uni 	BB6_1023;

BB6_178:
	setp.eq.s32	%p134, %r60, 30;
	@%p134 bra 	BB6_182;
	bra.uni 	BB6_179;

BB6_182:
	mov.u32 	%r11649, 0;
	mov.u32 	%r2743, 16;
	// inline asm
	shf.r.wrap.b32 %r11652, %r11649, %r55, %r2743;
	// inline asm
	bra.uni 	BB6_181;

BB6_273:
	setp.eq.s32	%p222, %r60, 18;
	@%p222 bra 	BB6_307;
	bra.uni 	BB6_274;

BB6_307:
	mov.u32 	%r3506, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r57, %r58, %r3506;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r56, %r57, %r3506;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r55, %r56, %r3506;
	// inline asm
	mov.u32 	%r11508, 0;
	// inline asm
	shf.r.wrap.b32 %r55, %r11508, %r55, %r3506;
	// inline asm
	bra.uni 	BB6_306;

BB6_257:
	setp.eq.s32	%p234, %r60, 10;
	@%p234 bra 	BB6_311;
	bra.uni 	BB6_258;

BB6_311:
	mov.u32 	%r3656, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r51, %r52, %r3656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r58, %r51, %r3656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r57, %r58, %r3656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3645, %r56, %r57, %r3656;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11508, %r55, %r56, %r3656;
	// inline asm
	mov.u32 	%r11510, 0;
	// inline asm
	shf.r.wrap.b32 %r11509, %r11510, %r55, %r3656;
	// inline asm
	mov.u32 	%r11511, %r11510;
	mov.u32 	%r55, %r3645;
	bra.uni 	BB6_322;

BB6_289:
	setp.eq.s32	%p211, %r60, 26;
	@%p211 bra 	BB6_301;
	bra.uni 	BB6_290;

BB6_301:
	mov.u32 	%r3388, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r55, %r56, %r3388;
	// inline asm
	mov.u32 	%r11508, 0;
	// inline asm
	shf.r.wrap.b32 %r53, %r11508, %r55, %r3388;
	// inline asm
	bra.uni 	BB6_292;

BB6_700:
	setp.ne.s32	%p554, %r60, 30;
	@%p554 bra 	BB6_669;

	mov.u32 	%r11641, 0;
	mov.u32 	%r5944, 16;
	// inline asm
	shf.r.wrap.b32 %r58, %r54, %r11641, %r5944;
	// inline asm

BB6_721:
	mov.u32 	%r11642, %r11641;
	mov.u32 	%r11643, %r11641;

BB6_722:
	mov.u32 	%r11645, %r11641;
	mov.u32 	%r11646, %r11641;
	mov.u32 	%r11647, %r11641;
	mov.u32 	%r54, %r11641;
	bra.uni 	BB6_723;

BB6_669:
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r58, %r55;
	mov.u32 	%r11645, %r54;
	mov.u32 	%r11646, %r53;
	mov.u32 	%r11647, %r52;
	mov.u32 	%r54, %r51;

BB6_723:
	and.b32  	%r6486, %r61, 3;
	shl.b32 	%r6487, %r6486, 3;
	mov.u32 	%r6488, 1;
	shl.b32 	%r6489, %r6488, %r6487;
	add.s32 	%r989, %r6489, -1;
	shr.u32 	%r6485, %r62, 2;
	setp.gt.s32	%p594, %r6485, 3;
	@%p594 bra 	BB6_731;

	setp.gt.s32	%p600, %r6485, 1;
	@%p600 bra 	BB6_728;

	setp.eq.s32	%p603, %r6485, 0;
	@%p603 bra 	BB6_745;
	bra.uni 	BB6_726;

BB6_745:
	and.b32  	%r11644, %r58, %r989;
	mov.u32 	%r11641, 0;
	mov.u32 	%r11642, %r11641;
	mov.u32 	%r11643, %r11641;
	bra.uni 	BB6_744;

BB6_731:
	setp.gt.s32	%p595, %r6485, 5;
	@%p595 bra 	BB6_735;

	setp.eq.s32	%p598, %r6485, 4;
	@%p598 bra 	BB6_741;
	bra.uni 	BB6_733;

BB6_741:
	and.b32  	%r11648, %r54, %r989;
	mov.u32 	%r11645, 0;
	mov.u32 	%r11644, %r58;
	mov.u32 	%r11646, %r11645;
	mov.u32 	%r11647, %r11645;
	mov.u32 	%r11657, %r62;
	bra.uni 	BB6_1030;

BB6_728:
	setp.eq.s32	%p601, %r6485, 2;
	@%p601 bra 	BB6_742;
	bra.uni 	BB6_729;

BB6_742:
	and.b32  	%r11642, %r11642, %r989;
	mov.u32 	%r11641, 0;
	bra.uni 	BB6_743;

BB6_735:
	setp.eq.s32	%p596, %r6485, 6;
	@%p596 bra 	BB6_740;
	bra.uni 	BB6_736;

BB6_740:
	and.b32  	%r11646, %r11646, %r989;
	mov.u32 	%r11645, 0;
	bra.uni 	BB6_737;

BB6_726:
	setp.eq.s32	%p604, %r6485, 1;
	@%p604 bra 	BB6_727;
	bra.uni 	BB6_737;

BB6_727:
	and.b32  	%r11643, %r11643, %r989;
	mov.u32 	%r11641, 0;
	mov.u32 	%r11642, %r11641;

BB6_743:
	mov.u32 	%r11644, %r58;

BB6_744:
	mov.u32 	%r11645, %r11641;
	mov.u32 	%r11646, %r11641;
	mov.u32 	%r11647, %r11641;
	mov.u32 	%r11648, %r11641;
	mov.u32 	%r11657, %r62;
	bra.uni 	BB6_1030;

BB6_733:
	setp.eq.s32	%p599, %r6485, 5;
	@%p599 bra 	BB6_734;
	bra.uni 	BB6_737;

BB6_734:
	and.b32  	%r11647, %r11647, %r989;
	mov.u32 	%r11645, 0;
	mov.u32 	%r11644, %r58;
	mov.u32 	%r11646, %r11645;
	bra.uni 	BB6_738;

BB6_729:
	setp.eq.s32	%p602, %r6485, 3;
	@%p602 bra 	BB6_730;
	bra.uni 	BB6_737;

BB6_730:
	and.b32  	%r11641, %r11641, %r989;
	mov.u32 	%r11645, 0;
	mov.u32 	%r11644, %r58;
	mov.u32 	%r11646, %r11645;
	mov.u32 	%r11647, %r11645;
	mov.u32 	%r11648, %r11645;
	mov.u32 	%r11657, %r62;
	bra.uni 	BB6_1030;

BB6_736:
	setp.ne.s32	%p597, %r6485, 7;
	@%p597 bra 	BB6_737;

	and.b32  	%r11645, %r11645, %r989;

BB6_737:
	mov.u32 	%r11644, %r58;

BB6_738:
	mov.u32 	%r11648, %r54;
	mov.u32 	%r11657, %r62;
	bra.uni 	BB6_1030;

BB6_1000:
	setp.ne.s32	%p750, %r7992, 30;
	@%p750 bra 	BB6_977;

	mov.u32 	%r11649, 0;
	mov.u32 	%r7997, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r11649, %r55, %r7997;
	// inline asm

BB6_1019:
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;

BB6_1020:
	mov.u32 	%r11654, %r11649;

BB6_1021:
	mov.u32 	%r11655, %r11649;

BB6_1022:
	mov.u32 	%r11656, %r11649;
	bra.uni 	BB6_1023;

BB6_977:
	mov.u32 	%r11649, %r58;
	mov.u32 	%r11650, %r57;
	mov.u32 	%r11651, %r56;
	mov.u32 	%r11652, %r55;
	mov.u32 	%r11653, %r54;
	mov.u32 	%r11654, %r53;
	mov.u32 	%r11655, %r52;
	mov.u32 	%r11656, %r51;

BB6_1023:
	// inline asm
	prmt.b32 %r11644, %r11653, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r11643, %r11654, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r11642, %r11655, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r11641, %r11656, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r11648, %r11649, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r11647, %r11650, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r11646, %r11651, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r11645, %r11652, 0, 0x0123;
	// inline asm
	bra.uni 	BB6_1029;

BB6_249:
	setp.eq.s32	%p240, %r60, 6;
	@%p240 bra 	BB6_313;
	bra.uni 	BB6_250;

BB6_313:
	mov.u32 	%r3743, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r52, %r53, %r3743;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r51, %r52, %r3743;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r58, %r51, %r3743;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3728, %r57, %r58, %r3743;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11508, %r56, %r57, %r3743;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11509, %r55, %r56, %r3743;
	// inline asm
	mov.u32 	%r11511, 0;
	// inline asm
	shf.r.wrap.b32 %r11510, %r11511, %r55, %r3743;
	// inline asm
	mov.u32 	%r55, %r3728;
	bra.uni 	BB6_322;

BB6_280:
	setp.eq.s32	%p217, %r60, 22;
	@%p217 bra 	BB6_303;
	bra.uni 	BB6_281;

BB6_303:
	mov.u32 	%r3443, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r56, %r57, %r3443;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r55, %r56, %r3443;
	// inline asm
	mov.u32 	%r11508, 0;
	// inline asm
	shf.r.wrap.b32 %r52, %r11508, %r55, %r3443;
	// inline asm
	bra.uni 	BB6_283;

BB6_264:
	setp.eq.s32	%p229, %r60, 14;
	@%p229 bra 	BB6_309;
	bra.uni 	BB6_265;

BB6_309:
	mov.u32 	%r3577, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r58, %r51, %r3577;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r57, %r58, %r3577;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r56, %r57, %r3577;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3570, %r55, %r56, %r3577;
	// inline asm
	mov.u32 	%r11509, 0;
	// inline asm
	shf.r.wrap.b32 %r11508, %r11509, %r55, %r3577;
	// inline asm
	mov.u32 	%r11510, %r11509;
	mov.u32 	%r11511, %r11509;
	mov.u32 	%r55, %r3570;
	bra.uni 	BB6_322;

BB6_297:
	setp.eq.s32	%p206, %r60, 31;
	@%p206 bra 	BB6_317;
	bra.uni 	BB6_298;

BB6_317:
	mov.u32 	%r11508, 0;
	mov.u32 	%r3874, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r11508, %r55, %r3874;
	// inline asm
	bra.uni 	BB6_318;

BB6_423:
	mov.u32 	%r4691, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r53, %r54, %r4691;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r52, %r53, %r4691;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r51, %r52, %r4691;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4672, %r58, %r51, %r4691;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11527, %r57, %r58, %r4691;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11528, %r56, %r57, %r4691;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11529, %r55, %r56, %r4691;
	// inline asm
	mov.u32 	%r4689, 0;
	// inline asm
	shf.r.wrap.b32 %r11530, %r4689, %r55, %r4691;
	// inline asm
	mov.u32 	%r55, %r4672;
	bra.uni 	BB6_429;

BB6_350:
	setp.eq.s32	%p327, %r60, 3;
	@%p327 bra 	BB6_351;
	bra.uni 	BB6_382;

BB6_351:
	mov.u32 	%r4627, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r53, %r54, %r4627;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r52, %r53, %r4627;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r51, %r52, %r4627;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4608, %r58, %r51, %r4627;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11527, %r57, %r58, %r4627;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11528, %r56, %r57, %r4627;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11529, %r55, %r56, %r4627;
	// inline asm
	mov.u32 	%r4625, 0;
	// inline asm
	shf.r.wrap.b32 %r11530, %r4625, %r55, %r4627;
	// inline asm
	mov.u32 	%r55, %r4608;
	bra.uni 	BB6_429;

BB6_533:
	setp.eq.s32	%p534, %r62, 1;
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r11652;
	mov.u32 	%r11654, %r11652;
	mov.u32 	%r11655, %r11652;
	mov.u32 	%r11656, %r11652;
	@%p534 bra 	BB6_534;
	bra.uni 	BB6_614;

BB6_534:
	mov.u32 	%r5918, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r55, %r56, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r56, %r57, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r57, %r58, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r58, %r51, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r51, %r52, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r52, %r53, %r5918;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r53, %r54, %r5918;
	// inline asm
	mov.u32 	%r5917, 0;
	// inline asm
	shf.r.wrap.b32 %r11652, %r54, %r5917, %r5918;
	// inline asm
	bra.uni 	BB6_614;

BB6_803:
	setp.eq.s32	%p704, %r6825, 2;
	@%p704 bra 	BB6_879;
	bra.uni 	BB6_804;

BB6_879:
	mov.u32 	%r7338, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r53, %r54, %r7338;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r52, %r53, %r7338;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r51, %r52, %r7338;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r58, %r51, %r7338;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r57, %r58, %r7338;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r56, %r57, %r7338;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r55, %r56, %r7338;
	// inline asm
	mov.u32 	%r7336, 0;
	// inline asm
	shf.r.wrap.b32 %r11652, %r7336, %r55, %r7338;
	// inline asm
	bra.uni 	BB6_880;

BB6_565:
	setp.eq.s32	%p511, %r62, 17;
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r11652;
	mov.u32 	%r11654, %r11652;
	mov.u32 	%r11655, %r11652;
	mov.u32 	%r11656, %r11652;
	@%p511 bra 	BB6_566;
	bra.uni 	BB6_614;

BB6_566:
	mov.u32 	%r5574, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r51, %r52, %r5574;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r52, %r53, %r5574;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r53, %r54, %r5574;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11656, %r54, %r11649, %r5574;
	// inline asm
	bra.uni 	BB6_570;

BB6_378:
	setp.eq.s32	%p306, %r60, 17;
	@%p306 bra 	BB6_379;
	bra.uni 	BB6_382;

BB6_379:
	mov.u32 	%r4347, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r57, %r58, %r4347;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r56, %r57, %r4347;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r55, %r56, %r4347;
	// inline asm
	mov.u32 	%r11527, 0;
	// inline asm
	shf.r.wrap.b32 %r55, %r11527, %r55, %r4347;
	// inline asm
	bra.uni 	BB6_413;

BB6_548:
	setp.eq.s32	%p523, %r62, 9;
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r11652;
	mov.u32 	%r11654, %r11652;
	mov.u32 	%r11655, %r11652;
	mov.u32 	%r11656, %r11652;
	@%p523 bra 	BB6_549;
	bra.uni 	BB6_614;

BB6_549:
	mov.u32 	%r5730, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r57, %r58, %r5730;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r58, %r51, %r5730;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r51, %r52, %r5730;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r52, %r53, %r5730;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r53, %r54, %r5730;
	// inline asm
	mov.u32 	%r11651, 0;
	// inline asm
	shf.r.wrap.b32 %r11650, %r54, %r11651, %r5730;
	// inline asm
	mov.u32 	%r11652, %r11651;
	bra.uni 	BB6_614;

BB6_835:
	setp.eq.s32	%p681, %r6825, 18;
	@%p681 bra 	BB6_871;
	bra.uni 	BB6_836;

BB6_871:
	mov.u32 	%r7006, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r57, %r58, %r7006;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r56, %r57, %r7006;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r55, %r56, %r7006;
	// inline asm
	mov.u32 	%r11652, 0;
	// inline asm
	shf.r.wrap.b32 %r11656, %r11652, %r55, %r7006;
	// inline asm
	bra.uni 	BB6_838;

BB6_362:
	setp.eq.s32	%p318, %r60, 9;
	@%p318 bra 	BB6_363;
	bra.uni 	BB6_382;

BB6_363:
	mov.u32 	%r4503, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r51, %r52, %r4503;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r58, %r51, %r4503;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r57, %r58, %r4503;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4492, %r56, %r57, %r4503;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11527, %r55, %r56, %r4503;
	// inline asm
	mov.u32 	%r11529, 0;
	// inline asm
	shf.r.wrap.b32 %r11528, %r11529, %r55, %r4503;
	// inline asm
	mov.u32 	%r11530, %r11529;
	mov.u32 	%r55, %r4492;
	bra.uni 	BB6_429;

BB6_582:
	setp.eq.s32	%p500, %r62, 25;
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r11652;
	mov.u32 	%r11654, %r11652;
	mov.u32 	%r11655, %r11652;
	mov.u32 	%r11656, %r11652;
	@%p500 bra 	BB6_583;
	bra.uni 	BB6_614;

BB6_583:
	mov.u32 	%r5450, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r53, %r54, %r5450;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11654, %r54, %r11649, %r5450;
	// inline asm
	bra.uni 	BB6_587;

BB6_394:
	setp.eq.s32	%p295, %r60, 25;
	@%p295 bra 	BB6_395;
	bra.uni 	BB6_382;

BB6_395:
	mov.u32 	%r4223, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r55, %r56, %r4223;
	// inline asm
	mov.u32 	%r11527, 0;
	// inline asm
	shf.r.wrap.b32 %r53, %r11527, %r55, %r4223;
	// inline asm
	bra.uni 	BB6_399;

BB6_540:
	setp.eq.s32	%p529, %r62, 5;
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r11652;
	mov.u32 	%r11654, %r11652;
	mov.u32 	%r11655, %r11652;
	mov.u32 	%r11656, %r11652;
	@%p529 bra 	BB6_541;
	bra.uni 	BB6_614;

BB6_541:
	mov.u32 	%r5820, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r56, %r57, %r5820;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r57, %r58, %r5820;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r58, %r51, %r5820;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r51, %r52, %r5820;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r52, %r53, %r5820;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r53, %r54, %r5820;
	// inline asm
	mov.u32 	%r11652, 0;
	// inline asm
	shf.r.wrap.b32 %r11651, %r54, %r11652, %r5820;
	// inline asm
	bra.uni 	BB6_614;

BB6_818:
	setp.eq.s32	%p693, %r6825, 10;
	@%p693 bra 	BB6_875;
	bra.uni 	BB6_819;

BB6_875:
	mov.u32 	%r7156, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r51, %r52, %r7156;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r58, %r51, %r7156;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r57, %r58, %r7156;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r56, %r57, %r7156;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r55, %r56, %r7156;
	// inline asm
	mov.u32 	%r11652, 0;
	// inline asm
	shf.r.wrap.b32 %r11650, %r11652, %r55, %r7156;
	// inline asm
	mov.u32 	%r11651, %r11652;
	bra.uni 	BB6_880;

BB6_354:
	setp.eq.s32	%p324, %r60, 5;
	@%p324 bra 	BB6_355;
	bra.uni 	BB6_382;

BB6_355:
	mov.u32 	%r4593, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r52, %r53, %r4593;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r51, %r52, %r4593;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r58, %r51, %r4593;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4578, %r57, %r58, %r4593;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11527, %r56, %r57, %r4593;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11528, %r55, %r56, %r4593;
	// inline asm
	mov.u32 	%r11530, 0;
	// inline asm
	shf.r.wrap.b32 %r11529, %r11530, %r55, %r4593;
	// inline asm
	mov.u32 	%r55, %r4578;
	bra.uni 	BB6_429;

BB6_573:
	setp.eq.s32	%p506, %r62, 21;
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r11652;
	mov.u32 	%r11654, %r11652;
	mov.u32 	%r11655, %r11652;
	mov.u32 	%r11656, %r11652;
	@%p506 bra 	BB6_574;
	bra.uni 	BB6_614;

BB6_574:
	mov.u32 	%r5508, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r52, %r53, %r5508;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r53, %r54, %r5508;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11655, %r54, %r11649, %r5508;
	// inline asm
	bra.uni 	BB6_578;

BB6_385:
	setp.eq.s32	%p301, %r60, 21;
	@%p301 bra 	BB6_386;
	bra.uni 	BB6_382;

BB6_386:
	mov.u32 	%r4281, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r56, %r57, %r4281;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r55, %r56, %r4281;
	// inline asm
	mov.u32 	%r11527, 0;
	// inline asm
	shf.r.wrap.b32 %r52, %r11527, %r55, %r4281;
	// inline asm
	bra.uni 	BB6_390;

BB6_555:
	setp.eq.s32	%p518, %r62, 13;
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r11652;
	mov.u32 	%r11654, %r11652;
	mov.u32 	%r11655, %r11652;
	mov.u32 	%r11656, %r11652;
	@%p518 bra 	BB6_556;
	bra.uni 	BB6_614;

BB6_556:
	mov.u32 	%r5648, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r58, %r51, %r5648;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r51, %r52, %r5648;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r52, %r53, %r5648;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r53, %r54, %r5648;
	// inline asm
	mov.u32 	%r11650, 0;
	// inline asm
	shf.r.wrap.b32 %r11649, %r54, %r11650, %r5648;
	// inline asm
	bra.uni 	BB6_560;

BB6_851:
	setp.eq.s32	%p670, %r6825, 26;
	@%p670 bra 	BB6_867;
	bra.uni 	BB6_852;

BB6_867:
	mov.u32 	%r6888, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r55, %r56, %r6888;
	// inline asm
	mov.u32 	%r11656, 0;
	// inline asm
	shf.r.wrap.b32 %r11654, %r11656, %r55, %r6888;
	// inline asm
	bra.uni 	BB6_854;

BB6_369:
	setp.eq.s32	%p313, %r60, 13;
	@%p313 bra 	BB6_370;
	bra.uni 	BB6_382;

BB6_370:
	mov.u32 	%r4421, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r58, %r51, %r4421;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r57, %r58, %r4421;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r56, %r57, %r4421;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4414, %r55, %r56, %r4421;
	// inline asm
	mov.u32 	%r11528, 0;
	// inline asm
	shf.r.wrap.b32 %r11527, %r11528, %r55, %r4421;
	// inline asm
	mov.u32 	%r11529, %r11528;
	mov.u32 	%r11530, %r11528;
	mov.u32 	%r55, %r4414;
	bra.uni 	BB6_429;

BB6_591:
	setp.eq.s32	%p495, %r62, 29;
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r11652;
	mov.u32 	%r11654, %r11652;
	mov.u32 	%r11655, %r11652;
	mov.u32 	%r11656, %r11652;
	@%p495 bra 	BB6_592;
	bra.uni 	BB6_614;

BB6_592:
	mov.u32 	%r11649, 0;
	mov.u32 	%r5400, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r54, %r11649, %r5400;
	// inline asm
	bra.uni 	BB6_596;

BB6_402:
	setp.eq.s32	%p290, %r60, 29;
	@%p290 bra 	BB6_403;
	bra.uni 	BB6_382;

BB6_403:
	mov.u32 	%r11527, 0;
	mov.u32 	%r4173, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r11527, %r55, %r4173;
	// inline asm
	bra.uni 	BB6_425;

BB6_316:
	mov.u32 	%r3870, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r53, %r54, %r3870;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r52, %r53, %r3870;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r51, %r52, %r3870;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3851, %r58, %r51, %r3870;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11508, %r57, %r58, %r3870;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11509, %r56, %r57, %r3870;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11510, %r55, %r56, %r3870;
	// inline asm
	mov.u32 	%r3868, 0;
	// inline asm
	shf.r.wrap.b32 %r11511, %r3868, %r55, %r3870;
	// inline asm
	mov.u32 	%r55, %r3851;
	bra.uni 	BB6_322;

BB6_243:
	setp.eq.s32	%p246, %r60, 3;
	@%p246 bra 	BB6_244;
	bra.uni 	BB6_275;

BB6_244:
	mov.u32 	%r3806, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r53, %r54, %r3806;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r52, %r53, %r3806;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r51, %r52, %r3806;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3787, %r58, %r51, %r3806;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11508, %r57, %r58, %r3806;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11509, %r56, %r57, %r3806;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11510, %r55, %r56, %r3806;
	// inline asm
	mov.u32 	%r3804, 0;
	// inline asm
	shf.r.wrap.b32 %r11511, %r3804, %r55, %r3806;
	// inline asm
	mov.u32 	%r55, %r3787;
	bra.uni 	BB6_322;

BB6_536:
	setp.eq.s32	%p532, %r62, 3;
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r11652;
	mov.u32 	%r11654, %r11652;
	mov.u32 	%r11655, %r11652;
	mov.u32 	%r11656, %r11652;
	@%p532 bra 	BB6_537;
	bra.uni 	BB6_614;

BB6_537:
	mov.u32 	%r5854, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r55, %r56, %r5854;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r56, %r57, %r5854;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r57, %r58, %r5854;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r58, %r51, %r5854;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r51, %r52, %r5854;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r52, %r53, %r5854;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r53, %r54, %r5854;
	// inline asm
	mov.u32 	%r5853, 0;
	// inline asm
	shf.r.wrap.b32 %r11652, %r54, %r5853, %r5854;
	// inline asm
	bra.uni 	BB6_614;

BB6_810:
	setp.eq.s32	%p699, %r6825, 6;
	@%p699 bra 	BB6_877;
	bra.uni 	BB6_811;

BB6_877:
	mov.u32 	%r7243, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r52, %r53, %r7243;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r51, %r52, %r7243;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r58, %r51, %r7243;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r57, %r58, %r7243;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r56, %r57, %r7243;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r55, %r56, %r7243;
	// inline asm
	mov.u32 	%r11652, 0;
	// inline asm
	shf.r.wrap.b32 %r11651, %r11652, %r55, %r7243;
	// inline asm
	bra.uni 	BB6_880;

BB6_568:
	setp.eq.s32	%p509, %r62, 19;
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r11652;
	mov.u32 	%r11654, %r11652;
	mov.u32 	%r11655, %r11652;
	mov.u32 	%r11656, %r11652;
	@%p509 bra 	BB6_569;
	bra.uni 	BB6_614;

BB6_569:
	mov.u32 	%r5534, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r51, %r52, %r5534;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r52, %r53, %r5534;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r53, %r54, %r5534;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11656, %r54, %r11649, %r5534;
	// inline asm

BB6_570:
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;
	bra.uni 	BB6_614;

BB6_381:
	setp.eq.s32	%p304, %r60, 19;
	@%p304 bra 	BB6_412;
	bra.uni 	BB6_382;

BB6_412:
	mov.u32 	%r4307, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r57, %r58, %r4307;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r56, %r57, %r4307;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r55, %r56, %r4307;
	// inline asm
	mov.u32 	%r11527, 0;
	// inline asm
	shf.r.wrap.b32 %r55, %r11527, %r55, %r4307;
	// inline asm

BB6_413:
	mov.u32 	%r11528, %r11527;
	mov.u32 	%r11529, %r11527;
	mov.u32 	%r11530, %r11527;
	bra.uni 	BB6_429;

BB6_551:
	setp.eq.s32	%p521, %r62, 11;
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r11652;
	mov.u32 	%r11654, %r11652;
	mov.u32 	%r11655, %r11652;
	mov.u32 	%r11656, %r11652;
	@%p521 bra 	BB6_552;
	bra.uni 	BB6_614;

BB6_552:
	mov.u32 	%r5678, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r57, %r58, %r5678;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r58, %r51, %r5678;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r51, %r52, %r5678;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r52, %r53, %r5678;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r53, %r54, %r5678;
	// inline asm
	mov.u32 	%r11651, 0;
	// inline asm
	shf.r.wrap.b32 %r11650, %r54, %r11651, %r5678;
	// inline asm
	mov.u32 	%r11652, %r11651;
	bra.uni 	BB6_614;

BB6_843:
	setp.eq.s32	%p676, %r6825, 22;
	@%p676 bra 	BB6_869;
	bra.uni 	BB6_844;

BB6_869:
	mov.u32 	%r6943, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r56, %r57, %r6943;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r55, %r56, %r6943;
	// inline asm
	mov.u32 	%r11656, 0;
	// inline asm
	shf.r.wrap.b32 %r11655, %r11656, %r55, %r6943;
	// inline asm
	bra.uni 	BB6_855;

BB6_365:
	setp.eq.s32	%p316, %r60, 11;
	@%p316 bra 	BB6_366;
	bra.uni 	BB6_382;

BB6_366:
	mov.u32 	%r4451, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r51, %r52, %r4451;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r58, %r51, %r4451;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r57, %r58, %r4451;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4440, %r56, %r57, %r4451;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11527, %r55, %r56, %r4451;
	// inline asm
	mov.u32 	%r11529, 0;
	// inline asm
	shf.r.wrap.b32 %r11528, %r11529, %r55, %r4451;
	// inline asm
	mov.u32 	%r11530, %r11529;
	mov.u32 	%r55, %r4440;
	bra.uni 	BB6_429;

BB6_585:
	setp.eq.s32	%p498, %r62, 27;
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r11652;
	mov.u32 	%r11654, %r11652;
	mov.u32 	%r11655, %r11652;
	mov.u32 	%r11656, %r11652;
	@%p498 bra 	BB6_586;
	bra.uni 	BB6_614;

BB6_586:
	mov.u32 	%r5422, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r53, %r54, %r5422;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11654, %r54, %r11649, %r5422;
	// inline asm

BB6_587:
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;
	bra.uni 	BB6_588;

BB6_397:
	setp.eq.s32	%p293, %r60, 27;
	@%p293 bra 	BB6_398;
	bra.uni 	BB6_382;

BB6_398:
	mov.u32 	%r4195, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r55, %r56, %r4195;
	// inline asm
	mov.u32 	%r11527, 0;
	// inline asm
	shf.r.wrap.b32 %r53, %r11527, %r55, %r4195;
	// inline asm

BB6_399:
	mov.u32 	%r11528, %r11527;
	mov.u32 	%r11529, %r11527;
	mov.u32 	%r11530, %r11527;
	bra.uni 	BB6_427;

BB6_543:
	setp.eq.s32	%p527, %r62, 7;
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r11652;
	mov.u32 	%r11654, %r11652;
	mov.u32 	%r11655, %r11652;
	mov.u32 	%r11656, %r11652;
	@%p527 bra 	BB6_544;
	bra.uni 	BB6_614;

BB6_544:
	mov.u32 	%r5762, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r56, %r57, %r5762;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r57, %r58, %r5762;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r58, %r51, %r5762;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r51, %r52, %r5762;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r52, %r53, %r5762;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r53, %r54, %r5762;
	// inline asm
	mov.u32 	%r11652, 0;
	// inline asm
	shf.r.wrap.b32 %r11651, %r54, %r11652, %r5762;
	// inline asm
	bra.uni 	BB6_614;

BB6_825:
	setp.eq.s32	%p688, %r6825, 14;
	@%p688 bra 	BB6_873;
	bra.uni 	BB6_826;

BB6_873:
	mov.u32 	%r7077, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r58, %r51, %r7077;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r57, %r58, %r7077;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r56, %r57, %r7077;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r55, %r56, %r7077;
	// inline asm
	mov.u32 	%r11652, 0;
	// inline asm
	shf.r.wrap.b32 %r11649, %r11652, %r55, %r7077;
	// inline asm
	bra.uni 	BB6_828;

BB6_357:
	setp.eq.s32	%p322, %r60, 7;
	@%p322 bra 	BB6_358;
	bra.uni 	BB6_382;

BB6_358:
	mov.u32 	%r4535, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r52, %r53, %r4535;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r51, %r52, %r4535;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r58, %r51, %r4535;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4520, %r57, %r58, %r4535;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11527, %r56, %r57, %r4535;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11528, %r55, %r56, %r4535;
	// inline asm
	mov.u32 	%r11530, 0;
	// inline asm
	shf.r.wrap.b32 %r11529, %r11530, %r55, %r4535;
	// inline asm
	mov.u32 	%r55, %r4520;
	bra.uni 	BB6_429;

BB6_576:
	setp.eq.s32	%p504, %r62, 23;
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r11652;
	mov.u32 	%r11654, %r11652;
	mov.u32 	%r11655, %r11652;
	mov.u32 	%r11656, %r11652;
	@%p504 bra 	BB6_577;
	bra.uni 	BB6_614;

BB6_577:
	mov.u32 	%r5474, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r52, %r53, %r5474;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r53, %r54, %r5474;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11655, %r54, %r11649, %r5474;
	// inline asm

BB6_578:
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;
	mov.u32 	%r11656, %r11649;
	bra.uni 	BB6_614;

BB6_388:
	setp.eq.s32	%p299, %r60, 23;
	@%p299 bra 	BB6_389;
	bra.uni 	BB6_382;

BB6_389:
	mov.u32 	%r4247, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r56, %r57, %r4247;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r55, %r56, %r4247;
	// inline asm
	mov.u32 	%r11527, 0;
	// inline asm
	shf.r.wrap.b32 %r52, %r11527, %r55, %r4247;
	// inline asm

BB6_390:
	mov.u32 	%r11528, %r11527;
	mov.u32 	%r11529, %r11527;
	mov.u32 	%r11530, %r11527;
	bra.uni 	BB6_428;

BB6_558:
	setp.eq.s32	%p516, %r62, 15;
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r11652;
	mov.u32 	%r11654, %r11652;
	mov.u32 	%r11655, %r11652;
	mov.u32 	%r11656, %r11652;
	@%p516 bra 	BB6_559;
	bra.uni 	BB6_614;

BB6_559:
	mov.u32 	%r5602, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r58, %r51, %r5602;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r51, %r52, %r5602;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r52, %r53, %r5602;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r53, %r54, %r5602;
	// inline asm
	mov.u32 	%r11650, 0;
	// inline asm
	shf.r.wrap.b32 %r11649, %r54, %r11650, %r5602;
	// inline asm

BB6_560:
	mov.u32 	%r11651, %r11650;
	mov.u32 	%r11652, %r11650;
	bra.uni 	BB6_614;

BB6_860:
	setp.eq.s32	%p665, %r6825, 30;
	@%p665 bra 	BB6_864;
	bra.uni 	BB6_861;

BB6_864:
	mov.u32 	%r11656, 0;
	mov.u32 	%r6841, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r11656, %r55, %r6841;
	// inline asm
	bra.uni 	BB6_863;

BB6_372:
	setp.eq.s32	%p311, %r60, 15;
	@%p311 bra 	BB6_373;
	bra.uni 	BB6_382;

BB6_373:
	mov.u32 	%r4375, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r58, %r51, %r4375;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r57, %r58, %r4375;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r56, %r57, %r4375;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4368, %r55, %r56, %r4375;
	// inline asm
	mov.u32 	%r11528, 0;
	// inline asm
	shf.r.wrap.b32 %r11527, %r11528, %r55, %r4375;
	// inline asm
	mov.u32 	%r11529, %r11528;
	mov.u32 	%r11530, %r11528;
	mov.u32 	%r55, %r4368;
	bra.uni 	BB6_429;

BB6_594:
	setp.ne.s32	%p493, %r62, 31;
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r11652;
	mov.u32 	%r11654, %r11652;
	mov.u32 	%r11655, %r11652;
	mov.u32 	%r11656, %r11652;
	@%p493 bra 	BB6_614;

	mov.u32 	%r11649, 0;
	mov.u32 	%r5378, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r54, %r11649, %r5378;
	// inline asm

BB6_596:
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;
	mov.u32 	%r11654, %r11649;

BB6_588:
	mov.u32 	%r11655, %r11649;
	mov.u32 	%r11656, %r11649;

BB6_614:
	and.b32  	%r5920, %r59, 3;
	shl.b32 	%r5921, %r5920, 3;
	mov.u32 	%r5922, 1;
	shl.b32 	%r5923, %r5922, %r5921;
	add.s32 	%r854, %r5923, -1;
	neg.s32 	%r855, %r5923;
	shr.u32 	%r5919, %r60, 2;
	setp.gt.s32	%p535, %r5919, 3;
	@%p535 bra 	BB6_624;

	setp.gt.s32	%p541, %r5919, 1;
	@%p541 bra 	BB6_619;

	setp.eq.s32	%p544, %r5919, 0;
	@%p544 bra 	BB6_637;
	bra.uni 	BB6_617;

BB6_637:
	and.b32  	%r5938, %r854, %r55;
	and.b32  	%r5939, %r11653, %r855;
	or.b32  	%r11644, %r5939, %r5938;
	mov.u32 	%r11641, %r11656;
	mov.u32 	%r11642, %r11655;
	mov.u32 	%r11643, %r11654;
	bra.uni 	BB6_638;

BB6_624:
	setp.gt.s32	%p536, %r5919, 5;
	@%p536 bra 	BB6_628;

	setp.eq.s32	%p539, %r5919, 4;
	@%p539 bra 	BB6_635;
	bra.uni 	BB6_626;

BB6_635:
	and.b32  	%r5930, %r854, %r51;
	and.b32  	%r5931, %r11649, %r855;
	or.b32  	%r11648, %r5931, %r5930;
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11645, %r11652;
	mov.u32 	%r11646, %r11651;
	mov.u32 	%r11647, %r11650;
	bra.uni 	BB6_639;

BB6_619:
	setp.eq.s32	%p542, %r5919, 2;
	@%p542 bra 	BB6_636;
	bra.uni 	BB6_620;

BB6_636:
	and.b32  	%r5934, %r854, %r57;
	and.b32  	%r5935, %r11655, %r855;
	or.b32  	%r11642, %r5935, %r5934;
	mov.u32 	%r11641, %r11656;
	bra.uni 	BB6_622;

BB6_628:
	setp.eq.s32	%p537, %r5919, 6;
	@%p537 bra 	BB6_634;
	bra.uni 	BB6_629;

BB6_634:
	and.b32  	%r5926, %r854, %r53;
	and.b32  	%r5927, %r11651, %r855;
	or.b32  	%r11646, %r5927, %r5926;
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11645, %r11652;
	bra.uni 	BB6_632;

BB6_617:
	setp.eq.s32	%p545, %r5919, 1;
	@%p545 bra 	BB6_618;
	bra.uni 	BB6_630;

BB6_618:
	and.b32  	%r5936, %r854, %r56;
	and.b32  	%r5937, %r11654, %r855;
	or.b32  	%r11643, %r5937, %r5936;
	mov.u32 	%r11641, %r11656;
	mov.u32 	%r11642, %r11655;
	bra.uni 	BB6_623;

BB6_626:
	setp.eq.s32	%p540, %r5919, 5;
	@%p540 bra 	BB6_627;
	bra.uni 	BB6_630;

BB6_627:
	and.b32  	%r5928, %r854, %r52;
	and.b32  	%r5929, %r11650, %r855;
	or.b32  	%r11647, %r5929, %r5928;
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11645, %r11652;
	mov.u32 	%r11646, %r11651;
	mov.u32 	%r11648, %r51;
	bra.uni 	BB6_639;

BB6_620:
	setp.eq.s32	%p543, %r5919, 3;
	@%p543 bra 	BB6_621;
	bra.uni 	BB6_630;

BB6_621:
	and.b32  	%r5932, %r854, %r58;
	and.b32  	%r5933, %r11656, %r855;
	or.b32  	%r11641, %r5933, %r5932;
	mov.u32 	%r11642, %r57;

BB6_622:
	mov.u32 	%r11643, %r56;

BB6_623:
	mov.u32 	%r11644, %r55;

BB6_638:
	mov.u32 	%r11645, %r11652;
	mov.u32 	%r11646, %r11651;
	mov.u32 	%r11647, %r11650;
	mov.u32 	%r11648, %r11649;
	bra.uni 	BB6_639;

BB6_629:
	setp.ne.s32	%p538, %r5919, 7;
	@%p538 bra 	BB6_630;

	and.b32  	%r5924, %r854, %r54;
	and.b32  	%r5925, %r11652, %r855;
	or.b32  	%r11645, %r5925, %r5924;
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	bra.uni 	BB6_631;

BB6_630:
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11645, %r54;

BB6_631:
	mov.u32 	%r11646, %r53;

BB6_632:
	mov.u32 	%r11647, %r52;
	mov.u32 	%r11648, %r51;

BB6_639:
	sub.s32 	%r11657, %r42, %r62;
	bra.uni 	BB6_1030;

BB6_405:
	setp.ne.s32	%p288, %r60, 30;
	@%p288 bra 	BB6_382;

	mov.u32 	%r11527, 0;
	mov.u32 	%r4162, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r11527, %r55, %r4162;
	// inline asm

BB6_425:
	mov.u32 	%r11528, %r11527;
	mov.u32 	%r11529, %r11527;
	mov.u32 	%r11530, %r11527;

BB6_426:
	mov.u32 	%r53, %r11527;

BB6_427:
	mov.u32 	%r52, %r11527;

BB6_428:
	mov.u32 	%r55, %r11527;
	bra.uni 	BB6_429;

BB6_382:
	mov.u32 	%r11527, %r58;
	mov.u32 	%r11528, %r57;
	mov.u32 	%r11529, %r56;
	mov.u32 	%r11530, %r55;
	mov.u32 	%r55, %r51;

BB6_429:
	and.b32  	%r4704, %r59, 3;
	shl.b32 	%r4705, %r4704, 3;
	mov.u32 	%r4706, 1;
	shl.b32 	%r4707, %r4706, %r4705;
	add.s32 	%r627, %r4707, -1;
	shr.u32 	%r4703, %r60, 2;
	setp.gt.s32	%p328, %r4703, 3;
	@%p328 bra 	BB6_437;

	setp.gt.s32	%p334, %r4703, 1;
	@%p334 bra 	BB6_434;

	setp.eq.s32	%p337, %r4703, 0;
	@%p337 bra 	BB6_449;
	bra.uni 	BB6_432;

BB6_449:
	and.b32  	%r11653, %r11653, %r627;
	mov.u32 	%r11649, 0;
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;
	mov.u32 	%r11654, %r11649;
	bra.uni 	BB6_450;

BB6_437:
	setp.gt.s32	%p329, %r4703, 5;
	@%p329 bra 	BB6_441;

	setp.eq.s32	%p332, %r4703, 4;
	@%p332 bra 	BB6_447;
	bra.uni 	BB6_439;

BB6_447:
	and.b32  	%r11649, %r11653, %r627;
	mov.u32 	%r11650, 0;
	mov.u32 	%r11651, %r11650;
	mov.u32 	%r11652, %r11650;
	bra.uni 	BB6_445;

BB6_434:
	setp.eq.s32	%p335, %r4703, 2;
	@%p335 bra 	BB6_448;
	bra.uni 	BB6_435;

BB6_448:
	and.b32  	%r11655, %r11653, %r627;
	mov.u32 	%r11649, 0;
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;
	mov.u32 	%r11654, %r11653;
	bra.uni 	BB6_451;

BB6_441:
	setp.eq.s32	%p330, %r4703, 6;
	@%p330 bra 	BB6_446;
	bra.uni 	BB6_442;

BB6_446:
	and.b32  	%r11651, %r11653, %r627;
	mov.u32 	%r11652, 0;
	mov.u32 	%r11649, %r11653;
	mov.u32 	%r11650, %r11653;
	bra.uni 	BB6_445;

BB6_432:
	setp.eq.s32	%p338, %r4703, 1;
	@%p338 bra 	BB6_433;
	bra.uni 	BB6_443;

BB6_433:
	and.b32  	%r11654, %r11653, %r627;
	mov.u32 	%r11649, 0;
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;

BB6_450:
	mov.u32 	%r11655, %r11649;

BB6_451:
	mov.u32 	%r11656, %r11649;
	bra.uni 	BB6_452;

BB6_439:
	setp.eq.s32	%p333, %r4703, 5;
	@%p333 bra 	BB6_440;
	bra.uni 	BB6_443;

BB6_440:
	and.b32  	%r11650, %r11653, %r627;
	mov.u32 	%r11651, 0;
	mov.u32 	%r11649, %r11653;
	mov.u32 	%r11652, %r11651;
	bra.uni 	BB6_445;

BB6_435:
	setp.eq.s32	%p336, %r4703, 3;
	@%p336 bra 	BB6_436;
	bra.uni 	BB6_443;

BB6_436:
	and.b32  	%r11656, %r11653, %r627;
	mov.u32 	%r11649, 0;
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;
	mov.u32 	%r11654, %r11653;
	mov.u32 	%r11655, %r11653;
	bra.uni 	BB6_452;

BB6_442:
	setp.ne.s32	%p331, %r4703, 7;
	@%p331 bra 	BB6_443;

	and.b32  	%r11652, %r11653, %r627;
	mov.u32 	%r11649, %r11653;
	mov.u32 	%r11650, %r11653;
	mov.u32 	%r11651, %r11653;
	bra.uni 	BB6_445;

BB6_443:
	mov.u32 	%r11649, %r11653;
	mov.u32 	%r11650, %r11653;
	mov.u32 	%r11651, %r11653;
	mov.u32 	%r11652, %r11653;

BB6_445:
	mov.u32 	%r11654, %r11653;
	mov.u32 	%r11655, %r11653;
	mov.u32 	%r11656, %r11653;

BB6_452:
	or.b32  	%r11644, %r11653, %r11530;
	or.b32  	%r11643, %r11654, %r11529;
	or.b32  	%r11642, %r11655, %r11528;
	or.b32  	%r11641, %r11656, %r11527;
	or.b32  	%r11648, %r11649, %r55;
	bra.uni 	BB6_220;

BB6_121:
	setp.eq.s32	%p176, %r60, 1;
	@%p176 bra 	BB6_122;
	bra.uni 	BB6_199;

BB6_122:
	mov.u32 	%r3272, 24;
	// inline asm
	shf.r.wrap.b32 %r11652, %r53, %r54, %r3272;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r52, %r53, %r3272;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r51, %r52, %r3272;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r58, %r51, %r3272;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r57, %r58, %r3272;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r56, %r57, %r3272;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r55, %r56, %r3272;
	// inline asm
	mov.u32 	%r3270, 0;
	// inline asm
	shf.r.wrap.b32 %r11653, %r3270, %r55, %r3272;
	// inline asm
	bra.uni 	BB6_199;

BB6_153:
	setp.eq.s32	%p153, %r60, 17;
	@%p153 bra 	BB6_154;
	bra.uni 	BB6_199;

BB6_154:
	mov.u32 	%r2928, 24;
	// inline asm
	shf.r.wrap.b32 %r11652, %r57, %r58, %r2928;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r56, %r57, %r2928;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r55, %r56, %r2928;
	// inline asm
	mov.u32 	%r11653, 0;
	// inline asm
	shf.r.wrap.b32 %r11649, %r11653, %r55, %r2928;
	// inline asm
	bra.uni 	BB6_191;

BB6_136:
	setp.eq.s32	%p165, %r60, 9;
	@%p165 bra 	BB6_137;
	bra.uni 	BB6_199;

BB6_137:
	mov.u32 	%r3084, 24;
	// inline asm
	shf.r.wrap.b32 %r11652, %r51, %r52, %r3084;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r58, %r51, %r3084;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r57, %r58, %r3084;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r56, %r57, %r3084;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r55, %r56, %r3084;
	// inline asm
	mov.u32 	%r11653, 0;
	// inline asm
	shf.r.wrap.b32 %r11655, %r11653, %r55, %r3084;
	// inline asm
	mov.u32 	%r11654, %r11653;
	bra.uni 	BB6_199;

BB6_168:
	setp.eq.s32	%p142, %r60, 25;
	@%p142 bra 	BB6_169;
	bra.uni 	BB6_199;

BB6_169:
	mov.u32 	%r2804, 24;
	// inline asm
	shf.r.wrap.b32 %r11652, %r55, %r56, %r2804;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11651, %r11649, %r55, %r2804;
	// inline asm
	bra.uni 	BB6_173;

BB6_128:
	setp.eq.s32	%p171, %r60, 5;
	@%p171 bra 	BB6_129;
	bra.uni 	BB6_199;

BB6_129:
	mov.u32 	%r3174, 24;
	// inline asm
	shf.r.wrap.b32 %r11652, %r52, %r53, %r3174;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r51, %r52, %r3174;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r58, %r51, %r3174;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r57, %r58, %r3174;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r56, %r57, %r3174;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r55, %r56, %r3174;
	// inline asm
	mov.u32 	%r11653, 0;
	// inline asm
	shf.r.wrap.b32 %r11654, %r11653, %r55, %r3174;
	// inline asm
	bra.uni 	BB6_199;

BB6_160:
	setp.eq.s32	%p148, %r60, 21;
	@%p148 bra 	BB6_161;
	bra.uni 	BB6_199;

BB6_161:
	mov.u32 	%r2862, 24;
	// inline asm
	shf.r.wrap.b32 %r11652, %r56, %r57, %r2862;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r55, %r56, %r2862;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11650, %r11649, %r55, %r2862;
	// inline asm
	bra.uni 	BB6_184;

BB6_143:
	setp.eq.s32	%p160, %r60, 13;
	@%p160 bra 	BB6_144;
	bra.uni 	BB6_199;

BB6_144:
	mov.u32 	%r3002, 24;
	// inline asm
	shf.r.wrap.b32 %r11652, %r58, %r51, %r3002;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r57, %r58, %r3002;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r56, %r57, %r3002;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r55, %r56, %r3002;
	// inline asm
	mov.u32 	%r11653, 0;
	// inline asm
	shf.r.wrap.b32 %r11656, %r11653, %r55, %r3002;
	// inline asm
	bra.uni 	BB6_148;

BB6_176:
	setp.eq.s32	%p137, %r60, 29;
	@%p137 bra 	BB6_177;
	bra.uni 	BB6_199;

BB6_177:
	mov.u32 	%r11649, 0;
	mov.u32 	%r2754, 24;
	// inline asm
	shf.r.wrap.b32 %r11652, %r11649, %r55, %r2754;
	// inline asm
	bra.uni 	BB6_181;

BB6_271:
	setp.eq.s32	%p225, %r60, 17;
	@%p225 bra 	BB6_272;
	bra.uni 	BB6_275;

BB6_272:
	mov.u32 	%r3526, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r57, %r58, %r3526;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r56, %r57, %r3526;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r55, %r56, %r3526;
	// inline asm
	mov.u32 	%r11508, 0;
	// inline asm
	shf.r.wrap.b32 %r55, %r11508, %r55, %r3526;
	// inline asm
	bra.uni 	BB6_306;

BB6_255:
	setp.eq.s32	%p237, %r60, 9;
	@%p237 bra 	BB6_256;
	bra.uni 	BB6_275;

BB6_256:
	mov.u32 	%r3682, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r51, %r52, %r3682;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r58, %r51, %r3682;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r57, %r58, %r3682;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3671, %r56, %r57, %r3682;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11508, %r55, %r56, %r3682;
	// inline asm
	mov.u32 	%r11510, 0;
	// inline asm
	shf.r.wrap.b32 %r11509, %r11510, %r55, %r3682;
	// inline asm
	mov.u32 	%r11511, %r11510;
	mov.u32 	%r55, %r3671;
	bra.uni 	BB6_322;

BB6_287:
	setp.eq.s32	%p214, %r60, 25;
	@%p214 bra 	BB6_288;
	bra.uni 	BB6_275;

BB6_288:
	mov.u32 	%r3402, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r55, %r56, %r3402;
	// inline asm
	mov.u32 	%r11508, 0;
	// inline asm
	shf.r.wrap.b32 %r53, %r11508, %r55, %r3402;
	// inline asm
	bra.uni 	BB6_292;

BB6_247:
	setp.eq.s32	%p243, %r60, 5;
	@%p243 bra 	BB6_248;
	bra.uni 	BB6_275;

BB6_248:
	mov.u32 	%r3772, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r52, %r53, %r3772;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r51, %r52, %r3772;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r58, %r51, %r3772;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3757, %r57, %r58, %r3772;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11508, %r56, %r57, %r3772;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11509, %r55, %r56, %r3772;
	// inline asm
	mov.u32 	%r11511, 0;
	// inline asm
	shf.r.wrap.b32 %r11510, %r11511, %r55, %r3772;
	// inline asm
	mov.u32 	%r55, %r3757;
	bra.uni 	BB6_322;

BB6_278:
	setp.eq.s32	%p220, %r60, 21;
	@%p220 bra 	BB6_279;
	bra.uni 	BB6_275;

BB6_279:
	mov.u32 	%r3460, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r56, %r57, %r3460;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r55, %r56, %r3460;
	// inline asm
	mov.u32 	%r11508, 0;
	// inline asm
	shf.r.wrap.b32 %r52, %r11508, %r55, %r3460;
	// inline asm
	bra.uni 	BB6_283;

BB6_262:
	setp.eq.s32	%p232, %r60, 13;
	@%p232 bra 	BB6_263;
	bra.uni 	BB6_275;

BB6_263:
	mov.u32 	%r3600, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r58, %r51, %r3600;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r57, %r58, %r3600;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r56, %r57, %r3600;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3593, %r55, %r56, %r3600;
	// inline asm
	mov.u32 	%r11509, 0;
	// inline asm
	shf.r.wrap.b32 %r11508, %r11509, %r55, %r3600;
	// inline asm
	mov.u32 	%r11510, %r11509;
	mov.u32 	%r11511, %r11509;
	mov.u32 	%r55, %r3593;
	bra.uni 	BB6_322;

BB6_295:
	setp.eq.s32	%p209, %r60, 29;
	@%p209 bra 	BB6_296;
	bra.uni 	BB6_275;

BB6_296:
	mov.u32 	%r11508, 0;
	mov.u32 	%r3352, 24;
	// inline asm
	shf.r.wrap.b32 %r54, %r11508, %r55, %r3352;
	// inline asm
	bra.uni 	BB6_318;

BB6_124:
	setp.eq.s32	%p174, %r60, 3;
	@%p174 bra 	BB6_125;
	bra.uni 	BB6_199;

BB6_125:
	mov.u32 	%r3208, 8;
	// inline asm
	shf.r.wrap.b32 %r11652, %r53, %r54, %r3208;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r52, %r53, %r3208;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r51, %r52, %r3208;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r58, %r51, %r3208;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r57, %r58, %r3208;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r56, %r57, %r3208;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r55, %r56, %r3208;
	// inline asm
	mov.u32 	%r3206, 0;
	// inline asm
	shf.r.wrap.b32 %r11653, %r3206, %r55, %r3208;
	// inline asm
	bra.uni 	BB6_199;

BB6_156:
	setp.eq.s32	%p151, %r60, 19;
	@%p151 bra 	BB6_157;
	bra.uni 	BB6_199;

BB6_157:
	mov.u32 	%r2888, 8;
	// inline asm
	shf.r.wrap.b32 %r11652, %r57, %r58, %r2888;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r56, %r57, %r2888;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r55, %r56, %r2888;
	// inline asm
	mov.u32 	%r11653, 0;
	// inline asm
	shf.r.wrap.b32 %r11649, %r11653, %r55, %r2888;
	// inline asm

BB6_191:
	mov.u32 	%r11654, %r11653;
	mov.u32 	%r11655, %r11653;
	mov.u32 	%r11656, %r11653;
	bra.uni 	BB6_199;

BB6_139:
	setp.eq.s32	%p163, %r60, 11;
	@%p163 bra 	BB6_140;
	bra.uni 	BB6_199;

BB6_140:
	mov.u32 	%r3032, 8;
	// inline asm
	shf.r.wrap.b32 %r11652, %r51, %r52, %r3032;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r58, %r51, %r3032;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r57, %r58, %r3032;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r56, %r57, %r3032;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r55, %r56, %r3032;
	// inline asm
	mov.u32 	%r11653, 0;
	// inline asm
	shf.r.wrap.b32 %r11655, %r11653, %r55, %r3032;
	// inline asm
	mov.u32 	%r11654, %r11653;
	bra.uni 	BB6_199;

BB6_171:
	setp.eq.s32	%p140, %r60, 27;
	@%p140 bra 	BB6_172;
	bra.uni 	BB6_199;

BB6_172:
	mov.u32 	%r2776, 8;
	// inline asm
	shf.r.wrap.b32 %r11652, %r55, %r56, %r2776;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11651, %r11649, %r55, %r2776;
	// inline asm

BB6_173:
	mov.u32 	%r11650, %r11649;
	bra.uni 	BB6_184;

BB6_131:
	setp.eq.s32	%p169, %r60, 7;
	@%p169 bra 	BB6_132;
	bra.uni 	BB6_199;

BB6_132:
	mov.u32 	%r3116, 8;
	// inline asm
	shf.r.wrap.b32 %r11652, %r52, %r53, %r3116;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r51, %r52, %r3116;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r58, %r51, %r3116;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r57, %r58, %r3116;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r56, %r57, %r3116;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r55, %r56, %r3116;
	// inline asm
	mov.u32 	%r11653, 0;
	// inline asm
	shf.r.wrap.b32 %r11654, %r11653, %r55, %r3116;
	// inline asm
	bra.uni 	BB6_199;

BB6_163:
	setp.eq.s32	%p146, %r60, 23;
	@%p146 bra 	BB6_164;
	bra.uni 	BB6_199;

BB6_164:
	mov.u32 	%r2828, 8;
	// inline asm
	shf.r.wrap.b32 %r11652, %r56, %r57, %r2828;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r55, %r56, %r2828;
	// inline asm
	mov.u32 	%r11649, 0;
	// inline asm
	shf.r.wrap.b32 %r11650, %r11649, %r55, %r2828;
	// inline asm
	bra.uni 	BB6_184;

BB6_146:
	setp.eq.s32	%p158, %r60, 15;
	@%p158 bra 	BB6_147;
	bra.uni 	BB6_199;

BB6_147:
	mov.u32 	%r2956, 8;
	// inline asm
	shf.r.wrap.b32 %r11652, %r58, %r51, %r2956;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r57, %r58, %r2956;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r56, %r57, %r2956;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r55, %r56, %r2956;
	// inline asm
	mov.u32 	%r11653, 0;
	// inline asm
	shf.r.wrap.b32 %r11656, %r11653, %r55, %r2956;
	// inline asm

BB6_148:
	mov.u32 	%r11654, %r11653;
	mov.u32 	%r11655, %r11653;
	bra.uni 	BB6_199;

BB6_179:
	setp.ne.s32	%p135, %r60, 31;
	@%p135 bra 	BB6_199;

	mov.u32 	%r11649, 0;
	mov.u32 	%r2732, 8;
	// inline asm
	shf.r.wrap.b32 %r11652, %r11649, %r55, %r2732;
	// inline asm

BB6_181:
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;

BB6_184:
	mov.u32 	%r11653, %r11649;
	mov.u32 	%r11654, %r11649;
	mov.u32 	%r11655, %r11649;
	mov.u32 	%r11656, %r11649;

BB6_199:
	and.b32  	%r3274, %r42, 3;
	shl.b32 	%r3275, %r3274, 3;
	mov.u32 	%r3276, -1;
	shl.b32 	%r226, %r3276, %r3275;
	shr.u32 	%r3273, %r42, 2;
	setp.gt.s32	%p177, %r3273, 3;
	@%p177 bra 	BB6_207;

	setp.gt.s32	%p183, %r3273, 1;
	@%p183 bra 	BB6_204;

	setp.eq.s32	%p186, %r3273, 0;
	@%p186 bra 	BB6_218;
	bra.uni 	BB6_202;

BB6_218:
	and.b32  	%r11653, %r11653, %r226;
	bra.uni 	BB6_219;

BB6_207:
	setp.gt.s32	%p178, %r3273, 5;
	@%p178 bra 	BB6_211;

	setp.eq.s32	%p181, %r3273, 4;
	@%p181 bra 	BB6_216;
	bra.uni 	BB6_209;

BB6_216:
	and.b32  	%r11649, %r11649, %r226;
	mov.u32 	%r11653, 0;
	mov.u32 	%r11654, %r11653;
	mov.u32 	%r11655, %r11653;
	mov.u32 	%r11656, %r11653;
	bra.uni 	BB6_219;

BB6_204:
	setp.eq.s32	%p184, %r3273, 2;
	@%p184 bra 	BB6_217;
	bra.uni 	BB6_205;

BB6_217:
	and.b32  	%r11655, %r11655, %r226;
	mov.u32 	%r11653, 0;
	mov.u32 	%r11654, %r11653;
	bra.uni 	BB6_219;

BB6_211:
	setp.eq.s32	%p179, %r3273, 6;
	@%p179 bra 	BB6_214;
	bra.uni 	BB6_212;

BB6_214:
	and.b32  	%r11651, %r11651, %r226;
	mov.u32 	%r11649, 0;
	mov.u32 	%r11650, %r11649;
	bra.uni 	BB6_215;

BB6_202:
	setp.eq.s32	%p187, %r3273, 1;
	@%p187 bra 	BB6_203;
	bra.uni 	BB6_219;

BB6_203:
	and.b32  	%r11654, %r11654, %r226;
	mov.u32 	%r11653, 0;
	bra.uni 	BB6_219;

BB6_209:
	setp.eq.s32	%p182, %r3273, 5;
	@%p182 bra 	BB6_210;
	bra.uni 	BB6_219;

BB6_210:
	and.b32  	%r11650, %r11650, %r226;
	mov.u32 	%r11649, 0;
	bra.uni 	BB6_215;

BB6_205:
	setp.eq.s32	%p185, %r3273, 3;
	@%p185 bra 	BB6_206;
	bra.uni 	BB6_219;

BB6_206:
	and.b32  	%r11656, %r11656, %r226;
	mov.u32 	%r11653, 0;
	mov.u32 	%r11654, %r11653;
	mov.u32 	%r11655, %r11653;
	bra.uni 	BB6_219;

BB6_212:
	setp.ne.s32	%p180, %r3273, 7;
	@%p180 bra 	BB6_219;

	and.b32  	%r11652, %r11652, %r226;
	mov.u32 	%r11649, 0;
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;

BB6_215:
	mov.u32 	%r11653, %r11649;
	mov.u32 	%r11654, %r11649;
	mov.u32 	%r11655, %r11649;
	mov.u32 	%r11656, %r11649;

BB6_219:
	or.b32  	%r11644, %r11653, %r55;
	or.b32  	%r11643, %r11654, %r56;
	or.b32  	%r11642, %r11655, %r57;
	or.b32  	%r11641, %r11656, %r58;
	or.b32  	%r11648, %r11649, %r51;

BB6_220:
	or.b32  	%r11647, %r11650, %r52;
	or.b32  	%r11646, %r11651, %r53;
	or.b32  	%r11645, %r11652, %r54;
	bra.uni 	BB6_1030;

BB6_274:
	setp.eq.s32	%p223, %r60, 19;
	@%p223 bra 	BB6_305;
	bra.uni 	BB6_275;

BB6_305:
	mov.u32 	%r3486, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r57, %r58, %r3486;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r56, %r57, %r3486;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r55, %r56, %r3486;
	// inline asm
	mov.u32 	%r11508, 0;
	// inline asm
	shf.r.wrap.b32 %r55, %r11508, %r55, %r3486;
	// inline asm

BB6_306:
	mov.u32 	%r11509, %r11508;
	mov.u32 	%r11510, %r11508;
	mov.u32 	%r11511, %r11508;
	bra.uni 	BB6_322;

BB6_258:
	setp.eq.s32	%p235, %r60, 11;
	@%p235 bra 	BB6_259;
	bra.uni 	BB6_275;

BB6_259:
	mov.u32 	%r3630, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r51, %r52, %r3630;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r58, %r51, %r3630;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r57, %r58, %r3630;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3619, %r56, %r57, %r3630;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11508, %r55, %r56, %r3630;
	// inline asm
	mov.u32 	%r11510, 0;
	// inline asm
	shf.r.wrap.b32 %r11509, %r11510, %r55, %r3630;
	// inline asm
	mov.u32 	%r11511, %r11510;
	mov.u32 	%r55, %r3619;
	bra.uni 	BB6_322;

BB6_290:
	setp.eq.s32	%p212, %r60, 27;
	@%p212 bra 	BB6_291;
	bra.uni 	BB6_275;

BB6_291:
	mov.u32 	%r3374, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r55, %r56, %r3374;
	// inline asm
	mov.u32 	%r11508, 0;
	// inline asm
	shf.r.wrap.b32 %r53, %r11508, %r55, %r3374;
	// inline asm

BB6_292:
	mov.u32 	%r11509, %r11508;
	mov.u32 	%r11510, %r11508;
	mov.u32 	%r11511, %r11508;
	bra.uni 	BB6_320;

BB6_250:
	setp.eq.s32	%p241, %r60, 7;
	@%p241 bra 	BB6_251;
	bra.uni 	BB6_275;

BB6_251:
	mov.u32 	%r3714, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r52, %r53, %r3714;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r51, %r52, %r3714;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r58, %r51, %r3714;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3699, %r57, %r58, %r3714;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11508, %r56, %r57, %r3714;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11509, %r55, %r56, %r3714;
	// inline asm
	mov.u32 	%r11511, 0;
	// inline asm
	shf.r.wrap.b32 %r11510, %r11511, %r55, %r3714;
	// inline asm
	mov.u32 	%r55, %r3699;
	bra.uni 	BB6_322;

BB6_281:
	setp.eq.s32	%p218, %r60, 23;
	@%p218 bra 	BB6_282;
	bra.uni 	BB6_275;

BB6_282:
	mov.u32 	%r3426, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r56, %r57, %r3426;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r55, %r56, %r3426;
	// inline asm
	mov.u32 	%r11508, 0;
	// inline asm
	shf.r.wrap.b32 %r52, %r11508, %r55, %r3426;
	// inline asm

BB6_283:
	mov.u32 	%r11509, %r11508;
	mov.u32 	%r11510, %r11508;
	mov.u32 	%r11511, %r11508;
	bra.uni 	BB6_321;

BB6_265:
	setp.eq.s32	%p230, %r60, 15;
	@%p230 bra 	BB6_266;
	bra.uni 	BB6_275;

BB6_266:
	mov.u32 	%r3554, 8;
	// inline asm
	shf.r.wrap.b32 %r54, %r58, %r51, %r3554;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r57, %r58, %r3554;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r56, %r57, %r3554;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r3547, %r55, %r56, %r3554;
	// inline asm
	mov.u32 	%r11509, 0;
	// inline asm
	shf.r.wrap.b32 %r11508, %r11509, %r55, %r3554;
	// inline asm
	mov.u32 	%r11510, %r11509;
	mov.u32 	%r11511, %r11509;
	mov.u32 	%r55, %r3547;
	bra.uni 	BB6_322;

BB6_298:
	setp.ne.s32	%p207, %r60, 30;
	@%p207 bra 	BB6_275;

	mov.u32 	%r11508, 0;
	mov.u32 	%r3341, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r11508, %r55, %r3341;
	// inline asm

BB6_318:
	mov.u32 	%r11509, %r11508;
	mov.u32 	%r11510, %r11508;
	mov.u32 	%r11511, %r11508;

BB6_319:
	mov.u32 	%r53, %r11508;

BB6_320:
	mov.u32 	%r52, %r11508;

BB6_321:
	mov.u32 	%r55, %r11508;
	bra.uni 	BB6_322;

BB6_275:
	mov.u32 	%r11508, %r58;
	mov.u32 	%r11509, %r57;
	mov.u32 	%r11510, %r56;
	mov.u32 	%r11511, %r55;
	mov.u32 	%r55, %r51;

BB6_322:
	or.b32  	%r11644, %r11511, %r11653;
	or.b32  	%r11643, %r11510, %r11654;
	or.b32  	%r11642, %r11509, %r11655;
	or.b32  	%r11641, %r11508, %r11656;
	or.b32  	%r11648, %r55, %r11649;
	or.b32  	%r11647, %r52, %r11650;
	or.b32  	%r11646, %r53, %r11651;
	or.b32  	%r11645, %r54, %r11652;
	bra.uni 	BB6_1030;

BB6_801:
	setp.eq.s32	%p707, %r6825, 1;
	mov.u32 	%r11656, %r11599;
	mov.u32 	%r11655, %r11599;
	mov.u32 	%r11654, %r11599;
	mov.u32 	%r11653, %r11599;
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r11599;
	mov.u32 	%r11650, %r11599;
	mov.u32 	%r11649, %r11599;
	@%p707 bra 	BB6_802;
	bra.uni 	BB6_880;

BB6_802:
	mov.u32 	%r7370, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r53, %r54, %r7370;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r52, %r53, %r7370;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r51, %r52, %r7370;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r58, %r51, %r7370;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r57, %r58, %r7370;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r56, %r57, %r7370;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r55, %r56, %r7370;
	// inline asm
	mov.u32 	%r7368, 0;
	// inline asm
	shf.r.wrap.b32 %r11652, %r7368, %r55, %r7370;
	// inline asm
	bra.uni 	BB6_880;

BB6_833:
	setp.eq.s32	%p684, %r6825, 17;
	mov.u32 	%r11656, %r11599;
	mov.u32 	%r11655, %r11599;
	mov.u32 	%r11654, %r11599;
	mov.u32 	%r11653, %r11599;
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r11599;
	mov.u32 	%r11650, %r11599;
	mov.u32 	%r11649, %r11599;
	@%p684 bra 	BB6_834;
	bra.uni 	BB6_880;

BB6_834:
	mov.u32 	%r7026, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r57, %r58, %r7026;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r56, %r57, %r7026;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r55, %r56, %r7026;
	// inline asm
	mov.u32 	%r11652, 0;
	// inline asm
	shf.r.wrap.b32 %r11656, %r11652, %r55, %r7026;
	// inline asm
	bra.uni 	BB6_838;

BB6_816:
	setp.eq.s32	%p696, %r6825, 9;
	mov.u32 	%r11656, %r11599;
	mov.u32 	%r11655, %r11599;
	mov.u32 	%r11654, %r11599;
	mov.u32 	%r11653, %r11599;
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r11599;
	mov.u32 	%r11650, %r11599;
	mov.u32 	%r11649, %r11599;
	@%p696 bra 	BB6_817;
	bra.uni 	BB6_880;

BB6_817:
	mov.u32 	%r7182, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r51, %r52, %r7182;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r58, %r51, %r7182;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r57, %r58, %r7182;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r56, %r57, %r7182;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r55, %r56, %r7182;
	// inline asm
	mov.u32 	%r11652, 0;
	// inline asm
	shf.r.wrap.b32 %r11650, %r11652, %r55, %r7182;
	// inline asm
	mov.u32 	%r11651, %r11652;
	bra.uni 	BB6_880;

BB6_849:
	setp.eq.s32	%p673, %r6825, 25;
	mov.u32 	%r11656, %r11599;
	mov.u32 	%r11655, %r11599;
	mov.u32 	%r11654, %r11599;
	mov.u32 	%r11653, %r11599;
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r11599;
	mov.u32 	%r11650, %r11599;
	mov.u32 	%r11649, %r11599;
	@%p673 bra 	BB6_850;
	bra.uni 	BB6_880;

BB6_850:
	mov.u32 	%r6902, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r55, %r56, %r6902;
	// inline asm
	mov.u32 	%r11656, 0;
	// inline asm
	shf.r.wrap.b32 %r11654, %r11656, %r55, %r6902;
	// inline asm
	bra.uni 	BB6_854;

BB6_808:
	setp.eq.s32	%p702, %r6825, 5;
	mov.u32 	%r11656, %r11599;
	mov.u32 	%r11655, %r11599;
	mov.u32 	%r11654, %r11599;
	mov.u32 	%r11653, %r11599;
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r11599;
	mov.u32 	%r11650, %r11599;
	mov.u32 	%r11649, %r11599;
	@%p702 bra 	BB6_809;
	bra.uni 	BB6_880;

BB6_809:
	mov.u32 	%r7272, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r52, %r53, %r7272;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r51, %r52, %r7272;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r58, %r51, %r7272;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r57, %r58, %r7272;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r56, %r57, %r7272;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r55, %r56, %r7272;
	// inline asm
	mov.u32 	%r11652, 0;
	// inline asm
	shf.r.wrap.b32 %r11651, %r11652, %r55, %r7272;
	// inline asm
	bra.uni 	BB6_880;

BB6_841:
	setp.eq.s32	%p679, %r6825, 21;
	mov.u32 	%r11656, %r11599;
	mov.u32 	%r11655, %r11599;
	mov.u32 	%r11654, %r11599;
	mov.u32 	%r11653, %r11599;
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r11599;
	mov.u32 	%r11650, %r11599;
	mov.u32 	%r11649, %r11599;
	@%p679 bra 	BB6_842;
	bra.uni 	BB6_880;

BB6_842:
	mov.u32 	%r6960, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r56, %r57, %r6960;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r55, %r56, %r6960;
	// inline asm
	mov.u32 	%r11656, 0;
	// inline asm
	shf.r.wrap.b32 %r11655, %r11656, %r55, %r6960;
	// inline asm
	bra.uni 	BB6_855;

BB6_823:
	setp.eq.s32	%p691, %r6825, 13;
	mov.u32 	%r11656, %r11599;
	mov.u32 	%r11655, %r11599;
	mov.u32 	%r11654, %r11599;
	mov.u32 	%r11653, %r11599;
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r11599;
	mov.u32 	%r11650, %r11599;
	mov.u32 	%r11649, %r11599;
	@%p691 bra 	BB6_824;
	bra.uni 	BB6_880;

BB6_824:
	mov.u32 	%r7100, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r58, %r51, %r7100;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r57, %r58, %r7100;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r56, %r57, %r7100;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r55, %r56, %r7100;
	// inline asm
	mov.u32 	%r11652, 0;
	// inline asm
	shf.r.wrap.b32 %r11649, %r11652, %r55, %r7100;
	// inline asm
	bra.uni 	BB6_828;

BB6_858:
	setp.eq.s32	%p668, %r6825, 29;
	mov.u32 	%r11656, %r11599;
	mov.u32 	%r11655, %r11599;
	mov.u32 	%r11654, %r11599;
	mov.u32 	%r11653, %r11599;
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r11599;
	mov.u32 	%r11650, %r11599;
	mov.u32 	%r11649, %r11599;
	@%p668 bra 	BB6_859;
	bra.uni 	BB6_880;

BB6_859:
	mov.u32 	%r11656, 0;
	mov.u32 	%r6852, 24;
	// inline asm
	shf.r.wrap.b32 %r11653, %r11656, %r55, %r6852;
	// inline asm
	bra.uni 	BB6_863;

BB6_804:
	setp.eq.s32	%p705, %r6825, 3;
	mov.u32 	%r11656, %r11599;
	mov.u32 	%r11655, %r11599;
	mov.u32 	%r11654, %r11599;
	mov.u32 	%r11653, %r11599;
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r11599;
	mov.u32 	%r11650, %r11599;
	mov.u32 	%r11649, %r11599;
	@%p705 bra 	BB6_805;
	bra.uni 	BB6_880;

BB6_805:
	mov.u32 	%r7306, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r53, %r54, %r7306;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r52, %r53, %r7306;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r51, %r52, %r7306;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r58, %r51, %r7306;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r57, %r58, %r7306;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r56, %r57, %r7306;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r55, %r56, %r7306;
	// inline asm
	mov.u32 	%r7304, 0;
	// inline asm
	shf.r.wrap.b32 %r11652, %r7304, %r55, %r7306;
	// inline asm
	bra.uni 	BB6_880;

BB6_836:
	setp.eq.s32	%p682, %r6825, 19;
	mov.u32 	%r11656, %r11599;
	mov.u32 	%r11655, %r11599;
	mov.u32 	%r11654, %r11599;
	mov.u32 	%r11653, %r11599;
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r11599;
	mov.u32 	%r11650, %r11599;
	mov.u32 	%r11649, %r11599;
	@%p682 bra 	BB6_837;
	bra.uni 	BB6_880;

BB6_837:
	mov.u32 	%r6986, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r57, %r58, %r6986;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r56, %r57, %r6986;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r55, %r56, %r6986;
	// inline asm
	mov.u32 	%r11652, 0;
	// inline asm
	shf.r.wrap.b32 %r11656, %r11652, %r55, %r6986;
	// inline asm

BB6_838:
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11649, %r11652;
	bra.uni 	BB6_880;

BB6_819:
	setp.eq.s32	%p694, %r6825, 11;
	mov.u32 	%r11656, %r11599;
	mov.u32 	%r11655, %r11599;
	mov.u32 	%r11654, %r11599;
	mov.u32 	%r11653, %r11599;
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r11599;
	mov.u32 	%r11650, %r11599;
	mov.u32 	%r11649, %r11599;
	@%p694 bra 	BB6_820;
	bra.uni 	BB6_880;

BB6_820:
	mov.u32 	%r7130, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r51, %r52, %r7130;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r58, %r51, %r7130;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r57, %r58, %r7130;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r56, %r57, %r7130;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r55, %r56, %r7130;
	// inline asm
	mov.u32 	%r11652, 0;
	// inline asm
	shf.r.wrap.b32 %r11650, %r11652, %r55, %r7130;
	// inline asm
	mov.u32 	%r11651, %r11652;
	bra.uni 	BB6_880;

BB6_852:
	setp.eq.s32	%p671, %r6825, 27;
	mov.u32 	%r11656, %r11599;
	mov.u32 	%r11655, %r11599;
	mov.u32 	%r11654, %r11599;
	mov.u32 	%r11653, %r11599;
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r11599;
	mov.u32 	%r11650, %r11599;
	mov.u32 	%r11649, %r11599;
	@%p671 bra 	BB6_853;
	bra.uni 	BB6_880;

BB6_853:
	mov.u32 	%r6874, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r55, %r56, %r6874;
	// inline asm
	mov.u32 	%r11656, 0;
	// inline asm
	shf.r.wrap.b32 %r11654, %r11656, %r55, %r6874;
	// inline asm

BB6_854:
	mov.u32 	%r11655, %r11656;
	bra.uni 	BB6_855;

BB6_811:
	setp.eq.s32	%p700, %r6825, 7;
	mov.u32 	%r11656, %r11599;
	mov.u32 	%r11655, %r11599;
	mov.u32 	%r11654, %r11599;
	mov.u32 	%r11653, %r11599;
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r11599;
	mov.u32 	%r11650, %r11599;
	mov.u32 	%r11649, %r11599;
	@%p700 bra 	BB6_812;
	bra.uni 	BB6_880;

BB6_812:
	mov.u32 	%r7214, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r52, %r53, %r7214;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r51, %r52, %r7214;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r58, %r51, %r7214;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r57, %r58, %r7214;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r56, %r57, %r7214;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r55, %r56, %r7214;
	// inline asm
	mov.u32 	%r11652, 0;
	// inline asm
	shf.r.wrap.b32 %r11651, %r11652, %r55, %r7214;
	// inline asm
	bra.uni 	BB6_880;

BB6_844:
	setp.eq.s32	%p677, %r6825, 23;
	mov.u32 	%r11656, %r11599;
	mov.u32 	%r11655, %r11599;
	mov.u32 	%r11654, %r11599;
	mov.u32 	%r11653, %r11599;
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r11599;
	mov.u32 	%r11650, %r11599;
	mov.u32 	%r11649, %r11599;
	@%p677 bra 	BB6_845;
	bra.uni 	BB6_880;

BB6_845:
	mov.u32 	%r6926, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r56, %r57, %r6926;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r55, %r56, %r6926;
	// inline asm
	mov.u32 	%r11656, 0;
	// inline asm
	shf.r.wrap.b32 %r11655, %r11656, %r55, %r6926;
	// inline asm
	bra.uni 	BB6_855;

BB6_826:
	setp.eq.s32	%p689, %r6825, 15;
	mov.u32 	%r11656, %r11599;
	mov.u32 	%r11655, %r11599;
	mov.u32 	%r11654, %r11599;
	mov.u32 	%r11653, %r11599;
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r11599;
	mov.u32 	%r11650, %r11599;
	mov.u32 	%r11649, %r11599;
	@%p689 bra 	BB6_827;
	bra.uni 	BB6_880;

BB6_827:
	mov.u32 	%r7054, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r58, %r51, %r7054;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r57, %r58, %r7054;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r56, %r57, %r7054;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r55, %r56, %r7054;
	// inline asm
	mov.u32 	%r11652, 0;
	// inline asm
	shf.r.wrap.b32 %r11649, %r11652, %r55, %r7054;
	// inline asm

BB6_828:
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11650, %r11652;
	bra.uni 	BB6_880;

BB6_861:
	setp.ne.s32	%p666, %r6825, 31;
	mov.u32 	%r11656, %r11599;
	mov.u32 	%r11655, %r11599;
	mov.u32 	%r11654, %r11599;
	mov.u32 	%r11653, %r11599;
	mov.u32 	%r11652, %r11599;
	mov.u32 	%r11651, %r11599;
	mov.u32 	%r11650, %r11599;
	mov.u32 	%r11649, %r11599;
	@%p666 bra 	BB6_880;

	mov.u32 	%r11656, 0;
	mov.u32 	%r6830, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r11656, %r55, %r6830;
	// inline asm

BB6_863:
	mov.u32 	%r11655, %r11656;
	mov.u32 	%r11654, %r11656;

BB6_855:
	mov.u32 	%r11652, %r11656;
	mov.u32 	%r11651, %r11656;
	mov.u32 	%r11650, %r11656;
	mov.u32 	%r11649, %r11656;

BB6_880:
	// inline asm
	prmt.b32 %r7371, %r11653, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r7373, %r11654, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r7375, %r11655, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r7377, %r11656, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r7379, %r11649, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r7381, %r11650, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r7383, %r11651, 0, 0x0123;
	// inline asm
	shr.u32 	%r7393, %r42, 2;
	setp.gt.s32	%p708, %r7393, 3;
	@%p708 bra 	BB6_888;

	setp.gt.s32	%p714, %r7393, 1;
	@%p714 bra 	BB6_885;

	setp.eq.s32	%p717, %r7393, 0;
	@%p717 bra 	BB6_898;
	bra.uni 	BB6_883;

BB6_898:
	// inline asm
	prmt.b32 %r7576, %r11652, 0, 0x0123;
	// inline asm
	and.b32  	%r7610, %r42, 3;
	mov.u32 	%r7611, 4;
	sub.s32 	%r7612, %r7611, %r7610;
	shl.b32 	%r7613, %r7612, 2;
	mov.u32 	%r7614, 1985229328;
	shr.u32 	%r7615, %r7614, %r7613;
	and.b32  	%r7609, %r7615, 65535;
	// inline asm
	prmt.b32 %r11606, %r7383, %r7576, %r7609;
	// inline asm
	// inline asm
	prmt.b32 %r11605, %r7381, %r7383, %r7609;
	// inline asm
	// inline asm
	prmt.b32 %r11604, %r7379, %r7381, %r7609;
	// inline asm
	// inline asm
	prmt.b32 %r11603, %r7377, %r7379, %r7609;
	// inline asm
	// inline asm
	prmt.b32 %r11602, %r7375, %r7377, %r7609;
	// inline asm
	// inline asm
	prmt.b32 %r11601, %r7373, %r7375, %r7609;
	// inline asm
	// inline asm
	prmt.b32 %r11600, %r7371, %r7373, %r7609;
	// inline asm
	mov.u32 	%r7607, 0;
	// inline asm
	prmt.b32 %r11599, %r7607, %r7371, %r7609;
	// inline asm
	bra.uni 	BB6_899;

BB6_888:
	setp.gt.s32	%p709, %r7393, 5;
	@%p709 bra 	BB6_892;

	setp.eq.s32	%p712, %r7393, 4;
	@%p712 bra 	BB6_896;
	bra.uni 	BB6_890;

BB6_896:
	and.b32  	%r7474, %r42, 3;
	mov.u32 	%r7475, 4;
	sub.s32 	%r7476, %r7475, %r7474;
	shl.b32 	%r7477, %r7476, 2;
	mov.u32 	%r7478, 1985229328;
	shr.u32 	%r7479, %r7478, %r7477;
	and.b32  	%r7469, %r7479, 65535;
	// inline asm
	prmt.b32 %r11606, %r7375, %r7377, %r7469;
	// inline asm
	// inline asm
	prmt.b32 %r11605, %r7373, %r7375, %r7469;
	// inline asm
	// inline asm
	prmt.b32 %r11604, %r7371, %r7373, %r7469;
	// inline asm
	mov.u32 	%r11599, 0;
	// inline asm
	prmt.b32 %r11603, %r11599, %r7371, %r7469;
	// inline asm
	mov.u32 	%r11600, %r11599;
	mov.u32 	%r11601, %r11599;
	mov.u32 	%r11602, %r11599;
	bra.uni 	BB6_899;

BB6_885:
	setp.eq.s32	%p715, %r7393, 2;
	@%p715 bra 	BB6_897;
	bra.uni 	BB6_886;

BB6_897:
	and.b32  	%r7535, %r42, 3;
	mov.u32 	%r7536, 4;
	sub.s32 	%r7537, %r7536, %r7535;
	shl.b32 	%r7538, %r7537, 2;
	mov.u32 	%r7539, 1985229328;
	shr.u32 	%r7540, %r7539, %r7538;
	and.b32  	%r7532, %r7540, 65535;
	// inline asm
	prmt.b32 %r11606, %r7379, %r7381, %r7532;
	// inline asm
	// inline asm
	prmt.b32 %r11605, %r7377, %r7379, %r7532;
	// inline asm
	// inline asm
	prmt.b32 %r11604, %r7375, %r7377, %r7532;
	// inline asm
	// inline asm
	prmt.b32 %r11603, %r7373, %r7375, %r7532;
	// inline asm
	// inline asm
	prmt.b32 %r11602, %r7371, %r7373, %r7532;
	// inline asm
	mov.u32 	%r11599, 0;
	// inline asm
	prmt.b32 %r11601, %r11599, %r7371, %r7532;
	// inline asm
	mov.u32 	%r11600, %r11599;
	bra.uni 	BB6_899;

BB6_892:
	setp.eq.s32	%p710, %r7393, 6;
	@%p710 bra 	BB6_895;
	bra.uni 	BB6_893;

BB6_895:
	and.b32  	%r7425, %r42, 3;
	mov.u32 	%r7426, 4;
	sub.s32 	%r7427, %r7426, %r7425;
	shl.b32 	%r7428, %r7427, 2;
	mov.u32 	%r7429, 1985229328;
	shr.u32 	%r7430, %r7429, %r7428;
	and.b32  	%r7418, %r7430, 65535;
	// inline asm
	prmt.b32 %r11606, %r7371, %r7373, %r7418;
	// inline asm
	mov.u32 	%r11599, 0;
	// inline asm
	prmt.b32 %r11605, %r11599, %r7371, %r7418;
	// inline asm
	mov.u32 	%r11600, %r11599;
	mov.u32 	%r11601, %r11599;
	mov.u32 	%r11602, %r11599;
	mov.u32 	%r11603, %r11599;
	mov.u32 	%r11604, %r11599;
	bra.uni 	BB6_899;

BB6_883:
	setp.eq.s32	%p718, %r7393, 1;
	mov.u32 	%r11600, %r11599;
	mov.u32 	%r11601, %r11599;
	mov.u32 	%r11602, %r11599;
	mov.u32 	%r11603, %r11599;
	mov.u32 	%r11604, %r11599;
	mov.u32 	%r11605, %r11599;
	mov.u32 	%r11606, %r11599;
	@%p718 bra 	BB6_884;
	bra.uni 	BB6_899;

BB6_884:
	and.b32  	%r7570, %r42, 3;
	mov.u32 	%r7571, 4;
	sub.s32 	%r7572, %r7571, %r7570;
	shl.b32 	%r7573, %r7572, 2;
	mov.u32 	%r7574, 1985229328;
	shr.u32 	%r7575, %r7574, %r7573;
	and.b32  	%r7568, %r7575, 65535;
	// inline asm
	prmt.b32 %r11606, %r7381, %r7383, %r7568;
	// inline asm
	// inline asm
	prmt.b32 %r11605, %r7379, %r7381, %r7568;
	// inline asm
	// inline asm
	prmt.b32 %r11604, %r7377, %r7379, %r7568;
	// inline asm
	// inline asm
	prmt.b32 %r11603, %r7375, %r7377, %r7568;
	// inline asm
	// inline asm
	prmt.b32 %r11602, %r7373, %r7375, %r7568;
	// inline asm
	// inline asm
	prmt.b32 %r11601, %r7371, %r7373, %r7568;
	// inline asm
	mov.u32 	%r11599, 0;
	// inline asm
	prmt.b32 %r11600, %r11599, %r7371, %r7568;
	// inline asm
	bra.uni 	BB6_899;

BB6_890:
	setp.eq.s32	%p713, %r7393, 5;
	mov.u32 	%r11600, %r11599;
	mov.u32 	%r11601, %r11599;
	mov.u32 	%r11602, %r11599;
	mov.u32 	%r11603, %r11599;
	mov.u32 	%r11604, %r11599;
	mov.u32 	%r11605, %r11599;
	mov.u32 	%r11606, %r11599;
	@%p713 bra 	BB6_891;
	bra.uni 	BB6_899;

BB6_891:
	and.b32  	%r7448, %r42, 3;
	mov.u32 	%r7449, 4;
	sub.s32 	%r7450, %r7449, %r7448;
	shl.b32 	%r7451, %r7450, 2;
	mov.u32 	%r7452, 1985229328;
	shr.u32 	%r7453, %r7452, %r7451;
	and.b32  	%r7442, %r7453, 65535;
	// inline asm
	prmt.b32 %r11606, %r7373, %r7375, %r7442;
	// inline asm
	// inline asm
	prmt.b32 %r11605, %r7371, %r7373, %r7442;
	// inline asm
	mov.u32 	%r11599, 0;
	// inline asm
	prmt.b32 %r11604, %r11599, %r7371, %r7442;
	// inline asm
	mov.u32 	%r11600, %r11599;
	mov.u32 	%r11601, %r11599;
	mov.u32 	%r11602, %r11599;
	mov.u32 	%r11603, %r11599;
	bra.uni 	BB6_899;

BB6_886:
	setp.eq.s32	%p716, %r7393, 3;
	mov.u32 	%r11600, %r11599;
	mov.u32 	%r11601, %r11599;
	mov.u32 	%r11602, %r11599;
	mov.u32 	%r11603, %r11599;
	mov.u32 	%r11604, %r11599;
	mov.u32 	%r11605, %r11599;
	mov.u32 	%r11606, %r11599;
	@%p716 bra 	BB6_887;
	bra.uni 	BB6_899;

BB6_887:
	and.b32  	%r7503, %r42, 3;
	mov.u32 	%r7504, 4;
	sub.s32 	%r7505, %r7504, %r7503;
	shl.b32 	%r7506, %r7505, 2;
	mov.u32 	%r7507, 1985229328;
	shr.u32 	%r7508, %r7507, %r7506;
	and.b32  	%r7499, %r7508, 65535;
	// inline asm
	prmt.b32 %r11606, %r7377, %r7379, %r7499;
	// inline asm
	// inline asm
	prmt.b32 %r11605, %r7375, %r7377, %r7499;
	// inline asm
	// inline asm
	prmt.b32 %r11604, %r7373, %r7375, %r7499;
	// inline asm
	// inline asm
	prmt.b32 %r11603, %r7371, %r7373, %r7499;
	// inline asm
	mov.u32 	%r11599, 0;
	// inline asm
	prmt.b32 %r11602, %r11599, %r7371, %r7499;
	// inline asm
	mov.u32 	%r11600, %r11599;
	mov.u32 	%r11601, %r11599;
	bra.uni 	BB6_899;

BB6_893:
	setp.ne.s32	%p711, %r7393, 7;
	mov.u32 	%r11600, %r11599;
	mov.u32 	%r11601, %r11599;
	mov.u32 	%r11602, %r11599;
	mov.u32 	%r11603, %r11599;
	mov.u32 	%r11604, %r11599;
	mov.u32 	%r11605, %r11599;
	mov.u32 	%r11606, %r11599;
	@%p711 bra 	BB6_899;

	and.b32  	%r7405, %r42, 3;
	mov.u32 	%r7406, 4;
	sub.s32 	%r7407, %r7406, %r7405;
	shl.b32 	%r7408, %r7407, 2;
	mov.u32 	%r7409, 1985229328;
	shr.u32 	%r7410, %r7409, %r7408;
	and.b32  	%r7397, %r7410, 65535;
	mov.u32 	%r11599, 0;
	// inline asm
	prmt.b32 %r11606, %r11599, %r7371, %r7397;
	// inline asm
	mov.u32 	%r11600, %r11599;
	mov.u32 	%r11601, %r11599;
	mov.u32 	%r11602, %r11599;
	mov.u32 	%r11603, %r11599;
	mov.u32 	%r11604, %r11599;
	mov.u32 	%r11605, %r11599;

BB6_899:
	or.b32  	%r11644, %r11599, %r55;
	or.b32  	%r11643, %r11600, %r56;
	or.b32  	%r11642, %r11601, %r57;
	or.b32  	%r11641, %r11602, %r58;
	or.b32  	%r11648, %r11603, %r51;
	or.b32  	%r11647, %r11604, %r52;
	or.b32  	%r11646, %r11605, %r53;
	or.b32  	%r11645, %r11606, %r54;
	bra.uni 	BB6_1030;

BB6_4:
	mov.u32 	%r58, %r11641;
	mov.u32 	%r57, %r11642;
	mov.u32 	%r56, %r11643;
	mov.u32 	%r55, %r11644;
	mov.u32 	%r54, %r11645;
	mov.u32 	%r53, %r11646;
	mov.u32 	%r52, %r11647;
	mov.u32 	%r51, %r11648;
	mov.u32 	%r42, %r11657;
	shr.u32 	%r59, %r11465, 8;
	bfe.u32 	%r60, %r11465, 8, 8;
	shr.u32 	%r61, %r11465, 16;
	bfe.u32 	%r62, %r11465, 16, 8;
	add.u64 	%rd123, %SP, 32;
	cvta.to.local.u64 	%rd86, %rd123;
	add.u64 	%rd124, %SP, 0;
	cvta.to.local.u64 	%rd87, %rd124;
	and.b32  	%r1766, %r59, 252;
	cvt.u64.u32	%rd125, %r1766;
	add.s64 	%rd88, %rd87, %rd125;
	add.s64 	%rd89, %rd86, %rd125;
	and.b32  	%r1765, %r11465, 255;
	setp.gt.s32	%p4, %r1765, 93;
	@%p4 bra 	BB6_59;

	setp.gt.s32	%p32, %r1765, 68;
	@%p32 bra 	BB6_33;

	setp.gt.s32	%p46, %r1765, 44;
	@%p46 bra 	BB6_19;

	setp.gt.s32	%p53, %r1765, 41;
	@%p53 bra 	BB6_15;

	setp.eq.s32	%p57, %r1765, 36;
	@%p57 bra 	BB6_799;
	bra.uni 	BB6_9;

BB6_799:
	add.s32 	%r11657, %r42, 1;
	setp.gt.u32	%p653, %r11657, 31;
	@%p653 bra 	BB6_110;

	mov.u32 	%r6794, 64;
	prmt.b32 	%r6795, %r60, %r60, %r6794;
	mov.u32 	%r6796, 1040;
	prmt.b32 	%r6797, %r6795, %r60, %r6796;
	mov.u32 	%r6798, 16912;
	prmt.b32 	%r6799, %r6797, %r60, %r6798;
	bfe.u32 	%r6800, %r42, 2, 2;
	and.b32  	%r6801, %r42, 3;
	shl.b32 	%r6802, %r6801, 3;
	mov.u32 	%r6803, 255;
	shl.b32 	%r6804, %r6803, %r6802;
	setp.eq.s32	%p654, %r6800, 0;
	selp.b32	%r11653, %r6804, 0, %p654;
	setp.eq.s32	%p655, %r6800, 1;
	selp.b32	%r11654, %r6804, 0, %p655;
	setp.eq.s32	%p656, %r6800, 2;
	selp.b32	%r11655, %r6804, 0, %p656;
	setp.eq.s32	%p657, %r6800, 3;
	selp.b32	%r11656, %r6804, 0, %p657;
	shr.u32 	%r6805, %r42, 4;
	setp.eq.s32	%p658, %r6805, 0;
	selp.b32	%r6806, %r6799, 0, %p658;
	and.b32  	%r6807, %r6806, %r11653;
	or.b32  	%r11644, %r6807, %r55;
	and.b32  	%r6808, %r6806, %r11654;
	or.b32  	%r11643, %r6808, %r56;
	and.b32  	%r6809, %r6806, %r11655;
	or.b32  	%r11642, %r6809, %r57;
	and.b32  	%r6810, %r6806, %r11656;
	or.b32  	%r11641, %r6810, %r58;
	setp.eq.s32	%p659, %r6805, 1;
	selp.b32	%r6811, %r6799, 0, %p659;
	and.b32  	%r6812, %r6811, %r11653;
	or.b32  	%r11648, %r6812, %r51;
	and.b32  	%r6813, %r6811, %r11654;
	or.b32  	%r11647, %r6813, %r52;
	and.b32  	%r6814, %r6811, %r11655;
	or.b32  	%r11646, %r6814, %r53;
	and.b32  	%r6815, %r6811, %r11656;
	or.b32  	%r11645, %r6815, %r54;
	bra.uni 	BB6_1030;

BB6_59:
	setp.gt.s32	%p5, %r1765, 112;
	@%p5 bra 	BB6_84;

	setp.gt.s32	%p19, %r1765, 104;
	@%p19 bra 	BB6_75;

	setp.gt.s32	%p26, %r1765, 99;
	@%p26 bra 	BB6_65;

	setp.eq.s32	%p30, %r1765, 94;
	@%p30 bra 	BB6_797;
	bra.uni 	BB6_63;

BB6_797:
	add.s32 	%r11657, %r42, 1;
	setp.gt.u32	%p652, %r11657, 31;
	@%p652 bra 	BB6_110;

	mov.u32 	%r6793, 24;
	// inline asm
	shf.r.wrap.b32 %r11645, %r53, %r54, %r6793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11646, %r52, %r53, %r6793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11647, %r51, %r52, %r6793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11648, %r58, %r51, %r6793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11641, %r57, %r58, %r6793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11642, %r56, %r57, %r6793;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r55, %r56, %r6793;
	// inline asm
	mov.u32 	%r6791, 0;
	// inline asm
	shf.r.wrap.b32 %r6790, %r6791, %r55, %r6793;
	// inline asm
	or.b32  	%r11644, %r6790, %r60;
	bra.uni 	BB6_1030;

BB6_33:
	setp.gt.s32	%p33, %r1765, 83;
	@%p33 bra 	BB6_44;

	setp.gt.s32	%p40, %r1765, 75;
	@%p40 bra 	BB6_39;

	setp.eq.s32	%p44, %r1765, 69;
	@%p44 bra 	BB6_112;
	bra.uni 	BB6_36;

BB6_112:
	setp.eq.s32	%p59, %r42, 0;
	mov.u32 	%r11657, 0;
	@%p59 bra 	BB6_113;

	mov.u32 	%r1882, 0;
	mov.u32 	%r1895, 8;
	// inline asm
	bfe.u32 %r1768, %r55, %r1882, %r1895;
	// inline asm
	setp.eq.s32	%p60, %r1768, 32;
	selp.u32	%r1896, 1, 0, %p60;
	// inline asm
	bfe.u32 %r1772, %r55, %r1895, %r1895;
	// inline asm
	setp.eq.s32	%p61, %r1772, 32;
	or.b32  	%r1897, %r1896, 2;
	selp.b32	%r1898, %r1897, %r1896, %p61;
	mov.u32 	%r1890, 16;
	// inline asm
	bfe.u32 %r1776, %r55, %r1890, %r1895;
	// inline asm
	setp.eq.s32	%p62, %r1776, 32;
	or.b32  	%r1899, %r1898, 4;
	selp.b32	%r1900, %r1899, %r1898, %p62;
	mov.u32 	%r1894, 24;
	// inline asm
	bfe.u32 %r1780, %r55, %r1894, %r1895;
	// inline asm
	setp.eq.s32	%p63, %r1780, 32;
	or.b32  	%r1901, %r1900, 8;
	selp.b32	%r63, %r1901, %r1900, %p63;
	// inline asm
	bfe.u32 %r1784, %r56, %r1882, %r1895;
	// inline asm
	setp.eq.s32	%p64, %r1784, 32;
	selp.u32	%r1902, 1, 0, %p64;
	// inline asm
	bfe.u32 %r1788, %r56, %r1895, %r1895;
	// inline asm
	setp.eq.s32	%p65, %r1788, 32;
	or.b32  	%r1903, %r1902, 2;
	selp.b32	%r1904, %r1903, %r1902, %p65;
	// inline asm
	bfe.u32 %r1792, %r56, %r1890, %r1895;
	// inline asm
	setp.eq.s32	%p66, %r1792, 32;
	or.b32  	%r1905, %r1904, 4;
	selp.b32	%r1906, %r1905, %r1904, %p66;
	// inline asm
	bfe.u32 %r1796, %r56, %r1894, %r1895;
	// inline asm
	setp.eq.s32	%p67, %r1796, 32;
	or.b32  	%r1907, %r1906, 8;
	selp.b32	%r64, %r1907, %r1906, %p67;
	// inline asm
	bfe.u32 %r1800, %r57, %r1882, %r1895;
	// inline asm
	setp.eq.s32	%p68, %r1800, 32;
	selp.u32	%r1908, 1, 0, %p68;
	// inline asm
	bfe.u32 %r1804, %r57, %r1895, %r1895;
	// inline asm
	setp.eq.s32	%p69, %r1804, 32;
	or.b32  	%r1909, %r1908, 2;
	selp.b32	%r1910, %r1909, %r1908, %p69;
	// inline asm
	bfe.u32 %r1808, %r57, %r1890, %r1895;
	// inline asm
	setp.eq.s32	%p70, %r1808, 32;
	or.b32  	%r1911, %r1910, 4;
	selp.b32	%r1912, %r1911, %r1910, %p70;
	// inline asm
	bfe.u32 %r1812, %r57, %r1894, %r1895;
	// inline asm
	setp.eq.s32	%p71, %r1812, 32;
	or.b32  	%r1913, %r1912, 8;
	selp.b32	%r65, %r1913, %r1912, %p71;
	// inline asm
	bfe.u32 %r1816, %r58, %r1882, %r1895;
	// inline asm
	setp.eq.s32	%p72, %r1816, 32;
	selp.u32	%r1914, 1, 0, %p72;
	// inline asm
	bfe.u32 %r1820, %r58, %r1895, %r1895;
	// inline asm
	setp.eq.s32	%p73, %r1820, 32;
	or.b32  	%r1915, %r1914, 2;
	selp.b32	%r1916, %r1915, %r1914, %p73;
	// inline asm
	bfe.u32 %r1824, %r58, %r1890, %r1895;
	// inline asm
	setp.eq.s32	%p74, %r1824, 32;
	or.b32  	%r1917, %r1916, 4;
	selp.b32	%r1918, %r1917, %r1916, %p74;
	// inline asm
	bfe.u32 %r1828, %r58, %r1894, %r1895;
	// inline asm
	setp.eq.s32	%p75, %r1828, 32;
	or.b32  	%r1919, %r1918, 8;
	selp.b32	%r66, %r1919, %r1918, %p75;
	// inline asm
	bfe.u32 %r1832, %r51, %r1882, %r1895;
	// inline asm
	setp.eq.s32	%p76, %r1832, 32;
	selp.u32	%r1920, 1, 0, %p76;
	// inline asm
	bfe.u32 %r1836, %r51, %r1895, %r1895;
	// inline asm
	setp.eq.s32	%p77, %r1836, 32;
	or.b32  	%r1921, %r1920, 2;
	selp.b32	%r1922, %r1921, %r1920, %p77;
	// inline asm
	bfe.u32 %r1840, %r51, %r1890, %r1895;
	// inline asm
	setp.eq.s32	%p78, %r1840, 32;
	or.b32  	%r1923, %r1922, 4;
	selp.b32	%r1924, %r1923, %r1922, %p78;
	// inline asm
	bfe.u32 %r1844, %r51, %r1894, %r1895;
	// inline asm
	setp.eq.s32	%p79, %r1844, 32;
	or.b32  	%r1925, %r1924, 8;
	selp.b32	%r1926, %r1925, %r1924, %p79;
	// inline asm
	bfe.u32 %r1848, %r52, %r1882, %r1895;
	// inline asm
	setp.eq.s32	%p80, %r1848, 32;
	selp.u32	%r1927, 1, 0, %p80;
	// inline asm
	bfe.u32 %r1852, %r52, %r1895, %r1895;
	// inline asm
	setp.eq.s32	%p81, %r1852, 32;
	or.b32  	%r1928, %r1927, 2;
	selp.b32	%r1929, %r1928, %r1927, %p81;
	// inline asm
	bfe.u32 %r1856, %r52, %r1890, %r1895;
	// inline asm
	setp.eq.s32	%p82, %r1856, 32;
	or.b32  	%r1930, %r1929, 4;
	selp.b32	%r1931, %r1930, %r1929, %p82;
	// inline asm
	bfe.u32 %r1860, %r52, %r1894, %r1895;
	// inline asm
	setp.eq.s32	%p83, %r1860, 32;
	or.b32  	%r1932, %r1931, 8;
	selp.b32	%r1933, %r1932, %r1931, %p83;
	// inline asm
	bfe.u32 %r1864, %r53, %r1882, %r1895;
	// inline asm
	setp.eq.s32	%p84, %r1864, 32;
	selp.u32	%r1934, 1, 0, %p84;
	// inline asm
	bfe.u32 %r1868, %r53, %r1895, %r1895;
	// inline asm
	setp.eq.s32	%p85, %r1868, 32;
	or.b32  	%r1935, %r1934, 2;
	selp.b32	%r1936, %r1935, %r1934, %p85;
	// inline asm
	bfe.u32 %r1872, %r53, %r1890, %r1895;
	// inline asm
	setp.eq.s32	%p86, %r1872, 32;
	or.b32  	%r1937, %r1936, 4;
	selp.b32	%r1938, %r1937, %r1936, %p86;
	// inline asm
	bfe.u32 %r1876, %r53, %r1894, %r1895;
	// inline asm
	setp.eq.s32	%p87, %r1876, 32;
	or.b32  	%r1939, %r1938, 8;
	selp.b32	%r1940, %r1939, %r1938, %p87;
	// inline asm
	bfe.u32 %r1880, %r54, %r1882, %r1895;
	// inline asm
	setp.eq.s32	%p88, %r1880, 32;
	selp.u32	%r1941, 1, 0, %p88;
	// inline asm
	bfe.u32 %r1884, %r54, %r1895, %r1895;
	// inline asm
	setp.eq.s32	%p89, %r1884, 32;
	or.b32  	%r1942, %r1941, 2;
	selp.b32	%r1943, %r1942, %r1941, %p89;
	// inline asm
	bfe.u32 %r1888, %r54, %r1890, %r1895;
	// inline asm
	setp.eq.s32	%p90, %r1888, 32;
	or.b32  	%r1944, %r1943, 4;
	selp.b32	%r1945, %r1944, %r1943, %p90;
	// inline asm
	bfe.u32 %r1892, %r54, %r1894, %r1895;
	// inline asm
	setp.eq.s32	%p91, %r1892, 32;
	or.b32  	%r1946, %r1945, 8;
	selp.b32	%r1947, %r1946, %r1945, %p91;
	and.b32  	%r1948, %r55, 1077952576;
	shr.u32 	%r1949, %r1948, 1;
	and.b32  	%r1950, %r55, -2139062144;
	shr.u32 	%r1951, %r1950, 2;
	not.b32 	%r1952, %r1951;
	and.b32  	%r1953, %r1949, %r1952;
	and.b32  	%r1954, %r55, 522133279;
	add.s32 	%r1955, %r1954, 522133279;
	mov.u32 	%r1956, -84215046;
	sub.s32 	%r1957, %r1956, %r1954;
	and.b32  	%r1958, %r1953, %r1957;
	and.b32  	%r1959, %r1958, %r1955;
	or.b32  	%r67, %r1959, %r55;
	and.b32  	%r1960, %r56, 1077952576;
	shr.u32 	%r1961, %r1960, 1;
	and.b32  	%r1962, %r56, -2139062144;
	shr.u32 	%r1963, %r1962, 2;
	not.b32 	%r1964, %r1963;
	and.b32  	%r1965, %r1961, %r1964;
	and.b32  	%r1966, %r56, 522133279;
	add.s32 	%r1967, %r1966, 522133279;
	sub.s32 	%r1968, %r1956, %r1966;
	and.b32  	%r1969, %r1965, %r1968;
	and.b32  	%r1970, %r1969, %r1967;
	or.b32  	%r11643, %r1970, %r56;
	and.b32  	%r1971, %r57, 1077952576;
	shr.u32 	%r1972, %r1971, 1;
	and.b32  	%r1973, %r57, -2139062144;
	shr.u32 	%r1974, %r1973, 2;
	not.b32 	%r1975, %r1974;
	and.b32  	%r1976, %r1972, %r1975;
	and.b32  	%r1977, %r57, 522133279;
	add.s32 	%r1978, %r1977, 522133279;
	sub.s32 	%r1979, %r1956, %r1977;
	and.b32  	%r1980, %r1976, %r1979;
	and.b32  	%r1981, %r1980, %r1978;
	or.b32  	%r11642, %r1981, %r57;
	and.b32  	%r1982, %r58, 1077952576;
	shr.u32 	%r1983, %r1982, 1;
	and.b32  	%r1984, %r58, -2139062144;
	shr.u32 	%r1985, %r1984, 2;
	not.b32 	%r1986, %r1985;
	and.b32  	%r1987, %r1983, %r1986;
	and.b32  	%r1988, %r58, 522133279;
	add.s32 	%r1989, %r1988, 522133279;
	sub.s32 	%r1990, %r1956, %r1988;
	and.b32  	%r1991, %r1987, %r1990;
	and.b32  	%r1992, %r1991, %r1989;
	or.b32  	%r11641, %r1992, %r58;
	and.b32  	%r1993, %r51, 1077952576;
	shr.u32 	%r1994, %r1993, 1;
	and.b32  	%r1995, %r51, -2139062144;
	shr.u32 	%r1996, %r1995, 2;
	not.b32 	%r1997, %r1996;
	and.b32  	%r1998, %r1994, %r1997;
	and.b32  	%r1999, %r51, 522133279;
	add.s32 	%r2000, %r1999, 522133279;
	sub.s32 	%r2001, %r1956, %r1999;
	and.b32  	%r2002, %r1998, %r2001;
	and.b32  	%r2003, %r2002, %r2000;
	or.b32  	%r11648, %r2003, %r51;
	and.b32  	%r2004, %r52, 1077952576;
	shr.u32 	%r2005, %r2004, 1;
	and.b32  	%r2006, %r52, -2139062144;
	shr.u32 	%r2007, %r2006, 2;
	not.b32 	%r2008, %r2007;
	and.b32  	%r2009, %r2005, %r2008;
	and.b32  	%r2010, %r52, 522133279;
	add.s32 	%r2011, %r2010, 522133279;
	sub.s32 	%r2012, %r1956, %r2010;
	and.b32  	%r2013, %r2009, %r2012;
	and.b32  	%r2014, %r2013, %r2011;
	or.b32  	%r11647, %r2014, %r52;
	and.b32  	%r2015, %r53, 1077952576;
	shr.u32 	%r2016, %r2015, 1;
	and.b32  	%r2017, %r53, -2139062144;
	shr.u32 	%r2018, %r2017, 2;
	not.b32 	%r2019, %r2018;
	and.b32  	%r2020, %r2016, %r2019;
	and.b32  	%r2021, %r53, 522133279;
	add.s32 	%r2022, %r2021, 522133279;
	sub.s32 	%r2023, %r1956, %r2021;
	and.b32  	%r2024, %r2020, %r2023;
	and.b32  	%r2025, %r2024, %r2022;
	or.b32  	%r11646, %r2025, %r53;
	and.b32  	%r2026, %r54, 1077952576;
	shr.u32 	%r2027, %r2026, 1;
	and.b32  	%r2028, %r54, -2139062144;
	shr.u32 	%r2029, %r2028, 2;
	not.b32 	%r2030, %r2029;
	and.b32  	%r2031, %r2027, %r2030;
	and.b32  	%r2032, %r54, 522133279;
	add.s32 	%r2033, %r2032, 522133279;
	sub.s32 	%r2034, %r1956, %r2032;
	and.b32  	%r2035, %r2031, %r2034;
	and.b32  	%r2036, %r2035, %r2033;
	or.b32  	%r11645, %r2036, %r54;
	and.b32  	%r2037, %r67, 64;
	shr.u32 	%r2038, %r2037, 1;
	and.b32  	%r2039, %r67, 128;
	shr.u32 	%r2040, %r2039, 2;
	not.b32 	%r2041, %r2040;
	and.b32  	%r2042, %r2038, %r2041;
	and.b32  	%r75, %r67, 522133279;
	add.s32 	%r2043, %r75, 31;
	sub.s32 	%r76, %r1956, %r75;
	and.b32  	%r2044, %r2042, %r76;
	and.b32  	%r2045, %r2044, %r2043;
	not.b32 	%r2046, %r2045;
	or.b32  	%r2047, %r2046, -33;
	and.b32  	%r11644, %r2047, %r67;
	add.s32 	%r2048, %r64, %r63;
	add.s32 	%r2049, %r2048, %r65;
	add.s32 	%r2050, %r2049, %r66;
	add.s32 	%r2051, %r2050, %r1926;
	add.s32 	%r2052, %r2051, %r1933;
	add.s32 	%r2053, %r2052, %r1940;
	neg.s32 	%r2054, %r1947;
	setp.eq.s32	%p92, %r2053, %r2054;
	@%p92 bra 	BB6_1029;

	shl.b32 	%r2055, %r64, 1;
	bfe.u32 	%r2056, %r63, 3, 28;
	or.b32  	%r2057, %r2055, %r2056;
	shr.u32 	%r2058, %r2057, 4;
	shl.b32 	%r2059, %r65, 1;
	or.b32  	%r2060, %r2059, %r2058;
	shr.u32 	%r2061, %r2060, 4;
	shl.b32 	%r2062, %r66, 1;
	or.b32  	%r2063, %r2062, %r2061;
	and.b32  	%r2064, %r67, 1077952576;
	shr.u32 	%r2065, %r2064, 1;
	and.b32  	%r2066, %r67, -2139062144;
	shr.u32 	%r2067, %r2066, 2;
	not.b32 	%r2068, %r2067;
	and.b32  	%r2069, %r2065, %r2068;
	and.b32  	%r2070, %r2069, %r76;
	add.s32 	%r2071, %r75, 522133279;
	and.b32  	%r2072, %r2070, %r2071;
	shl.b32 	%r2073, %r63, 13;
	and.b32  	%r2074, %r2072, %r2073;
	and.b32  	%r2075, %r2074, 8192;
	xor.b32  	%r2076, %r11644, %r2075;
	shl.b32 	%r2077, %r63, 20;
	and.b32  	%r2078, %r2072, %r2077;
	and.b32  	%r2079, %r2078, 2097152;
	xor.b32  	%r2080, %r2076, %r2079;
	shl.b32 	%r2081, %r63, 27;
	and.b32  	%r2082, %r2072, %r2081;
	and.b32  	%r2083, %r2082, 536870912;
	xor.b32  	%r11644, %r2080, %r2083;
	and.b32  	%r2084, %r11643, 1077952576;
	shr.u32 	%r2085, %r2084, 1;
	and.b32  	%r2086, %r11643, -2139062144;
	shr.u32 	%r2087, %r2086, 2;
	not.b32 	%r2088, %r2087;
	and.b32  	%r2089, %r2085, %r2088;
	and.b32  	%r2090, %r11643, 522133279;
	add.s32 	%r2091, %r2090, 522133279;
	sub.s32 	%r2093, %r1956, %r2090;
	and.b32  	%r2094, %r2089, %r2093;
	and.b32  	%r2095, %r2094, %r2091;
	shl.b32 	%r2096, %r63, 2;
	and.b32  	%r2097, %r2095, %r2096;
	and.b32  	%r2098, %r2097, 32;
	xor.b32  	%r2099, %r2098, %r11643;
	shl.b32 	%r2100, %r2057, 12;
	and.b32  	%r2101, %r2095, %r2100;
	and.b32  	%r2102, %r2101, 8192;
	xor.b32  	%r2103, %r2099, %r2102;
	shl.b32 	%r2104, %r2057, 19;
	and.b32  	%r2105, %r2095, %r2104;
	and.b32  	%r2106, %r2105, 2097152;
	xor.b32  	%r2107, %r2103, %r2106;
	shl.b32 	%r2108, %r2057, 26;
	and.b32  	%r2109, %r2095, %r2108;
	and.b32  	%r2110, %r2109, 536870912;
	xor.b32  	%r11643, %r2107, %r2110;
	and.b32  	%r2111, %r11642, 1077952576;
	shr.u32 	%r2112, %r2111, 1;
	and.b32  	%r2113, %r11642, -2139062144;
	shr.u32 	%r2114, %r2113, 2;
	not.b32 	%r2115, %r2114;
	and.b32  	%r2116, %r2112, %r2115;
	and.b32  	%r2117, %r11642, 522133279;
	add.s32 	%r2118, %r2117, 522133279;
	sub.s32 	%r2119, %r1956, %r2117;
	and.b32  	%r2120, %r2116, %r2119;
	and.b32  	%r2121, %r2120, %r2118;
	shl.b32 	%r2122, %r2057, 1;
	and.b32  	%r2123, %r2121, %r2122;
	and.b32  	%r2124, %r2123, 32;
	xor.b32  	%r2125, %r2124, %r11642;
	shl.b32 	%r2126, %r2060, 12;
	and.b32  	%r2127, %r2121, %r2126;
	and.b32  	%r2128, %r2127, 8192;
	xor.b32  	%r2129, %r2125, %r2128;
	shl.b32 	%r2130, %r2060, 19;
	and.b32  	%r2131, %r2121, %r2130;
	and.b32  	%r2132, %r2131, 2097152;
	xor.b32  	%r2133, %r2129, %r2132;
	shl.b32 	%r2134, %r2060, 26;
	and.b32  	%r2135, %r2121, %r2134;
	and.b32  	%r2136, %r2135, 536870912;
	xor.b32  	%r11642, %r2133, %r2136;
	and.b32  	%r2137, %r11641, 1077952576;
	shr.u32 	%r2138, %r2137, 1;
	and.b32  	%r2139, %r11641, -2139062144;
	shr.u32 	%r2140, %r2139, 2;
	not.b32 	%r2141, %r2140;
	and.b32  	%r2142, %r2138, %r2141;
	and.b32  	%r2143, %r11641, 522133279;
	add.s32 	%r2144, %r2143, 522133279;
	sub.s32 	%r2145, %r1956, %r2143;
	and.b32  	%r2146, %r2142, %r2145;
	and.b32  	%r2147, %r2146, %r2144;
	shl.b32 	%r2148, %r2060, 1;
	and.b32  	%r2149, %r2147, %r2148;
	and.b32  	%r2150, %r2149, 32;
	xor.b32  	%r2151, %r2150, %r11641;
	shl.b32 	%r2152, %r2063, 12;
	and.b32  	%r2153, %r2147, %r2152;
	and.b32  	%r2154, %r2153, 8192;
	xor.b32  	%r2155, %r2151, %r2154;
	shl.b32 	%r2156, %r2063, 19;
	and.b32  	%r2157, %r2147, %r2156;
	and.b32  	%r2158, %r2157, 2097152;
	xor.b32  	%r2159, %r2155, %r2158;
	shl.b32 	%r2160, %r2063, 26;
	and.b32  	%r2161, %r2147, %r2160;
	and.b32  	%r2162, %r2161, 536870912;
	xor.b32  	%r11641, %r2159, %r2162;
	and.b32  	%r2163, %r11648, 1077952576;
	shr.u32 	%r2164, %r2163, 1;
	and.b32  	%r2165, %r11648, -2139062144;
	shr.u32 	%r2166, %r2165, 2;
	not.b32 	%r2167, %r2166;
	and.b32  	%r2168, %r2164, %r2167;
	and.b32  	%r2169, %r11648, 522133279;
	add.s32 	%r2170, %r2169, 522133279;
	sub.s32 	%r2171, %r1956, %r2169;
	and.b32  	%r2172, %r2168, %r2171;
	and.b32  	%r2173, %r2172, %r2170;
	and.b32  	%r2174, %r2173, %r2073;
	and.b32  	%r2175, %r2174, 8192;
	xor.b32  	%r2176, %r11648, %r2175;
	and.b32  	%r2177, %r2173, %r2077;
	and.b32  	%r2178, %r2177, 2097152;
	xor.b32  	%r2179, %r2176, %r2178;
	and.b32  	%r2180, %r2173, %r2081;
	and.b32  	%r2181, %r2180, 536870912;
	xor.b32  	%r11648, %r2179, %r2181;
	and.b32  	%r2182, %r11647, 1077952576;
	shr.u32 	%r2183, %r2182, 1;
	and.b32  	%r2184, %r11647, -2139062144;
	shr.u32 	%r2185, %r2184, 2;
	not.b32 	%r2186, %r2185;
	and.b32  	%r2187, %r2183, %r2186;
	and.b32  	%r2188, %r11647, 522133279;
	add.s32 	%r2189, %r2188, 522133279;
	sub.s32 	%r2190, %r1956, %r2188;
	and.b32  	%r2191, %r2187, %r2190;
	and.b32  	%r2192, %r2191, %r2189;
	and.b32  	%r2193, %r2192, %r2096;
	and.b32  	%r2194, %r2193, 32;
	xor.b32  	%r2195, %r2194, %r11647;
	and.b32  	%r2196, %r2192, %r2100;
	and.b32  	%r2197, %r2196, 8192;
	xor.b32  	%r2198, %r2195, %r2197;
	and.b32  	%r2199, %r2192, %r2104;
	and.b32  	%r2200, %r2199, 2097152;
	xor.b32  	%r2201, %r2198, %r2200;
	and.b32  	%r2202, %r2192, %r2108;
	and.b32  	%r2203, %r2202, 536870912;
	xor.b32  	%r11647, %r2201, %r2203;
	and.b32  	%r2204, %r11646, 1077952576;
	shr.u32 	%r2205, %r2204, 1;
	and.b32  	%r2206, %r11646, -2139062144;
	shr.u32 	%r2207, %r2206, 2;
	not.b32 	%r2208, %r2207;
	and.b32  	%r2209, %r2205, %r2208;
	and.b32  	%r2210, %r11646, 522133279;
	add.s32 	%r2211, %r2210, 522133279;
	sub.s32 	%r2212, %r1956, %r2210;
	and.b32  	%r2213, %r2209, %r2212;
	and.b32  	%r2214, %r2213, %r2211;
	and.b32  	%r2215, %r2214, %r2122;
	and.b32  	%r2216, %r2215, 32;
	xor.b32  	%r2217, %r2216, %r11646;
	and.b32  	%r2218, %r2214, %r2126;
	and.b32  	%r2219, %r2218, 8192;
	xor.b32  	%r2220, %r2217, %r2219;
	and.b32  	%r2221, %r2214, %r2130;
	and.b32  	%r2222, %r2221, 2097152;
	xor.b32  	%r2223, %r2220, %r2222;
	and.b32  	%r2224, %r2214, %r2134;
	and.b32  	%r2225, %r2224, 536870912;
	xor.b32  	%r11646, %r2223, %r2225;
	and.b32  	%r2226, %r11645, 1077952576;
	shr.u32 	%r2227, %r2226, 1;
	and.b32  	%r2228, %r11645, -2139062144;
	shr.u32 	%r2229, %r2228, 2;
	not.b32 	%r2230, %r2229;
	and.b32  	%r2231, %r2227, %r2230;
	and.b32  	%r2232, %r11645, 522133279;
	add.s32 	%r2233, %r2232, 522133279;
	sub.s32 	%r2234, %r1956, %r2232;
	and.b32  	%r2235, %r2231, %r2234;
	and.b32  	%r2236, %r2235, %r2233;
	and.b32  	%r2237, %r2236, %r2148;
	and.b32  	%r2238, %r2237, 32;
	xor.b32  	%r2239, %r2238, %r11645;
	and.b32  	%r2240, %r2236, %r2152;
	and.b32  	%r2241, %r2240, 8192;
	xor.b32  	%r2242, %r2239, %r2241;
	and.b32  	%r2243, %r2236, %r2156;
	and.b32  	%r2244, %r2243, 2097152;
	xor.b32  	%r2245, %r2242, %r2244;
	and.b32  	%r2246, %r2236, %r2160;
	and.b32  	%r2247, %r2246, 536870912;
	xor.b32  	%r11645, %r2245, %r2247;
	bra.uni 	BB6_1029;

BB6_84:
	setp.gt.s32	%p6, %r1765, 119;
	@%p6 bra 	BB6_98;

	setp.gt.s32	%p13, %r1765, 114;
	@%p13 bra 	BB6_94;

	setp.eq.s32	%p17, %r1765, 113;
	@%p17 bra 	BB6_334;
	bra.uni 	BB6_87;

BB6_334:
	setp.eq.s32	%p260, %r42, 0;
	add.s32 	%r11657, %r42, %r42;
	setp.gt.u32	%p261, %r11657, 31;
	or.pred  	%p262, %p260, %p261;
	@%p262 bra 	BB6_110;

	and.b32  	%r4083, %r55, 255;
	and.b32  	%r4084, %r55, 65280;
	prmt.b32 	%r11653, %r4084, %r4083, 8452;
	bfe.u32 	%r4085, %r55, 16, 8;
	and.b32  	%r4086, %r55, -16777216;
	shr.u32 	%r4087, %r4086, 8;
	or.b32  	%r11654, %r4085, %r4087;
	and.b32  	%r4088, %r56, 65280;
	and.b32  	%r4089, %r56, 255;
	prmt.b32 	%r11655, %r4088, %r4089, 8452;
	bfe.u32 	%r4090, %r56, 16, 8;
	and.b32  	%r4091, %r56, -16777216;
	shr.u32 	%r4092, %r4091, 8;
	or.b32  	%r11656, %r4090, %r4092;
	and.b32  	%r4093, %r57, 65280;
	and.b32  	%r4094, %r57, 255;
	prmt.b32 	%r11649, %r4093, %r4094, 8452;
	bfe.u32 	%r4095, %r57, 16, 8;
	and.b32  	%r4096, %r57, -16777216;
	shr.u32 	%r4097, %r4096, 8;
	or.b32  	%r11650, %r4095, %r4097;
	and.b32  	%r4098, %r58, 65280;
	and.b32  	%r4099, %r58, 255;
	prmt.b32 	%r11651, %r4098, %r4099, 8452;
	bfe.u32 	%r4100, %r58, 16, 8;
	and.b32  	%r4101, %r58, -16777216;
	shr.u32 	%r4102, %r4101, 8;
	or.b32  	%r11652, %r4100, %r4102;
	shl.b32 	%r4103, %r11653, 8;
	or.b32  	%r11644, %r4103, %r11653;
	shl.b32 	%r4104, %r11654, 8;
	or.b32  	%r11643, %r4104, %r11654;
	shl.b32 	%r4105, %r11655, 8;
	or.b32  	%r11642, %r4105, %r11655;
	shl.b32 	%r4106, %r11656, 8;
	or.b32  	%r11641, %r4106, %r11656;
	shl.b32 	%r4107, %r11649, 8;
	or.b32  	%r11648, %r4107, %r11649;
	shl.b32 	%r4108, %r11650, 8;
	or.b32  	%r11647, %r4108, %r11650;
	shl.b32 	%r4109, %r11651, 8;
	or.b32  	%r11646, %r4109, %r11651;
	shl.b32 	%r4110, %r11652, 8;
	or.b32  	%r11645, %r4110, %r11652;
	bra.uni 	BB6_1030;

BB6_19:
	setp.gt.s32	%p47, %r1765, 63;
	@%p47 bra 	BB6_24;

	setp.eq.s32	%p51, %r1765, 45;
	@%p51 bra 	BB6_325;
	bra.uni 	BB6_21;

BB6_325:
	setp.ge.u32	%p251, %r60, %r42;
	@%p251 bra 	BB6_110;

	and.b32  	%r3983, %r59, 3;
	shl.b32 	%r3984, %r3983, 3;
	mov.u32 	%r3985, 255;
	shl.b32 	%r3986, %r3985, %r3984;
	not.b32 	%r3987, %r3986;
	and.b32  	%r3988, %r3986, 16843009;
	st.local.v4.u32 	[%rd86], {%r55, %r56, %r57, %r58};
	st.local.v4.u32 	[%rd86+16], {%r51, %r52, %r53, %r54};
	ld.local.u32 	%r3989, [%rd89];
	and.b32  	%r3990, %r3989, %r3987;
	and.b32  	%r3991, %r3989, %r3986;
	sub.s32 	%r3992, %r3991, %r3988;
	and.b32  	%r3993, %r3992, %r3986;
	or.b32  	%r3994, %r3993, %r3990;
	st.local.u32 	[%rd89], %r3994;
	bra.uni 	BB6_333;

BB6_75:
	setp.gt.s32	%p20, %r1765, 107;
	@%p20 bra 	BB6_80;

	setp.eq.s32	%p24, %r1765, 105;
	@%p24 bra 	BB6_504;
	bra.uni 	BB6_77;

BB6_504:
	setp.gt.u32	%p471, %r60, %r42;
	add.s32 	%r11657, %r42, 1;
	setp.gt.u32	%p472, %r11657, 31;
	or.pred  	%p473, %p471, %p472;
	@%p473 bra 	BB6_110;

	mov.u32 	%r5335, 24;
	// inline asm
	shf.r.wrap.b32 %r11652, %r53, %r54, %r5335;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r52, %r53, %r5335;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r51, %r52, %r5335;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r58, %r51, %r5335;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r57, %r58, %r5335;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r56, %r57, %r5335;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r55, %r56, %r5335;
	// inline asm
	mov.u32 	%r5333, 0;
	// inline asm
	shf.r.wrap.b32 %r11653, %r5333, %r55, %r5335;
	// inline asm
	and.b32  	%r5337, %r59, 3;
	shl.b32 	%r5338, %r5337, 3;
	shl.b32 	%r727, %r62, %r5338;
	mov.u32 	%r5339, 1;
	shl.b32 	%r5340, %r5339, %r5338;
	add.s32 	%r728, %r5340, -1;
	mov.u32 	%r5341, -256;
	shl.b32 	%r729, %r5341, %r5338;
	shr.u32 	%r5336, %r60, 2;
	setp.gt.s32	%p474, %r5336, 3;
	@%p474 bra 	BB6_516;

	setp.gt.s32	%p480, %r5336, 1;
	@%p480 bra 	BB6_510;

	setp.eq.s32	%p483, %r5336, 0;
	@%p483 bra 	BB6_526;
	bra.uni 	BB6_508;

BB6_526:
	and.b32  	%r5363, %r728, %r55;
	or.b32  	%r5364, %r5363, %r727;
	and.b32  	%r5365, %r11653, %r729;
	or.b32  	%r11644, %r5364, %r5365;
	mov.u32 	%r11641, %r11656;
	mov.u32 	%r11642, %r11655;
	mov.u32 	%r11643, %r11654;
	bra.uni 	BB6_515;

BB6_44:
	setp.gt.s32	%p34, %r1765, 89;
	@%p34 bra 	BB6_54;

	setp.eq.s32	%p38, %r1765, 84;
	@%p38 bra 	BB6_1024;
	bra.uni 	BB6_46;

BB6_1024:
	setp.ge.u32	%p790, %r60, %r42;
	@%p790 bra 	BB6_110;

	st.local.v4.u32 	[%rd86], {%r55, %r56, %r57, %r58};
	st.local.v4.u32 	[%rd86+16], {%r51, %r52, %r53, %r54};
	and.b32  	%r8554, %r59, 3;
	shl.b32 	%r8555, %r8554, 3;
	mov.u32 	%r8556, 32;
	shl.b32 	%r8557, %r8556, %r8555;
	ld.local.u32 	%r8558, [%rd89];
	and.b32  	%r8559, %r8558, 1077952576;
	shr.u32 	%r8560, %r8559, 1;
	shr.u32 	%r8561, %r8558, 2;
	not.b32 	%r8562, %r8561;
	and.b32  	%r8563, %r8558, 522133279;
	add.s32 	%r8564, %r8563, 522133279;
	mov.u32 	%r8565, -84215046;
	sub.s32 	%r8566, %r8565, %r8563;
	and.b32  	%r8567, %r8557, %r8562;
	and.b32  	%r8568, %r8567, %r8560;
	and.b32  	%r8569, %r8568, %r8566;
	and.b32  	%r8570, %r8569, %r8564;
	xor.b32  	%r8571, %r8570, %r8558;
	st.local.u32 	[%rd89], %r8571;
	bra.uni 	BB6_333;

BB6_98:
	setp.gt.s32	%p7, %r1765, 121;
	@%p7 bra 	BB6_107;

	setp.eq.s32	%p11, %r1765, 120;
	@%p11 bra 	BB6_640;
	bra.uni 	BB6_100;

BB6_640:
	setp.ge.u32	%p546, %r60, %r42;
	add.s32 	%r5940, %r60, %r62;
	setp.gt.u32	%p547, %r5940, %r42;
	or.pred  	%p548, %p546, %p547;
	@%p548 bra 	BB6_110;

	setp.gt.s32	%p549, %r60, 15;
	@%p549 bra 	BB6_670;

	setp.gt.s32	%p573, %r60, 7;
	@%p573 bra 	BB6_655;

	setp.gt.s32	%p585, %r60, 3;
	@%p585 bra 	BB6_648;

	setp.eq.s32	%p591, %r60, 1;
	@%p591 bra 	BB6_719;

	setp.eq.s32	%p592, %r60, 2;
	@%p592 bra 	BB6_718;
	bra.uni 	BB6_646;

BB6_718:
	mov.u32 	%r6441, 16;
	// inline asm
	shf.r.wrap.b32 %r6410, %r55, %r56, %r6441;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r56, %r57, %r6441;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11642, %r57, %r58, %r6441;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11641, %r58, %r51, %r6441;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6426, %r51, %r52, %r6441;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11647, %r52, %r53, %r6441;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11646, %r53, %r54, %r6441;
	// inline asm
	mov.u32 	%r6440, 0;
	// inline asm
	shf.r.wrap.b32 %r11645, %r54, %r6440, %r6441;
	// inline asm
	mov.u32 	%r58, %r6410;
	mov.u32 	%r54, %r6426;
	bra.uni 	BB6_723;

BB6_15:
	setp.eq.s32	%p54, %r1765, 42;
	@%p54 bra 	BB6_331;

	setp.eq.s32	%p55, %r1765, 43;
	@%p55 bra 	BB6_327;
	bra.uni 	BB6_17;

BB6_327:
	setp.ge.u32	%p252, %r60, %r42;
	@%p252 bra 	BB6_110;

	and.b32  	%r4003, %r59, 3;
	shl.b32 	%r4004, %r4003, 3;
	mov.u32 	%r4005, 255;
	shl.b32 	%r4006, %r4005, %r4004;
	not.b32 	%r4007, %r4006;
	and.b32  	%r4008, %r4006, 16843009;
	st.local.v4.u32 	[%rd86], {%r55, %r56, %r57, %r58};
	st.local.v4.u32 	[%rd86+16], {%r51, %r52, %r53, %r54};
	ld.local.u32 	%r4009, [%rd89];
	and.b32  	%r4010, %r4009, %r4007;
	and.b32  	%r4011, %r4009, %r4006;
	add.s32 	%r4012, %r4011, %r4008;
	and.b32  	%r4013, %r4012, %r4006;
	or.b32  	%r4014, %r4013, %r4010;
	st.local.u32 	[%rd89], %r4014;
	bra.uni 	BB6_333;

BB6_65:
	setp.eq.s32	%p27, %r1765, 100;
	@%p27 bra 	BB6_923;

	setp.eq.s32	%p28, %r1765, 101;
	@%p28 bra 	BB6_118;
	bra.uni 	BB6_67;

BB6_118:
	setp.eq.s32	%p93, %r42, 0;
	mov.u32 	%r11657, 0;
	@%p93 bra 	BB6_113;

	mov.u32 	%r2363, 0;
	mov.u32 	%r2376, 8;
	// inline asm
	bfe.u32 %r2249, %r55, %r2363, %r2376;
	// inline asm
	setp.eq.s32	%p94, %r2249, %r60;
	selp.u32	%r2377, 1, 0, %p94;
	// inline asm
	bfe.u32 %r2253, %r55, %r2376, %r2376;
	// inline asm
	setp.eq.s32	%p95, %r2253, %r60;
	or.b32  	%r2378, %r2377, 2;
	selp.b32	%r2379, %r2378, %r2377, %p95;
	mov.u32 	%r2371, 16;
	// inline asm
	bfe.u32 %r2257, %r55, %r2371, %r2376;
	// inline asm
	setp.eq.s32	%p96, %r2257, %r60;
	or.b32  	%r2380, %r2379, 4;
	selp.b32	%r2381, %r2380, %r2379, %p96;
	mov.u32 	%r2375, 24;
	// inline asm
	bfe.u32 %r2261, %r55, %r2375, %r2376;
	// inline asm
	setp.eq.s32	%p97, %r2261, %r60;
	or.b32  	%r2382, %r2381, 8;
	selp.b32	%r86, %r2382, %r2381, %p97;
	// inline asm
	bfe.u32 %r2265, %r56, %r2363, %r2376;
	// inline asm
	setp.eq.s32	%p98, %r2265, %r60;
	selp.u32	%r2383, 1, 0, %p98;
	// inline asm
	bfe.u32 %r2269, %r56, %r2376, %r2376;
	// inline asm
	setp.eq.s32	%p99, %r2269, %r60;
	or.b32  	%r2384, %r2383, 2;
	selp.b32	%r2385, %r2384, %r2383, %p99;
	// inline asm
	bfe.u32 %r2273, %r56, %r2371, %r2376;
	// inline asm
	setp.eq.s32	%p100, %r2273, %r60;
	or.b32  	%r2386, %r2385, 4;
	selp.b32	%r2387, %r2386, %r2385, %p100;
	// inline asm
	bfe.u32 %r2277, %r56, %r2375, %r2376;
	// inline asm
	setp.eq.s32	%p101, %r2277, %r60;
	or.b32  	%r2388, %r2387, 8;
	selp.b32	%r87, %r2388, %r2387, %p101;
	// inline asm
	bfe.u32 %r2281, %r57, %r2363, %r2376;
	// inline asm
	setp.eq.s32	%p102, %r2281, %r60;
	selp.u32	%r2389, 1, 0, %p102;
	// inline asm
	bfe.u32 %r2285, %r57, %r2376, %r2376;
	// inline asm
	setp.eq.s32	%p103, %r2285, %r60;
	or.b32  	%r2390, %r2389, 2;
	selp.b32	%r2391, %r2390, %r2389, %p103;
	// inline asm
	bfe.u32 %r2289, %r57, %r2371, %r2376;
	// inline asm
	setp.eq.s32	%p104, %r2289, %r60;
	or.b32  	%r2392, %r2391, 4;
	selp.b32	%r2393, %r2392, %r2391, %p104;
	// inline asm
	bfe.u32 %r2293, %r57, %r2375, %r2376;
	// inline asm
	setp.eq.s32	%p105, %r2293, %r60;
	or.b32  	%r2394, %r2393, 8;
	selp.b32	%r88, %r2394, %r2393, %p105;
	// inline asm
	bfe.u32 %r2297, %r58, %r2363, %r2376;
	// inline asm
	setp.eq.s32	%p106, %r2297, %r60;
	selp.u32	%r2395, 1, 0, %p106;
	// inline asm
	bfe.u32 %r2301, %r58, %r2376, %r2376;
	// inline asm
	setp.eq.s32	%p107, %r2301, %r60;
	or.b32  	%r2396, %r2395, 2;
	selp.b32	%r2397, %r2396, %r2395, %p107;
	// inline asm
	bfe.u32 %r2305, %r58, %r2371, %r2376;
	// inline asm
	setp.eq.s32	%p108, %r2305, %r60;
	or.b32  	%r2398, %r2397, 4;
	selp.b32	%r2399, %r2398, %r2397, %p108;
	// inline asm
	bfe.u32 %r2309, %r58, %r2375, %r2376;
	// inline asm
	setp.eq.s32	%p109, %r2309, %r60;
	or.b32  	%r2400, %r2399, 8;
	selp.b32	%r89, %r2400, %r2399, %p109;
	// inline asm
	bfe.u32 %r2313, %r51, %r2363, %r2376;
	// inline asm
	setp.eq.s32	%p110, %r2313, %r60;
	selp.u32	%r2401, 1, 0, %p110;
	// inline asm
	bfe.u32 %r2317, %r51, %r2376, %r2376;
	// inline asm
	setp.eq.s32	%p111, %r2317, %r60;
	or.b32  	%r2402, %r2401, 2;
	selp.b32	%r2403, %r2402, %r2401, %p111;
	// inline asm
	bfe.u32 %r2321, %r51, %r2371, %r2376;
	// inline asm
	setp.eq.s32	%p112, %r2321, %r60;
	or.b32  	%r2404, %r2403, 4;
	selp.b32	%r2405, %r2404, %r2403, %p112;
	// inline asm
	bfe.u32 %r2325, %r51, %r2375, %r2376;
	// inline asm
	setp.eq.s32	%p113, %r2325, %r60;
	or.b32  	%r2406, %r2405, 8;
	selp.b32	%r2407, %r2406, %r2405, %p113;
	// inline asm
	bfe.u32 %r2329, %r52, %r2363, %r2376;
	// inline asm
	setp.eq.s32	%p114, %r2329, %r60;
	selp.u32	%r2408, 1, 0, %p114;
	// inline asm
	bfe.u32 %r2333, %r52, %r2376, %r2376;
	// inline asm
	setp.eq.s32	%p115, %r2333, %r60;
	or.b32  	%r2409, %r2408, 2;
	selp.b32	%r2410, %r2409, %r2408, %p115;
	// inline asm
	bfe.u32 %r2337, %r52, %r2371, %r2376;
	// inline asm
	setp.eq.s32	%p116, %r2337, %r60;
	or.b32  	%r2411, %r2410, 4;
	selp.b32	%r2412, %r2411, %r2410, %p116;
	// inline asm
	bfe.u32 %r2341, %r52, %r2375, %r2376;
	// inline asm
	setp.eq.s32	%p117, %r2341, %r60;
	or.b32  	%r2413, %r2412, 8;
	selp.b32	%r2414, %r2413, %r2412, %p117;
	// inline asm
	bfe.u32 %r2345, %r53, %r2363, %r2376;
	// inline asm
	setp.eq.s32	%p118, %r2345, %r60;
	selp.u32	%r2415, 1, 0, %p118;
	// inline asm
	bfe.u32 %r2349, %r53, %r2376, %r2376;
	// inline asm
	setp.eq.s32	%p119, %r2349, %r60;
	or.b32  	%r2416, %r2415, 2;
	selp.b32	%r2417, %r2416, %r2415, %p119;
	// inline asm
	bfe.u32 %r2353, %r53, %r2371, %r2376;
	// inline asm
	setp.eq.s32	%p120, %r2353, %r60;
	or.b32  	%r2418, %r2417, 4;
	selp.b32	%r2419, %r2418, %r2417, %p120;
	// inline asm
	bfe.u32 %r2357, %r53, %r2375, %r2376;
	// inline asm
	setp.eq.s32	%p121, %r2357, %r60;
	or.b32  	%r2420, %r2419, 8;
	selp.b32	%r2421, %r2420, %r2419, %p121;
	// inline asm
	bfe.u32 %r2361, %r54, %r2363, %r2376;
	// inline asm
	setp.eq.s32	%p122, %r2361, %r60;
	selp.u32	%r2422, 1, 0, %p122;
	// inline asm
	bfe.u32 %r2365, %r54, %r2376, %r2376;
	// inline asm
	setp.eq.s32	%p123, %r2365, %r60;
	or.b32  	%r2423, %r2422, 2;
	selp.b32	%r2424, %r2423, %r2422, %p123;
	// inline asm
	bfe.u32 %r2369, %r54, %r2371, %r2376;
	// inline asm
	setp.eq.s32	%p124, %r2369, %r60;
	or.b32  	%r2425, %r2424, 4;
	selp.b32	%r2426, %r2425, %r2424, %p124;
	// inline asm
	bfe.u32 %r2373, %r54, %r2375, %r2376;
	// inline asm
	setp.eq.s32	%p125, %r2373, %r60;
	or.b32  	%r2427, %r2426, 8;
	selp.b32	%r2428, %r2427, %r2426, %p125;
	and.b32  	%r2429, %r55, 1077952576;
	shr.u32 	%r2430, %r2429, 1;
	and.b32  	%r2431, %r55, -2139062144;
	shr.u32 	%r2432, %r2431, 2;
	not.b32 	%r2433, %r2432;
	and.b32  	%r2434, %r2430, %r2433;
	and.b32  	%r2435, %r55, 522133279;
	add.s32 	%r2436, %r2435, 522133279;
	mov.u32 	%r2437, -84215046;
	sub.s32 	%r2438, %r2437, %r2435;
	and.b32  	%r2439, %r2434, %r2438;
	and.b32  	%r2440, %r2439, %r2436;
	or.b32  	%r90, %r2440, %r55;
	and.b32  	%r2441, %r56, 1077952576;
	shr.u32 	%r2442, %r2441, 1;
	and.b32  	%r2443, %r56, -2139062144;
	shr.u32 	%r2444, %r2443, 2;
	not.b32 	%r2445, %r2444;
	and.b32  	%r2446, %r2442, %r2445;
	and.b32  	%r2447, %r56, 522133279;
	add.s32 	%r2448, %r2447, 522133279;
	sub.s32 	%r2449, %r2437, %r2447;
	and.b32  	%r2450, %r2446, %r2449;
	and.b32  	%r2451, %r2450, %r2448;
	or.b32  	%r11643, %r2451, %r56;
	and.b32  	%r2452, %r57, 1077952576;
	shr.u32 	%r2453, %r2452, 1;
	and.b32  	%r2454, %r57, -2139062144;
	shr.u32 	%r2455, %r2454, 2;
	not.b32 	%r2456, %r2455;
	and.b32  	%r2457, %r2453, %r2456;
	and.b32  	%r2458, %r57, 522133279;
	add.s32 	%r2459, %r2458, 522133279;
	sub.s32 	%r2460, %r2437, %r2458;
	and.b32  	%r2461, %r2457, %r2460;
	and.b32  	%r2462, %r2461, %r2459;
	or.b32  	%r11642, %r2462, %r57;
	and.b32  	%r2463, %r58, 1077952576;
	shr.u32 	%r2464, %r2463, 1;
	and.b32  	%r2465, %r58, -2139062144;
	shr.u32 	%r2466, %r2465, 2;
	not.b32 	%r2467, %r2466;
	and.b32  	%r2468, %r2464, %r2467;
	and.b32  	%r2469, %r58, 522133279;
	add.s32 	%r2470, %r2469, 522133279;
	sub.s32 	%r2471, %r2437, %r2469;
	and.b32  	%r2472, %r2468, %r2471;
	and.b32  	%r2473, %r2472, %r2470;
	or.b32  	%r11641, %r2473, %r58;
	and.b32  	%r2474, %r51, 1077952576;
	shr.u32 	%r2475, %r2474, 1;
	and.b32  	%r2476, %r51, -2139062144;
	shr.u32 	%r2477, %r2476, 2;
	not.b32 	%r2478, %r2477;
	and.b32  	%r2479, %r2475, %r2478;
	and.b32  	%r2480, %r51, 522133279;
	add.s32 	%r2481, %r2480, 522133279;
	sub.s32 	%r2482, %r2437, %r2480;
	and.b32  	%r2483, %r2479, %r2482;
	and.b32  	%r2484, %r2483, %r2481;
	or.b32  	%r11648, %r2484, %r51;
	and.b32  	%r2485, %r52, 1077952576;
	shr.u32 	%r2486, %r2485, 1;
	and.b32  	%r2487, %r52, -2139062144;
	shr.u32 	%r2488, %r2487, 2;
	not.b32 	%r2489, %r2488;
	and.b32  	%r2490, %r2486, %r2489;
	and.b32  	%r2491, %r52, 522133279;
	add.s32 	%r2492, %r2491, 522133279;
	sub.s32 	%r2493, %r2437, %r2491;
	and.b32  	%r2494, %r2490, %r2493;
	and.b32  	%r2495, %r2494, %r2492;
	or.b32  	%r11647, %r2495, %r52;
	and.b32  	%r2496, %r53, 1077952576;
	shr.u32 	%r2497, %r2496, 1;
	and.b32  	%r2498, %r53, -2139062144;
	shr.u32 	%r2499, %r2498, 2;
	not.b32 	%r2500, %r2499;
	and.b32  	%r2501, %r2497, %r2500;
	and.b32  	%r2502, %r53, 522133279;
	add.s32 	%r2503, %r2502, 522133279;
	sub.s32 	%r2504, %r2437, %r2502;
	and.b32  	%r2505, %r2501, %r2504;
	and.b32  	%r2506, %r2505, %r2503;
	or.b32  	%r11646, %r2506, %r53;
	and.b32  	%r2507, %r54, 1077952576;
	shr.u32 	%r2508, %r2507, 1;
	and.b32  	%r2509, %r54, -2139062144;
	shr.u32 	%r2510, %r2509, 2;
	not.b32 	%r2511, %r2510;
	and.b32  	%r2512, %r2508, %r2511;
	and.b32  	%r2513, %r54, 522133279;
	add.s32 	%r2514, %r2513, 522133279;
	sub.s32 	%r2515, %r2437, %r2513;
	and.b32  	%r2516, %r2512, %r2515;
	and.b32  	%r2517, %r2516, %r2514;
	or.b32  	%r11645, %r2517, %r54;
	and.b32  	%r2518, %r90, 64;
	shr.u32 	%r2519, %r2518, 1;
	and.b32  	%r2520, %r90, 128;
	shr.u32 	%r2521, %r2520, 2;
	not.b32 	%r2522, %r2521;
	and.b32  	%r2523, %r2519, %r2522;
	and.b32  	%r98, %r90, 522133279;
	add.s32 	%r2524, %r98, 31;
	sub.s32 	%r99, %r2437, %r98;
	and.b32  	%r2525, %r2523, %r99;
	and.b32  	%r2526, %r2525, %r2524;
	not.b32 	%r2527, %r2526;
	or.b32  	%r2528, %r2527, -33;
	and.b32  	%r11644, %r2528, %r90;
	add.s32 	%r2529, %r87, %r86;
	add.s32 	%r2530, %r2529, %r88;
	add.s32 	%r2531, %r2530, %r89;
	add.s32 	%r2532, %r2531, %r2407;
	add.s32 	%r2533, %r2532, %r2414;
	add.s32 	%r2534, %r2533, %r2421;
	neg.s32 	%r2535, %r2428;
	setp.eq.s32	%p126, %r2534, %r2535;
	@%p126 bra 	BB6_1029;

	shl.b32 	%r2536, %r87, 1;
	bfe.u32 	%r2537, %r86, 3, 28;
	or.b32  	%r2538, %r2536, %r2537;
	shr.u32 	%r2539, %r2538, 4;
	shl.b32 	%r2540, %r88, 1;
	or.b32  	%r2541, %r2540, %r2539;
	shr.u32 	%r2542, %r2541, 4;
	shl.b32 	%r2543, %r89, 1;
	or.b32  	%r2544, %r2543, %r2542;
	and.b32  	%r2545, %r90, 1077952576;
	shr.u32 	%r2546, %r2545, 1;
	and.b32  	%r2547, %r90, -2139062144;
	shr.u32 	%r2548, %r2547, 2;
	not.b32 	%r2549, %r2548;
	and.b32  	%r2550, %r2546, %r2549;
	and.b32  	%r2551, %r2550, %r99;
	add.s32 	%r2552, %r98, 522133279;
	and.b32  	%r2553, %r2551, %r2552;
	shl.b32 	%r2554, %r86, 13;
	and.b32  	%r2555, %r2553, %r2554;
	and.b32  	%r2556, %r2555, 8192;
	xor.b32  	%r2557, %r11644, %r2556;
	shl.b32 	%r2558, %r86, 20;
	and.b32  	%r2559, %r2553, %r2558;
	and.b32  	%r2560, %r2559, 2097152;
	xor.b32  	%r2561, %r2557, %r2560;
	shl.b32 	%r2562, %r86, 27;
	and.b32  	%r2563, %r2553, %r2562;
	and.b32  	%r2564, %r2563, 536870912;
	xor.b32  	%r11644, %r2561, %r2564;
	and.b32  	%r2565, %r11643, 1077952576;
	shr.u32 	%r2566, %r2565, 1;
	and.b32  	%r2567, %r11643, -2139062144;
	shr.u32 	%r2568, %r2567, 2;
	not.b32 	%r2569, %r2568;
	and.b32  	%r2570, %r2566, %r2569;
	and.b32  	%r2571, %r11643, 522133279;
	add.s32 	%r2572, %r2571, 522133279;
	sub.s32 	%r2574, %r2437, %r2571;
	and.b32  	%r2575, %r2570, %r2574;
	and.b32  	%r2576, %r2575, %r2572;
	shl.b32 	%r2577, %r86, 2;
	and.b32  	%r2578, %r2576, %r2577;
	and.b32  	%r2579, %r2578, 32;
	xor.b32  	%r2580, %r2579, %r11643;
	shl.b32 	%r2581, %r2538, 12;
	and.b32  	%r2582, %r2576, %r2581;
	and.b32  	%r2583, %r2582, 8192;
	xor.b32  	%r2584, %r2580, %r2583;
	shl.b32 	%r2585, %r2538, 19;
	and.b32  	%r2586, %r2576, %r2585;
	and.b32  	%r2587, %r2586, 2097152;
	xor.b32  	%r2588, %r2584, %r2587;
	shl.b32 	%r2589, %r2538, 26;
	and.b32  	%r2590, %r2576, %r2589;
	and.b32  	%r2591, %r2590, 536870912;
	xor.b32  	%r11643, %r2588, %r2591;
	and.b32  	%r2592, %r11642, 1077952576;
	shr.u32 	%r2593, %r2592, 1;
	and.b32  	%r2594, %r11642, -2139062144;
	shr.u32 	%r2595, %r2594, 2;
	not.b32 	%r2596, %r2595;
	and.b32  	%r2597, %r2593, %r2596;
	and.b32  	%r2598, %r11642, 522133279;
	add.s32 	%r2599, %r2598, 522133279;
	sub.s32 	%r2600, %r2437, %r2598;
	and.b32  	%r2601, %r2597, %r2600;
	and.b32  	%r2602, %r2601, %r2599;
	shl.b32 	%r2603, %r2538, 1;
	and.b32  	%r2604, %r2602, %r2603;
	and.b32  	%r2605, %r2604, 32;
	xor.b32  	%r2606, %r2605, %r11642;
	shl.b32 	%r2607, %r2541, 12;
	and.b32  	%r2608, %r2602, %r2607;
	and.b32  	%r2609, %r2608, 8192;
	xor.b32  	%r2610, %r2606, %r2609;
	shl.b32 	%r2611, %r2541, 19;
	and.b32  	%r2612, %r2602, %r2611;
	and.b32  	%r2613, %r2612, 2097152;
	xor.b32  	%r2614, %r2610, %r2613;
	shl.b32 	%r2615, %r2541, 26;
	and.b32  	%r2616, %r2602, %r2615;
	and.b32  	%r2617, %r2616, 536870912;
	xor.b32  	%r11642, %r2614, %r2617;
	and.b32  	%r2618, %r11641, 1077952576;
	shr.u32 	%r2619, %r2618, 1;
	and.b32  	%r2620, %r11641, -2139062144;
	shr.u32 	%r2621, %r2620, 2;
	not.b32 	%r2622, %r2621;
	and.b32  	%r2623, %r2619, %r2622;
	and.b32  	%r2624, %r11641, 522133279;
	add.s32 	%r2625, %r2624, 522133279;
	sub.s32 	%r2626, %r2437, %r2624;
	and.b32  	%r2627, %r2623, %r2626;
	and.b32  	%r2628, %r2627, %r2625;
	shl.b32 	%r2629, %r2541, 1;
	and.b32  	%r2630, %r2628, %r2629;
	and.b32  	%r2631, %r2630, 32;
	xor.b32  	%r2632, %r2631, %r11641;
	shl.b32 	%r2633, %r2544, 12;
	and.b32  	%r2634, %r2628, %r2633;
	and.b32  	%r2635, %r2634, 8192;
	xor.b32  	%r2636, %r2632, %r2635;
	shl.b32 	%r2637, %r2544, 19;
	and.b32  	%r2638, %r2628, %r2637;
	and.b32  	%r2639, %r2638, 2097152;
	xor.b32  	%r2640, %r2636, %r2639;
	shl.b32 	%r2641, %r2544, 26;
	and.b32  	%r2642, %r2628, %r2641;
	and.b32  	%r2643, %r2642, 536870912;
	xor.b32  	%r11641, %r2640, %r2643;
	and.b32  	%r2644, %r11648, 1077952576;
	shr.u32 	%r2645, %r2644, 1;
	and.b32  	%r2646, %r11648, -2139062144;
	shr.u32 	%r2647, %r2646, 2;
	not.b32 	%r2648, %r2647;
	and.b32  	%r2649, %r2645, %r2648;
	and.b32  	%r2650, %r11648, 522133279;
	add.s32 	%r2651, %r2650, 522133279;
	sub.s32 	%r2652, %r2437, %r2650;
	and.b32  	%r2653, %r2649, %r2652;
	and.b32  	%r2654, %r2653, %r2651;
	and.b32  	%r2655, %r2654, %r2554;
	and.b32  	%r2656, %r2655, 8192;
	xor.b32  	%r2657, %r11648, %r2656;
	and.b32  	%r2658, %r2654, %r2558;
	and.b32  	%r2659, %r2658, 2097152;
	xor.b32  	%r2660, %r2657, %r2659;
	and.b32  	%r2661, %r2654, %r2562;
	and.b32  	%r2662, %r2661, 536870912;
	xor.b32  	%r11648, %r2660, %r2662;
	and.b32  	%r2663, %r11647, 1077952576;
	shr.u32 	%r2664, %r2663, 1;
	and.b32  	%r2665, %r11647, -2139062144;
	shr.u32 	%r2666, %r2665, 2;
	not.b32 	%r2667, %r2666;
	and.b32  	%r2668, %r2664, %r2667;
	and.b32  	%r2669, %r11647, 522133279;
	add.s32 	%r2670, %r2669, 522133279;
	sub.s32 	%r2671, %r2437, %r2669;
	and.b32  	%r2672, %r2668, %r2671;
	and.b32  	%r2673, %r2672, %r2670;
	and.b32  	%r2674, %r2673, %r2577;
	and.b32  	%r2675, %r2674, 32;
	xor.b32  	%r2676, %r2675, %r11647;
	and.b32  	%r2677, %r2673, %r2581;
	and.b32  	%r2678, %r2677, 8192;
	xor.b32  	%r2679, %r2676, %r2678;
	and.b32  	%r2680, %r2673, %r2585;
	and.b32  	%r2681, %r2680, 2097152;
	xor.b32  	%r2682, %r2679, %r2681;
	and.b32  	%r2683, %r2673, %r2589;
	and.b32  	%r2684, %r2683, 536870912;
	xor.b32  	%r11647, %r2682, %r2684;
	and.b32  	%r2685, %r11646, 1077952576;
	shr.u32 	%r2686, %r2685, 1;
	and.b32  	%r2687, %r11646, -2139062144;
	shr.u32 	%r2688, %r2687, 2;
	not.b32 	%r2689, %r2688;
	and.b32  	%r2690, %r2686, %r2689;
	and.b32  	%r2691, %r11646, 522133279;
	add.s32 	%r2692, %r2691, 522133279;
	sub.s32 	%r2693, %r2437, %r2691;
	and.b32  	%r2694, %r2690, %r2693;
	and.b32  	%r2695, %r2694, %r2692;
	and.b32  	%r2696, %r2695, %r2603;
	and.b32  	%r2697, %r2696, 32;
	xor.b32  	%r2698, %r2697, %r11646;
	and.b32  	%r2699, %r2695, %r2607;
	and.b32  	%r2700, %r2699, 8192;
	xor.b32  	%r2701, %r2698, %r2700;
	and.b32  	%r2702, %r2695, %r2611;
	and.b32  	%r2703, %r2702, 2097152;
	xor.b32  	%r2704, %r2701, %r2703;
	and.b32  	%r2705, %r2695, %r2615;
	and.b32  	%r2706, %r2705, 536870912;
	xor.b32  	%r11646, %r2704, %r2706;
	and.b32  	%r2707, %r11645, 1077952576;
	shr.u32 	%r2708, %r2707, 1;
	and.b32  	%r2709, %r11645, -2139062144;
	shr.u32 	%r2710, %r2709, 2;
	not.b32 	%r2711, %r2710;
	and.b32  	%r2712, %r2708, %r2711;
	and.b32  	%r2713, %r11645, 522133279;
	add.s32 	%r2714, %r2713, 522133279;
	sub.s32 	%r2715, %r2437, %r2713;
	and.b32  	%r2716, %r2712, %r2715;
	and.b32  	%r2717, %r2716, %r2714;
	and.b32  	%r2718, %r2717, %r2629;
	and.b32  	%r2719, %r2718, 32;
	xor.b32  	%r2720, %r2719, %r11645;
	and.b32  	%r2721, %r2717, %r2633;
	and.b32  	%r2722, %r2721, 8192;
	xor.b32  	%r2723, %r2720, %r2722;
	and.b32  	%r2724, %r2717, %r2637;
	and.b32  	%r2725, %r2724, 2097152;
	xor.b32  	%r2726, %r2723, %r2725;
	and.b32  	%r2727, %r2717, %r2641;
	and.b32  	%r2728, %r2727, 536870912;
	xor.b32  	%r11645, %r2726, %r2728;
	bra.uni 	BB6_1029;

BB6_39:
	setp.eq.s32	%p41, %r1765, 76;
	@%p41 bra 	BB6_329;

	setp.eq.s32	%p42, %r1765, 79;
	@%p42 bra 	BB6_527;
	bra.uni 	BB6_41;

BB6_527:
	setp.ge.u32	%p485, %r60, %r42;
	add.s32 	%r5366, %r60, %r62;
	setp.gt.u32	%p486, %r5366, %r42;
	or.pred  	%p487, %p485, %p486;
	@%p487 bra 	BB6_110;

	mov.u32 	%r11652, 0;
	setp.gt.s32	%p488, %r62, 15;
	@%p488 bra 	BB6_561;

	setp.gt.s32	%p512, %r62, 7;
	@%p512 bra 	BB6_545;

	setp.gt.s32	%p524, %r62, 3;
	@%p524 bra 	BB6_538;

	setp.gt.s32	%p530, %r62, 1;
	@%p530 bra 	BB6_535;

	setp.eq.s32	%p533, %r62, 0;
	@%p533 bra 	BB6_613;
	bra.uni 	BB6_533;

BB6_613:
	mov.u32 	%r11649, %r51;
	mov.u32 	%r11650, %r52;
	mov.u32 	%r11651, %r53;
	mov.u32 	%r11652, %r54;
	mov.u32 	%r11653, %r55;
	mov.u32 	%r11654, %r56;
	mov.u32 	%r11655, %r57;
	mov.u32 	%r11656, %r58;
	bra.uni 	BB6_614;

BB6_94:
	setp.eq.s32	%p14, %r1765, 115;
	@%p14 bra 	BB6_479;

	setp.eq.s32	%p15, %r1765, 116;
	@%p15 bra 	BB6_1026;
	bra.uni 	BB6_96;

BB6_1026:
	and.b32  	%r8580, %r55, 1077952576;
	shr.u32 	%r8581, %r8580, 1;
	and.b32  	%r8582, %r55, -2139062144;
	shr.u32 	%r8583, %r8582, 2;
	not.b32 	%r8584, %r8583;
	and.b32  	%r8585, %r8581, %r8584;
	and.b32  	%r8586, %r55, 522133279;
	add.s32 	%r8587, %r8586, 522133279;
	mov.u32 	%r8588, -84215046;
	sub.s32 	%r8589, %r8588, %r8586;
	and.b32  	%r8590, %r8585, %r8589;
	and.b32  	%r8591, %r8590, %r8587;
	xor.b32  	%r11644, %r8591, %r55;
	and.b32  	%r8592, %r56, 1077952576;
	shr.u32 	%r8593, %r8592, 1;
	and.b32  	%r8594, %r56, -2139062144;
	shr.u32 	%r8595, %r8594, 2;
	not.b32 	%r8596, %r8595;
	and.b32  	%r8597, %r8593, %r8596;
	and.b32  	%r8598, %r56, 522133279;
	add.s32 	%r8599, %r8598, 522133279;
	sub.s32 	%r8600, %r8588, %r8598;
	and.b32  	%r8601, %r8597, %r8600;
	and.b32  	%r8602, %r8601, %r8599;
	xor.b32  	%r11643, %r8602, %r56;
	and.b32  	%r8603, %r57, 1077952576;
	shr.u32 	%r8604, %r8603, 1;
	and.b32  	%r8605, %r57, -2139062144;
	shr.u32 	%r8606, %r8605, 2;
	not.b32 	%r8607, %r8606;
	and.b32  	%r8608, %r8604, %r8607;
	and.b32  	%r8609, %r57, 522133279;
	add.s32 	%r8610, %r8609, 522133279;
	sub.s32 	%r8611, %r8588, %r8609;
	and.b32  	%r8612, %r8608, %r8611;
	and.b32  	%r8613, %r8612, %r8610;
	xor.b32  	%r11642, %r8613, %r57;
	and.b32  	%r8614, %r58, 1077952576;
	shr.u32 	%r8615, %r8614, 1;
	and.b32  	%r8616, %r58, -2139062144;
	shr.u32 	%r8617, %r8616, 2;
	not.b32 	%r8618, %r8617;
	and.b32  	%r8619, %r8615, %r8618;
	and.b32  	%r8620, %r58, 522133279;
	add.s32 	%r8621, %r8620, 522133279;
	sub.s32 	%r8622, %r8588, %r8620;
	and.b32  	%r8623, %r8619, %r8622;
	and.b32  	%r8624, %r8623, %r8621;
	xor.b32  	%r11641, %r8624, %r58;
	and.b32  	%r8625, %r51, 1077952576;
	shr.u32 	%r8626, %r8625, 1;
	and.b32  	%r8627, %r51, -2139062144;
	shr.u32 	%r8628, %r8627, 2;
	not.b32 	%r8629, %r8628;
	and.b32  	%r8630, %r8626, %r8629;
	and.b32  	%r8631, %r51, 522133279;
	add.s32 	%r8632, %r8631, 522133279;
	sub.s32 	%r8633, %r8588, %r8631;
	and.b32  	%r8634, %r8630, %r8633;
	and.b32  	%r8635, %r8634, %r8632;
	xor.b32  	%r11648, %r8635, %r51;
	and.b32  	%r8636, %r52, 1077952576;
	shr.u32 	%r8637, %r8636, 1;
	and.b32  	%r8638, %r52, -2139062144;
	shr.u32 	%r8639, %r8638, 2;
	not.b32 	%r8640, %r8639;
	and.b32  	%r8641, %r8637, %r8640;
	and.b32  	%r8642, %r52, 522133279;
	add.s32 	%r8643, %r8642, 522133279;
	sub.s32 	%r8644, %r8588, %r8642;
	and.b32  	%r8645, %r8641, %r8644;
	and.b32  	%r8646, %r8645, %r8643;
	xor.b32  	%r11647, %r8646, %r52;
	and.b32  	%r8647, %r53, 1077952576;
	shr.u32 	%r8648, %r8647, 1;
	and.b32  	%r8649, %r53, -2139062144;
	shr.u32 	%r8650, %r8649, 2;
	not.b32 	%r8651, %r8650;
	and.b32  	%r8652, %r8648, %r8651;
	and.b32  	%r8653, %r53, 522133279;
	add.s32 	%r8654, %r8653, 522133279;
	sub.s32 	%r8655, %r8588, %r8653;
	and.b32  	%r8656, %r8652, %r8655;
	and.b32  	%r8657, %r8656, %r8654;
	xor.b32  	%r11646, %r8657, %r53;
	and.b32  	%r8658, %r54, 1077952576;
	shr.u32 	%r8659, %r8658, 1;
	and.b32  	%r8660, %r54, -2139062144;
	shr.u32 	%r8661, %r8660, 2;
	not.b32 	%r8662, %r8661;
	and.b32  	%r8663, %r8659, %r8662;
	and.b32  	%r8664, %r54, 522133279;
	add.s32 	%r8665, %r8664, 522133279;
	sub.s32 	%r8666, %r8588, %r8664;
	and.b32  	%r8667, %r8663, %r8666;
	and.b32  	%r8668, %r8667, %r8665;
	xor.b32  	%r11645, %r8668, %r54;
	bra.uni 	BB6_1029;

BB6_24:
	setp.eq.s32	%p48, %r1765, 64;
	@%p48 bra 	BB6_453;

	setp.eq.s32	%p49, %r1765, 67;
	@%p49 bra 	BB6_1027;
	bra.uni 	BB6_26;

BB6_1027:
	and.b32  	%r8669, %r55, 1077952576;
	shr.u32 	%r8670, %r8669, 1;
	and.b32  	%r8671, %r55, -2139062144;
	shr.u32 	%r8672, %r8671, 2;
	not.b32 	%r8673, %r8672;
	and.b32  	%r8674, %r8670, %r8673;
	and.b32  	%r8675, %r55, 522133279;
	add.s32 	%r8676, %r8675, 522133279;
	mov.u32 	%r8677, -84215046;
	sub.s32 	%r8678, %r8677, %r8675;
	and.b32  	%r8679, %r8674, %r8678;
	and.b32  	%r8680, %r8679, %r8676;
	not.b32 	%r8681, %r8680;
	and.b32  	%r8682, %r55, %r8681;
	and.b32  	%r8683, %r56, 1077952576;
	shr.u32 	%r8684, %r8683, 1;
	and.b32  	%r8685, %r56, -2139062144;
	shr.u32 	%r8686, %r8685, 2;
	not.b32 	%r8687, %r8686;
	and.b32  	%r8688, %r8684, %r8687;
	and.b32  	%r8689, %r56, 522133279;
	add.s32 	%r8690, %r8689, 522133279;
	sub.s32 	%r8691, %r8677, %r8689;
	and.b32  	%r8692, %r8688, %r8691;
	and.b32  	%r8693, %r8692, %r8690;
	not.b32 	%r8694, %r8693;
	and.b32  	%r11643, %r56, %r8694;
	and.b32  	%r8695, %r57, 1077952576;
	shr.u32 	%r8696, %r8695, 1;
	and.b32  	%r8697, %r57, -2139062144;
	shr.u32 	%r8698, %r8697, 2;
	not.b32 	%r8699, %r8698;
	and.b32  	%r8700, %r8696, %r8699;
	and.b32  	%r8701, %r57, 522133279;
	add.s32 	%r8702, %r8701, 522133279;
	sub.s32 	%r8703, %r8677, %r8701;
	and.b32  	%r8704, %r8700, %r8703;
	and.b32  	%r8705, %r8704, %r8702;
	not.b32 	%r8706, %r8705;
	and.b32  	%r11642, %r57, %r8706;
	and.b32  	%r8707, %r58, 1077952576;
	shr.u32 	%r8708, %r8707, 1;
	and.b32  	%r8709, %r58, -2139062144;
	shr.u32 	%r8710, %r8709, 2;
	not.b32 	%r8711, %r8710;
	and.b32  	%r8712, %r8708, %r8711;
	and.b32  	%r8713, %r58, 522133279;
	add.s32 	%r8714, %r8713, 522133279;
	sub.s32 	%r8715, %r8677, %r8713;
	and.b32  	%r8716, %r8712, %r8715;
	and.b32  	%r8717, %r8716, %r8714;
	not.b32 	%r8718, %r8717;
	and.b32  	%r11641, %r58, %r8718;
	and.b32  	%r8719, %r51, 1077952576;
	shr.u32 	%r8720, %r8719, 1;
	and.b32  	%r8721, %r51, -2139062144;
	shr.u32 	%r8722, %r8721, 2;
	not.b32 	%r8723, %r8722;
	and.b32  	%r8724, %r8720, %r8723;
	and.b32  	%r8725, %r51, 522133279;
	add.s32 	%r8726, %r8725, 522133279;
	sub.s32 	%r8727, %r8677, %r8725;
	and.b32  	%r8728, %r8724, %r8727;
	and.b32  	%r8729, %r8728, %r8726;
	not.b32 	%r8730, %r8729;
	and.b32  	%r11648, %r51, %r8730;
	and.b32  	%r8731, %r52, 1077952576;
	shr.u32 	%r8732, %r8731, 1;
	and.b32  	%r8733, %r52, -2139062144;
	shr.u32 	%r8734, %r8733, 2;
	not.b32 	%r8735, %r8734;
	and.b32  	%r8736, %r8732, %r8735;
	and.b32  	%r8737, %r52, 522133279;
	add.s32 	%r8738, %r8737, 522133279;
	sub.s32 	%r8739, %r8677, %r8737;
	and.b32  	%r8740, %r8736, %r8739;
	and.b32  	%r8741, %r8740, %r8738;
	not.b32 	%r8742, %r8741;
	and.b32  	%r11647, %r52, %r8742;
	and.b32  	%r8743, %r53, 1077952576;
	shr.u32 	%r8744, %r8743, 1;
	and.b32  	%r8745, %r53, -2139062144;
	shr.u32 	%r8746, %r8745, 2;
	not.b32 	%r8747, %r8746;
	and.b32  	%r8748, %r8744, %r8747;
	and.b32  	%r8749, %r53, 522133279;
	add.s32 	%r8750, %r8749, 522133279;
	sub.s32 	%r8751, %r8677, %r8749;
	and.b32  	%r8752, %r8748, %r8751;
	and.b32  	%r8753, %r8752, %r8750;
	not.b32 	%r8754, %r8753;
	and.b32  	%r11646, %r53, %r8754;
	and.b32  	%r8755, %r54, 1077952576;
	shr.u32 	%r8756, %r8755, 1;
	and.b32  	%r8757, %r54, -2139062144;
	shr.u32 	%r8758, %r8757, 2;
	not.b32 	%r8759, %r8758;
	and.b32  	%r8760, %r8756, %r8759;
	and.b32  	%r8761, %r54, 522133279;
	add.s32 	%r8762, %r8761, 522133279;
	sub.s32 	%r8763, %r8677, %r8761;
	and.b32  	%r8764, %r8760, %r8763;
	and.b32  	%r8765, %r8764, %r8762;
	not.b32 	%r8766, %r8765;
	and.b32  	%r11645, %r54, %r8766;
	and.b32  	%r8767, %r8682, 1077952576;
	shr.u32 	%r8768, %r8767, 1;
	and.b32  	%r8769, %r8682, 128;
	shr.u32 	%r8770, %r8769, 2;
	and.b32  	%r8771, %r8682, 522133279;
	add.s32 	%r8772, %r8771, 522133279;
	sub.s32 	%r8773, %r8677, %r8771;
	xor.b32  	%r8774, %r8770, 32;
	and.b32  	%r8775, %r8774, %r8768;
	and.b32  	%r8776, %r8775, %r8773;
	and.b32  	%r8777, %r8776, %r8772;
	or.b32  	%r11644, %r8777, %r8682;
	bra.uni 	BB6_1029;

BB6_80:
	setp.eq.s32	%p21, %r1765, 108;
	@%p21 bra 	BB6_1028;

	setp.eq.s32	%p22, %r1765, 111;
	@%p22 bra 	BB6_502;
	bra.uni 	BB6_82;

BB6_502:
	setp.ge.u32	%p470, %r60, %r42;
	@%p470 bra 	BB6_110;

	and.b32  	%r5287, %r59, 3;
	shl.b32 	%r5288, %r5287, 3;
	shl.b32 	%r5289, %r62, %r5288;
	mov.u32 	%r5290, 255;
	shl.b32 	%r5291, %r5290, %r5288;
	not.b32 	%r5292, %r5291;
	st.local.v4.u32 	[%rd86], {%r55, %r56, %r57, %r58};
	st.local.v4.u32 	[%rd86+16], {%r51, %r52, %r53, %r54};
	ld.local.u32 	%r5293, [%rd89];
	and.b32  	%r5294, %r5293, %r5292;
	or.b32  	%r5295, %r5294, %r5289;
	st.local.u32 	[%rd89], %r5295;
	bra.uni 	BB6_333;

BB6_54:
	setp.eq.s32	%p35, %r1765, 90;
	@%p35 bra 	BB6_336;

	setp.eq.s32	%p36, %r1765, 91;
	@%p36 bra 	BB6_768;
	bra.uni 	BB6_56;

BB6_768:
	setp.eq.s32	%p626, %r42, 0;
	mov.u32 	%r11657, 0;
	@%p626 bra 	BB6_113;

	add.s32 	%r11657, %r42, -1;
	mov.u32 	%r6618, 8;
	// inline asm
	shf.r.wrap.b32 %r11644, %r55, %r56, %r6618;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11643, %r56, %r57, %r6618;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11642, %r57, %r58, %r6618;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11641, %r58, %r51, %r6618;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11648, %r51, %r52, %r6618;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11647, %r52, %r53, %r6618;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11646, %r53, %r54, %r6618;
	// inline asm
	mov.u32 	%r6617, 0;
	// inline asm
	shf.r.wrap.b32 %r11645, %r54, %r6617, %r6618;
	// inline asm
	bra.uni 	BB6_1030;

BB6_107:
	setp.eq.s32	%p8, %r1765, 122;
	@%p8 bra 	BB6_344;

	setp.eq.s32	%p9, %r1765, 123;
	@%p9 bra 	BB6_795;
	bra.uni 	BB6_109;

BB6_795:
	setp.eq.s32	%p645, %r42, 0;
	mov.u32 	%r11657, 0;
	@%p645 bra 	BB6_113;

	add.s32 	%r6739, %r42, -1;
	mov.u32 	%r6738, 8;
	// inline asm
	shf.r.wrap.b32 %r6707, %r55, %r56, %r6738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6711, %r56, %r57, %r6738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6715, %r57, %r58, %r6738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6719, %r58, %r51, %r6738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6723, %r51, %r52, %r6738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6727, %r52, %r53, %r6738;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r6731, %r53, %r54, %r6738;
	// inline asm
	mov.u32 	%r6737, 0;
	// inline asm
	shf.r.wrap.b32 %r6735, %r54, %r6737, %r6738;
	// inline asm
	mov.u32 	%r6740, 64;
	prmt.b32 	%r6741, %r55, %r55, %r6740;
	mov.u32 	%r6742, 1040;
	prmt.b32 	%r6743, %r6741, %r55, %r6742;
	mov.u32 	%r6744, 16912;
	prmt.b32 	%r6745, %r6743, %r55, %r6744;
	bfe.u32 	%r6746, %r6739, 2, 2;
	and.b32  	%r6747, %r6739, 3;
	shl.b32 	%r6748, %r6747, 3;
	mov.u32 	%r6749, 255;
	shl.b32 	%r6750, %r6749, %r6748;
	setp.eq.s32	%p646, %r6746, 0;
	selp.b32	%r11653, %r6750, 0, %p646;
	setp.eq.s32	%p647, %r6746, 1;
	selp.b32	%r11654, %r6750, 0, %p647;
	setp.eq.s32	%p648, %r6746, 2;
	selp.b32	%r11655, %r6750, 0, %p648;
	setp.eq.s32	%p649, %r6746, 3;
	selp.b32	%r11656, %r6750, 0, %p649;
	shr.u32 	%r6751, %r6739, 4;
	setp.eq.s32	%p650, %r6751, 0;
	selp.b32	%r6752, %r6745, 0, %p650;
	and.b32  	%r6753, %r6752, %r11653;
	or.b32  	%r11644, %r6753, %r6707;
	and.b32  	%r6754, %r6752, %r11654;
	or.b32  	%r11643, %r6754, %r6711;
	and.b32  	%r6755, %r6752, %r11655;
	or.b32  	%r11642, %r6755, %r6715;
	and.b32  	%r6756, %r6752, %r11656;
	or.b32  	%r11641, %r6756, %r6719;
	setp.eq.s32	%p651, %r6751, 1;
	selp.b32	%r6757, %r6745, 0, %p651;
	and.b32  	%r6758, %r6757, %r11653;
	or.b32  	%r11648, %r6758, %r6723;
	and.b32  	%r6759, %r6757, %r11654;
	or.b32  	%r11647, %r6759, %r6727;
	and.b32  	%r6760, %r6757, %r11655;
	or.b32  	%r11646, %r6760, %r6731;
	and.b32  	%r6761, %r6757, %r11656;
	or.b32  	%r11645, %r6761, %r6735;
	bra.uni 	BB6_1029;

BB6_9:
	setp.eq.s32	%p58, %r1765, 39;
	@%p58 bra 	BB6_10;
	bra.uni 	BB6_110;

BB6_10:
	setp.ge.u32	%p458, %r60, %r42;
	@%p458 bra 	BB6_110;

	and.b32  	%r5255, %r59, 3;
	shl.b32 	%r5256, %r5255, 3;
	mov.u32 	%r5257, 1;
	shl.b32 	%r5258, %r5257, %r5256;
	add.s32 	%r701, %r5258, -1;
	shr.u32 	%r5254, %r60, 2;
	setp.gt.s32	%p459, %r5254, 3;
	@%p459 bra 	BB6_486;

	setp.gt.s32	%p465, %r5254, 1;
	@%p465 bra 	BB6_483;

	setp.eq.s32	%p468, %r5254, 0;
	@%p468 bra 	BB6_14;
	bra.uni 	BB6_481;

BB6_14:
	and.b32  	%r11644, %r701, %r55;
	mov.u32 	%r11641, 0;
	mov.u32 	%r11642, %r11641;
	mov.u32 	%r11643, %r11641;
	bra.uni 	BB6_501;

BB6_63:
	setp.eq.s32	%p31, %r1765, 99;
	@%p31 bra 	BB6_64;
	bra.uni 	BB6_110;

BB6_64:
	and.b32  	%r8778, %r55, 1077952576;
	shr.u32 	%r8779, %r8778, 1;
	and.b32  	%r8780, %r55, -2139062144;
	shr.u32 	%r8781, %r8780, 2;
	not.b32 	%r8782, %r8781;
	and.b32  	%r8783, %r8779, %r8782;
	and.b32  	%r8784, %r55, 522133279;
	add.s32 	%r8785, %r8784, 522133279;
	mov.u32 	%r8786, -84215046;
	sub.s32 	%r8787, %r8786, %r8784;
	and.b32  	%r8788, %r8783, %r8787;
	and.b32  	%r8789, %r8788, %r8785;
	or.b32  	%r8790, %r8789, %r55;
	and.b32  	%r8791, %r56, 1077952576;
	shr.u32 	%r8792, %r8791, 1;
	and.b32  	%r8793, %r56, -2139062144;
	shr.u32 	%r8794, %r8793, 2;
	not.b32 	%r8795, %r8794;
	and.b32  	%r8796, %r8792, %r8795;
	and.b32  	%r8797, %r56, 522133279;
	add.s32 	%r8798, %r8797, 522133279;
	sub.s32 	%r8799, %r8786, %r8797;
	and.b32  	%r8800, %r8796, %r8799;
	and.b32  	%r8801, %r8800, %r8798;
	or.b32  	%r11643, %r8801, %r56;
	and.b32  	%r8802, %r57, 1077952576;
	shr.u32 	%r8803, %r8802, 1;
	and.b32  	%r8804, %r57, -2139062144;
	shr.u32 	%r8805, %r8804, 2;
	not.b32 	%r8806, %r8805;
	and.b32  	%r8807, %r8803, %r8806;
	and.b32  	%r8808, %r57, 522133279;
	add.s32 	%r8809, %r8808, 522133279;
	sub.s32 	%r8810, %r8786, %r8808;
	and.b32  	%r8811, %r8807, %r8810;
	and.b32  	%r8812, %r8811, %r8809;
	or.b32  	%r11642, %r8812, %r57;
	and.b32  	%r8813, %r58, 1077952576;
	shr.u32 	%r8814, %r8813, 1;
	and.b32  	%r8815, %r58, -2139062144;
	shr.u32 	%r8816, %r8815, 2;
	not.b32 	%r8817, %r8816;
	and.b32  	%r8818, %r8814, %r8817;
	and.b32  	%r8819, %r58, 522133279;
	add.s32 	%r8820, %r8819, 522133279;
	sub.s32 	%r8821, %r8786, %r8819;
	and.b32  	%r8822, %r8818, %r8821;
	and.b32  	%r8823, %r8822, %r8820;
	or.b32  	%r11641, %r8823, %r58;
	and.b32  	%r8824, %r51, 1077952576;
	shr.u32 	%r8825, %r8824, 1;
	and.b32  	%r8826, %r51, -2139062144;
	shr.u32 	%r8827, %r8826, 2;
	not.b32 	%r8828, %r8827;
	and.b32  	%r8829, %r8825, %r8828;
	and.b32  	%r8830, %r51, 522133279;
	add.s32 	%r8831, %r8830, 522133279;
	sub.s32 	%r8832, %r8786, %r8830;
	and.b32  	%r8833, %r8829, %r8832;
	and.b32  	%r8834, %r8833, %r8831;
	or.b32  	%r11648, %r8834, %r51;
	and.b32  	%r8835, %r52, 1077952576;
	shr.u32 	%r8836, %r8835, 1;
	and.b32  	%r8837, %r52, -2139062144;
	shr.u32 	%r8838, %r8837, 2;
	not.b32 	%r8839, %r8838;
	and.b32  	%r8840, %r8836, %r8839;
	and.b32  	%r8841, %r52, 522133279;
	add.s32 	%r8842, %r8841, 522133279;
	sub.s32 	%r8843, %r8786, %r8841;
	and.b32  	%r8844, %r8840, %r8843;
	and.b32  	%r8845, %r8844, %r8842;
	or.b32  	%r11647, %r8845, %r52;
	and.b32  	%r8846, %r53, 1077952576;
	shr.u32 	%r8847, %r8846, 1;
	and.b32  	%r8848, %r53, -2139062144;
	shr.u32 	%r8849, %r8848, 2;
	not.b32 	%r8850, %r8849;
	and.b32  	%r8851, %r8847, %r8850;
	and.b32  	%r8852, %r53, 522133279;
	add.s32 	%r8853, %r8852, 522133279;
	sub.s32 	%r8854, %r8786, %r8852;
	and.b32  	%r8855, %r8851, %r8854;
	and.b32  	%r8856, %r8855, %r8853;
	or.b32  	%r11646, %r8856, %r53;
	and.b32  	%r8857, %r54, 1077952576;
	shr.u32 	%r8858, %r8857, 1;
	and.b32  	%r8859, %r54, -2139062144;
	shr.u32 	%r8860, %r8859, 2;
	not.b32 	%r8861, %r8860;
	and.b32  	%r8862, %r8858, %r8861;
	and.b32  	%r8863, %r54, 522133279;
	add.s32 	%r8864, %r8863, 522133279;
	sub.s32 	%r8865, %r8786, %r8863;
	and.b32  	%r8866, %r8862, %r8865;
	and.b32  	%r8867, %r8866, %r8864;
	or.b32  	%r11645, %r8867, %r54;
	and.b32  	%r8868, %r8790, 64;
	shr.u32 	%r8869, %r8868, 1;
	and.b32  	%r8870, %r8790, 128;
	shr.u32 	%r8871, %r8870, 2;
	not.b32 	%r8872, %r8871;
	and.b32  	%r8873, %r8869, %r8872;
	and.b32  	%r8874, %r8790, 522133279;
	add.s32 	%r8875, %r8874, 31;
	sub.s32 	%r8876, %r8786, %r8874;
	and.b32  	%r8877, %r8873, %r8876;
	and.b32  	%r8878, %r8877, %r8875;
	not.b32 	%r8879, %r8878;
	or.b32  	%r8880, %r8879, -33;
	and.b32  	%r11644, %r8880, %r8790;
	bra.uni 	BB6_1029;

BB6_36:
	setp.eq.s32	%p45, %r1765, 75;
	@%p45 bra 	BB6_37;
	bra.uni 	BB6_110;

BB6_37:
	setp.lt.u32	%p258, %r42, 2;
	@%p258 bra 	BB6_110;

	st.local.v4.u32 	[%rd86], {%r55, %r56, %r57, %r58};
	st.local.v4.u32 	[%rd86+16], {%r51, %r52, %r53, %r54};
	add.s32 	%r4069, %r42, -2;
	cvt.s64.s32	%rd132, %r4069;
	add.s64 	%rd135, %rd86, %rd132;
	ld.local.u8 	%rs12, [%rd135];
	ld.local.u8 	%rs13, [%rd135+1];
	st.local.u8 	[%rd135], %rs13;
	st.local.u8 	[%rd135+1], %rs12;
	bra.uni 	BB6_333;

BB6_87:
	setp.eq.s32	%p18, %r1765, 114;
	@%p18 bra 	BB6_88;
	bra.uni 	BB6_110;

BB6_88:
	mov.u32 	%r7993, 32;
	sub.s32 	%r7992, %r7993, %r42;
	setp.gt.s32	%p745, %r7992, 15;
	@%p745 bra 	BB6_969;

	setp.gt.s32	%p769, %r7992, 7;
	@%p769 bra 	BB6_953;

	setp.gt.s32	%p781, %r7992, 3;
	@%p781 bra 	BB6_946;

	setp.eq.s32	%p787, %r7992, 1;
	@%p787 bra 	BB6_1017;

	setp.eq.s32	%p788, %r7992, 2;
	@%p788 bra 	BB6_93;
	bra.uni 	BB6_944;

BB6_93:
	mov.u32 	%r8494, 16;
	// inline asm
	shf.r.wrap.b32 %r11653, %r53, %r54, %r8494;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r52, %r53, %r8494;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r51, %r52, %r8494;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r58, %r51, %r8494;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r57, %r58, %r8494;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r56, %r57, %r8494;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r55, %r56, %r8494;
	// inline asm
	mov.u32 	%r8492, 0;
	// inline asm
	shf.r.wrap.b32 %r11652, %r8492, %r55, %r8494;
	// inline asm
	bra.uni 	BB6_1023;

BB6_21:
	setp.eq.s32	%p52, %r1765, 46;
	@%p52 bra 	BB6_22;
	bra.uni 	BB6_110;

BB6_22:
	add.s32 	%r3932, %r60, 1;
	setp.ge.u32	%p250, %r3932, %r42;
	@%p250 bra 	BB6_110;

	mov.u32 	%r3964, 8;
	// inline asm
	shf.r.wrap.b32 %r3933, %r55, %r56, %r3964;
	// inline asm
	st.local.u32 	[%rd86], %r3933;
	// inline asm
	shf.r.wrap.b32 %r3937, %r56, %r57, %r3964;
	// inline asm
	st.local.u32 	[%rd86+4], %r3937;
	// inline asm
	shf.r.wrap.b32 %r3941, %r57, %r58, %r3964;
	// inline asm
	st.local.u32 	[%rd86+8], %r3941;
	// inline asm
	shf.r.wrap.b32 %r3945, %r58, %r51, %r3964;
	// inline asm
	st.local.u32 	[%rd86+12], %r3945;
	// inline asm
	shf.r.wrap.b32 %r3949, %r51, %r52, %r3964;
	// inline asm
	st.local.u32 	[%rd86+16], %r3949;
	// inline asm
	shf.r.wrap.b32 %r3953, %r52, %r53, %r3964;
	// inline asm
	st.local.u32 	[%rd86+20], %r3953;
	// inline asm
	shf.r.wrap.b32 %r3957, %r53, %r54, %r3964;
	// inline asm
	st.local.u32 	[%rd86+24], %r3957;
	mov.u32 	%r3963, 0;
	// inline asm
	shf.r.wrap.b32 %r3961, %r54, %r3963, %r3964;
	// inline asm
	st.local.u32 	[%rd86+28], %r3961;
	and.b32  	%r3965, %r59, 3;
	shl.b32 	%r3966, %r3965, 3;
	mov.u32 	%r3967, 255;
	shl.b32 	%r3968, %r3967, %r3966;
	not.b32 	%r3969, %r3968;
	st.local.v4.u32 	[%rd87], {%r55, %r56, %r57, %r58};
	st.local.v4.u32 	[%rd87+16], {%r51, %r52, %r53, %r54};
	ld.local.u32 	%r3970, [%rd88];
	and.b32  	%r3971, %r3970, %r3969;
	ld.local.u32 	%r3972, [%rd89];
	and.b32  	%r3973, %r3972, %r3968;
	or.b32  	%r3974, %r3973, %r3971;
	st.local.u32 	[%rd88], %r3974;
	bra.uni 	BB6_324;

BB6_77:
	setp.eq.s32	%p25, %r1765, 107;
	@%p25 bra 	BB6_78;
	bra.uni 	BB6_110;

BB6_78:
	setp.lt.u32	%p259, %r42, 2;
	@%p259 bra 	BB6_110;

	and.b32  	%r4078, %r55, -65536;
	shl.b32 	%r4079, %r55, 8;
	and.b32  	%r4080, %r4079, 65280;
	or.b32  	%r4081, %r4080, %r4078;
	bfe.u32 	%r4082, %r55, 8, 8;
	or.b32  	%r11644, %r4081, %r4082;
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	bra.uni 	BB6_111;

BB6_46:
	setp.eq.s32	%p39, %r1765, 89;
	@%p39 bra 	BB6_47;
	bra.uni 	BB6_110;

BB6_47:
	setp.gt.u32	%p127, %r60, %r42;
	add.s32 	%r11657, %r60, %r42;
	setp.gt.u32	%p128, %r11657, 31;
	or.pred  	%p129, %p127, %p128;
	@%p129 bra 	BB6_110;

	setp.gt.s32	%p130, %r60, 15;
	@%p130 bra 	BB6_149;

	setp.gt.s32	%p154, %r60, 7;
	@%p154 bra 	BB6_133;

	setp.gt.s32	%p166, %r60, 3;
	@%p166 bra 	BB6_126;

	setp.gt.s32	%p172, %r60, 1;
	@%p172 bra 	BB6_123;

	setp.eq.s32	%p175, %r60, 0;
	@%p175 bra 	BB6_53;
	bra.uni 	BB6_121;

BB6_53:
	mov.u32 	%r11649, %r51;
	mov.u32 	%r11650, %r52;
	mov.u32 	%r11651, %r53;
	mov.u32 	%r11652, %r54;
	mov.u32 	%r11653, %r55;
	mov.u32 	%r11654, %r56;
	mov.u32 	%r11655, %r57;
	mov.u32 	%r11656, %r58;
	bra.uni 	BB6_199;

BB6_100:
	setp.eq.s32	%p12, %r1765, 121;
	@%p12 bra 	BB6_101;
	bra.uni 	BB6_110;

BB6_101:
	setp.gt.u32	%p188, %r60, %r42;
	add.s32 	%r11657, %r60, %r42;
	setp.gt.u32	%p189, %r11657, 31;
	or.pred  	%p190, %p188, %p189;
	@%p190 bra 	BB6_110;

	and.b32  	%r3306, %r59, 3;
	shl.b32 	%r3307, %r3306, 3;
	mov.u32 	%r3308, 1;
	shl.b32 	%r3309, %r3308, %r3307;
	add.s32 	%r252, %r3309, -1;
	shr.u32 	%r3305, %r60, 2;
	setp.gt.s32	%p191, %r3305, 3;
	@%p191 bra 	BB6_226;

	setp.gt.s32	%p197, %r3305, 1;
	@%p197 bra 	BB6_223;

	setp.eq.s32	%p200, %r3305, 0;
	@%p200 bra 	BB6_105;
	bra.uni 	BB6_221;

BB6_105:
	and.b32  	%r11653, %r252, %r55;
	mov.u32 	%r11649, 0;
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;
	mov.u32 	%r11654, %r11649;
	bra.uni 	BB6_106;

BB6_331:
	setp.gt.u32	%p255, %r42, %r62;
	setp.lt.u32	%p256, %r60, %r42;
	and.pred  	%p257, %p256, %p255;
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11645, %r54;
	mov.u32 	%r11646, %r53;
	mov.u32 	%r11647, %r52;
	mov.u32 	%r11648, %r51;
	mov.u32 	%r11657, %r42;
	@!%p257 bra 	BB6_1030;
	bra.uni 	BB6_332;

BB6_332:
	st.local.v4.u32 	[%rd86], {%r55, %r56, %r57, %r58};
	st.local.v4.u32 	[%rd86+16], {%r51, %r52, %r53, %r54};
	cvt.u64.u32	%rd128, %r60;
	add.s64 	%rd129, %rd86, %rd128;
	ld.local.u8 	%rs10, [%rd129];
	cvt.u64.u32	%rd130, %r62;
	add.s64 	%rd131, %rd86, %rd130;
	ld.local.u8 	%rs11, [%rd131];
	st.local.u8 	[%rd129], %rs11;
	st.local.u8 	[%rd131], %rs10;
	bra.uni 	BB6_333;

BB6_17:
	setp.eq.s32	%p56, %r1765, 44;
	@%p56 bra 	BB6_18;
	bra.uni 	BB6_110;

BB6_18:
	setp.lt.u32	%p247, %r60, %r42;
	setp.ne.s32	%p248, %r60, 0;
	and.pred  	%p249, %p248, %p247;
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11645, %r54;
	mov.u32 	%r11646, %r53;
	mov.u32 	%r11647, %r52;
	mov.u32 	%r11648, %r51;
	mov.u32 	%r11657, %r42;
	@!%p249 bra 	BB6_1030;
	bra.uni 	BB6_323;

BB6_323:
	mov.u32 	%r3913, 24;
	// inline asm
	shf.r.wrap.b32 %r3882, %r53, %r54, %r3913;
	// inline asm
	st.local.u32 	[%rd86+28], %r3882;
	// inline asm
	shf.r.wrap.b32 %r3886, %r52, %r53, %r3913;
	// inline asm
	st.local.u32 	[%rd86+24], %r3886;
	// inline asm
	shf.r.wrap.b32 %r3890, %r51, %r52, %r3913;
	// inline asm
	st.local.u32 	[%rd86+20], %r3890;
	// inline asm
	shf.r.wrap.b32 %r3894, %r58, %r51, %r3913;
	// inline asm
	st.local.u32 	[%rd86+16], %r3894;
	// inline asm
	shf.r.wrap.b32 %r3898, %r57, %r58, %r3913;
	// inline asm
	st.local.u32 	[%rd86+12], %r3898;
	// inline asm
	shf.r.wrap.b32 %r3902, %r56, %r57, %r3913;
	// inline asm
	st.local.u32 	[%rd86+8], %r3902;
	// inline asm
	shf.r.wrap.b32 %r3906, %r55, %r56, %r3913;
	// inline asm
	st.local.u32 	[%rd86+4], %r3906;
	mov.u32 	%r3911, 0;
	// inline asm
	shf.r.wrap.b32 %r3910, %r3911, %r55, %r3913;
	// inline asm
	st.local.u32 	[%rd86], %r3910;
	and.b32  	%r3914, %r59, 3;
	shl.b32 	%r3915, %r3914, 3;
	mov.u32 	%r3916, 255;
	shl.b32 	%r3917, %r3916, %r3915;
	not.b32 	%r3918, %r3917;
	st.local.v4.u32 	[%rd87], {%r55, %r56, %r57, %r58};
	st.local.v4.u32 	[%rd87+16], {%r51, %r52, %r53, %r54};
	ld.local.u32 	%r3919, [%rd88];
	and.b32  	%r3920, %r3919, %r3918;
	ld.local.u32 	%r3921, [%rd89];
	and.b32  	%r3922, %r3921, %r3917;
	or.b32  	%r3923, %r3922, %r3920;
	st.local.u32 	[%rd88], %r3923;

BB6_324:
	ld.local.v4.u32 	{%r11644, %r11643, %r11642, %r11641}, [%rd87];
	ld.local.v4.u32 	{%r11648, %r11647, %r11646, %r11645}, [%rd87+16];
	bra.uni 	BB6_1029;

BB6_923:
	add.s32 	%r11657, %r42, %r42;
	setp.gt.u32	%p733, %r11657, 31;
	@%p733 bra 	BB6_110;

	shr.u32 	%r7813, %r42, 2;
	and.b32  	%r7814, %r42, 3;
	mov.u32 	%r7815, 4;
	sub.s32 	%r7816, %r7815, %r7814;
	shl.b32 	%r7817, %r7816, 2;
	mov.u32 	%r7818, 1985229328;
	shr.u32 	%r7819, %r7818, %r7817;
	and.b32  	%r1344, %r7819, 65535;
	mov.u32 	%r11625, 0;
	setp.gt.s32	%p734, %r7813, 3;
	@%p734 bra 	BB6_932;

	setp.gt.s32	%p740, %r7813, 1;
	@%p740 bra 	BB6_929;

	setp.eq.s32	%p743, %r7813, 0;
	@%p743 bra 	BB6_942;
	bra.uni 	BB6_927;

BB6_942:
	// inline asm
	prmt.b32 %r11632, %r53, %r54, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11631, %r52, %r53, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11630, %r51, %r52, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11629, %r58, %r51, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11628, %r57, %r58, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11627, %r56, %r57, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11626, %r55, %r56, %r1344;
	// inline asm
	mov.u32 	%r7989, 0;
	// inline asm
	prmt.b32 %r11625, %r7989, %r55, %r1344;
	// inline asm
	bra.uni 	BB6_943;

BB6_67:
	setp.eq.s32	%p29, %r1765, 102;
	@%p29 bra 	BB6_68;
	bra.uni 	BB6_110;

BB6_68:
	add.s32 	%r11657, %r42, %r42;
	setp.gt.u32	%p660, %r11657, 31;
	@%p660 bra 	BB6_110;

	mov.u32 	%r6826, 32;
	sub.s32 	%r6825, %r6826, %r42;
	mov.u32 	%r11599, 0;
	setp.gt.s32	%p661, %r6825, 15;
	@%p661 bra 	BB6_829;

	setp.gt.s32	%p685, %r6825, 7;
	@%p685 bra 	BB6_813;

	setp.gt.s32	%p697, %r6825, 3;
	@%p697 bra 	BB6_806;

	setp.gt.s32	%p703, %r6825, 1;
	@%p703 bra 	BB6_803;

	setp.eq.s32	%p706, %r6825, 0;
	@%p706 bra 	BB6_74;
	bra.uni 	BB6_801;

BB6_74:
	mov.u32 	%r11656, %r51;
	mov.u32 	%r11655, %r52;
	mov.u32 	%r11654, %r53;
	mov.u32 	%r11653, %r54;
	mov.u32 	%r11652, %r55;
	mov.u32 	%r11651, %r56;
	mov.u32 	%r11650, %r57;
	mov.u32 	%r11649, %r58;
	bra.uni 	BB6_880;

BB6_329:
	setp.ge.u32	%p254, %r60, %r42;
	@%p254 bra 	BB6_110;

	and.b32  	%r4042, %r59, 3;
	shl.b32 	%r4043, %r4042, 3;
	mov.u32 	%r4044, 255;
	shl.b32 	%r4045, %r4044, %r4043;
	not.b32 	%r4046, %r4045;
	st.local.v4.u32 	[%rd86], {%r55, %r56, %r57, %r58};
	st.local.v4.u32 	[%rd86+16], {%r51, %r52, %r53, %r54};
	ld.local.u32 	%r4047, [%rd89];
	and.b32  	%r4048, %r4047, %r4046;
	and.b32  	%r4049, %r4047, %r4045;
	shl.b32 	%r4050, %r4049, 1;
	and.b32  	%r4051, %r4050, %r4045;
	or.b32  	%r4052, %r4051, %r4048;
	st.local.u32 	[%rd89], %r4052;
	bra.uni 	BB6_333;

BB6_41:
	setp.eq.s32	%p43, %r1765, 82;
	@%p43 bra 	BB6_42;
	bra.uni 	BB6_110;

BB6_42:
	setp.ge.u32	%p253, %r60, %r42;
	@%p253 bra 	BB6_110;

	and.b32  	%r4023, %r59, 3;
	shl.b32 	%r4024, %r4023, 3;
	mov.u32 	%r4025, 255;
	shl.b32 	%r4026, %r4025, %r4024;
	not.b32 	%r4027, %r4026;
	st.local.v4.u32 	[%rd86], {%r55, %r56, %r57, %r58};
	st.local.v4.u32 	[%rd86+16], {%r51, %r52, %r53, %r54};
	ld.local.u32 	%r4028, [%rd89];
	and.b32  	%r4029, %r4028, %r4027;
	and.b32  	%r4030, %r4028, %r4026;
	shr.u32 	%r4031, %r4030, 1;
	and.b32  	%r4032, %r4031, %r4026;
	or.b32  	%r4033, %r4032, %r4029;
	st.local.u32 	[%rd89], %r4033;

BB6_333:
	ld.local.v4.u32 	{%r11644, %r11643, %r11642, %r11641}, [%rd86];
	ld.local.v4.u32 	{%r11648, %r11647, %r11646, %r11645}, [%rd86+16];
	bra.uni 	BB6_1029;

BB6_479:
	mov.u32 	%r5063, 0;
	mov.u32 	%r5076, 8;
	// inline asm
	bfe.u32 %r4949, %r55, %r5063, %r5076;
	// inline asm
	setp.eq.s32	%p385, %r4949, %r60;
	selp.u32	%r5077, 1, 0, %p385;
	// inline asm
	bfe.u32 %r4953, %r55, %r5076, %r5076;
	// inline asm
	setp.eq.s32	%p386, %r4953, %r60;
	or.b32  	%r5078, %r5077, 2;
	selp.b32	%r5079, %r5078, %r5077, %p386;
	mov.u32 	%r5071, 16;
	// inline asm
	bfe.u32 %r4957, %r55, %r5071, %r5076;
	// inline asm
	setp.eq.s32	%p387, %r4957, %r60;
	or.b32  	%r5080, %r5079, 4;
	selp.b32	%r5081, %r5080, %r5079, %p387;
	mov.u32 	%r5075, 24;
	// inline asm
	bfe.u32 %r4961, %r55, %r5075, %r5076;
	// inline asm
	setp.eq.s32	%p388, %r4961, %r60;
	or.b32  	%r5082, %r5081, 8;
	selp.b32	%r685, %r5082, %r5081, %p388;
	// inline asm
	bfe.u32 %r4965, %r56, %r5063, %r5076;
	// inline asm
	setp.eq.s32	%p389, %r4965, %r60;
	selp.u32	%r5083, 1, 0, %p389;
	// inline asm
	bfe.u32 %r4969, %r56, %r5076, %r5076;
	// inline asm
	setp.eq.s32	%p390, %r4969, %r60;
	or.b32  	%r5084, %r5083, 2;
	selp.b32	%r5085, %r5084, %r5083, %p390;
	// inline asm
	bfe.u32 %r4973, %r56, %r5071, %r5076;
	// inline asm
	setp.eq.s32	%p391, %r4973, %r60;
	or.b32  	%r5086, %r5085, 4;
	selp.b32	%r5087, %r5086, %r5085, %p391;
	// inline asm
	bfe.u32 %r4977, %r56, %r5075, %r5076;
	// inline asm
	setp.eq.s32	%p392, %r4977, %r60;
	or.b32  	%r5088, %r5087, 8;
	selp.b32	%r686, %r5088, %r5087, %p392;
	// inline asm
	bfe.u32 %r4981, %r57, %r5063, %r5076;
	// inline asm
	setp.eq.s32	%p393, %r4981, %r60;
	selp.u32	%r5089, 1, 0, %p393;
	// inline asm
	bfe.u32 %r4985, %r57, %r5076, %r5076;
	// inline asm
	setp.eq.s32	%p394, %r4985, %r60;
	or.b32  	%r5090, %r5089, 2;
	selp.b32	%r5091, %r5090, %r5089, %p394;
	// inline asm
	bfe.u32 %r4989, %r57, %r5071, %r5076;
	// inline asm
	setp.eq.s32	%p395, %r4989, %r60;
	or.b32  	%r5092, %r5091, 4;
	selp.b32	%r5093, %r5092, %r5091, %p395;
	// inline asm
	bfe.u32 %r4993, %r57, %r5075, %r5076;
	// inline asm
	setp.eq.s32	%p396, %r4993, %r60;
	or.b32  	%r5094, %r5093, 8;
	selp.b32	%r687, %r5094, %r5093, %p396;
	// inline asm
	bfe.u32 %r4997, %r58, %r5063, %r5076;
	// inline asm
	setp.eq.s32	%p397, %r4997, %r60;
	selp.u32	%r5095, 1, 0, %p397;
	// inline asm
	bfe.u32 %r5001, %r58, %r5076, %r5076;
	// inline asm
	setp.eq.s32	%p398, %r5001, %r60;
	or.b32  	%r5096, %r5095, 2;
	selp.b32	%r5097, %r5096, %r5095, %p398;
	// inline asm
	bfe.u32 %r5005, %r58, %r5071, %r5076;
	// inline asm
	setp.eq.s32	%p399, %r5005, %r60;
	or.b32  	%r5098, %r5097, 4;
	selp.b32	%r5099, %r5098, %r5097, %p399;
	// inline asm
	bfe.u32 %r5009, %r58, %r5075, %r5076;
	// inline asm
	setp.eq.s32	%p400, %r5009, %r60;
	or.b32  	%r5100, %r5099, 8;
	selp.b32	%r688, %r5100, %r5099, %p400;
	// inline asm
	bfe.u32 %r5013, %r51, %r5063, %r5076;
	// inline asm
	setp.eq.s32	%p401, %r5013, %r60;
	selp.u32	%r5101, 1, 0, %p401;
	// inline asm
	bfe.u32 %r5017, %r51, %r5076, %r5076;
	// inline asm
	setp.eq.s32	%p402, %r5017, %r60;
	or.b32  	%r5102, %r5101, 2;
	selp.b32	%r5103, %r5102, %r5101, %p402;
	// inline asm
	bfe.u32 %r5021, %r51, %r5071, %r5076;
	// inline asm
	setp.eq.s32	%p403, %r5021, %r60;
	or.b32  	%r5104, %r5103, 4;
	selp.b32	%r5105, %r5104, %r5103, %p403;
	// inline asm
	bfe.u32 %r5025, %r51, %r5075, %r5076;
	// inline asm
	setp.eq.s32	%p404, %r5025, %r60;
	or.b32  	%r5106, %r5105, 8;
	selp.b32	%r689, %r5106, %r5105, %p404;
	// inline asm
	bfe.u32 %r5029, %r52, %r5063, %r5076;
	// inline asm
	setp.eq.s32	%p405, %r5029, %r60;
	selp.u32	%r5107, 1, 0, %p405;
	// inline asm
	bfe.u32 %r5033, %r52, %r5076, %r5076;
	// inline asm
	setp.eq.s32	%p406, %r5033, %r60;
	or.b32  	%r5108, %r5107, 2;
	selp.b32	%r5109, %r5108, %r5107, %p406;
	// inline asm
	bfe.u32 %r5037, %r52, %r5071, %r5076;
	// inline asm
	setp.eq.s32	%p407, %r5037, %r60;
	or.b32  	%r5110, %r5109, 4;
	selp.b32	%r5111, %r5110, %r5109, %p407;
	// inline asm
	bfe.u32 %r5041, %r52, %r5075, %r5076;
	// inline asm
	setp.eq.s32	%p408, %r5041, %r60;
	or.b32  	%r5112, %r5111, 8;
	selp.b32	%r690, %r5112, %r5111, %p408;
	// inline asm
	bfe.u32 %r5045, %r53, %r5063, %r5076;
	// inline asm
	setp.eq.s32	%p409, %r5045, %r60;
	selp.u32	%r5113, 1, 0, %p409;
	// inline asm
	bfe.u32 %r5049, %r53, %r5076, %r5076;
	// inline asm
	setp.eq.s32	%p410, %r5049, %r60;
	or.b32  	%r5114, %r5113, 2;
	selp.b32	%r5115, %r5114, %r5113, %p410;
	// inline asm
	bfe.u32 %r5053, %r53, %r5071, %r5076;
	// inline asm
	setp.eq.s32	%p411, %r5053, %r60;
	or.b32  	%r5116, %r5115, 4;
	selp.b32	%r5117, %r5116, %r5115, %p411;
	// inline asm
	bfe.u32 %r5057, %r53, %r5075, %r5076;
	// inline asm
	setp.eq.s32	%p412, %r5057, %r60;
	or.b32  	%r5118, %r5117, 8;
	selp.b32	%r691, %r5118, %r5117, %p412;
	// inline asm
	bfe.u32 %r5061, %r54, %r5063, %r5076;
	// inline asm
	setp.eq.s32	%p413, %r5061, %r60;
	selp.u32	%r5119, 1, 0, %p413;
	// inline asm
	bfe.u32 %r5065, %r54, %r5076, %r5076;
	// inline asm
	setp.eq.s32	%p414, %r5065, %r60;
	or.b32  	%r5120, %r5119, 2;
	selp.b32	%r5121, %r5120, %r5119, %p414;
	// inline asm
	bfe.u32 %r5069, %r54, %r5071, %r5076;
	// inline asm
	setp.eq.s32	%p415, %r5069, %r60;
	or.b32  	%r5122, %r5121, 4;
	selp.b32	%r5123, %r5122, %r5121, %p415;
	// inline asm
	bfe.u32 %r5073, %r54, %r5075, %r5076;
	// inline asm
	setp.eq.s32	%p416, %r5073, %r60;
	or.b32  	%r5124, %r5123, 8;
	selp.b32	%r692, %r5124, %r5123, %p416;
	add.s32 	%r5125, %r686, %r685;
	add.s32 	%r5126, %r5125, %r687;
	add.s32 	%r5127, %r5126, %r688;
	add.s32 	%r5128, %r5127, %r689;
	add.s32 	%r5129, %r5128, %r690;
	add.s32 	%r5130, %r5129, %r691;
	neg.s32 	%r5131, %r692;
	setp.eq.s32	%p417, %r5130, %r5131;
	@%p417 bra 	BB6_110;

	and.b32  	%r5132, %r685, 1;
	setp.eq.b32	%p418, %r5132, 1;
	not.pred 	%p419, %p418;
	and.b32  	%r5133, %r55, -256;
	or.b32  	%r5134, %r62, %r5133;
	selp.b32	%r5135, %r55, %r5134, %p419;
	and.b32  	%r5136, %r685, 2;
	setp.eq.s32	%p420, %r5136, 0;
	and.b32  	%r5137, %r5135, -65281;
	shl.b32 	%r5138, %r62, 8;
	or.b32  	%r5139, %r5137, %r5138;
	selp.b32	%r5140, %r5135, %r5139, %p420;
	and.b32  	%r5141, %r685, 4;
	setp.eq.s32	%p421, %r5141, 0;
	and.b32  	%r5142, %r5140, -16711681;
	shl.b32 	%r5143, %r62, 16;
	or.b32  	%r5144, %r5142, %r5143;
	selp.b32	%r5145, %r5140, %r5144, %p421;
	and.b32  	%r5146, %r685, 8;
	setp.eq.s32	%p422, %r5146, 0;
	and.b32  	%r5147, %r5145, 16777215;
	prmt.b32 	%r5148, %r61, %r5147, 1620;
	selp.b32	%r11644, %r5145, %r5148, %p422;
	and.b32  	%r5149, %r686, 1;
	setp.eq.b32	%p423, %r5149, 1;
	not.pred 	%p424, %p423;
	and.b32  	%r5150, %r56, -256;
	or.b32  	%r5151, %r62, %r5150;
	selp.b32	%r5152, %r56, %r5151, %p424;
	and.b32  	%r5153, %r686, 2;
	setp.eq.s32	%p425, %r5153, 0;
	and.b32  	%r5154, %r5152, -65281;
	or.b32  	%r5155, %r5154, %r5138;
	selp.b32	%r5156, %r5152, %r5155, %p425;
	and.b32  	%r5157, %r686, 4;
	setp.eq.s32	%p426, %r5157, 0;
	and.b32  	%r5158, %r5156, -16711681;
	or.b32  	%r5159, %r5158, %r5143;
	selp.b32	%r5160, %r5156, %r5159, %p426;
	and.b32  	%r5161, %r686, 8;
	setp.eq.s32	%p427, %r5161, 0;
	and.b32  	%r5162, %r5160, 16777215;
	prmt.b32 	%r5163, %r61, %r5162, 1620;
	selp.b32	%r11643, %r5160, %r5163, %p427;
	and.b32  	%r5164, %r687, 1;
	setp.eq.b32	%p428, %r5164, 1;
	not.pred 	%p429, %p428;
	and.b32  	%r5165, %r57, -256;
	or.b32  	%r5166, %r62, %r5165;
	selp.b32	%r5167, %r57, %r5166, %p429;
	and.b32  	%r5168, %r687, 2;
	setp.eq.s32	%p430, %r5168, 0;
	and.b32  	%r5169, %r5167, -65281;
	or.b32  	%r5170, %r5169, %r5138;
	selp.b32	%r5171, %r5167, %r5170, %p430;
	and.b32  	%r5172, %r687, 4;
	setp.eq.s32	%p431, %r5172, 0;
	and.b32  	%r5173, %r5171, -16711681;
	or.b32  	%r5174, %r5173, %r5143;
	selp.b32	%r5175, %r5171, %r5174, %p431;
	and.b32  	%r5176, %r687, 8;
	setp.eq.s32	%p432, %r5176, 0;
	and.b32  	%r5177, %r5175, 16777215;
	prmt.b32 	%r5178, %r61, %r5177, 1620;
	selp.b32	%r11642, %r5175, %r5178, %p432;
	and.b32  	%r5179, %r688, 1;
	setp.eq.b32	%p433, %r5179, 1;
	not.pred 	%p434, %p433;
	and.b32  	%r5180, %r58, -256;
	or.b32  	%r5181, %r62, %r5180;
	selp.b32	%r5182, %r58, %r5181, %p434;
	and.b32  	%r5183, %r688, 2;
	setp.eq.s32	%p435, %r5183, 0;
	and.b32  	%r5184, %r5182, -65281;
	or.b32  	%r5185, %r5184, %r5138;
	selp.b32	%r5186, %r5182, %r5185, %p435;
	and.b32  	%r5187, %r688, 4;
	setp.eq.s32	%p436, %r5187, 0;
	and.b32  	%r5188, %r5186, -16711681;
	or.b32  	%r5189, %r5188, %r5143;
	selp.b32	%r5190, %r5186, %r5189, %p436;
	and.b32  	%r5191, %r688, 8;
	setp.eq.s32	%p437, %r5191, 0;
	and.b32  	%r5192, %r5190, 16777215;
	prmt.b32 	%r5193, %r61, %r5192, 1620;
	selp.b32	%r11641, %r5190, %r5193, %p437;
	and.b32  	%r5194, %r689, 1;
	setp.eq.b32	%p438, %r5194, 1;
	not.pred 	%p439, %p438;
	and.b32  	%r5195, %r51, -256;
	or.b32  	%r5196, %r62, %r5195;
	selp.b32	%r5197, %r51, %r5196, %p439;
	and.b32  	%r5198, %r689, 2;
	setp.eq.s32	%p440, %r5198, 0;
	and.b32  	%r5199, %r5197, -65281;
	or.b32  	%r5200, %r5199, %r5138;
	selp.b32	%r5201, %r5197, %r5200, %p440;
	and.b32  	%r5202, %r689, 4;
	setp.eq.s32	%p441, %r5202, 0;
	and.b32  	%r5203, %r5201, -16711681;
	or.b32  	%r5204, %r5203, %r5143;
	selp.b32	%r5205, %r5201, %r5204, %p441;
	and.b32  	%r5206, %r689, 8;
	setp.eq.s32	%p442, %r5206, 0;
	and.b32  	%r5207, %r5205, 16777215;
	prmt.b32 	%r5208, %r61, %r5207, 1620;
	selp.b32	%r11648, %r5205, %r5208, %p442;
	and.b32  	%r5209, %r690, 1;
	setp.eq.b32	%p443, %r5209, 1;
	not.pred 	%p444, %p443;
	and.b32  	%r5210, %r52, -256;
	or.b32  	%r5211, %r62, %r5210;
	selp.b32	%r5212, %r52, %r5211, %p444;
	and.b32  	%r5213, %r690, 2;
	setp.eq.s32	%p445, %r5213, 0;
	and.b32  	%r5214, %r5212, -65281;
	or.b32  	%r5215, %r5214, %r5138;
	selp.b32	%r5216, %r5212, %r5215, %p445;
	and.b32  	%r5217, %r690, 4;
	setp.eq.s32	%p446, %r5217, 0;
	and.b32  	%r5218, %r5216, -16711681;
	or.b32  	%r5219, %r5218, %r5143;
	selp.b32	%r5220, %r5216, %r5219, %p446;
	and.b32  	%r5221, %r690, 8;
	setp.eq.s32	%p447, %r5221, 0;
	and.b32  	%r5222, %r5220, 16777215;
	prmt.b32 	%r5223, %r61, %r5222, 1620;
	selp.b32	%r11647, %r5220, %r5223, %p447;
	and.b32  	%r5224, %r691, 1;
	setp.eq.b32	%p448, %r5224, 1;
	not.pred 	%p449, %p448;
	and.b32  	%r5225, %r53, -256;
	or.b32  	%r5226, %r62, %r5225;
	selp.b32	%r5227, %r53, %r5226, %p449;
	and.b32  	%r5228, %r691, 2;
	setp.eq.s32	%p450, %r5228, 0;
	and.b32  	%r5229, %r5227, -65281;
	or.b32  	%r5230, %r5229, %r5138;
	selp.b32	%r5231, %r5227, %r5230, %p450;
	and.b32  	%r5232, %r691, 4;
	setp.eq.s32	%p451, %r5232, 0;
	and.b32  	%r5233, %r5231, -16711681;
	or.b32  	%r5234, %r5233, %r5143;
	selp.b32	%r5235, %r5231, %r5234, %p451;
	and.b32  	%r5236, %r691, 8;
	setp.eq.s32	%p452, %r5236, 0;
	and.b32  	%r5237, %r5235, 16777215;
	prmt.b32 	%r5238, %r61, %r5237, 1620;
	selp.b32	%r11646, %r5235, %r5238, %p452;
	and.b32  	%r5239, %r692, 1;
	setp.eq.b32	%p453, %r5239, 1;
	not.pred 	%p454, %p453;
	and.b32  	%r5240, %r54, -256;
	or.b32  	%r5241, %r62, %r5240;
	selp.b32	%r5242, %r54, %r5241, %p454;
	and.b32  	%r5243, %r692, 2;
	setp.eq.s32	%p455, %r5243, 0;
	and.b32  	%r5244, %r5242, -65281;
	or.b32  	%r5245, %r5244, %r5138;
	selp.b32	%r5246, %r5242, %r5245, %p455;
	and.b32  	%r5247, %r692, 4;
	setp.eq.s32	%p456, %r5247, 0;
	and.b32  	%r5248, %r5246, -16711681;
	or.b32  	%r5249, %r5248, %r5143;
	selp.b32	%r5250, %r5246, %r5249, %p456;
	and.b32  	%r5251, %r692, 8;
	setp.eq.s32	%p457, %r5251, 0;
	and.b32  	%r5252, %r5250, 16777215;
	prmt.b32 	%r5253, %r61, %r5252, 1620;
	selp.b32	%r11645, %r5250, %r5253, %p457;
	bra.uni 	BB6_1029;

BB6_96:
	setp.eq.s32	%p16, %r1765, 117;
	@%p16 bra 	BB6_97;
	bra.uni 	BB6_110;

BB6_97:
	and.b32  	%r8881, %r55, 1077952576;
	shr.u32 	%r8882, %r8881, 1;
	and.b32  	%r8883, %r55, -2139062144;
	shr.u32 	%r8884, %r8883, 2;
	not.b32 	%r8885, %r8884;
	and.b32  	%r8886, %r8882, %r8885;
	and.b32  	%r8887, %r55, 522133279;
	add.s32 	%r8888, %r8887, 522133279;
	mov.u32 	%r8889, -84215046;
	sub.s32 	%r8890, %r8889, %r8887;
	and.b32  	%r8891, %r8886, %r8890;
	and.b32  	%r8892, %r8891, %r8888;
	not.b32 	%r8893, %r8892;
	and.b32  	%r11644, %r55, %r8893;
	and.b32  	%r8894, %r56, 1077952576;
	shr.u32 	%r8895, %r8894, 1;
	and.b32  	%r8896, %r56, -2139062144;
	shr.u32 	%r8897, %r8896, 2;
	not.b32 	%r8898, %r8897;
	and.b32  	%r8899, %r8895, %r8898;
	and.b32  	%r8900, %r56, 522133279;
	add.s32 	%r8901, %r8900, 522133279;
	sub.s32 	%r8902, %r8889, %r8900;
	and.b32  	%r8903, %r8899, %r8902;
	and.b32  	%r8904, %r8903, %r8901;
	not.b32 	%r8905, %r8904;
	and.b32  	%r11643, %r56, %r8905;
	and.b32  	%r8906, %r57, 1077952576;
	shr.u32 	%r8907, %r8906, 1;
	and.b32  	%r8908, %r57, -2139062144;
	shr.u32 	%r8909, %r8908, 2;
	not.b32 	%r8910, %r8909;
	and.b32  	%r8911, %r8907, %r8910;
	and.b32  	%r8912, %r57, 522133279;
	add.s32 	%r8913, %r8912, 522133279;
	sub.s32 	%r8914, %r8889, %r8912;
	and.b32  	%r8915, %r8911, %r8914;
	and.b32  	%r8916, %r8915, %r8913;
	not.b32 	%r8917, %r8916;
	and.b32  	%r11642, %r57, %r8917;
	and.b32  	%r8918, %r58, 1077952576;
	shr.u32 	%r8919, %r8918, 1;
	and.b32  	%r8920, %r58, -2139062144;
	shr.u32 	%r8921, %r8920, 2;
	not.b32 	%r8922, %r8921;
	and.b32  	%r8923, %r8919, %r8922;
	and.b32  	%r8924, %r58, 522133279;
	add.s32 	%r8925, %r8924, 522133279;
	sub.s32 	%r8926, %r8889, %r8924;
	and.b32  	%r8927, %r8923, %r8926;
	and.b32  	%r8928, %r8927, %r8925;
	not.b32 	%r8929, %r8928;
	and.b32  	%r11641, %r58, %r8929;
	and.b32  	%r8930, %r51, 1077952576;
	shr.u32 	%r8931, %r8930, 1;
	and.b32  	%r8932, %r51, -2139062144;
	shr.u32 	%r8933, %r8932, 2;
	not.b32 	%r8934, %r8933;
	and.b32  	%r8935, %r8931, %r8934;
	and.b32  	%r8936, %r51, 522133279;
	add.s32 	%r8937, %r8936, 522133279;
	sub.s32 	%r8938, %r8889, %r8936;
	and.b32  	%r8939, %r8935, %r8938;
	and.b32  	%r8940, %r8939, %r8937;
	not.b32 	%r8941, %r8940;
	and.b32  	%r11648, %r51, %r8941;
	and.b32  	%r8942, %r52, 1077952576;
	shr.u32 	%r8943, %r8942, 1;
	and.b32  	%r8944, %r52, -2139062144;
	shr.u32 	%r8945, %r8944, 2;
	not.b32 	%r8946, %r8945;
	and.b32  	%r8947, %r8943, %r8946;
	and.b32  	%r8948, %r52, 522133279;
	add.s32 	%r8949, %r8948, 522133279;
	sub.s32 	%r8950, %r8889, %r8948;
	and.b32  	%r8951, %r8947, %r8950;
	and.b32  	%r8952, %r8951, %r8949;
	not.b32 	%r8953, %r8952;
	and.b32  	%r11647, %r52, %r8953;
	and.b32  	%r8954, %r53, 1077952576;
	shr.u32 	%r8955, %r8954, 1;
	and.b32  	%r8956, %r53, -2139062144;
	shr.u32 	%r8957, %r8956, 2;
	not.b32 	%r8958, %r8957;
	and.b32  	%r8959, %r8955, %r8958;
	and.b32  	%r8960, %r53, 522133279;
	add.s32 	%r8961, %r8960, 522133279;
	sub.s32 	%r8962, %r8889, %r8960;
	and.b32  	%r8963, %r8959, %r8962;
	and.b32  	%r8964, %r8963, %r8961;
	not.b32 	%r8965, %r8964;
	and.b32  	%r11646, %r53, %r8965;
	and.b32  	%r8966, %r54, 1077952576;
	shr.u32 	%r8967, %r8966, 1;
	and.b32  	%r8968, %r54, -2139062144;
	shr.u32 	%r8969, %r8968, 2;
	not.b32 	%r8970, %r8969;
	and.b32  	%r8971, %r8967, %r8970;
	and.b32  	%r8972, %r54, 522133279;
	add.s32 	%r8973, %r8972, 522133279;
	sub.s32 	%r8974, %r8889, %r8972;
	and.b32  	%r8975, %r8971, %r8974;
	and.b32  	%r8976, %r8975, %r8973;
	not.b32 	%r8977, %r8976;
	and.b32  	%r11645, %r54, %r8977;
	bra.uni 	BB6_1029;

BB6_453:
	mov.u32 	%r11657, 0;
	mov.u32 	%r4863, 8;
	// inline asm
	bfe.u32 %r4736, %r55, %r11657, %r4863;
	// inline asm
	setp.eq.s32	%p339, %r4736, %r60;
	selp.u32	%r4864, 1, 0, %p339;
	// inline asm
	bfe.u32 %r4740, %r55, %r4863, %r4863;
	// inline asm
	setp.eq.s32	%p340, %r4740, %r60;
	or.b32  	%r4865, %r4864, 2;
	selp.b32	%r4866, %r4865, %r4864, %p340;
	mov.u32 	%r4858, 16;
	// inline asm
	bfe.u32 %r4744, %r55, %r4858, %r4863;
	// inline asm
	setp.eq.s32	%p341, %r4744, %r60;
	or.b32  	%r4867, %r4866, 4;
	selp.b32	%r4868, %r4867, %r4866, %p341;
	mov.u32 	%r4862, 24;
	// inline asm
	bfe.u32 %r4748, %r55, %r4862, %r4863;
	// inline asm
	setp.eq.s32	%p342, %r4748, %r60;
	or.b32  	%r4869, %r4868, 8;
	selp.b32	%r4870, %r4869, %r4868, %p342;
	// inline asm
	bfe.u32 %r4752, %r56, %r11657, %r4863;
	// inline asm
	setp.eq.s32	%p343, %r4752, %r60;
	selp.u32	%r4871, 1, 0, %p343;
	// inline asm
	bfe.u32 %r4756, %r56, %r4863, %r4863;
	// inline asm
	setp.eq.s32	%p344, %r4756, %r60;
	or.b32  	%r4872, %r4871, 2;
	selp.b32	%r4873, %r4872, %r4871, %p344;
	// inline asm
	bfe.u32 %r4760, %r56, %r4858, %r4863;
	// inline asm
	setp.eq.s32	%p345, %r4760, %r60;
	or.b32  	%r4874, %r4873, 4;
	selp.b32	%r4875, %r4874, %r4873, %p345;
	// inline asm
	bfe.u32 %r4764, %r56, %r4862, %r4863;
	// inline asm
	setp.eq.s32	%p346, %r4764, %r60;
	or.b32  	%r4876, %r4875, 8;
	selp.b32	%r4877, %r4876, %r4875, %p346;
	// inline asm
	bfe.u32 %r4768, %r57, %r11657, %r4863;
	// inline asm
	setp.eq.s32	%p347, %r4768, %r60;
	selp.u32	%r4878, 1, 0, %p347;
	// inline asm
	bfe.u32 %r4772, %r57, %r4863, %r4863;
	// inline asm
	setp.eq.s32	%p348, %r4772, %r60;
	or.b32  	%r4879, %r4878, 2;
	selp.b32	%r4880, %r4879, %r4878, %p348;
	// inline asm
	bfe.u32 %r4776, %r57, %r4858, %r4863;
	// inline asm
	setp.eq.s32	%p349, %r4776, %r60;
	or.b32  	%r4881, %r4880, 4;
	selp.b32	%r4882, %r4881, %r4880, %p349;
	// inline asm
	bfe.u32 %r4780, %r57, %r4862, %r4863;
	// inline asm
	setp.eq.s32	%p350, %r4780, %r60;
	or.b32  	%r4883, %r4882, 8;
	selp.b32	%r4884, %r4883, %r4882, %p350;
	// inline asm
	bfe.u32 %r4784, %r58, %r11657, %r4863;
	// inline asm
	setp.eq.s32	%p351, %r4784, %r60;
	selp.u32	%r4885, 1, 0, %p351;
	// inline asm
	bfe.u32 %r4788, %r58, %r4863, %r4863;
	// inline asm
	setp.eq.s32	%p352, %r4788, %r60;
	or.b32  	%r4886, %r4885, 2;
	selp.b32	%r4887, %r4886, %r4885, %p352;
	// inline asm
	bfe.u32 %r4792, %r58, %r4858, %r4863;
	// inline asm
	setp.eq.s32	%p353, %r4792, %r60;
	or.b32  	%r4888, %r4887, 4;
	selp.b32	%r4889, %r4888, %r4887, %p353;
	// inline asm
	bfe.u32 %r4796, %r58, %r4862, %r4863;
	// inline asm
	setp.eq.s32	%p354, %r4796, %r60;
	or.b32  	%r4890, %r4889, 8;
	selp.b32	%r4891, %r4890, %r4889, %p354;
	// inline asm
	bfe.u32 %r4800, %r51, %r11657, %r4863;
	// inline asm
	setp.eq.s32	%p355, %r4800, %r60;
	selp.u32	%r4892, 1, 0, %p355;
	// inline asm
	bfe.u32 %r4804, %r51, %r4863, %r4863;
	// inline asm
	setp.eq.s32	%p356, %r4804, %r60;
	or.b32  	%r4893, %r4892, 2;
	selp.b32	%r4894, %r4893, %r4892, %p356;
	// inline asm
	bfe.u32 %r4808, %r51, %r4858, %r4863;
	// inline asm
	setp.eq.s32	%p357, %r4808, %r60;
	or.b32  	%r4895, %r4894, 4;
	selp.b32	%r4896, %r4895, %r4894, %p357;
	// inline asm
	bfe.u32 %r4812, %r51, %r4862, %r4863;
	// inline asm
	setp.eq.s32	%p358, %r4812, %r60;
	or.b32  	%r4897, %r4896, 8;
	selp.b32	%r4898, %r4897, %r4896, %p358;
	// inline asm
	bfe.u32 %r4816, %r52, %r11657, %r4863;
	// inline asm
	setp.eq.s32	%p359, %r4816, %r60;
	selp.u32	%r4899, 1, 0, %p359;
	// inline asm
	bfe.u32 %r4820, %r52, %r4863, %r4863;
	// inline asm
	setp.eq.s32	%p360, %r4820, %r60;
	or.b32  	%r4900, %r4899, 2;
	selp.b32	%r4901, %r4900, %r4899, %p360;
	// inline asm
	bfe.u32 %r4824, %r52, %r4858, %r4863;
	// inline asm
	setp.eq.s32	%p361, %r4824, %r60;
	or.b32  	%r4902, %r4901, 4;
	selp.b32	%r4903, %r4902, %r4901, %p361;
	// inline asm
	bfe.u32 %r4828, %r52, %r4862, %r4863;
	// inline asm
	setp.eq.s32	%p362, %r4828, %r60;
	or.b32  	%r4904, %r4903, 8;
	selp.b32	%r4905, %r4904, %r4903, %p362;
	// inline asm
	bfe.u32 %r4832, %r53, %r11657, %r4863;
	// inline asm
	setp.eq.s32	%p363, %r4832, %r60;
	selp.u32	%r4906, 1, 0, %p363;
	// inline asm
	bfe.u32 %r4836, %r53, %r4863, %r4863;
	// inline asm
	setp.eq.s32	%p364, %r4836, %r60;
	or.b32  	%r4907, %r4906, 2;
	selp.b32	%r4908, %r4907, %r4906, %p364;
	// inline asm
	bfe.u32 %r4840, %r53, %r4858, %r4863;
	// inline asm
	setp.eq.s32	%p365, %r4840, %r60;
	or.b32  	%r4909, %r4908, 4;
	selp.b32	%r4910, %r4909, %r4908, %p365;
	// inline asm
	bfe.u32 %r4844, %r53, %r4862, %r4863;
	// inline asm
	setp.eq.s32	%p366, %r4844, %r60;
	or.b32  	%r4911, %r4910, 8;
	selp.b32	%r4912, %r4911, %r4910, %p366;
	// inline asm
	bfe.u32 %r4848, %r54, %r11657, %r4863;
	// inline asm
	setp.eq.s32	%p367, %r4848, %r60;
	selp.u32	%r4913, 1, 0, %p367;
	// inline asm
	bfe.u32 %r4852, %r54, %r4863, %r4863;
	// inline asm
	setp.eq.s32	%p368, %r4852, %r60;
	or.b32  	%r4914, %r4913, 2;
	selp.b32	%r4915, %r4914, %r4913, %p368;
	// inline asm
	bfe.u32 %r4856, %r54, %r4858, %r4863;
	// inline asm
	setp.eq.s32	%p369, %r4856, %r60;
	or.b32  	%r4916, %r4915, 4;
	selp.b32	%r4917, %r4916, %r4915, %p369;
	// inline asm
	bfe.u32 %r4860, %r54, %r4862, %r4863;
	// inline asm
	setp.eq.s32	%p370, %r4860, %r60;
	or.b32  	%r4918, %r4917, 8;
	selp.b32	%r4919, %r4918, %r4917, %p370;
	add.s32 	%r4920, %r4877, %r4870;
	add.s32 	%r4921, %r4920, %r4884;
	add.s32 	%r4922, %r4921, %r4891;
	add.s32 	%r4923, %r4922, %r4898;
	add.s32 	%r4924, %r4923, %r4905;
	add.s32 	%r4925, %r4924, %r4912;
	neg.s32 	%r4926, %r4919;
	setp.eq.s32	%p371, %r4925, %r4926;
	@%p371 bra 	BB6_110;

	mov.u64 	%rd2752, 0;
	st.local.v4.u32 	[%rd86], {%r55, %r56, %r57, %r58};
	st.local.v4.u32 	[%rd86+16], {%r51, %r52, %r53, %r54};
	st.local.v2.u64 	[%rd87], {%rd2752, %rd2752};
	st.local.v2.u64 	[%rd87+16], {%rd2752, %rd2752};
	cvt.u16.u32	%rs23, %r55;
	setp.eq.s32	%p372, %r42, 0;
	@%p372 bra 	BB6_478;

	cvt.u16.u32	%rs2, %r59;
	and.b32  	%r652, %r42, 3;
	setp.eq.s32	%p373, %r652, 0;
	mov.u32 	%r11551, 0;
	mov.u32 	%r11657, %r11551;
	@%p373 bra 	BB6_468;

	setp.eq.s32	%p374, %r652, 1;
	mov.u32 	%r11546, 0;
	@%p374 bra 	BB6_457;
	bra.uni 	BB6_458;

BB6_457:
	mov.u32 	%r11657, %r11546;
	bra.uni 	BB6_465;

BB6_26:
	setp.eq.s32	%p50, %r1765, 68;
	@%p50 bra 	BB6_27;
	bra.uni 	BB6_110;

BB6_27:
	setp.ge.u32	%p605, %r60, %r42;
	@%p605 bra 	BB6_110;

	mov.u32 	%r6549, 8;
	// inline asm
	shf.r.wrap.b32 %r11653, %r55, %r56, %r6549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11654, %r56, %r57, %r6549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11655, %r57, %r58, %r6549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11656, %r58, %r51, %r6549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11649, %r51, %r52, %r6549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11650, %r52, %r53, %r6549;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11651, %r53, %r54, %r6549;
	// inline asm
	mov.u32 	%r6548, 0;
	// inline asm
	shf.r.wrap.b32 %r11652, %r54, %r6548, %r6549;
	// inline asm
	and.b32  	%r6551, %r59, 3;
	shl.b32 	%r6552, %r6551, 3;
	mov.u32 	%r6553, 1;
	shl.b32 	%r6554, %r6553, %r6552;
	add.s32 	%r1006, %r6554, -1;
	neg.s32 	%r1007, %r6554;
	shr.u32 	%r6550, %r60, 2;
	setp.gt.s32	%p606, %r6550, 3;
	@%p606 bra 	BB6_753;

	setp.gt.s32	%p612, %r6550, 1;
	@%p612 bra 	BB6_748;

	setp.eq.s32	%p615, %r6550, 0;
	@%p615 bra 	BB6_31;
	bra.uni 	BB6_746;

BB6_31:
	and.b32  	%r6569, %r1006, %r55;
	and.b32  	%r6570, %r11653, %r1007;
	or.b32  	%r11644, %r6570, %r6569;
	mov.u32 	%r11641, %r11656;
	mov.u32 	%r11642, %r11655;
	mov.u32 	%r11643, %r11654;
	bra.uni 	BB6_32;

BB6_1028:
	and.b32  	%r8978, %r55, 1077952576;
	shr.u32 	%r8979, %r8978, 1;
	and.b32  	%r8980, %r55, -2139062144;
	shr.u32 	%r8981, %r8980, 2;
	not.b32 	%r8982, %r8981;
	and.b32  	%r8983, %r8979, %r8982;
	and.b32  	%r8984, %r55, 522133279;
	add.s32 	%r8985, %r8984, 522133279;
	mov.u32 	%r8986, -84215046;
	sub.s32 	%r8987, %r8986, %r8984;
	and.b32  	%r8988, %r8983, %r8987;
	and.b32  	%r8989, %r8988, %r8985;
	or.b32  	%r11644, %r8989, %r55;
	and.b32  	%r8990, %r56, 1077952576;
	shr.u32 	%r8991, %r8990, 1;
	and.b32  	%r8992, %r56, -2139062144;
	shr.u32 	%r8993, %r8992, 2;
	not.b32 	%r8994, %r8993;
	and.b32  	%r8995, %r8991, %r8994;
	and.b32  	%r8996, %r56, 522133279;
	add.s32 	%r8997, %r8996, 522133279;
	sub.s32 	%r8998, %r8986, %r8996;
	and.b32  	%r8999, %r8995, %r8998;
	and.b32  	%r9000, %r8999, %r8997;
	or.b32  	%r11643, %r9000, %r56;
	and.b32  	%r9001, %r57, 1077952576;
	shr.u32 	%r9002, %r9001, 1;
	and.b32  	%r9003, %r57, -2139062144;
	shr.u32 	%r9004, %r9003, 2;
	not.b32 	%r9005, %r9004;
	and.b32  	%r9006, %r9002, %r9005;
	and.b32  	%r9007, %r57, 522133279;
	add.s32 	%r9008, %r9007, 522133279;
	sub.s32 	%r9009, %r8986, %r9007;
	and.b32  	%r9010, %r9006, %r9009;
	and.b32  	%r9011, %r9010, %r9008;
	or.b32  	%r11642, %r9011, %r57;
	and.b32  	%r9012, %r58, 1077952576;
	shr.u32 	%r9013, %r9012, 1;
	and.b32  	%r9014, %r58, -2139062144;
	shr.u32 	%r9015, %r9014, 2;
	not.b32 	%r9016, %r9015;
	and.b32  	%r9017, %r9013, %r9016;
	and.b32  	%r9018, %r58, 522133279;
	add.s32 	%r9019, %r9018, 522133279;
	sub.s32 	%r9020, %r8986, %r9018;
	and.b32  	%r9021, %r9017, %r9020;
	and.b32  	%r9022, %r9021, %r9019;
	or.b32  	%r11641, %r9022, %r58;
	and.b32  	%r9023, %r51, 1077952576;
	shr.u32 	%r9024, %r9023, 1;
	and.b32  	%r9025, %r51, -2139062144;
	shr.u32 	%r9026, %r9025, 2;
	not.b32 	%r9027, %r9026;
	and.b32  	%r9028, %r9024, %r9027;
	and.b32  	%r9029, %r51, 522133279;
	add.s32 	%r9030, %r9029, 522133279;
	sub.s32 	%r9031, %r8986, %r9029;
	and.b32  	%r9032, %r9028, %r9031;
	and.b32  	%r9033, %r9032, %r9030;
	or.b32  	%r11648, %r9033, %r51;
	and.b32  	%r9034, %r52, 1077952576;
	shr.u32 	%r9035, %r9034, 1;
	and.b32  	%r9036, %r52, -2139062144;
	shr.u32 	%r9037, %r9036, 2;
	not.b32 	%r9038, %r9037;
	and.b32  	%r9039, %r9035, %r9038;
	and.b32  	%r9040, %r52, 522133279;
	add.s32 	%r9041, %r9040, 522133279;
	sub.s32 	%r9042, %r8986, %r9040;
	and.b32  	%r9043, %r9039, %r9042;
	and.b32  	%r9044, %r9043, %r9041;
	or.b32  	%r11647, %r9044, %r52;
	and.b32  	%r9045, %r53, 1077952576;
	shr.u32 	%r9046, %r9045, 1;
	and.b32  	%r9047, %r53, -2139062144;
	shr.u32 	%r9048, %r9047, 2;
	not.b32 	%r9049, %r9048;
	and.b32  	%r9050, %r9046, %r9049;
	and.b32  	%r9051, %r53, 522133279;
	add.s32 	%r9052, %r9051, 522133279;
	sub.s32 	%r9053, %r8986, %r9051;
	and.b32  	%r9054, %r9050, %r9053;
	and.b32  	%r9055, %r9054, %r9052;
	or.b32  	%r11646, %r9055, %r53;
	and.b32  	%r9056, %r54, 1077952576;
	shr.u32 	%r9057, %r9056, 1;
	and.b32  	%r9058, %r54, -2139062144;
	shr.u32 	%r9059, %r9058, 2;
	not.b32 	%r9060, %r9059;
	and.b32  	%r9061, %r9057, %r9060;
	and.b32  	%r9062, %r54, 522133279;
	add.s32 	%r9063, %r9062, 522133279;
	sub.s32 	%r9064, %r8986, %r9062;
	and.b32  	%r9065, %r9061, %r9064;
	and.b32  	%r9066, %r9065, %r9063;
	or.b32  	%r11645, %r9066, %r54;
	bra.uni 	BB6_1029;

BB6_82:
	setp.eq.s32	%p23, %r1765, 112;
	@%p23 bra 	BB6_83;
	bra.uni 	BB6_110;

BB6_83:
	mad.lo.s32 	%r7616, %r60, %r42, %r42;
	setp.gt.u32	%p719, %r7616, 31;
	@%p719 bra 	BB6_110;

	setp.eq.s32	%p720, %r60, 0;
	mov.u32 	%r7617, 0;
	mov.u32 	%r11607, %r7617;
	mov.u32 	%r11657, %r42;
	mov.u32 	%r11648, %r51;
	mov.u32 	%r11647, %r52;
	mov.u32 	%r11646, %r53;
	mov.u32 	%r11645, %r54;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11641, %r58;
	@%p720 bra 	BB6_901;
	bra.uni 	BB6_902;

BB6_901:
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11645, %r54;
	mov.u32 	%r11646, %r53;
	mov.u32 	%r11647, %r52;
	mov.u32 	%r11648, %r51;
	mov.u32 	%r11649, %r51;
	mov.u32 	%r11650, %r52;
	mov.u32 	%r11651, %r53;
	mov.u32 	%r11652, %r54;
	mov.u32 	%r11653, %r55;
	mov.u32 	%r11654, %r56;
	mov.u32 	%r11655, %r57;
	mov.u32 	%r11656, %r58;
	bra.uni 	BB6_1029;

BB6_902:
	and.b32  	%r7627, %r11657, 3;
	mov.u32 	%r7628, 4;
	sub.s32 	%r7629, %r7628, %r7627;
	shl.b32 	%r7630, %r7629, 2;
	mov.u32 	%r7631, 1985229328;
	shr.u32 	%r7632, %r7631, %r7630;
	and.b32  	%r1288, %r7632, 65535;
	shr.u32 	%r7626, %r11657, 2;
	setp.gt.s32	%p721, %r7626, 3;
	@%p721 bra 	BB6_910;

	setp.gt.s32	%p727, %r7626, 1;
	@%p727 bra 	BB6_907;

	setp.eq.s32	%p730, %r7626, 0;
	@%p730 bra 	BB6_920;
	bra.uni 	BB6_905;

BB6_920:
	// inline asm
	prmt.b32 %r11624, %r53, %r54, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11623, %r52, %r53, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11622, %r51, %r52, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11621, %r58, %r51, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11620, %r57, %r58, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11619, %r56, %r57, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11618, %r55, %r56, %r1288;
	// inline asm
	mov.u32 	%r7802, 0;
	// inline asm
	prmt.b32 %r11617, %r7802, %r55, %r1288;
	// inline asm
	bra.uni 	BB6_921;

BB6_910:
	setp.gt.s32	%p722, %r7626, 5;
	@%p722 bra 	BB6_914;

	setp.eq.s32	%p725, %r7626, 4;
	@%p725 bra 	BB6_918;
	bra.uni 	BB6_912;

BB6_918:
	// inline asm
	prmt.b32 %r11624, %r57, %r58, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11623, %r56, %r57, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11622, %r55, %r56, %r1288;
	// inline asm
	mov.u32 	%r11617, 0;
	// inline asm
	prmt.b32 %r11621, %r11617, %r55, %r1288;
	// inline asm
	mov.u32 	%r11618, %r11617;
	mov.u32 	%r11619, %r11617;
	mov.u32 	%r11620, %r11617;
	bra.uni 	BB6_921;

BB6_907:
	setp.eq.s32	%p728, %r7626, 2;
	@%p728 bra 	BB6_919;
	bra.uni 	BB6_908;

BB6_919:
	// inline asm
	prmt.b32 %r11624, %r51, %r52, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11623, %r58, %r51, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11622, %r57, %r58, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11621, %r56, %r57, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11620, %r55, %r56, %r1288;
	// inline asm
	mov.u32 	%r11617, 0;
	// inline asm
	prmt.b32 %r11619, %r11617, %r55, %r1288;
	// inline asm
	mov.u32 	%r11618, %r11617;
	bra.uni 	BB6_921;

BB6_914:
	setp.eq.s32	%p723, %r7626, 6;
	@%p723 bra 	BB6_917;
	bra.uni 	BB6_915;

BB6_917:
	// inline asm
	prmt.b32 %r11624, %r55, %r56, %r1288;
	// inline asm
	mov.u32 	%r11617, 0;
	// inline asm
	prmt.b32 %r11623, %r11617, %r55, %r1288;
	// inline asm
	mov.u32 	%r11618, %r11617;
	mov.u32 	%r11619, %r11617;
	mov.u32 	%r11620, %r11617;
	mov.u32 	%r11621, %r11617;
	mov.u32 	%r11622, %r11617;
	bra.uni 	BB6_921;

BB6_905:
	setp.eq.s32	%p731, %r7626, 1;
	mov.u32 	%r11617, %r7617;
	mov.u32 	%r11618, %r7617;
	mov.u32 	%r11619, %r7617;
	mov.u32 	%r11620, %r7617;
	mov.u32 	%r11621, %r7617;
	mov.u32 	%r11622, %r7617;
	mov.u32 	%r11623, %r7617;
	mov.u32 	%r11624, %r7617;
	@%p731 bra 	BB6_906;
	bra.uni 	BB6_921;

BB6_906:
	// inline asm
	prmt.b32 %r11624, %r52, %r53, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11623, %r51, %r52, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11622, %r58, %r51, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11621, %r57, %r58, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11620, %r56, %r57, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11619, %r55, %r56, %r1288;
	// inline asm
	mov.u32 	%r11617, 0;
	// inline asm
	prmt.b32 %r11618, %r11617, %r55, %r1288;
	// inline asm
	bra.uni 	BB6_921;

BB6_912:
	setp.eq.s32	%p726, %r7626, 5;
	mov.u32 	%r11617, %r7617;
	mov.u32 	%r11618, %r7617;
	mov.u32 	%r11619, %r7617;
	mov.u32 	%r11620, %r7617;
	mov.u32 	%r11621, %r7617;
	mov.u32 	%r11622, %r7617;
	mov.u32 	%r11623, %r7617;
	mov.u32 	%r11624, %r7617;
	@%p726 bra 	BB6_913;
	bra.uni 	BB6_921;

BB6_913:
	// inline asm
	prmt.b32 %r11624, %r56, %r57, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11623, %r55, %r56, %r1288;
	// inline asm
	mov.u32 	%r11617, 0;
	// inline asm
	prmt.b32 %r11622, %r11617, %r55, %r1288;
	// inline asm
	mov.u32 	%r11618, %r11617;
	mov.u32 	%r11619, %r11617;
	mov.u32 	%r11620, %r11617;
	mov.u32 	%r11621, %r11617;
	bra.uni 	BB6_921;

BB6_908:
	setp.eq.s32	%p729, %r7626, 3;
	mov.u32 	%r11617, %r7617;
	mov.u32 	%r11618, %r7617;
	mov.u32 	%r11619, %r7617;
	mov.u32 	%r11620, %r7617;
	mov.u32 	%r11621, %r7617;
	mov.u32 	%r11622, %r7617;
	mov.u32 	%r11623, %r7617;
	mov.u32 	%r11624, %r7617;
	@%p729 bra 	BB6_909;
	bra.uni 	BB6_921;

BB6_909:
	// inline asm
	prmt.b32 %r11624, %r58, %r51, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11623, %r57, %r58, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11622, %r56, %r57, %r1288;
	// inline asm
	// inline asm
	prmt.b32 %r11621, %r55, %r56, %r1288;
	// inline asm
	mov.u32 	%r11617, 0;
	// inline asm
	prmt.b32 %r11620, %r11617, %r55, %r1288;
	// inline asm
	mov.u32 	%r11618, %r11617;
	mov.u32 	%r11619, %r11617;
	bra.uni 	BB6_921;

BB6_915:
	setp.ne.s32	%p724, %r7626, 7;
	mov.u32 	%r11617, %r7617;
	mov.u32 	%r11618, %r7617;
	mov.u32 	%r11619, %r7617;
	mov.u32 	%r11620, %r7617;
	mov.u32 	%r11621, %r7617;
	mov.u32 	%r11622, %r7617;
	mov.u32 	%r11623, %r7617;
	mov.u32 	%r11624, %r7617;
	@%p724 bra 	BB6_921;

	mov.u32 	%r11617, 0;
	// inline asm
	prmt.b32 %r11624, %r11617, %r55, %r1288;
	// inline asm
	mov.u32 	%r11618, %r11617;
	mov.u32 	%r11619, %r11617;
	mov.u32 	%r11620, %r11617;
	mov.u32 	%r11621, %r11617;
	mov.u32 	%r11622, %r11617;
	mov.u32 	%r11623, %r11617;

BB6_921:
	or.b32  	%r11644, %r11617, %r11644;
	or.b32  	%r11643, %r11618, %r11643;
	or.b32  	%r11642, %r11619, %r11642;
	or.b32  	%r11641, %r11620, %r11641;
	or.b32  	%r11648, %r11621, %r11648;
	or.b32  	%r11647, %r11622, %r11647;
	or.b32  	%r11646, %r11623, %r11646;
	or.b32  	%r11645, %r11624, %r11645;
	add.s32 	%r11657, %r11657, %r42;
	add.s32 	%r11607, %r11607, 1;
	setp.lt.u32	%p732, %r11607, %r60;
	@%p732 bra 	BB6_902;

	mov.u32 	%r11649, %r51;
	mov.u32 	%r11650, %r52;
	mov.u32 	%r11651, %r53;
	mov.u32 	%r11652, %r54;
	mov.u32 	%r11653, %r55;
	mov.u32 	%r11654, %r56;
	mov.u32 	%r11655, %r57;
	mov.u32 	%r11656, %r58;
	bra.uni 	BB6_1030;

BB6_336:
	setp.eq.s32	%p263, %r42, 0;
	add.s32 	%r4111, %r60, %r42;
	setp.gt.u32	%p264, %r4111, 31;
	or.pred  	%p265, %p263, %p264;
	@%p265 bra 	BB6_110;

	add.s32 	%r4114, %r42, -1;
	and.b32  	%r4115, %r4114, 3;
	shl.b32 	%r475, %r4115, 3;
	bfe.u32 	%r4116, %r4114, 2, 2;
	mov.u32 	%r4117, 255;
	shl.b32 	%r4118, %r4117, %r475;
	setp.eq.s32	%p266, %r4116, 0;
	selp.b32	%r11649, %r4118, 0, %p266;
	setp.eq.s32	%p267, %r4116, 1;
	selp.b32	%r11650, %r4118, 0, %p267;
	setp.eq.s32	%p268, %r4116, 2;
	selp.b32	%r11651, %r4118, 0, %p268;
	setp.eq.s32	%p269, %r4116, 3;
	selp.b32	%r11652, %r4118, 0, %p269;
	shr.u32 	%r4113, %r4114, 4;
	mov.u32 	%r11516, 0;
	setp.eq.s32	%p270, %r4113, 0;
	@%p270 bra 	BB6_340;
	bra.uni 	BB6_338;

BB6_340:
	and.b32  	%r4125, %r11649, %r55;
	and.b32  	%r4126, %r11650, %r56;
	or.b32  	%r4127, %r4125, %r4126;
	and.b32  	%r4128, %r11651, %r57;
	or.b32  	%r4129, %r4127, %r4128;
	and.b32  	%r4130, %r11652, %r58;
	or.b32  	%r11516, %r4129, %r4130;
	bra.uni 	BB6_341;

BB6_56:
	setp.eq.s32	%p37, %r1765, 93;
	@%p37 bra 	BB6_57;
	bra.uni 	BB6_110;

BB6_57:
	setp.eq.s32	%p617, %r42, 0;
	mov.u32 	%r11657, 0;
	@%p617 bra 	BB6_113;

	add.s32 	%r11657, %r42, -1;
	and.b32  	%r6572, %r11657, 3;
	shl.b32 	%r6573, %r6572, 3;
	mov.u32 	%r6574, 1;
	shl.b32 	%r6575, %r6574, %r6573;
	add.s32 	%r6576, %r6575, -1;
	setp.lt.u32	%p618, %r11657, 4;
	selp.b32	%r6577, %r6576, -1, %p618;
	and.b32  	%r11644, %r6577, %r55;
	and.b32  	%r6578, %r11657, -4;
	setp.eq.s32	%p619, %r6578, 4;
	selp.b32	%r6579, %r6576, -1, %p619;
	and.b32  	%r11643, %r6579, %r56;
	setp.eq.s32	%p620, %r6578, 8;
	selp.b32	%r6580, %r6576, -1, %p620;
	and.b32  	%r11642, %r6580, %r57;
	setp.eq.s32	%p621, %r6578, 12;
	selp.b32	%r6581, %r6576, -1, %p621;
	and.b32  	%r11641, %r6581, %r58;
	setp.eq.s32	%p622, %r6578, 16;
	selp.b32	%r6582, %r6576, -1, %p622;
	and.b32  	%r11648, %r6582, %r51;
	setp.eq.s32	%p623, %r6578, 20;
	selp.b32	%r6583, %r6576, -1, %p623;
	and.b32  	%r11647, %r6583, %r52;
	setp.eq.s32	%p624, %r6578, 24;
	selp.b32	%r6584, %r6576, -1, %p624;
	and.b32  	%r11646, %r6584, %r53;
	setp.gt.u32	%p625, %r11657, 27;
	selp.b32	%r6585, %r6576, -1, %p625;
	and.b32  	%r11645, %r6585, %r54;
	bra.uni 	BB6_1030;

BB6_344:
	setp.eq.s32	%p280, %r42, 0;
	add.s32 	%r11657, %r60, %r42;
	setp.gt.u32	%p281, %r11657, 31;
	or.pred  	%p282, %p280, %p281;
	@%p282 bra 	BB6_110;

	mov.u32 	%r4154, 64;
	prmt.b32 	%r4155, %r55, %r55, %r4154;
	mov.u32 	%r4156, 1040;
	prmt.b32 	%r4157, %r4155, %r55, %r4156;
	mov.u32 	%r4158, 16912;
	prmt.b32 	%r11653, %r4157, %r55, %r4158;
	setp.gt.s32	%p283, %r60, 15;
	@%p283 bra 	BB6_374;

	setp.gt.s32	%p307, %r60, 7;
	@%p307 bra 	BB6_359;

	setp.gt.s32	%p319, %r60, 3;
	@%p319 bra 	BB6_352;

	setp.eq.s32	%p325, %r60, 1;
	@%p325 bra 	BB6_423;

	setp.eq.s32	%p326, %r60, 2;
	@%p326 bra 	BB6_422;
	bra.uni 	BB6_350;

BB6_422:
	mov.u32 	%r4659, 16;
	// inline asm
	shf.r.wrap.b32 %r54, %r53, %r54, %r4659;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r53, %r52, %r53, %r4659;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r52, %r51, %r52, %r4659;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r4640, %r58, %r51, %r4659;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11527, %r57, %r58, %r4659;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11528, %r56, %r57, %r4659;
	// inline asm
	// inline asm
	shf.r.wrap.b32 %r11529, %r55, %r56, %r4659;
	// inline asm
	mov.u32 	%r4657, 0;
	// inline asm
	shf.r.wrap.b32 %r11530, %r4657, %r55, %r4659;
	// inline asm
	mov.u32 	%r55, %r4640;
	bra.uni 	BB6_429;

BB6_109:
	setp.eq.s32	%p10, %r1765, 125;
	@%p10 bra 	BB6_770;
	bra.uni 	BB6_110;

BB6_770:
	setp.eq.s32	%p627, %r42, 0;
	mov.u32 	%r11657, 0;
	@%p627 bra 	BB6_113;

	add.s32 	%r6622, %r42, -1;
	and.b32  	%r6623, %r6622, 3;
	shl.b32 	%r1043, %r6623, 3;
	bfe.u32 	%r6624, %r6622, 2, 2;
	mov.u32 	%r6625, 255;
	shl.b32 	%r6626, %r6625, %r1043;
	setp.eq.s32	%p628, %r6624, 0;
	selp.b32	%r11653, %r6626, 0, %p628;
	setp.eq.s32	%p629, %r6624, 1;
	selp.b32	%r11654, %r6626, 0, %p629;
	setp.eq.s32	%p630, %r6624, 2;
	selp.b32	%r11655, %r6626, 0, %p630;
	setp.eq.s32	%p631, %r6624, 3;
	selp.b32	%r11656, %r6626, 0, %p631;
	shr.u32 	%r6621, %r6622, 4;
	mov.u32 	%r6620, 0;
	setp.eq.s32	%p632, %r6621, 0;
	@%p632 bra 	BB6_774;
	bra.uni 	BB6_772;

BB6_774:
	and.b32  	%r6633, %r11653, %r55;
	and.b32  	%r6634, %r11654, %r56;
	or.b32  	%r6635, %r6633, %r6634;
	and.b32  	%r6636, %r11655, %r57;
	or.b32  	%r6637, %r6635, %r6636;
	and.b32  	%r6638, %r11656, %r58;
	or.b32  	%r11590, %r6637, %r6638;
	bra.uni 	BB6_775;

BB6_516:
	setp.gt.s32	%p475, %r5336, 5;
	@%p475 bra 	BB6_520;

	setp.eq.s32	%p478, %r5336, 4;
	@%p478 bra 	BB6_524;
	bra.uni 	BB6_518;

BB6_524:
	and.b32  	%r5351, %r728, %r51;
	or.b32  	%r5352, %r5351, %r727;
	and.b32  	%r5353, %r11649, %r729;
	or.b32  	%r11648, %r5352, %r5353;
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;
	mov.u32 	%r11645, %r11652;
	mov.u32 	%r11646, %r11651;
	mov.u32 	%r11647, %r11650;
	bra.uni 	BB6_1030;

BB6_670:
	setp.gt.s32	%p550, %r60, 23;
	@%p550 bra 	BB6_686;

	setp.gt.s32	%p562, %r60, 19;
	@%p562 bra 	BB6_679;

	setp.gt.s32	%p568, %r60, 17;
	@%p568 bra 	BB6_676;

	setp.eq.s32	%p571, %r60, 16;
	@%p571 bra 	BB6_708;
	bra.uni 	BB6_674;

BB6_708:
	mov.u32 	%r11645, 0;
	mov.u32 	%r11641, %r54;
	mov.u32 	%r11642, %r53;
	mov.u32 	%r11643, %r52;
	mov.u32 	%r58, %r51;
	bra.uni 	BB6_709;

BB6_969:
	setp.gt.s32	%p746, %r7992, 23;
	@%p746 bra 	BB6_986;

	setp.gt.s32	%p758, %r7992, 19;
	@%p758 bra 	BB6_978;

	setp.gt.s32	%p764, %r7992, 17;
	@%p764 bra 	BB6_975;

	setp.eq.s32	%p767, %r7992, 16;
	@%p767 bra 	BB6_1010;
	bra.uni 	BB6_973;

BB6_1010:
	mov.u32 	%r11649, 0;
	mov.u32 	%r11650, %r11649;
	mov.u32 	%r11651, %r11649;
	mov.u32 	%r11652, %r11649;
	mov.u32 	%r11653, %r58;
	mov.u32 	%r11654, %r57;
	mov.u32 	%r11655, %r56;
	mov.u32 	%r11656, %r55;
	bra.uni 	BB6_1023;

BB6_561:
	setp.gt.s32	%p489, %r62, 23;
	@%p489 bra 	BB6_579;

	setp.gt.s32	%p501, %r62, 19;
	@%p501 bra 	BB6_571;

	setp.gt.s32	%p507, %r62, 17;
	@%p507 bra 	BB6_567;

	setp.eq.s32	%p510, %r62, 16;
	@%p510 bra 	BB6_605;
	bra.uni 	BB6_565;

BB6_605:
	mov.u32 	%r11649, %r11652;
	mov.u32 	%r11650, %r11652;
	mov.u32 	%r11651, %r11652;
	mov.u32 	%r11653, %r51;
	mov.u32 	%r11654, %r52;
	mov.u32 	%r11655, %r53;
	mov.u32 	%r11656, %r54;
	bra.uni 	BB6_614;

BB6_932:
	setp.gt.s32	%p735, %r7813, 5;
	@%p735 bra 	BB6_936;

	setp.eq.s32	%p738, %r7813, 4;
	@%p738 bra 	BB6_940;
	bra.uni 	BB6_934;

BB6_940:
	// inline asm
	prmt.b32 %r11632, %r57, %r58, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11631, %r56, %r57, %r1344;
	// inline asm
	// inline asm
	prmt.b32 %r11630, %r55, %r56, %r1344;
	// inline asm
	mov.u32 	%r11625, 0;
	// inline asm
	prmt.b32 %r11629, %r11625, %r55, %r1344;
	// inline asm
	mov.u32 	%r11626, %r11625;
	mov.u32 	%r11627, %r11625;
	mov.u32 	%r11628, %r11625;
	bra.uni 	BB6_943;

BB6_338:
	setp.ne.s32	%p271, %r4113, 1;
	@%p271 bra 	BB6_341;

	and.b32  	%r4119, %r11649, %r51;
	and.b32  	%r4120, %r11650, %r52;
	or.b32  	%r4121, %r4119, %r4120;
	and.b32  	%r4122, %r11651, %r53;
	or.b32  	%r4123, %r4121, %r4122;
	and.b32  	%r4124, %r11652, %r54;
	or.b32  	%r11516, %r4123, %r4124;

BB6_341:
	shr.u32 	%r483, %r11516, %r475;
	setp.eq.s32	%p272, %r60, 0;
	@%p272 bra 	BB6_110;

	and.b32  	%r4132, %r483, 255;
	mov.u32 	%r4133, 64;
	prmt.b32 	%r4134, %r4132, %r4132, %r4133;
	mov.u32 	%r4135, 1040;
	prmt.b32 	%r4136, %r4134, %r4132, %r4135;
	mov.u32 	%r4137, 16912;
	prmt.b32 	%r484, %r4136, %r4132, %r4137;
	mov.u32 	%r11517, 0;

BB6_343:
	bfe.u32 	%r4138, %r42, 2, 2;
	and.b32  	%r4139, %r42, 3;
	shl.b32 	%r4140, %r4139, 3;
	shl.b32 	%r4142, %r4117, %r4140;
	setp.eq.s32	%p273, %r4138, 0;
	selp.b32	%r11653, %r4142, 0, %p273;
	setp.eq.s32	%p274, %r4138, 1;
	selp.b32	%r11654, %r4142, 0, %p274;
	setp.eq.s32	%p275, %r4138, 2;
	selp.b32	%r11655, %r4142, 0, %p275;
	setp.eq.s32	%p276, %r4138, 3;
	selp.b32	%r11656, %r4142, 0, %p276;
	shr.u32 	%r4143, %r42, 4;
	setp.eq.s32	%p277, %r4143, 0;
	selp.b32	%r4144, %r484, 0, %p277;
	and.b32  	%r4145, %r4144, %r11653;
	or.b32  	%r55, %r4145, %r55;
	and.b32  	%r4146, %r4144, %r11654;
	or.b32  	%r56, %r4146, %r56;
	and.b32  	%r4147, %r4144, %r11655;
	or.b32  	%r57, %r4147, %r57;
	and.b32  	%r4148, %r4144, %r11656;
	or.b32  	%r58, %r4148, %r58;
	setp.eq.s32	%p278, %r4143, 1;
	selp.b32	%r4149, %r484, 0, %p278;
	and.b32  	%r4150, %r4149, %r11653;
	or.b32  	%r51, %r4150, %r51;
	and.b32  	%r4151, %r4149, %r11654;
	or.b32  	%r52, %r4151, %r52;
	and.b32  	%r4152, %r4149, %r11655;
	or.b32  	%r53, %r4152, %r53;
	and.b32  	%r4153, %r4149, %r11656;
	or.b32  	%r54, %r4153, %r54;
	add.s32 	%r42, %r42, 1;
	add.s32 	%r11517, %r11517, 1;
	setp.lt.u32	%p279, %r11517, %r60;
	@%p279 bra 	BB6_343;

BB6_110:
	mov.u32 	%r11641, %r58;
	mov.u32 	%r11642, %r57;
	mov.u32 	%r11643, %r56;
	mov.u32 	%r11644, %r55;

BB6_111:
	mov.u32 	%r11645, %r54;
	mov.u32 	%r11646, %r53;
	mov.u32 	%r11647, %r52;
	mov.u32 	%r11648, %r51;

BB6_1029:
	mov.u32 	%r11657, %r42;

BB6_1030:
	add.s32 	%r11466, %r11466, 1;
	shl.b64 	%rd182, %rd85, 7;
	add.s64 	%rd183, %rd94, %rd182;
	mul.wide.u32 	%rd184, %r11466, 4;
	add.s64 	%rd185, %rd183, %rd184;
	ld.const.u32 	%r11465, [%rd185];
	setp.ne.s32	%p791, %r11465, 0;
	@%p791 bra 	BB6_4;

BB6_1031:
	and.b32  	%r9068, %r11657, 3;
	mov.u32 	%r9069, 4;
	sub.s32 	%r9070, %r9069, %r9068;
	shl.b32 	%r9071, %r9070, 2;
	mov.u32 	%r9072, 1985229328;
	shr.u32 	%r9073, %r9072, %r9071;
	and.b32  	%r1605, %r9073, 65535;
	shr.u32 	%r9067, %r11657, 2;
	setp.gt.s32	%p792, %r9067, 7;
	@%p792 bra 	BB6_1047;

	setp.gt.s32	%p804, %r9067, 3;
	@%p804 bra 	BB6_1040;

	setp.gt.s32	%p810, %r9067, 1;
	@%p810 bra 	BB6_1037;

	setp.eq.s32	%p813, %r9067, 0;
	@%p813 bra 	BB6_1072;
	bra.uni 	BB6_1035;

BB6_1072:
	// inline asm
	prmt.b32 %r11670, %r23, %r24, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11671, %r22, %r23, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11666, %r21, %r22, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11667, %r20, %r21, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11668, %r19, %r20, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11669, %r18, %r19, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11662, %r17, %r18, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11663, %r16, %r17, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11664, %r15, %r16, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11665, %r14, %r15, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11658, %r13, %r14, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11659, %r12, %r13, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11660, %r11, %r12, %r1605;
	// inline asm
	mov.u32 	%r9610, 0;
	// inline asm
	prmt.b32 %r11661, %r9610, %r11, %r1605;
	// inline asm
	bra.uni 	BB6_1073;

BB6_1047:
	setp.gt.s32	%p793, %r9067, 11;
	@%p793 bra 	BB6_1055;

	setp.gt.s32	%p799, %r9067, 9;
	@%p799 bra 	BB6_1052;

	setp.eq.s32	%p802, %r9067, 8;
	@%p802 bra 	BB6_1066;
	bra.uni 	BB6_1050;

BB6_1066:
	// inline asm
	prmt.b32 %r11670, %r15, %r16, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11671, %r14, %r15, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11666, %r13, %r14, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11667, %r12, %r13, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11668, %r11, %r12, %r1605;
	// inline asm
	mov.u32 	%r11658, 0;
	// inline asm
	prmt.b32 %r11669, %r11658, %r11, %r1605;
	// inline asm
	mov.u32 	%r11659, %r11658;
	mov.u32 	%r11660, %r11658;
	mov.u32 	%r11661, %r11658;
	mov.u32 	%r11662, %r11658;
	bra.uni 	BB6_1067;

BB6_1040:
	setp.gt.s32	%p805, %r9067, 5;
	@%p805 bra 	BB6_1044;

	setp.eq.s32	%p808, %r9067, 4;
	@%p808 bra 	BB6_1070;
	bra.uni 	BB6_1042;

BB6_1070:
	// inline asm
	prmt.b32 %r11670, %r19, %r20, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11671, %r18, %r19, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11666, %r17, %r18, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11667, %r16, %r17, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11668, %r15, %r16, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11669, %r14, %r15, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11662, %r13, %r14, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11663, %r12, %r13, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11664, %r11, %r12, %r1605;
	// inline asm
	mov.u32 	%r11658, 0;
	// inline asm
	prmt.b32 %r11665, %r11658, %r11, %r1605;
	// inline asm
	mov.u32 	%r11659, %r11658;
	mov.u32 	%r11660, %r11658;
	mov.u32 	%r11661, %r11658;
	bra.uni 	BB6_1073;

BB6_1055:
	setp.gt.s32	%p794, %r9067, 13;
	@%p794 bra 	BB6_1059;

	setp.eq.s32	%p797, %r9067, 12;
	@%p797 bra 	BB6_1062;
	bra.uni 	BB6_1057;

BB6_1062:
	// inline asm
	prmt.b32 %r11670, %r11, %r12, %r1605;
	// inline asm
	mov.u32 	%r11658, 0;
	// inline asm
	prmt.b32 %r11671, %r11658, %r11, %r1605;
	// inline asm
	mov.u32 	%r11659, %r11658;
	mov.u32 	%r11660, %r11658;
	mov.u32 	%r11661, %r11658;
	mov.u32 	%r11662, %r11658;
	mov.u32 	%r11663, %r11658;
	mov.u32 	%r11664, %r11658;
	mov.u32 	%r11665, %r11658;
	mov.u32 	%r11666, %r11658;
	bra.uni 	BB6_1063;

BB6_1037:
	setp.eq.s32	%p811, %r9067, 2;
	@%p811 bra 	BB6_1071;
	bra.uni 	BB6_1038;

BB6_1071:
	// inline asm
	prmt.b32 %r11670, %r21, %r22, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11671, %r20, %r21, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11666, %r19, %r20, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11667, %r18, %r19, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11668, %r17, %r18, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11669, %r16, %r17, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11662, %r15, %r16, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11663, %r14, %r15, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11664, %r13, %r14, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11665, %r12, %r13, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11658, %r11, %r12, %r1605;
	// inline asm
	mov.u32 	%r11660, 0;
	// inline asm
	prmt.b32 %r11659, %r11660, %r11, %r1605;
	// inline asm
	mov.u32 	%r11661, %r11660;
	bra.uni 	BB6_1073;

BB6_1052:
	setp.eq.s32	%p800, %r9067, 10;
	@%p800 bra 	BB6_1065;
	bra.uni 	BB6_1053;

BB6_1065:
	// inline asm
	prmt.b32 %r11670, %r13, %r14, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11671, %r12, %r13, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11666, %r11, %r12, %r1605;
	// inline asm
	mov.u32 	%r11658, 0;
	// inline asm
	prmt.b32 %r11667, %r11658, %r11, %r1605;
	// inline asm
	mov.u32 	%r11659, %r11658;
	mov.u32 	%r11660, %r11658;
	mov.u32 	%r11661, %r11658;
	mov.u32 	%r11662, %r11658;
	mov.u32 	%r11663, %r11658;
	mov.u32 	%r11664, %r11658;
	mov.u32 	%r11665, %r11658;
	bra.uni 	BB6_1064;

BB6_1044:
	setp.eq.s32	%p806, %r9067, 6;
	@%p806 bra 	BB6_1069;
	bra.uni 	BB6_1045;

BB6_1069:
	// inline asm
	prmt.b32 %r11670, %r17, %r18, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11671, %r16, %r17, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11666, %r15, %r16, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11667, %r14, %r15, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11668, %r13, %r14, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11669, %r12, %r13, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11662, %r11, %r12, %r1605;
	// inline asm
	mov.u32 	%r11658, 0;
	// inline asm
	prmt.b32 %r11663, %r11658, %r11, %r1605;
	// inline asm
	mov.u32 	%r11659, %r11658;
	mov.u32 	%r11660, %r11658;
	mov.u32 	%r11661, %r11658;
	bra.uni 	BB6_1068;

BB6_1059:
	setp.eq.s32	%p795, %r9067, 14;
	@%p795 bra 	BB6_1061;

	setp.ne.s32	%p796, %r9067, 15;
	mov.u32 	%r11658, %r14;
	mov.u32 	%r11659, %r13;
	mov.u32 	%r11660, %r12;
	mov.u32 	%r11661, %r11;
	mov.u32 	%r11662, %r18;
	mov.u32 	%r11663, %r17;
	mov.u32 	%r11664, %r16;
	mov.u32 	%r11665, %r15;
	mov.u32 	%r11666, %r22;
	mov.u32 	%r11667, %r21;
	mov.u32 	%r11668, %r20;
	mov.u32 	%r11669, %r19;
	mov.u32 	%r11670, %r24;
	mov.u32 	%r11671, %r23;
	@%p796 bra 	BB6_1073;

BB6_1061:
	mov.u32 	%r11658, 0;
	mov.u32 	%r11659, %r11658;
	mov.u32 	%r11660, %r11658;
	mov.u32 	%r11661, %r11658;
	mov.u32 	%r11662, %r11658;
	mov.u32 	%r11663, %r11658;
	mov.u32 	%r11664, %r11658;
	mov.u32 	%r11665, %r11658;
	mov.u32 	%r11666, %r11658;
	mov.u32 	%r11667, %r11658;
	mov.u32 	%r11668, %r11658;
	mov.u32 	%r11669, %r11658;
	mov.u32 	%r11670, %r11658;
	mov.u32 	%r11671, %r11658;
	bra.uni 	BB6_1073;

BB6_1035:
	setp.eq.s32	%p814, %r9067, 1;
	mov.u32 	%r11658, %r14;
	mov.u32 	%r11659, %r13;
	mov.u32 	%r11660, %r12;
	mov.u32 	%r11661, %r11;
	mov.u32 	%r11662, %r18;
	mov.u32 	%r11663, %r17;
	mov.u32 	%r11664, %r16;
	mov.u32 	%r11665, %r15;
	mov.u32 	%r11666, %r22;
	mov.u32 	%r11667, %r21;
	mov.u32 	%r11668, %r20;
	mov.u32 	%r11669, %r19;
	mov.u32 	%r11670, %r24;
	mov.u32 	%r11671, %r23;
	@%p814 bra 	BB6_1036;
	bra.uni 	BB6_1073;

BB6_1036:
	// inline asm
	prmt.b32 %r11670, %r22, %r23, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11671, %r21, %r22, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11666, %r20, %r21, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11667, %r19, %r20, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11668, %r18, %r19, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11669, %r17, %r18, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11662, %r16, %r17, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11663, %r15, %r16, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11664, %r14, %r15, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11665, %r13, %r14, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11658, %r12, %r13, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11659, %r11, %r12, %r1605;
	// inline asm
	mov.u32 	%r11661, 0;
	// inline asm
	prmt.b32 %r11660, %r11661, %r11, %r1605;
	// inline asm
	bra.uni 	BB6_1073;

BB6_1050:
	setp.eq.s32	%p803, %r9067, 9;
	mov.u32 	%r11658, %r14;
	mov.u32 	%r11659, %r13;
	mov.u32 	%r11660, %r12;
	mov.u32 	%r11661, %r11;
	mov.u32 	%r11662, %r18;
	mov.u32 	%r11663, %r17;
	mov.u32 	%r11664, %r16;
	mov.u32 	%r11665, %r15;
	mov.u32 	%r11666, %r22;
	mov.u32 	%r11667, %r21;
	mov.u32 	%r11668, %r20;
	mov.u32 	%r11669, %r19;
	mov.u32 	%r11670, %r24;
	mov.u32 	%r11671, %r23;
	@%p803 bra 	BB6_1051;
	bra.uni 	BB6_1073;

BB6_1051:
	// inline asm
	prmt.b32 %r11670, %r14, %r15, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11671, %r13, %r14, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11666, %r12, %r13, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11667, %r11, %r12, %r1605;
	// inline asm
	mov.u32 	%r11658, 0;
	// inline asm
	prmt.b32 %r11668, %r11658, %r11, %r1605;
	// inline asm
	mov.u32 	%r11659, %r11658;
	mov.u32 	%r11660, %r11658;
	mov.u32 	%r11661, %r11658;
	mov.u32 	%r11662, %r11658;
	mov.u32 	%r11663, %r11658;
	mov.u32 	%r11664, %r11658;
	mov.u32 	%r11665, %r11658;
	mov.u32 	%r11669, %r11658;
	bra.uni 	BB6_1073;

BB6_1042:
	setp.eq.s32	%p809, %r9067, 5;
	mov.u32 	%r11658, %r14;
	mov.u32 	%r11659, %r13;
	mov.u32 	%r11660, %r12;
	mov.u32 	%r11661, %r11;
	mov.u32 	%r11662, %r18;
	mov.u32 	%r11663, %r17;
	mov.u32 	%r11664, %r16;
	mov.u32 	%r11665, %r15;
	mov.u32 	%r11666, %r22;
	mov.u32 	%r11667, %r21;
	mov.u32 	%r11668, %r20;
	mov.u32 	%r11669, %r19;
	mov.u32 	%r11670, %r24;
	mov.u32 	%r11671, %r23;
	@%p809 bra 	BB6_1043;
	bra.uni 	BB6_1073;

BB6_1043:
	// inline asm
	prmt.b32 %r11670, %r18, %r19, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11671, %r17, %r18, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11666, %r16, %r17, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11667, %r15, %r16, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11668, %r14, %r15, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11669, %r13, %r14, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11662, %r12, %r13, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11663, %r11, %r12, %r1605;
	// inline asm
	mov.u32 	%r11658, 0;
	// inline asm
	prmt.b32 %r11664, %r11658, %r11, %r1605;
	// inline asm
	mov.u32 	%r11659, %r11658;
	mov.u32 	%r11660, %r11658;
	mov.u32 	%r11661, %r11658;
	mov.u32 	%r11665, %r11658;
	bra.uni 	BB6_1073;

BB6_1057:
	setp.eq.s32	%p798, %r9067, 13;
	mov.u32 	%r11658, %r14;
	mov.u32 	%r11659, %r13;
	mov.u32 	%r11660, %r12;
	mov.u32 	%r11661, %r11;
	mov.u32 	%r11662, %r18;
	mov.u32 	%r11663, %r17;
	mov.u32 	%r11664, %r16;
	mov.u32 	%r11665, %r15;
	mov.u32 	%r11666, %r22;
	mov.u32 	%r11667, %r21;
	mov.u32 	%r11668, %r20;
	mov.u32 	%r11669, %r19;
	mov.u32 	%r11670, %r24;
	mov.u32 	%r11671, %r23;
	@%p798 bra 	BB6_1058;
	bra.uni 	BB6_1073;

BB6_1058:
	mov.u32 	%r11658, 0;
	// inline asm
	prmt.b32 %r11670, %r11658, %r11, %r1605;
	// inline asm
	mov.u32 	%r11659, %r11658;
	mov.u32 	%r11660, %r11658;
	mov.u32 	%r11661, %r11658;
	mov.u32 	%r11662, %r11658;
	mov.u32 	%r11663, %r11658;
	mov.u32 	%r11664, %r11658;
	mov.u32 	%r11665, %r11658;
	mov.u32 	%r11666, %r11658;
	mov.u32 	%r11667, %r11658;
	mov.u32 	%r11668, %r11658;
	mov.u32 	%r11669, %r11658;
	mov.u32 	%r11671, %r11658;
	bra.uni 	BB6_1073;

BB6_1038:
	setp.eq.s32	%p812, %r9067, 3;
	mov.u32 	%r11658, %r14;
	mov.u32 	%r11659, %r13;
	mov.u32 	%r11660, %r12;
	mov.u32 	%r11661, %r11;
	mov.u32 	%r11662, %r18;
	mov.u32 	%r11663, %r17;
	mov.u32 	%r11664, %r16;
	mov.u32 	%r11665, %r15;
	mov.u32 	%r11666, %r22;
	mov.u32 	%r11667, %r21;
	mov.u32 	%r11668, %r20;
	mov.u32 	%r11669, %r19;
	mov.u32 	%r11670, %r24;
	mov.u32 	%r11671, %r23;
	@%p812 bra 	BB6_1039;
	bra.uni 	BB6_1073;

BB6_1039:
	// inline asm
	prmt.b32 %r11670, %r20, %r21, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11671, %r19, %r20, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11666, %r18, %r19, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11667, %r17, %r18, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11668, %r16, %r17, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11669, %r15, %r16, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11662, %r14, %r15, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11663, %r13, %r14, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11664, %r12, %r13, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11665, %r11, %r12, %r1605;
	// inline asm
	mov.u32 	%r11659, 0;
	// inline asm
	prmt.b32 %r11658, %r11659, %r11, %r1605;
	// inline asm
	mov.u32 	%r11660, %r11659;
	mov.u32 	%r11661, %r11659;
	bra.uni 	BB6_1073;

BB6_1053:
	setp.eq.s32	%p801, %r9067, 11;
	mov.u32 	%r11658, %r14;
	mov.u32 	%r11659, %r13;
	mov.u32 	%r11660, %r12;
	mov.u32 	%r11661, %r11;
	mov.u32 	%r11662, %r18;
	mov.u32 	%r11663, %r17;
	mov.u32 	%r11664, %r16;
	mov.u32 	%r11665, %r15;
	mov.u32 	%r11666, %r22;
	mov.u32 	%r11667, %r21;
	mov.u32 	%r11668, %r20;
	mov.u32 	%r11669, %r19;
	mov.u32 	%r11670, %r24;
	mov.u32 	%r11671, %r23;
	@%p801 bra 	BB6_1054;
	bra.uni 	BB6_1073;

BB6_1054:
	// inline asm
	prmt.b32 %r11670, %r12, %r13, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11671, %r11, %r12, %r1605;
	// inline asm
	mov.u32 	%r11658, 0;
	// inline asm
	prmt.b32 %r11666, %r11658, %r11, %r1605;
	// inline asm
	mov.u32 	%r11659, %r11658;
	mov.u32 	%r11660, %r11658;
	mov.u32 	%r11661, %r11658;
	mov.u32 	%r11662, %r11658;
	mov.u32 	%r11663, %r11658;
	mov.u32 	%r11664, %r11658;
	mov.u32 	%r11665, %r11658;

BB6_1063:
	mov.u32 	%r11667, %r11658;

BB6_1064:
	mov.u32 	%r11668, %r11658;
	mov.u32 	%r11669, %r11658;
	bra.uni 	BB6_1073;

BB6_1045:
	setp.eq.s32	%p807, %r9067, 7;
	mov.u32 	%r11658, %r14;
	mov.u32 	%r11659, %r13;
	mov.u32 	%r11660, %r12;
	mov.u32 	%r11661, %r11;
	mov.u32 	%r11662, %r18;
	mov.u32 	%r11663, %r17;
	mov.u32 	%r11664, %r16;
	mov.u32 	%r11665, %r15;
	mov.u32 	%r11666, %r22;
	mov.u32 	%r11667, %r21;
	mov.u32 	%r11668, %r20;
	mov.u32 	%r11669, %r19;
	mov.u32 	%r11670, %r24;
	mov.u32 	%r11671, %r23;
	@%p807 bra 	BB6_1046;
	bra.uni 	BB6_1073;

BB6_1046:
	// inline asm
	prmt.b32 %r11670, %r16, %r17, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11671, %r15, %r16, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11666, %r14, %r15, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11667, %r13, %r14, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11668, %r12, %r13, %r1605;
	// inline asm
	// inline asm
	prmt.b32 %r11669, %r11, %r12, %r1605;
	// inline asm
	mov.u32 	%r11658, 0;
	// inline asm
	prmt.b32 %r11662, %r11658, %r11, %r1605;
	// inline asm
	mov.u32 	%r11659, %r11658;
	mov.u32 	%r11660, %r11658;
	mov.u32 	%r11661, %r11658;

BB6_1067:
	mov.u32 	%r11663, %r11658;

BB6_1068:
	mov.u32 	%r11664, %r11658;
	mov.u32 	%r11665, %r11658;

BB6_1073:
	ld.const.u64 	%rd2829, [k_sha512+608];
	ld.const.u64 	%rd2828, [k_sha512+600];
	ld.const.u64 	%rd2827, [k_sha512+592];
	ld.const.u64 	%rd2826, [k_sha512+584];
	ld.const.u64 	%rd2825, [k_sha512+576];
	ld.const.u64 	%rd2824, [k_sha512+568];
	ld.const.u64 	%rd2823, [k_sha512+560];
	ld.const.u64 	%rd2822, [k_sha512+552];
	ld.const.u64 	%rd2821, [k_sha512+544];
	ld.const.u64 	%rd2820, [k_sha512+536];
	ld.const.u64 	%rd2819, [k_sha512+528];
	ld.const.u64 	%rd2818, [k_sha512+520];
	ld.const.u64 	%rd2817, [k_sha512+512];
	ld.const.u64 	%rd2816, [k_sha512+504];
	ld.const.u64 	%rd2815, [k_sha512+496];
	ld.const.u64 	%rd2814, [k_sha512+488];
	ld.const.u64 	%rd2813, [k_sha512+480];
	ld.const.u64 	%rd2812, [k_sha512+472];
	ld.const.u64 	%rd2811, [k_sha512+464];
	ld.const.u64 	%rd2810, [k_sha512+456];
	ld.const.u64 	%rd2809, [k_sha512+448];
	ld.const.u64 	%rd2808, [k_sha512+440];
	ld.const.u64 	%rd2807, [k_sha512+432];
	ld.const.u64 	%rd2806, [k_sha512+424];
	ld.const.u64 	%rd2805, [k_sha512+416];
	ld.const.u64 	%rd2804, [k_sha512+408];
	ld.const.u64 	%rd2803, [k_sha512+400];
	ld.const.u64 	%rd2802, [k_sha512+392];
	ld.const.u64 	%rd2801, [k_sha512+384];
	ld.const.u64 	%rd2800, [k_sha512+376];
	ld.const.u64 	%rd2799, [k_sha512+368];
	ld.const.u64 	%rd2798, [k_sha512+360];
	ld.const.u64 	%rd2797, [k_sha512+352];
	ld.const.u64 	%rd2796, [k_sha512+344];
	ld.const.u64 	%rd2795, [k_sha512+336];
	ld.const.u64 	%rd2794, [k_sha512+328];
	ld.const.u64 	%rd2793, [k_sha512+320];
	ld.const.u64 	%rd2792, [k_sha512+312];
	ld.const.u64 	%rd2791, [k_sha512+304];
	ld.const.u64 	%rd2790, [k_sha512+296];
	ld.const.u64 	%rd2789, [k_sha512+288];
	ld.const.u64 	%rd2788, [k_sha512+280];
	ld.const.u64 	%rd2787, [k_sha512+272];
	ld.const.u64 	%rd2786, [k_sha512+264];
	ld.const.u64 	%rd2785, [k_sha512+256];
	ld.const.u64 	%rd2784, [k_sha512+248];
	ld.const.u64 	%rd2783, [k_sha512+240];
	ld.const.u64 	%rd2782, [k_sha512+232];
	ld.const.u64 	%rd2781, [k_sha512+224];
	ld.const.u64 	%rd2780, [k_sha512+216];
	ld.const.u64 	%rd2779, [k_sha512+208];
	ld.const.u64 	%rd2778, [k_sha512+200];
	ld.const.u64 	%rd2777, [k_sha512+192];
	ld.const.u64 	%rd2776, [k_sha512+184];
	ld.const.u64 	%rd2775, [k_sha512+176];
	ld.const.u64 	%rd2774, [k_sha512+168];
	ld.const.u64 	%rd2773, [k_sha512+160];
	ld.const.u64 	%rd2772, [k_sha512+152];
	ld.const.u64 	%rd2771, [k_sha512+144];
	ld.const.u64 	%rd2770, [k_sha512+136];
	ld.const.u64 	%rd2769, [k_sha512+128];
	ld.const.u64 	%rd2768, [k_sha512+120];
	ld.const.u64 	%rd2767, [k_sha512+112];
	ld.const.u64 	%rd2766, [k_sha512+104];
	ld.const.u64 	%rd2765, [k_sha512+96];
	ld.const.u64 	%rd2764, [k_sha512+88];
	ld.const.u64 	%rd2763, [k_sha512+80];
	ld.const.u64 	%rd2762, [k_sha512+72];
	ld.const.u64 	%rd2761, [k_sha512+64];
	ld.const.u64 	%rd2760, [k_sha512+56];
	ld.const.u64 	%rd2759, [k_sha512+48];
	ld.const.u64 	%rd2758, [k_sha512+40];
	ld.const.u64 	%rd2757, [k_sha512+32];
	ld.const.u64 	%rd2756, [k_sha512+24];
	ld.const.u64 	%rd2755, [k_sha512+16];
	ld.const.u64 	%rd2754, [k_sha512+8];
	ld.const.u64 	%rd2753, [k_sha512];
	or.b32  	%r9614, %r11661, %r11644;
	or.b32  	%r9616, %r11660, %r11643;
	or.b32  	%r9618, %r11659, %r11642;
	or.b32  	%r9620, %r11658, %r11641;
	or.b32  	%r9622, %r11665, %r11648;
	or.b32  	%r9624, %r11664, %r11647;
	or.b32  	%r9626, %r11663, %r11646;
	or.b32  	%r9628, %r11662, %r11645;
	add.s32 	%r9641, %r11657, %r25;
	// inline asm
	prmt.b32 %r9613, %r9614, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r9615, %r9616, 0, 0x0123;
	// inline asm
	mov.b64	%rd186, {%r9615, %r9613};
	// inline asm
	prmt.b32 %r9617, %r9618, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r9619, %r9620, 0, 0x0123;
	// inline asm
	mov.b64	%rd187, {%r9619, %r9617};
	// inline asm
	prmt.b32 %r9621, %r9622, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r9623, %r9624, 0, 0x0123;
	// inline asm
	mov.b64	%rd188, {%r9623, %r9621};
	// inline asm
	prmt.b32 %r9625, %r9626, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r9627, %r9628, 0, 0x0123;
	// inline asm
	mov.b64	%rd189, {%r9627, %r9625};
	// inline asm
	prmt.b32 %r9629, %r11669, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r9631, %r11668, 0, 0x0123;
	// inline asm
	mov.b64	%rd190, {%r9631, %r9629};
	// inline asm
	prmt.b32 %r9633, %r11667, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r9635, %r11666, 0, 0x0123;
	// inline asm
	mov.b64	%rd191, {%r9635, %r9633};
	// inline asm
	prmt.b32 %r9637, %r11671, 0, 0x0123;
	// inline asm
	// inline asm
	prmt.b32 %r9639, %r11670, 0, 0x0123;
	// inline asm
	mov.b64	%rd192, {%r9639, %r9637};
	shl.b32 	%r9642, %r9641, 3;
	mov.u32 	%r9643, 0;
	mov.b64	%rd193, {%r9642, %r9643};
	add.s64 	%rd194, %rd186, %rd2753;
	add.s64 	%rd195, %rd194, %rd4;
	add.s64 	%rd196, %rd195, %rd5;
	add.s64 	%rd197, %rd195, 2357225248857953701;
	add.s64 	%rd198, %rd196, -5343946410804754465;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9644,%dummy}, %rd197;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9645}, %rd197;
	}
	shf.r.wrap.b32 	%r9646, %r9645, %r9644, 14;
	shf.r.wrap.b32 	%r9647, %r9644, %r9645, 14;
	mov.b64 	%rd199, {%r9647, %r9646};
	shf.r.wrap.b32 	%r9648, %r9645, %r9644, 18;
	shf.r.wrap.b32 	%r9649, %r9644, %r9645, 18;
	mov.b64 	%rd200, {%r9649, %r9648};
	xor.b64  	%rd201, %rd200, %rd199;
	shf.l.wrap.b32 	%r9650, %r9644, %r9645, 23;
	shf.l.wrap.b32 	%r9651, %r9645, %r9644, 23;
	mov.b64 	%rd202, {%r9651, %r9650};
	xor.b64  	%rd203, %rd201, %rd202;
	and.b64  	%rd204, %rd197, -3887949035690463538;
	xor.b64  	%rd205, %rd204, -7276294671716946913;
	add.s64 	%rd206, %rd187, %rd2754;
	add.s64 	%rd207, %rd206, %rd205;
	add.s64 	%rd208, %rd207, %rd203;
	xor.b64  	%rd209, %rd198, -4942790177534073029;
	xor.b64  	%rd210, %rd198, 7640891576956012808;
	and.b64  	%rd211, %rd210, %rd209;
	xor.b64  	%rd212, %rd211, %rd198;
	add.s64 	%rd213, %rd208, %rd212;
	add.s64 	%rd214, %rd208, 6625583534739731862;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9652,%dummy}, %rd198;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9653}, %rd198;
	}
	shf.r.wrap.b32 	%r9654, %r9653, %r9652, 28;
	shf.r.wrap.b32 	%r9655, %r9652, %r9653, 28;
	mov.b64 	%rd215, {%r9655, %r9654};
	shf.l.wrap.b32 	%r9656, %r9652, %r9653, 30;
	shf.l.wrap.b32 	%r9657, %r9653, %r9652, 30;
	mov.b64 	%rd216, {%r9657, %r9656};
	xor.b64  	%rd217, %rd216, %rd215;
	shf.l.wrap.b32 	%r9658, %r9652, %r9653, 25;
	shf.l.wrap.b32 	%r9659, %r9653, %r9652, 25;
	mov.b64 	%rd218, {%r9659, %r9658};
	xor.b64  	%rd219, %rd217, %rd218;
	add.s64 	%rd220, %rd213, %rd219;
	add.s64 	%rd221, %rd220, 2270897969802886507;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9660,%dummy}, %rd214;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9661}, %rd214;
	}
	shf.r.wrap.b32 	%r9662, %r9661, %r9660, 14;
	shf.r.wrap.b32 	%r9663, %r9660, %r9661, 14;
	mov.b64 	%rd222, {%r9663, %r9662};
	shf.r.wrap.b32 	%r9664, %r9661, %r9660, 18;
	shf.r.wrap.b32 	%r9665, %r9660, %r9661, 18;
	mov.b64 	%rd223, {%r9665, %r9664};
	xor.b64  	%rd224, %rd223, %rd222;
	shf.l.wrap.b32 	%r9666, %r9660, %r9661, 23;
	shf.l.wrap.b32 	%r9667, %r9661, %r9660, 23;
	mov.b64 	%rd225, {%r9667, %r9666};
	xor.b64  	%rd226, %rd224, %rd225;
	xor.b64  	%rd227, %rd197, 5840696475078001361;
	and.b64  	%rd228, %rd214, %rd227;
	xor.b64  	%rd229, %rd228, 5840696475078001361;
	add.s64 	%rd230, %rd188, %rd2755;
	add.s64 	%rd231, %rd230, %rd229;
	add.s64 	%rd232, %rd231, %rd226;
	xor.b64  	%rd233, %rd221, 7640891576956012808;
	xor.b64  	%rd234, %rd221, %rd198;
	and.b64  	%rd235, %rd234, %rd233;
	xor.b64  	%rd236, %rd235, %rd221;
	add.s64 	%rd237, %rd232, %rd236;
	add.s64 	%rd238, %rd232, 6227659224458531674;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9668,%dummy}, %rd221;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9669}, %rd221;
	}
	shf.r.wrap.b32 	%r9670, %r9669, %r9668, 28;
	shf.r.wrap.b32 	%r9671, %r9668, %r9669, 28;
	mov.b64 	%rd239, {%r9671, %r9670};
	shf.l.wrap.b32 	%r9672, %r9668, %r9669, 30;
	shf.l.wrap.b32 	%r9673, %r9669, %r9668, 30;
	mov.b64 	%rd240, {%r9673, %r9672};
	xor.b64  	%rd241, %rd240, %rd239;
	shf.l.wrap.b32 	%r9674, %r9668, %r9669, 25;
	shf.l.wrap.b32 	%r9675, %r9669, %r9668, 25;
	mov.b64 	%rd242, {%r9675, %r9674};
	xor.b64  	%rd243, %rd241, %rd242;
	add.s64 	%rd244, %rd237, %rd243;
	add.s64 	%rd245, %rd244, -7276294671716946913;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9676,%dummy}, %rd238;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9677}, %rd238;
	}
	shf.r.wrap.b32 	%r9678, %r9677, %r9676, 14;
	shf.r.wrap.b32 	%r9679, %r9676, %r9677, 14;
	mov.b64 	%rd246, {%r9679, %r9678};
	shf.r.wrap.b32 	%r9680, %r9677, %r9676, 18;
	shf.r.wrap.b32 	%r9681, %r9676, %r9677, 18;
	mov.b64 	%rd247, {%r9681, %r9680};
	xor.b64  	%rd248, %rd247, %rd246;
	shf.l.wrap.b32 	%r9682, %r9676, %r9677, 23;
	shf.l.wrap.b32 	%r9683, %r9677, %r9676, 23;
	mov.b64 	%rd249, {%r9683, %r9682};
	xor.b64  	%rd250, %rd248, %rd249;
	xor.b64  	%rd251, %rd214, %rd197;
	and.b64  	%rd252, %rd238, %rd251;
	xor.b64  	%rd253, %rd252, %rd197;
	add.s64 	%rd254, %rd189, %rd2756;
	add.s64 	%rd255, %rd254, %rd253;
	add.s64 	%rd256, %rd255, %rd250;
	xor.b64  	%rd257, %rd245, %rd198;
	xor.b64  	%rd258, %rd245, %rd221;
	and.b64  	%rd259, %rd258, %rd257;
	xor.b64  	%rd260, %rd259, %rd245;
	add.s64 	%rd261, %rd256, %rd260;
	add.s64 	%rd262, %rd256, -4965156021675537447;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9684,%dummy}, %rd245;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9685}, %rd245;
	}
	shf.r.wrap.b32 	%r9686, %r9685, %r9684, 28;
	shf.r.wrap.b32 	%r9687, %r9684, %r9685, 28;
	mov.b64 	%rd263, {%r9687, %r9686};
	shf.l.wrap.b32 	%r9688, %r9684, %r9685, 30;
	shf.l.wrap.b32 	%r9689, %r9685, %r9684, 30;
	mov.b64 	%rd264, {%r9689, %r9688};
	xor.b64  	%rd265, %rd264, %rd263;
	shf.l.wrap.b32 	%r9690, %r9684, %r9685, 25;
	shf.l.wrap.b32 	%r9691, %r9685, %r9684, 25;
	mov.b64 	%rd266, {%r9691, %r9690};
	xor.b64  	%rd267, %rd265, %rd266;
	add.s64 	%rd268, %rd261, %rd267;
	add.s64 	%rd269, %rd268, 5840696475078001361;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9692,%dummy}, %rd262;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9693}, %rd262;
	}
	shf.r.wrap.b32 	%r9694, %r9693, %r9692, 14;
	shf.r.wrap.b32 	%r9695, %r9692, %r9693, 14;
	mov.b64 	%rd270, {%r9695, %r9694};
	shf.r.wrap.b32 	%r9696, %r9693, %r9692, 18;
	shf.r.wrap.b32 	%r9697, %r9692, %r9693, 18;
	mov.b64 	%rd271, {%r9697, %r9696};
	xor.b64  	%rd272, %rd271, %rd270;
	shf.l.wrap.b32 	%r9698, %r9692, %r9693, 23;
	shf.l.wrap.b32 	%r9699, %r9693, %r9692, 23;
	mov.b64 	%rd273, {%r9699, %r9698};
	xor.b64  	%rd274, %rd272, %rd273;
	xor.b64  	%rd275, %rd238, %rd214;
	and.b64  	%rd276, %rd262, %rd275;
	xor.b64  	%rd277, %rd276, %rd214;
	add.s64 	%rd278, %rd197, %rd190;
	add.s64 	%rd279, %rd278, %rd2757;
	add.s64 	%rd280, %rd279, %rd277;
	add.s64 	%rd281, %rd280, %rd274;
	add.s64 	%rd282, %rd281, %rd198;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9700,%dummy}, %rd269;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9701}, %rd269;
	}
	shf.r.wrap.b32 	%r9702, %r9701, %r9700, 28;
	shf.r.wrap.b32 	%r9703, %r9700, %r9701, 28;
	mov.b64 	%rd283, {%r9703, %r9702};
	shf.l.wrap.b32 	%r9704, %r9700, %r9701, 30;
	shf.l.wrap.b32 	%r9705, %r9701, %r9700, 30;
	mov.b64 	%rd284, {%r9705, %r9704};
	xor.b64  	%rd285, %rd284, %rd283;
	shf.l.wrap.b32 	%r9706, %r9700, %r9701, 25;
	shf.l.wrap.b32 	%r9707, %r9701, %r9700, 25;
	mov.b64 	%rd286, {%r9707, %r9706};
	xor.b64  	%rd287, %rd285, %rd286;
	xor.b64  	%rd288, %rd269, %rd221;
	xor.b64  	%rd289, %rd269, %rd245;
	and.b64  	%rd290, %rd289, %rd288;
	xor.b64  	%rd291, %rd290, %rd269;
	add.s64 	%rd292, %rd281, %rd291;
	add.s64 	%rd293, %rd292, %rd287;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9708,%dummy}, %rd282;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9709}, %rd282;
	}
	shf.r.wrap.b32 	%r9710, %r9709, %r9708, 14;
	shf.r.wrap.b32 	%r9711, %r9708, %r9709, 14;
	mov.b64 	%rd294, {%r9711, %r9710};
	shf.r.wrap.b32 	%r9712, %r9709, %r9708, 18;
	shf.r.wrap.b32 	%r9713, %r9708, %r9709, 18;
	mov.b64 	%rd295, {%r9713, %r9712};
	xor.b64  	%rd296, %rd295, %rd294;
	shf.l.wrap.b32 	%r9714, %r9708, %r9709, 23;
	shf.l.wrap.b32 	%r9715, %r9709, %r9708, 23;
	mov.b64 	%rd297, {%r9715, %r9714};
	xor.b64  	%rd298, %rd296, %rd297;
	xor.b64  	%rd299, %rd262, %rd238;
	and.b64  	%rd300, %rd282, %rd299;
	xor.b64  	%rd301, %rd300, %rd238;
	add.s64 	%rd302, %rd214, %rd191;
	add.s64 	%rd303, %rd302, %rd2758;
	add.s64 	%rd304, %rd303, %rd301;
	add.s64 	%rd305, %rd304, %rd298;
	add.s64 	%rd306, %rd305, %rd221;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9716,%dummy}, %rd293;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9717}, %rd293;
	}
	shf.r.wrap.b32 	%r9718, %r9717, %r9716, 28;
	shf.r.wrap.b32 	%r9719, %r9716, %r9717, 28;
	mov.b64 	%rd307, {%r9719, %r9718};
	shf.l.wrap.b32 	%r9720, %r9716, %r9717, 30;
	shf.l.wrap.b32 	%r9721, %r9717, %r9716, 30;
	mov.b64 	%rd308, {%r9721, %r9720};
	xor.b64  	%rd309, %rd308, %rd307;
	shf.l.wrap.b32 	%r9722, %r9716, %r9717, 25;
	shf.l.wrap.b32 	%r9723, %r9717, %r9716, 25;
	mov.b64 	%rd310, {%r9723, %r9722};
	xor.b64  	%rd311, %rd309, %rd310;
	xor.b64  	%rd312, %rd293, %rd245;
	xor.b64  	%rd313, %rd293, %rd269;
	and.b64  	%rd314, %rd313, %rd312;
	xor.b64  	%rd315, %rd314, %rd293;
	add.s64 	%rd316, %rd305, %rd315;
	add.s64 	%rd317, %rd316, %rd311;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9724,%dummy}, %rd306;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9725}, %rd306;
	}
	shf.r.wrap.b32 	%r9726, %r9725, %r9724, 14;
	shf.r.wrap.b32 	%r9727, %r9724, %r9725, 14;
	mov.b64 	%rd318, {%r9727, %r9726};
	shf.r.wrap.b32 	%r9728, %r9725, %r9724, 18;
	shf.r.wrap.b32 	%r9729, %r9724, %r9725, 18;
	mov.b64 	%rd319, {%r9729, %r9728};
	xor.b64  	%rd320, %rd319, %rd318;
	shf.l.wrap.b32 	%r9730, %r9724, %r9725, 23;
	shf.l.wrap.b32 	%r9731, %r9725, %r9724, 23;
	mov.b64 	%rd321, {%r9731, %r9730};
	xor.b64  	%rd322, %rd320, %rd321;
	xor.b64  	%rd323, %rd282, %rd262;
	and.b64  	%rd324, %rd306, %rd323;
	xor.b64  	%rd325, %rd324, %rd262;
	add.s64 	%rd326, %rd238, %rd192;
	add.s64 	%rd327, %rd326, %rd2759;
	add.s64 	%rd328, %rd327, %rd325;
	add.s64 	%rd329, %rd328, %rd322;
	add.s64 	%rd330, %rd329, %rd245;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9732,%dummy}, %rd317;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9733}, %rd317;
	}
	shf.r.wrap.b32 	%r9734, %r9733, %r9732, 28;
	shf.r.wrap.b32 	%r9735, %r9732, %r9733, 28;
	mov.b64 	%rd331, {%r9735, %r9734};
	shf.l.wrap.b32 	%r9736, %r9732, %r9733, 30;
	shf.l.wrap.b32 	%r9737, %r9733, %r9732, 30;
	mov.b64 	%rd332, {%r9737, %r9736};
	xor.b64  	%rd333, %rd332, %rd331;
	shf.l.wrap.b32 	%r9738, %r9732, %r9733, 25;
	shf.l.wrap.b32 	%r9739, %r9733, %r9732, 25;
	mov.b64 	%rd334, {%r9739, %r9738};
	xor.b64  	%rd335, %rd333, %rd334;
	xor.b64  	%rd336, %rd317, %rd269;
	xor.b64  	%rd337, %rd317, %rd293;
	and.b64  	%rd338, %rd337, %rd336;
	xor.b64  	%rd339, %rd338, %rd317;
	add.s64 	%rd340, %rd329, %rd339;
	add.s64 	%rd341, %rd340, %rd335;
	add.s64 	%rd342, %rd2760, %rd262;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9740,%dummy}, %rd330;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9741}, %rd330;
	}
	shf.r.wrap.b32 	%r9742, %r9741, %r9740, 14;
	shf.r.wrap.b32 	%r9743, %r9740, %r9741, 14;
	mov.b64 	%rd343, {%r9743, %r9742};
	shf.r.wrap.b32 	%r9744, %r9741, %r9740, 18;
	shf.r.wrap.b32 	%r9745, %r9740, %r9741, 18;
	mov.b64 	%rd344, {%r9745, %r9744};
	xor.b64  	%rd345, %rd344, %rd343;
	shf.l.wrap.b32 	%r9746, %r9740, %r9741, 23;
	shf.l.wrap.b32 	%r9747, %r9741, %r9740, 23;
	mov.b64 	%rd346, {%r9747, %r9746};
	xor.b64  	%rd347, %rd345, %rd346;
	xor.b64  	%rd348, %rd306, %rd282;
	and.b64  	%rd349, %rd330, %rd348;
	xor.b64  	%rd350, %rd349, %rd282;
	add.s64 	%rd351, %rd342, %rd350;
	add.s64 	%rd352, %rd351, %rd347;
	add.s64 	%rd353, %rd352, %rd269;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9748,%dummy}, %rd341;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9749}, %rd341;
	}
	shf.r.wrap.b32 	%r9750, %r9749, %r9748, 28;
	shf.r.wrap.b32 	%r9751, %r9748, %r9749, 28;
	mov.b64 	%rd354, {%r9751, %r9750};
	shf.l.wrap.b32 	%r9752, %r9748, %r9749, 30;
	shf.l.wrap.b32 	%r9753, %r9749, %r9748, 30;
	mov.b64 	%rd355, {%r9753, %r9752};
	xor.b64  	%rd356, %rd355, %rd354;
	shf.l.wrap.b32 	%r9754, %r9748, %r9749, 25;
	shf.l.wrap.b32 	%r9755, %r9749, %r9748, 25;
	mov.b64 	%rd357, {%r9755, %r9754};
	xor.b64  	%rd358, %rd356, %rd357;
	xor.b64  	%rd359, %rd341, %rd293;
	xor.b64  	%rd360, %rd341, %rd317;
	and.b64  	%rd361, %rd360, %rd359;
	xor.b64  	%rd362, %rd361, %rd341;
	add.s64 	%rd363, %rd352, %rd362;
	add.s64 	%rd364, %rd363, %rd358;
	add.s64 	%rd365, %rd2761, %rd282;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9756,%dummy}, %rd353;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9757}, %rd353;
	}
	shf.r.wrap.b32 	%r9758, %r9757, %r9756, 14;
	shf.r.wrap.b32 	%r9759, %r9756, %r9757, 14;
	mov.b64 	%rd366, {%r9759, %r9758};
	shf.r.wrap.b32 	%r9760, %r9757, %r9756, 18;
	shf.r.wrap.b32 	%r9761, %r9756, %r9757, 18;
	mov.b64 	%rd367, {%r9761, %r9760};
	xor.b64  	%rd368, %rd367, %rd366;
	shf.l.wrap.b32 	%r9762, %r9756, %r9757, 23;
	shf.l.wrap.b32 	%r9763, %r9757, %r9756, 23;
	mov.b64 	%rd369, {%r9763, %r9762};
	xor.b64  	%rd370, %rd368, %rd369;
	xor.b64  	%rd371, %rd330, %rd306;
	and.b64  	%rd372, %rd353, %rd371;
	xor.b64  	%rd373, %rd372, %rd306;
	add.s64 	%rd374, %rd365, %rd373;
	add.s64 	%rd375, %rd374, %rd370;
	add.s64 	%rd376, %rd375, %rd293;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9764,%dummy}, %rd364;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9765}, %rd364;
	}
	shf.r.wrap.b32 	%r9766, %r9765, %r9764, 28;
	shf.r.wrap.b32 	%r9767, %r9764, %r9765, 28;
	mov.b64 	%rd377, {%r9767, %r9766};
	shf.l.wrap.b32 	%r9768, %r9764, %r9765, 30;
	shf.l.wrap.b32 	%r9769, %r9765, %r9764, 30;
	mov.b64 	%rd378, {%r9769, %r9768};
	xor.b64  	%rd379, %rd378, %rd377;
	shf.l.wrap.b32 	%r9770, %r9764, %r9765, 25;
	shf.l.wrap.b32 	%r9771, %r9765, %r9764, 25;
	mov.b64 	%rd380, {%r9771, %r9770};
	xor.b64  	%rd381, %rd379, %rd380;
	xor.b64  	%rd382, %rd364, %rd317;
	xor.b64  	%rd383, %rd364, %rd341;
	and.b64  	%rd384, %rd383, %rd382;
	xor.b64  	%rd385, %rd384, %rd364;
	add.s64 	%rd386, %rd375, %rd385;
	add.s64 	%rd387, %rd386, %rd381;
	add.s64 	%rd388, %rd2762, %rd306;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9772,%dummy}, %rd376;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9773}, %rd376;
	}
	shf.r.wrap.b32 	%r9774, %r9773, %r9772, 14;
	shf.r.wrap.b32 	%r9775, %r9772, %r9773, 14;
	mov.b64 	%rd389, {%r9775, %r9774};
	shf.r.wrap.b32 	%r9776, %r9773, %r9772, 18;
	shf.r.wrap.b32 	%r9777, %r9772, %r9773, 18;
	mov.b64 	%rd390, {%r9777, %r9776};
	xor.b64  	%rd391, %rd390, %rd389;
	shf.l.wrap.b32 	%r9778, %r9772, %r9773, 23;
	shf.l.wrap.b32 	%r9779, %r9773, %r9772, 23;
	mov.b64 	%rd392, {%r9779, %r9778};
	xor.b64  	%rd393, %rd391, %rd392;
	xor.b64  	%rd394, %rd353, %rd330;
	and.b64  	%rd395, %rd376, %rd394;
	xor.b64  	%rd396, %rd395, %rd330;
	add.s64 	%rd397, %rd388, %rd396;
	add.s64 	%rd398, %rd397, %rd393;
	add.s64 	%rd399, %rd398, %rd317;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9780,%dummy}, %rd387;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9781}, %rd387;
	}
	shf.r.wrap.b32 	%r9782, %r9781, %r9780, 28;
	shf.r.wrap.b32 	%r9783, %r9780, %r9781, 28;
	mov.b64 	%rd400, {%r9783, %r9782};
	shf.l.wrap.b32 	%r9784, %r9780, %r9781, 30;
	shf.l.wrap.b32 	%r9785, %r9781, %r9780, 30;
	mov.b64 	%rd401, {%r9785, %r9784};
	xor.b64  	%rd402, %rd401, %rd400;
	shf.l.wrap.b32 	%r9786, %r9780, %r9781, 25;
	shf.l.wrap.b32 	%r9787, %r9781, %r9780, 25;
	mov.b64 	%rd403, {%r9787, %r9786};
	xor.b64  	%rd404, %rd402, %rd403;
	xor.b64  	%rd405, %rd387, %rd341;
	xor.b64  	%rd406, %rd387, %rd364;
	and.b64  	%rd407, %rd406, %rd405;
	xor.b64  	%rd408, %rd407, %rd387;
	add.s64 	%rd409, %rd398, %rd408;
	add.s64 	%rd410, %rd409, %rd404;
	add.s64 	%rd411, %rd2763, %rd330;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9788,%dummy}, %rd399;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9789}, %rd399;
	}
	shf.r.wrap.b32 	%r9790, %r9789, %r9788, 14;
	shf.r.wrap.b32 	%r9791, %r9788, %r9789, 14;
	mov.b64 	%rd412, {%r9791, %r9790};
	shf.r.wrap.b32 	%r9792, %r9789, %r9788, 18;
	shf.r.wrap.b32 	%r9793, %r9788, %r9789, 18;
	mov.b64 	%rd413, {%r9793, %r9792};
	xor.b64  	%rd414, %rd413, %rd412;
	shf.l.wrap.b32 	%r9794, %r9788, %r9789, 23;
	shf.l.wrap.b32 	%r9795, %r9789, %r9788, 23;
	mov.b64 	%rd415, {%r9795, %r9794};
	xor.b64  	%rd416, %rd414, %rd415;
	xor.b64  	%rd417, %rd376, %rd353;
	and.b64  	%rd418, %rd399, %rd417;
	xor.b64  	%rd419, %rd418, %rd353;
	add.s64 	%rd420, %rd411, %rd419;
	add.s64 	%rd421, %rd420, %rd416;
	add.s64 	%rd422, %rd421, %rd341;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9796,%dummy}, %rd410;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9797}, %rd410;
	}
	shf.r.wrap.b32 	%r9798, %r9797, %r9796, 28;
	shf.r.wrap.b32 	%r9799, %r9796, %r9797, 28;
	mov.b64 	%rd423, {%r9799, %r9798};
	shf.l.wrap.b32 	%r9800, %r9796, %r9797, 30;
	shf.l.wrap.b32 	%r9801, %r9797, %r9796, 30;
	mov.b64 	%rd424, {%r9801, %r9800};
	xor.b64  	%rd425, %rd424, %rd423;
	shf.l.wrap.b32 	%r9802, %r9796, %r9797, 25;
	shf.l.wrap.b32 	%r9803, %r9797, %r9796, 25;
	mov.b64 	%rd426, {%r9803, %r9802};
	xor.b64  	%rd427, %rd425, %rd426;
	xor.b64  	%rd428, %rd410, %rd364;
	xor.b64  	%rd429, %rd410, %rd387;
	and.b64  	%rd430, %rd429, %rd428;
	xor.b64  	%rd431, %rd430, %rd410;
	add.s64 	%rd432, %rd421, %rd431;
	add.s64 	%rd433, %rd432, %rd427;
	add.s64 	%rd434, %rd2764, %rd353;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9804,%dummy}, %rd422;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9805}, %rd422;
	}
	shf.r.wrap.b32 	%r9806, %r9805, %r9804, 14;
	shf.r.wrap.b32 	%r9807, %r9804, %r9805, 14;
	mov.b64 	%rd435, {%r9807, %r9806};
	shf.r.wrap.b32 	%r9808, %r9805, %r9804, 18;
	shf.r.wrap.b32 	%r9809, %r9804, %r9805, 18;
	mov.b64 	%rd436, {%r9809, %r9808};
	xor.b64  	%rd437, %rd436, %rd435;
	shf.l.wrap.b32 	%r9810, %r9804, %r9805, 23;
	shf.l.wrap.b32 	%r9811, %r9805, %r9804, 23;
	mov.b64 	%rd438, {%r9811, %r9810};
	xor.b64  	%rd439, %rd437, %rd438;
	xor.b64  	%rd440, %rd399, %rd376;
	and.b64  	%rd441, %rd422, %rd440;
	xor.b64  	%rd442, %rd441, %rd376;
	add.s64 	%rd443, %rd434, %rd442;
	add.s64 	%rd444, %rd443, %rd439;
	add.s64 	%rd445, %rd444, %rd364;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9812,%dummy}, %rd433;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9813}, %rd433;
	}
	shf.r.wrap.b32 	%r9814, %r9813, %r9812, 28;
	shf.r.wrap.b32 	%r9815, %r9812, %r9813, 28;
	mov.b64 	%rd446, {%r9815, %r9814};
	shf.l.wrap.b32 	%r9816, %r9812, %r9813, 30;
	shf.l.wrap.b32 	%r9817, %r9813, %r9812, 30;
	mov.b64 	%rd447, {%r9817, %r9816};
	xor.b64  	%rd448, %rd447, %rd446;
	shf.l.wrap.b32 	%r9818, %r9812, %r9813, 25;
	shf.l.wrap.b32 	%r9819, %r9813, %r9812, 25;
	mov.b64 	%rd449, {%r9819, %r9818};
	xor.b64  	%rd450, %rd448, %rd449;
	xor.b64  	%rd451, %rd433, %rd387;
	xor.b64  	%rd452, %rd433, %rd410;
	and.b64  	%rd453, %rd452, %rd451;
	xor.b64  	%rd454, %rd453, %rd433;
	add.s64 	%rd455, %rd444, %rd454;
	add.s64 	%rd456, %rd455, %rd450;
	add.s64 	%rd457, %rd2765, %rd376;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9820,%dummy}, %rd445;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9821}, %rd445;
	}
	shf.r.wrap.b32 	%r9822, %r9821, %r9820, 14;
	shf.r.wrap.b32 	%r9823, %r9820, %r9821, 14;
	mov.b64 	%rd458, {%r9823, %r9822};
	shf.r.wrap.b32 	%r9824, %r9821, %r9820, 18;
	shf.r.wrap.b32 	%r9825, %r9820, %r9821, 18;
	mov.b64 	%rd459, {%r9825, %r9824};
	xor.b64  	%rd460, %rd459, %rd458;
	shf.l.wrap.b32 	%r9826, %r9820, %r9821, 23;
	shf.l.wrap.b32 	%r9827, %r9821, %r9820, 23;
	mov.b64 	%rd461, {%r9827, %r9826};
	xor.b64  	%rd462, %rd460, %rd461;
	xor.b64  	%rd463, %rd422, %rd399;
	and.b64  	%rd464, %rd445, %rd463;
	xor.b64  	%rd465, %rd464, %rd399;
	add.s64 	%rd466, %rd457, %rd465;
	add.s64 	%rd467, %rd466, %rd462;
	add.s64 	%rd468, %rd467, %rd387;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9828,%dummy}, %rd456;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9829}, %rd456;
	}
	shf.r.wrap.b32 	%r9830, %r9829, %r9828, 28;
	shf.r.wrap.b32 	%r9831, %r9828, %r9829, 28;
	mov.b64 	%rd469, {%r9831, %r9830};
	shf.l.wrap.b32 	%r9832, %r9828, %r9829, 30;
	shf.l.wrap.b32 	%r9833, %r9829, %r9828, 30;
	mov.b64 	%rd470, {%r9833, %r9832};
	xor.b64  	%rd471, %rd470, %rd469;
	shf.l.wrap.b32 	%r9834, %r9828, %r9829, 25;
	shf.l.wrap.b32 	%r9835, %r9829, %r9828, 25;
	mov.b64 	%rd472, {%r9835, %r9834};
	xor.b64  	%rd473, %rd471, %rd472;
	xor.b64  	%rd474, %rd456, %rd410;
	xor.b64  	%rd475, %rd456, %rd433;
	and.b64  	%rd476, %rd475, %rd474;
	xor.b64  	%rd477, %rd476, %rd456;
	add.s64 	%rd478, %rd467, %rd477;
	add.s64 	%rd479, %rd478, %rd473;
	add.s64 	%rd480, %rd2766, %rd399;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9836,%dummy}, %rd468;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9837}, %rd468;
	}
	shf.r.wrap.b32 	%r9838, %r9837, %r9836, 14;
	shf.r.wrap.b32 	%r9839, %r9836, %r9837, 14;
	mov.b64 	%rd481, {%r9839, %r9838};
	shf.r.wrap.b32 	%r9840, %r9837, %r9836, 18;
	shf.r.wrap.b32 	%r9841, %r9836, %r9837, 18;
	mov.b64 	%rd482, {%r9841, %r9840};
	xor.b64  	%rd483, %rd482, %rd481;
	shf.l.wrap.b32 	%r9842, %r9836, %r9837, 23;
	shf.l.wrap.b32 	%r9843, %r9837, %r9836, 23;
	mov.b64 	%rd484, {%r9843, %r9842};
	xor.b64  	%rd485, %rd483, %rd484;
	xor.b64  	%rd486, %rd445, %rd422;
	and.b64  	%rd487, %rd468, %rd486;
	xor.b64  	%rd488, %rd487, %rd422;
	add.s64 	%rd489, %rd480, %rd488;
	add.s64 	%rd490, %rd489, %rd485;
	add.s64 	%rd491, %rd490, %rd410;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9844,%dummy}, %rd479;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9845}, %rd479;
	}
	shf.r.wrap.b32 	%r9846, %r9845, %r9844, 28;
	shf.r.wrap.b32 	%r9847, %r9844, %r9845, 28;
	mov.b64 	%rd492, {%r9847, %r9846};
	shf.l.wrap.b32 	%r9848, %r9844, %r9845, 30;
	shf.l.wrap.b32 	%r9849, %r9845, %r9844, 30;
	mov.b64 	%rd493, {%r9849, %r9848};
	xor.b64  	%rd494, %rd493, %rd492;
	shf.l.wrap.b32 	%r9850, %r9844, %r9845, 25;
	shf.l.wrap.b32 	%r9851, %r9845, %r9844, 25;
	mov.b64 	%rd495, {%r9851, %r9850};
	xor.b64  	%rd496, %rd494, %rd495;
	xor.b64  	%rd497, %rd479, %rd433;
	xor.b64  	%rd498, %rd479, %rd456;
	and.b64  	%rd499, %rd498, %rd497;
	xor.b64  	%rd500, %rd499, %rd479;
	add.s64 	%rd501, %rd490, %rd500;
	add.s64 	%rd502, %rd501, %rd496;
	add.s64 	%rd503, %rd2767, %rd422;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9852,%dummy}, %rd491;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9853}, %rd491;
	}
	shf.r.wrap.b32 	%r9854, %r9853, %r9852, 14;
	shf.r.wrap.b32 	%r9855, %r9852, %r9853, 14;
	mov.b64 	%rd504, {%r9855, %r9854};
	shf.r.wrap.b32 	%r9856, %r9853, %r9852, 18;
	shf.r.wrap.b32 	%r9857, %r9852, %r9853, 18;
	mov.b64 	%rd505, {%r9857, %r9856};
	xor.b64  	%rd506, %rd505, %rd504;
	shf.l.wrap.b32 	%r9858, %r9852, %r9853, 23;
	shf.l.wrap.b32 	%r9859, %r9853, %r9852, 23;
	mov.b64 	%rd507, {%r9859, %r9858};
	xor.b64  	%rd508, %rd506, %rd507;
	xor.b64  	%rd509, %rd468, %rd445;
	and.b64  	%rd510, %rd491, %rd509;
	xor.b64  	%rd511, %rd510, %rd445;
	add.s64 	%rd512, %rd503, %rd511;
	add.s64 	%rd513, %rd512, %rd508;
	add.s64 	%rd514, %rd513, %rd433;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9860,%dummy}, %rd502;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9861}, %rd502;
	}
	shf.r.wrap.b32 	%r9862, %r9861, %r9860, 28;
	shf.r.wrap.b32 	%r9863, %r9860, %r9861, 28;
	mov.b64 	%rd515, {%r9863, %r9862};
	shf.l.wrap.b32 	%r9864, %r9860, %r9861, 30;
	shf.l.wrap.b32 	%r9865, %r9861, %r9860, 30;
	mov.b64 	%rd516, {%r9865, %r9864};
	xor.b64  	%rd517, %rd516, %rd515;
	shf.l.wrap.b32 	%r9866, %r9860, %r9861, 25;
	shf.l.wrap.b32 	%r9867, %r9861, %r9860, 25;
	mov.b64 	%rd518, {%r9867, %r9866};
	xor.b64  	%rd519, %rd517, %rd518;
	xor.b64  	%rd520, %rd502, %rd456;
	xor.b64  	%rd521, %rd502, %rd479;
	and.b64  	%rd522, %rd521, %rd520;
	xor.b64  	%rd523, %rd522, %rd502;
	add.s64 	%rd524, %rd513, %rd523;
	add.s64 	%rd525, %rd524, %rd519;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9868,%dummy}, %rd514;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9869}, %rd514;
	}
	shf.r.wrap.b32 	%r9870, %r9869, %r9868, 14;
	shf.r.wrap.b32 	%r9871, %r9868, %r9869, 14;
	mov.b64 	%rd526, {%r9871, %r9870};
	shf.r.wrap.b32 	%r9872, %r9869, %r9868, 18;
	shf.r.wrap.b32 	%r9873, %r9868, %r9869, 18;
	mov.b64 	%rd527, {%r9873, %r9872};
	xor.b64  	%rd528, %rd527, %rd526;
	shf.l.wrap.b32 	%r9874, %r9868, %r9869, 23;
	shf.l.wrap.b32 	%r9875, %r9869, %r9868, 23;
	mov.b64 	%rd529, {%r9875, %r9874};
	xor.b64  	%rd530, %rd528, %rd529;
	xor.b64  	%rd531, %rd491, %rd468;
	and.b64  	%rd532, %rd514, %rd531;
	xor.b64  	%rd533, %rd532, %rd468;
	add.s64 	%rd534, %rd445, %rd193;
	add.s64 	%rd535, %rd534, %rd2768;
	add.s64 	%rd536, %rd535, %rd533;
	add.s64 	%rd537, %rd536, %rd530;
	add.s64 	%rd538, %rd537, %rd456;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9876,%dummy}, %rd525;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9877}, %rd525;
	}
	shf.r.wrap.b32 	%r9878, %r9877, %r9876, 28;
	shf.r.wrap.b32 	%r9879, %r9876, %r9877, 28;
	mov.b64 	%rd539, {%r9879, %r9878};
	shf.l.wrap.b32 	%r9880, %r9876, %r9877, 30;
	shf.l.wrap.b32 	%r9881, %r9877, %r9876, 30;
	mov.b64 	%rd540, {%r9881, %r9880};
	xor.b64  	%rd541, %rd540, %rd539;
	shf.l.wrap.b32 	%r9882, %r9876, %r9877, 25;
	shf.l.wrap.b32 	%r9883, %r9877, %r9876, 25;
	mov.b64 	%rd542, {%r9883, %r9882};
	xor.b64  	%rd543, %rd541, %rd542;
	xor.b64  	%rd544, %rd525, %rd479;
	xor.b64  	%rd545, %rd525, %rd502;
	and.b64  	%rd546, %rd545, %rd544;
	xor.b64  	%rd547, %rd546, %rd525;
	add.s64 	%rd548, %rd537, %rd547;
	add.s64 	%rd549, %rd548, %rd543;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9884,%dummy}, %rd187;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9885}, %rd187;
	}
	shf.r.wrap.b32 	%r9886, %r9885, %r9884, 1;
	shf.r.wrap.b32 	%r9887, %r9884, %r9885, 1;
	mov.b64 	%rd550, {%r9887, %r9886};
	shf.r.wrap.b32 	%r9888, %r9885, %r9884, 8;
	shf.r.wrap.b32 	%r9889, %r9884, %r9885, 8;
	mov.b64 	%rd551, {%r9889, %r9888};
	shr.u64 	%rd552, %rd187, 7;
	xor.b64  	%rd553, %rd550, %rd552;
	xor.b64  	%rd554, %rd553, %rd551;
	add.s64 	%rd555, %rd186, %rd22;
	add.s64 	%rd556, %rd555, %rd554;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9890,%dummy}, %rd193;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9891}, %rd193;
	}
	shf.r.wrap.b32 	%r9892, %r9891, %r9890, 19;
	shf.r.wrap.b32 	%r9893, %r9890, %r9891, 19;
	mov.b64 	%rd557, {%r9893, %r9892};
	shf.l.wrap.b32 	%r9894, %r9890, %r9891, 3;
	shf.l.wrap.b32 	%r9895, %r9891, %r9890, 3;
	mov.b64 	%rd558, {%r9895, %r9894};
	shr.u64 	%rd559, %rd193, 6;
	xor.b64  	%rd560, %rd557, %rd559;
	xor.b64  	%rd561, %rd560, %rd558;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9896,%dummy}, %rd188;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9897}, %rd188;
	}
	shf.r.wrap.b32 	%r9898, %r9897, %r9896, 1;
	shf.r.wrap.b32 	%r9899, %r9896, %r9897, 1;
	mov.b64 	%rd562, {%r9899, %r9898};
	shf.r.wrap.b32 	%r9900, %r9897, %r9896, 8;
	shf.r.wrap.b32 	%r9901, %r9896, %r9897, 8;
	mov.b64 	%rd563, {%r9901, %r9900};
	shr.u64 	%rd564, %rd188, 7;
	xor.b64  	%rd565, %rd562, %rd564;
	xor.b64  	%rd566, %rd565, %rd563;
	add.s64 	%rd567, %rd187, %rd561;
	add.s64 	%rd568, %rd567, %rd566;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9902,%dummy}, %rd556;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9903}, %rd556;
	}
	shf.r.wrap.b32 	%r9904, %r9903, %r9902, 19;
	shf.r.wrap.b32 	%r9905, %r9902, %r9903, 19;
	mov.b64 	%rd569, {%r9905, %r9904};
	shf.l.wrap.b32 	%r9906, %r9902, %r9903, 3;
	shf.l.wrap.b32 	%r9907, %r9903, %r9902, 3;
	mov.b64 	%rd570, {%r9907, %r9906};
	shr.u64 	%rd571, %rd556, 6;
	xor.b64  	%rd572, %rd569, %rd571;
	xor.b64  	%rd573, %rd572, %rd570;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9908,%dummy}, %rd189;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9909}, %rd189;
	}
	shf.r.wrap.b32 	%r9910, %r9909, %r9908, 1;
	shf.r.wrap.b32 	%r9911, %r9908, %r9909, 1;
	mov.b64 	%rd574, {%r9911, %r9910};
	shf.r.wrap.b32 	%r9912, %r9909, %r9908, 8;
	shf.r.wrap.b32 	%r9913, %r9908, %r9909, 8;
	mov.b64 	%rd575, {%r9913, %r9912};
	shr.u64 	%rd576, %rd189, 7;
	xor.b64  	%rd577, %rd574, %rd576;
	xor.b64  	%rd578, %rd577, %rd575;
	add.s64 	%rd579, %rd188, %rd573;
	add.s64 	%rd580, %rd579, %rd578;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9914,%dummy}, %rd568;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9915}, %rd568;
	}
	shf.r.wrap.b32 	%r9916, %r9915, %r9914, 19;
	shf.r.wrap.b32 	%r9917, %r9914, %r9915, 19;
	mov.b64 	%rd581, {%r9917, %r9916};
	shf.l.wrap.b32 	%r9918, %r9914, %r9915, 3;
	shf.l.wrap.b32 	%r9919, %r9915, %r9914, 3;
	mov.b64 	%rd582, {%r9919, %r9918};
	shr.u64 	%rd583, %rd568, 6;
	xor.b64  	%rd584, %rd581, %rd583;
	xor.b64  	%rd585, %rd584, %rd582;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9920,%dummy}, %rd190;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9921}, %rd190;
	}
	shf.r.wrap.b32 	%r9922, %r9921, %r9920, 1;
	shf.r.wrap.b32 	%r9923, %r9920, %r9921, 1;
	mov.b64 	%rd586, {%r9923, %r9922};
	shf.r.wrap.b32 	%r9924, %r9921, %r9920, 8;
	shf.r.wrap.b32 	%r9925, %r9920, %r9921, 8;
	mov.b64 	%rd587, {%r9925, %r9924};
	shr.u64 	%rd588, %rd190, 7;
	xor.b64  	%rd589, %rd586, %rd588;
	xor.b64  	%rd590, %rd589, %rd587;
	add.s64 	%rd591, %rd189, %rd585;
	add.s64 	%rd592, %rd591, %rd590;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9926,%dummy}, %rd580;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9927}, %rd580;
	}
	shf.r.wrap.b32 	%r9928, %r9927, %r9926, 19;
	shf.r.wrap.b32 	%r9929, %r9926, %r9927, 19;
	mov.b64 	%rd593, {%r9929, %r9928};
	shf.l.wrap.b32 	%r9930, %r9926, %r9927, 3;
	shf.l.wrap.b32 	%r9931, %r9927, %r9926, 3;
	mov.b64 	%rd594, {%r9931, %r9930};
	shr.u64 	%rd595, %rd580, 6;
	xor.b64  	%rd596, %rd593, %rd595;
	xor.b64  	%rd597, %rd596, %rd594;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9932,%dummy}, %rd191;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9933}, %rd191;
	}
	shf.r.wrap.b32 	%r9934, %r9933, %r9932, 1;
	shf.r.wrap.b32 	%r9935, %r9932, %r9933, 1;
	mov.b64 	%rd598, {%r9935, %r9934};
	shf.r.wrap.b32 	%r9936, %r9933, %r9932, 8;
	shf.r.wrap.b32 	%r9937, %r9932, %r9933, 8;
	mov.b64 	%rd599, {%r9937, %r9936};
	shr.u64 	%rd600, %rd191, 7;
	xor.b64  	%rd601, %rd598, %rd600;
	xor.b64  	%rd602, %rd601, %rd599;
	add.s64 	%rd603, %rd190, %rd597;
	add.s64 	%rd604, %rd603, %rd602;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9938,%dummy}, %rd592;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9939}, %rd592;
	}
	shf.r.wrap.b32 	%r9940, %r9939, %r9938, 19;
	shf.r.wrap.b32 	%r9941, %r9938, %r9939, 19;
	mov.b64 	%rd605, {%r9941, %r9940};
	shf.l.wrap.b32 	%r9942, %r9938, %r9939, 3;
	shf.l.wrap.b32 	%r9943, %r9939, %r9938, 3;
	mov.b64 	%rd606, {%r9943, %r9942};
	shr.u64 	%rd607, %rd592, 6;
	xor.b64  	%rd608, %rd605, %rd607;
	xor.b64  	%rd609, %rd608, %rd606;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9944,%dummy}, %rd192;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9945}, %rd192;
	}
	shf.r.wrap.b32 	%r9946, %r9945, %r9944, 1;
	shf.r.wrap.b32 	%r9947, %r9944, %r9945, 1;
	mov.b64 	%rd610, {%r9947, %r9946};
	shf.r.wrap.b32 	%r9948, %r9945, %r9944, 8;
	shf.r.wrap.b32 	%r9949, %r9944, %r9945, 8;
	mov.b64 	%rd611, {%r9949, %r9948};
	shr.u64 	%rd612, %rd192, 7;
	xor.b64  	%rd613, %rd610, %rd612;
	xor.b64  	%rd614, %rd613, %rd611;
	add.s64 	%rd615, %rd191, %rd609;
	add.s64 	%rd616, %rd615, %rd614;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9950,%dummy}, %rd604;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9951}, %rd604;
	}
	shf.r.wrap.b32 	%r9952, %r9951, %r9950, 19;
	shf.r.wrap.b32 	%r9953, %r9950, %r9951, 19;
	mov.b64 	%rd617, {%r9953, %r9952};
	shf.l.wrap.b32 	%r9954, %r9950, %r9951, 3;
	shf.l.wrap.b32 	%r9955, %r9951, %r9950, 3;
	mov.b64 	%rd618, {%r9955, %r9954};
	shr.u64 	%rd619, %rd604, 6;
	xor.b64  	%rd620, %rd617, %rd619;
	xor.b64  	%rd621, %rd620, %rd618;
	add.s64 	%rd622, %rd192, %rd193;
	add.s64 	%rd623, %rd622, %rd621;
	add.s64 	%rd624, %rd623, %rd23;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9956,%dummy}, %rd616;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9957}, %rd616;
	}
	shf.r.wrap.b32 	%r9958, %r9957, %r9956, 19;
	shf.r.wrap.b32 	%r9959, %r9956, %r9957, 19;
	mov.b64 	%rd625, {%r9959, %r9958};
	shf.l.wrap.b32 	%r9960, %r9956, %r9957, 3;
	shf.l.wrap.b32 	%r9961, %r9957, %r9956, 3;
	mov.b64 	%rd626, {%r9961, %r9960};
	shr.u64 	%rd627, %rd616, 6;
	xor.b64  	%rd628, %rd625, %rd627;
	xor.b64  	%rd629, %rd628, %rd626;
	add.s64 	%rd630, %rd556, %rd629;
	add.s64 	%rd631, %rd630, %rd23;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9962,%dummy}, %rd624;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9963}, %rd624;
	}
	shf.r.wrap.b32 	%r9964, %r9963, %r9962, 19;
	shf.r.wrap.b32 	%r9965, %r9962, %r9963, 19;
	mov.b64 	%rd632, {%r9965, %r9964};
	shf.l.wrap.b32 	%r9966, %r9962, %r9963, 3;
	shf.l.wrap.b32 	%r9967, %r9963, %r9962, 3;
	mov.b64 	%rd633, {%r9967, %r9966};
	shr.u64 	%rd634, %rd624, 6;
	xor.b64  	%rd635, %rd632, %rd634;
	xor.b64  	%rd636, %rd635, %rd633;
	add.s64 	%rd637, %rd568, %rd636;
	add.s64 	%rd638, %rd637, %rd23;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9968,%dummy}, %rd631;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9969}, %rd631;
	}
	shf.r.wrap.b32 	%r9970, %r9969, %r9968, 19;
	shf.r.wrap.b32 	%r9971, %r9968, %r9969, 19;
	mov.b64 	%rd639, {%r9971, %r9970};
	shf.l.wrap.b32 	%r9972, %r9968, %r9969, 3;
	shf.l.wrap.b32 	%r9973, %r9969, %r9968, 3;
	mov.b64 	%rd640, {%r9973, %r9972};
	shr.u64 	%rd641, %rd631, 6;
	xor.b64  	%rd642, %rd639, %rd641;
	xor.b64  	%rd643, %rd642, %rd640;
	add.s64 	%rd644, %rd580, %rd643;
	add.s64 	%rd645, %rd644, %rd23;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9974,%dummy}, %rd638;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9975}, %rd638;
	}
	shf.r.wrap.b32 	%r9976, %r9975, %r9974, 19;
	shf.r.wrap.b32 	%r9977, %r9974, %r9975, 19;
	mov.b64 	%rd646, {%r9977, %r9976};
	shf.l.wrap.b32 	%r9978, %r9974, %r9975, 3;
	shf.l.wrap.b32 	%r9979, %r9975, %r9974, 3;
	mov.b64 	%rd647, {%r9979, %r9978};
	shr.u64 	%rd648, %rd638, 6;
	xor.b64  	%rd649, %rd646, %rd648;
	xor.b64  	%rd650, %rd649, %rd647;
	add.s64 	%rd651, %rd592, %rd650;
	add.s64 	%rd652, %rd651, %rd23;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9980,%dummy}, %rd645;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9981}, %rd645;
	}
	shf.r.wrap.b32 	%r9982, %r9981, %r9980, 19;
	shf.r.wrap.b32 	%r9983, %r9980, %r9981, 19;
	mov.b64 	%rd653, {%r9983, %r9982};
	shf.l.wrap.b32 	%r9984, %r9980, %r9981, 3;
	shf.l.wrap.b32 	%r9985, %r9981, %r9980, 3;
	mov.b64 	%rd654, {%r9985, %r9984};
	shr.u64 	%rd655, %rd645, 6;
	xor.b64  	%rd656, %rd653, %rd655;
	xor.b64  	%rd657, %rd656, %rd654;
	add.s64 	%rd658, %rd604, %rd657;
	add.s64 	%rd659, %rd658, %rd23;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9986,%dummy}, %rd652;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9987}, %rd652;
	}
	shf.r.wrap.b32 	%r9988, %r9987, %r9986, 19;
	shf.r.wrap.b32 	%r9989, %r9986, %r9987, 19;
	mov.b64 	%rd660, {%r9989, %r9988};
	shf.l.wrap.b32 	%r9990, %r9986, %r9987, 3;
	shf.l.wrap.b32 	%r9991, %r9987, %r9986, 3;
	mov.b64 	%rd661, {%r9991, %r9990};
	shr.u64 	%rd662, %rd652, 6;
	xor.b64  	%rd663, %rd660, %rd662;
	xor.b64  	%rd664, %rd663, %rd661;
	add.s64 	%rd665, %rd616, %rd664;
	add.s64 	%rd666, %rd665, %rd23;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9992,%dummy}, %rd659;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9993}, %rd659;
	}
	shf.r.wrap.b32 	%r9994, %r9993, %r9992, 19;
	shf.r.wrap.b32 	%r9995, %r9992, %r9993, 19;
	mov.b64 	%rd667, {%r9995, %r9994};
	shf.l.wrap.b32 	%r9996, %r9992, %r9993, 3;
	shf.l.wrap.b32 	%r9997, %r9993, %r9992, 3;
	mov.b64 	%rd668, {%r9997, %r9996};
	shr.u64 	%rd669, %rd659, 6;
	xor.b64  	%rd670, %rd667, %rd669;
	xor.b64  	%rd671, %rd670, %rd668;
	add.s64 	%rd672, %rd624, %rd671;
	add.s64 	%rd673, %rd672, %rd23;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r9998,%dummy}, %rd666;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r9999}, %rd666;
	}
	shf.r.wrap.b32 	%r10000, %r9999, %r9998, 19;
	shf.r.wrap.b32 	%r10001, %r9998, %r9999, 19;
	mov.b64 	%rd674, {%r10001, %r10000};
	shf.l.wrap.b32 	%r10002, %r9998, %r9999, 3;
	shf.l.wrap.b32 	%r10003, %r9999, %r9998, 3;
	mov.b64 	%rd675, {%r10003, %r10002};
	shr.u64 	%rd676, %rd666, 6;
	xor.b64  	%rd677, %rd674, %rd676;
	xor.b64  	%rd678, %rd677, %rd675;
	shf.r.wrap.b32 	%r10004, %r9891, %r9890, 1;
	shf.r.wrap.b32 	%r10005, %r9890, %r9891, 1;
	mov.b64 	%rd679, {%r10005, %r10004};
	shf.r.wrap.b32 	%r10006, %r9891, %r9890, 8;
	shf.r.wrap.b32 	%r10007, %r9890, %r9891, 8;
	mov.b64 	%rd680, {%r10007, %r10006};
	shr.u64 	%rd681, %rd193, 7;
	xor.b64  	%rd682, %rd679, %rd681;
	xor.b64  	%rd683, %rd682, %rd680;
	add.s64 	%rd684, %rd631, %rd678;
	add.s64 	%rd685, %rd684, %rd683;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10008,%dummy}, %rd673;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10009}, %rd673;
	}
	shf.r.wrap.b32 	%r10010, %r10009, %r10008, 19;
	shf.r.wrap.b32 	%r10011, %r10008, %r10009, 19;
	mov.b64 	%rd686, {%r10011, %r10010};
	shf.l.wrap.b32 	%r10012, %r10008, %r10009, 3;
	shf.l.wrap.b32 	%r10013, %r10009, %r10008, 3;
	mov.b64 	%rd687, {%r10013, %r10012};
	shr.u64 	%rd688, %rd673, 6;
	xor.b64  	%rd689, %rd686, %rd688;
	xor.b64  	%rd690, %rd689, %rd687;
	shf.r.wrap.b32 	%r10014, %r9903, %r9902, 1;
	shf.r.wrap.b32 	%r10015, %r9902, %r9903, 1;
	mov.b64 	%rd691, {%r10015, %r10014};
	shf.r.wrap.b32 	%r10016, %r9903, %r9902, 8;
	shf.r.wrap.b32 	%r10017, %r9902, %r9903, 8;
	mov.b64 	%rd692, {%r10017, %r10016};
	shr.u64 	%rd693, %rd556, 7;
	xor.b64  	%rd694, %rd691, %rd693;
	xor.b64  	%rd695, %rd694, %rd692;
	add.s64 	%rd696, %rd638, %rd193;
	add.s64 	%rd697, %rd696, %rd690;
	add.s64 	%rd698, %rd697, %rd695;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10018,%dummy}, %rd538;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10019}, %rd538;
	}
	shf.r.wrap.b32 	%r10020, %r10019, %r10018, 14;
	shf.r.wrap.b32 	%r10021, %r10018, %r10019, 14;
	mov.b64 	%rd699, {%r10021, %r10020};
	shf.r.wrap.b32 	%r10022, %r10019, %r10018, 18;
	shf.r.wrap.b32 	%r10023, %r10018, %r10019, 18;
	mov.b64 	%rd700, {%r10023, %r10022};
	xor.b64  	%rd701, %rd700, %rd699;
	shf.l.wrap.b32 	%r10024, %r10018, %r10019, 23;
	shf.l.wrap.b32 	%r10025, %r10019, %r10018, 23;
	mov.b64 	%rd702, {%r10025, %r10024};
	xor.b64  	%rd703, %rd701, %rd702;
	xor.b64  	%rd704, %rd491, %rd514;
	and.b64  	%rd705, %rd704, %rd538;
	xor.b64  	%rd706, %rd705, %rd491;
	add.s64 	%rd707, %rd706, %rd468;
	add.s64 	%rd708, %rd707, %rd556;
	add.s64 	%rd709, %rd708, %rd2769;
	add.s64 	%rd710, %rd709, %rd703;
	add.s64 	%rd711, %rd710, %rd479;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10026,%dummy}, %rd549;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10027}, %rd549;
	}
	shf.r.wrap.b32 	%r10028, %r10027, %r10026, 28;
	shf.r.wrap.b32 	%r10029, %r10026, %r10027, 28;
	mov.b64 	%rd712, {%r10029, %r10028};
	shf.l.wrap.b32 	%r10030, %r10026, %r10027, 30;
	shf.l.wrap.b32 	%r10031, %r10027, %r10026, 30;
	mov.b64 	%rd713, {%r10031, %r10030};
	xor.b64  	%rd714, %rd713, %rd712;
	shf.l.wrap.b32 	%r10032, %r10026, %r10027, 25;
	shf.l.wrap.b32 	%r10033, %r10027, %r10026, 25;
	mov.b64 	%rd715, {%r10033, %r10032};
	xor.b64  	%rd716, %rd714, %rd715;
	xor.b64  	%rd717, %rd549, %rd502;
	xor.b64  	%rd718, %rd549, %rd525;
	and.b64  	%rd719, %rd718, %rd717;
	xor.b64  	%rd720, %rd719, %rd549;
	add.s64 	%rd721, %rd710, %rd720;
	add.s64 	%rd722, %rd721, %rd716;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10034,%dummy}, %rd711;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10035}, %rd711;
	}
	shf.r.wrap.b32 	%r10036, %r10035, %r10034, 14;
	shf.r.wrap.b32 	%r10037, %r10034, %r10035, 14;
	mov.b64 	%rd723, {%r10037, %r10036};
	shf.r.wrap.b32 	%r10038, %r10035, %r10034, 18;
	shf.r.wrap.b32 	%r10039, %r10034, %r10035, 18;
	mov.b64 	%rd724, {%r10039, %r10038};
	xor.b64  	%rd725, %rd724, %rd723;
	shf.l.wrap.b32 	%r10040, %r10034, %r10035, 23;
	shf.l.wrap.b32 	%r10041, %r10035, %r10034, 23;
	mov.b64 	%rd726, {%r10041, %r10040};
	xor.b64  	%rd727, %rd725, %rd726;
	xor.b64  	%rd728, %rd514, %rd538;
	and.b64  	%rd729, %rd711, %rd728;
	xor.b64  	%rd730, %rd729, %rd514;
	add.s64 	%rd731, %rd568, %rd491;
	add.s64 	%rd732, %rd731, %rd2770;
	add.s64 	%rd733, %rd732, %rd730;
	add.s64 	%rd734, %rd733, %rd727;
	add.s64 	%rd735, %rd734, %rd502;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10042,%dummy}, %rd722;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10043}, %rd722;
	}
	shf.r.wrap.b32 	%r10044, %r10043, %r10042, 28;
	shf.r.wrap.b32 	%r10045, %r10042, %r10043, 28;
	mov.b64 	%rd736, {%r10045, %r10044};
	shf.l.wrap.b32 	%r10046, %r10042, %r10043, 30;
	shf.l.wrap.b32 	%r10047, %r10043, %r10042, 30;
	mov.b64 	%rd737, {%r10047, %r10046};
	xor.b64  	%rd738, %rd737, %rd736;
	shf.l.wrap.b32 	%r10048, %r10042, %r10043, 25;
	shf.l.wrap.b32 	%r10049, %r10043, %r10042, 25;
	mov.b64 	%rd739, {%r10049, %r10048};
	xor.b64  	%rd740, %rd738, %rd739;
	xor.b64  	%rd741, %rd722, %rd525;
	xor.b64  	%rd742, %rd722, %rd549;
	and.b64  	%rd743, %rd742, %rd741;
	xor.b64  	%rd744, %rd743, %rd722;
	add.s64 	%rd745, %rd734, %rd744;
	add.s64 	%rd746, %rd745, %rd740;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10050,%dummy}, %rd735;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10051}, %rd735;
	}
	shf.r.wrap.b32 	%r10052, %r10051, %r10050, 14;
	shf.r.wrap.b32 	%r10053, %r10050, %r10051, 14;
	mov.b64 	%rd747, {%r10053, %r10052};
	shf.r.wrap.b32 	%r10054, %r10051, %r10050, 18;
	shf.r.wrap.b32 	%r10055, %r10050, %r10051, 18;
	mov.b64 	%rd748, {%r10055, %r10054};
	xor.b64  	%rd749, %rd748, %rd747;
	shf.l.wrap.b32 	%r10056, %r10050, %r10051, 23;
	shf.l.wrap.b32 	%r10057, %r10051, %r10050, 23;
	mov.b64 	%rd750, {%r10057, %r10056};
	xor.b64  	%rd751, %rd749, %rd750;
	xor.b64  	%rd752, %rd711, %rd538;
	and.b64  	%rd753, %rd735, %rd752;
	xor.b64  	%rd754, %rd753, %rd538;
	add.s64 	%rd755, %rd580, %rd514;
	add.s64 	%rd756, %rd755, %rd2771;
	add.s64 	%rd757, %rd756, %rd754;
	add.s64 	%rd758, %rd757, %rd751;
	add.s64 	%rd759, %rd758, %rd525;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10058,%dummy}, %rd746;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10059}, %rd746;
	}
	shf.r.wrap.b32 	%r10060, %r10059, %r10058, 28;
	shf.r.wrap.b32 	%r10061, %r10058, %r10059, 28;
	mov.b64 	%rd760, {%r10061, %r10060};
	shf.l.wrap.b32 	%r10062, %r10058, %r10059, 30;
	shf.l.wrap.b32 	%r10063, %r10059, %r10058, 30;
	mov.b64 	%rd761, {%r10063, %r10062};
	xor.b64  	%rd762, %rd761, %rd760;
	shf.l.wrap.b32 	%r10064, %r10058, %r10059, 25;
	shf.l.wrap.b32 	%r10065, %r10059, %r10058, 25;
	mov.b64 	%rd763, {%r10065, %r10064};
	xor.b64  	%rd764, %rd762, %rd763;
	xor.b64  	%rd765, %rd746, %rd549;
	xor.b64  	%rd766, %rd746, %rd722;
	and.b64  	%rd767, %rd766, %rd765;
	xor.b64  	%rd768, %rd767, %rd746;
	add.s64 	%rd769, %rd758, %rd768;
	add.s64 	%rd770, %rd769, %rd764;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10066,%dummy}, %rd759;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10067}, %rd759;
	}
	shf.r.wrap.b32 	%r10068, %r10067, %r10066, 14;
	shf.r.wrap.b32 	%r10069, %r10066, %r10067, 14;
	mov.b64 	%rd771, {%r10069, %r10068};
	shf.r.wrap.b32 	%r10070, %r10067, %r10066, 18;
	shf.r.wrap.b32 	%r10071, %r10066, %r10067, 18;
	mov.b64 	%rd772, {%r10071, %r10070};
	xor.b64  	%rd773, %rd772, %rd771;
	shf.l.wrap.b32 	%r10072, %r10066, %r10067, 23;
	shf.l.wrap.b32 	%r10073, %r10067, %r10066, 23;
	mov.b64 	%rd774, {%r10073, %r10072};
	xor.b64  	%rd775, %rd773, %rd774;
	xor.b64  	%rd776, %rd735, %rd711;
	and.b64  	%rd777, %rd759, %rd776;
	xor.b64  	%rd778, %rd777, %rd711;
	add.s64 	%rd779, %rd592, %rd538;
	add.s64 	%rd780, %rd779, %rd2772;
	add.s64 	%rd781, %rd780, %rd778;
	add.s64 	%rd782, %rd781, %rd775;
	add.s64 	%rd783, %rd782, %rd549;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10074,%dummy}, %rd770;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10075}, %rd770;
	}
	shf.r.wrap.b32 	%r10076, %r10075, %r10074, 28;
	shf.r.wrap.b32 	%r10077, %r10074, %r10075, 28;
	mov.b64 	%rd784, {%r10077, %r10076};
	shf.l.wrap.b32 	%r10078, %r10074, %r10075, 30;
	shf.l.wrap.b32 	%r10079, %r10075, %r10074, 30;
	mov.b64 	%rd785, {%r10079, %r10078};
	xor.b64  	%rd786, %rd785, %rd784;
	shf.l.wrap.b32 	%r10080, %r10074, %r10075, 25;
	shf.l.wrap.b32 	%r10081, %r10075, %r10074, 25;
	mov.b64 	%rd787, {%r10081, %r10080};
	xor.b64  	%rd788, %rd786, %rd787;
	xor.b64  	%rd789, %rd770, %rd722;
	xor.b64  	%rd790, %rd770, %rd746;
	and.b64  	%rd791, %rd790, %rd789;
	xor.b64  	%rd792, %rd791, %rd770;
	add.s64 	%rd793, %rd782, %rd792;
	add.s64 	%rd794, %rd793, %rd788;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10082,%dummy}, %rd783;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10083}, %rd783;
	}
	shf.r.wrap.b32 	%r10084, %r10083, %r10082, 14;
	shf.r.wrap.b32 	%r10085, %r10082, %r10083, 14;
	mov.b64 	%rd795, {%r10085, %r10084};
	shf.r.wrap.b32 	%r10086, %r10083, %r10082, 18;
	shf.r.wrap.b32 	%r10087, %r10082, %r10083, 18;
	mov.b64 	%rd796, {%r10087, %r10086};
	xor.b64  	%rd797, %rd796, %rd795;
	shf.l.wrap.b32 	%r10088, %r10082, %r10083, 23;
	shf.l.wrap.b32 	%r10089, %r10083, %r10082, 23;
	mov.b64 	%rd798, {%r10089, %r10088};
	xor.b64  	%rd799, %rd797, %rd798;
	xor.b64  	%rd800, %rd759, %rd735;
	and.b64  	%rd801, %rd783, %rd800;
	xor.b64  	%rd802, %rd801, %rd735;
	add.s64 	%rd803, %rd711, %rd604;
	add.s64 	%rd804, %rd803, %rd2773;
	add.s64 	%rd805, %rd804, %rd802;
	add.s64 	%rd806, %rd805, %rd799;
	add.s64 	%rd807, %rd806, %rd722;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10090,%dummy}, %rd794;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10091}, %rd794;
	}
	shf.r.wrap.b32 	%r10092, %r10091, %r10090, 28;
	shf.r.wrap.b32 	%r10093, %r10090, %r10091, 28;
	mov.b64 	%rd808, {%r10093, %r10092};
	shf.l.wrap.b32 	%r10094, %r10090, %r10091, 30;
	shf.l.wrap.b32 	%r10095, %r10091, %r10090, 30;
	mov.b64 	%rd809, {%r10095, %r10094};
	xor.b64  	%rd810, %rd809, %rd808;
	shf.l.wrap.b32 	%r10096, %r10090, %r10091, 25;
	shf.l.wrap.b32 	%r10097, %r10091, %r10090, 25;
	mov.b64 	%rd811, {%r10097, %r10096};
	xor.b64  	%rd812, %rd810, %rd811;
	xor.b64  	%rd813, %rd794, %rd746;
	xor.b64  	%rd814, %rd794, %rd770;
	and.b64  	%rd815, %rd814, %rd813;
	xor.b64  	%rd816, %rd815, %rd794;
	add.s64 	%rd817, %rd806, %rd816;
	add.s64 	%rd818, %rd817, %rd812;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10098,%dummy}, %rd807;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10099}, %rd807;
	}
	shf.r.wrap.b32 	%r10100, %r10099, %r10098, 14;
	shf.r.wrap.b32 	%r10101, %r10098, %r10099, 14;
	mov.b64 	%rd819, {%r10101, %r10100};
	shf.r.wrap.b32 	%r10102, %r10099, %r10098, 18;
	shf.r.wrap.b32 	%r10103, %r10098, %r10099, 18;
	mov.b64 	%rd820, {%r10103, %r10102};
	xor.b64  	%rd821, %rd820, %rd819;
	shf.l.wrap.b32 	%r10104, %r10098, %r10099, 23;
	shf.l.wrap.b32 	%r10105, %r10099, %r10098, 23;
	mov.b64 	%rd822, {%r10105, %r10104};
	xor.b64  	%rd823, %rd821, %rd822;
	xor.b64  	%rd824, %rd783, %rd759;
	and.b64  	%rd825, %rd807, %rd824;
	xor.b64  	%rd826, %rd825, %rd759;
	add.s64 	%rd827, %rd735, %rd616;
	add.s64 	%rd828, %rd827, %rd2774;
	add.s64 	%rd829, %rd828, %rd826;
	add.s64 	%rd830, %rd829, %rd823;
	add.s64 	%rd831, %rd830, %rd746;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10106,%dummy}, %rd818;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10107}, %rd818;
	}
	shf.r.wrap.b32 	%r10108, %r10107, %r10106, 28;
	shf.r.wrap.b32 	%r10109, %r10106, %r10107, 28;
	mov.b64 	%rd832, {%r10109, %r10108};
	shf.l.wrap.b32 	%r10110, %r10106, %r10107, 30;
	shf.l.wrap.b32 	%r10111, %r10107, %r10106, 30;
	mov.b64 	%rd833, {%r10111, %r10110};
	xor.b64  	%rd834, %rd833, %rd832;
	shf.l.wrap.b32 	%r10112, %r10106, %r10107, 25;
	shf.l.wrap.b32 	%r10113, %r10107, %r10106, 25;
	mov.b64 	%rd835, {%r10113, %r10112};
	xor.b64  	%rd836, %rd834, %rd835;
	xor.b64  	%rd837, %rd818, %rd770;
	xor.b64  	%rd838, %rd818, %rd794;
	and.b64  	%rd839, %rd838, %rd837;
	xor.b64  	%rd840, %rd839, %rd818;
	add.s64 	%rd841, %rd830, %rd840;
	add.s64 	%rd842, %rd841, %rd836;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10114,%dummy}, %rd831;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10115}, %rd831;
	}
	shf.r.wrap.b32 	%r10116, %r10115, %r10114, 14;
	shf.r.wrap.b32 	%r10117, %r10114, %r10115, 14;
	mov.b64 	%rd843, {%r10117, %r10116};
	shf.r.wrap.b32 	%r10118, %r10115, %r10114, 18;
	shf.r.wrap.b32 	%r10119, %r10114, %r10115, 18;
	mov.b64 	%rd844, {%r10119, %r10118};
	xor.b64  	%rd845, %rd844, %rd843;
	shf.l.wrap.b32 	%r10120, %r10114, %r10115, 23;
	shf.l.wrap.b32 	%r10121, %r10115, %r10114, 23;
	mov.b64 	%rd846, {%r10121, %r10120};
	xor.b64  	%rd847, %rd845, %rd846;
	xor.b64  	%rd848, %rd807, %rd783;
	and.b64  	%rd849, %rd831, %rd848;
	xor.b64  	%rd850, %rd849, %rd783;
	add.s64 	%rd851, %rd759, %rd624;
	add.s64 	%rd852, %rd851, %rd2775;
	add.s64 	%rd853, %rd852, %rd850;
	add.s64 	%rd854, %rd853, %rd847;
	add.s64 	%rd855, %rd854, %rd770;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10122,%dummy}, %rd842;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10123}, %rd842;
	}
	shf.r.wrap.b32 	%r10124, %r10123, %r10122, 28;
	shf.r.wrap.b32 	%r10125, %r10122, %r10123, 28;
	mov.b64 	%rd856, {%r10125, %r10124};
	shf.l.wrap.b32 	%r10126, %r10122, %r10123, 30;
	shf.l.wrap.b32 	%r10127, %r10123, %r10122, 30;
	mov.b64 	%rd857, {%r10127, %r10126};
	xor.b64  	%rd858, %rd857, %rd856;
	shf.l.wrap.b32 	%r10128, %r10122, %r10123, 25;
	shf.l.wrap.b32 	%r10129, %r10123, %r10122, 25;
	mov.b64 	%rd859, {%r10129, %r10128};
	xor.b64  	%rd860, %rd858, %rd859;
	xor.b64  	%rd861, %rd842, %rd794;
	xor.b64  	%rd862, %rd842, %rd818;
	and.b64  	%rd863, %rd862, %rd861;
	xor.b64  	%rd864, %rd863, %rd842;
	add.s64 	%rd865, %rd854, %rd864;
	add.s64 	%rd866, %rd865, %rd860;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10130,%dummy}, %rd855;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10131}, %rd855;
	}
	shf.r.wrap.b32 	%r10132, %r10131, %r10130, 14;
	shf.r.wrap.b32 	%r10133, %r10130, %r10131, 14;
	mov.b64 	%rd867, {%r10133, %r10132};
	shf.r.wrap.b32 	%r10134, %r10131, %r10130, 18;
	shf.r.wrap.b32 	%r10135, %r10130, %r10131, 18;
	mov.b64 	%rd868, {%r10135, %r10134};
	xor.b64  	%rd869, %rd868, %rd867;
	shf.l.wrap.b32 	%r10136, %r10130, %r10131, 23;
	shf.l.wrap.b32 	%r10137, %r10131, %r10130, 23;
	mov.b64 	%rd870, {%r10137, %r10136};
	xor.b64  	%rd871, %rd869, %rd870;
	xor.b64  	%rd872, %rd831, %rd807;
	and.b64  	%rd873, %rd855, %rd872;
	xor.b64  	%rd874, %rd873, %rd807;
	add.s64 	%rd875, %rd783, %rd631;
	add.s64 	%rd876, %rd875, %rd2776;
	add.s64 	%rd877, %rd876, %rd874;
	add.s64 	%rd878, %rd877, %rd871;
	add.s64 	%rd879, %rd878, %rd794;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10138,%dummy}, %rd866;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10139}, %rd866;
	}
	shf.r.wrap.b32 	%r10140, %r10139, %r10138, 28;
	shf.r.wrap.b32 	%r10141, %r10138, %r10139, 28;
	mov.b64 	%rd880, {%r10141, %r10140};
	shf.l.wrap.b32 	%r10142, %r10138, %r10139, 30;
	shf.l.wrap.b32 	%r10143, %r10139, %r10138, 30;
	mov.b64 	%rd881, {%r10143, %r10142};
	xor.b64  	%rd882, %rd881, %rd880;
	shf.l.wrap.b32 	%r10144, %r10138, %r10139, 25;
	shf.l.wrap.b32 	%r10145, %r10139, %r10138, 25;
	mov.b64 	%rd883, {%r10145, %r10144};
	xor.b64  	%rd884, %rd882, %rd883;
	xor.b64  	%rd885, %rd866, %rd818;
	xor.b64  	%rd886, %rd866, %rd842;
	and.b64  	%rd887, %rd886, %rd885;
	xor.b64  	%rd888, %rd887, %rd866;
	add.s64 	%rd889, %rd878, %rd888;
	add.s64 	%rd890, %rd889, %rd884;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10146,%dummy}, %rd879;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10147}, %rd879;
	}
	shf.r.wrap.b32 	%r10148, %r10147, %r10146, 14;
	shf.r.wrap.b32 	%r10149, %r10146, %r10147, 14;
	mov.b64 	%rd891, {%r10149, %r10148};
	shf.r.wrap.b32 	%r10150, %r10147, %r10146, 18;
	shf.r.wrap.b32 	%r10151, %r10146, %r10147, 18;
	mov.b64 	%rd892, {%r10151, %r10150};
	xor.b64  	%rd893, %rd892, %rd891;
	shf.l.wrap.b32 	%r10152, %r10146, %r10147, 23;
	shf.l.wrap.b32 	%r10153, %r10147, %r10146, 23;
	mov.b64 	%rd894, {%r10153, %r10152};
	xor.b64  	%rd895, %rd893, %rd894;
	xor.b64  	%rd896, %rd855, %rd831;
	and.b64  	%rd897, %rd879, %rd896;
	xor.b64  	%rd898, %rd897, %rd831;
	add.s64 	%rd899, %rd807, %rd638;
	add.s64 	%rd900, %rd899, %rd2777;
	add.s64 	%rd901, %rd900, %rd898;
	add.s64 	%rd902, %rd901, %rd895;
	add.s64 	%rd903, %rd902, %rd818;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10154,%dummy}, %rd890;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10155}, %rd890;
	}
	shf.r.wrap.b32 	%r10156, %r10155, %r10154, 28;
	shf.r.wrap.b32 	%r10157, %r10154, %r10155, 28;
	mov.b64 	%rd904, {%r10157, %r10156};
	shf.l.wrap.b32 	%r10158, %r10154, %r10155, 30;
	shf.l.wrap.b32 	%r10159, %r10155, %r10154, 30;
	mov.b64 	%rd905, {%r10159, %r10158};
	xor.b64  	%rd906, %rd905, %rd904;
	shf.l.wrap.b32 	%r10160, %r10154, %r10155, 25;
	shf.l.wrap.b32 	%r10161, %r10155, %r10154, 25;
	mov.b64 	%rd907, {%r10161, %r10160};
	xor.b64  	%rd908, %rd906, %rd907;
	xor.b64  	%rd909, %rd890, %rd842;
	xor.b64  	%rd910, %rd890, %rd866;
	and.b64  	%rd911, %rd910, %rd909;
	xor.b64  	%rd912, %rd911, %rd890;
	add.s64 	%rd913, %rd902, %rd912;
	add.s64 	%rd914, %rd913, %rd908;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10162,%dummy}, %rd903;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10163}, %rd903;
	}
	shf.r.wrap.b32 	%r10164, %r10163, %r10162, 14;
	shf.r.wrap.b32 	%r10165, %r10162, %r10163, 14;
	mov.b64 	%rd915, {%r10165, %r10164};
	shf.r.wrap.b32 	%r10166, %r10163, %r10162, 18;
	shf.r.wrap.b32 	%r10167, %r10162, %r10163, 18;
	mov.b64 	%rd916, {%r10167, %r10166};
	xor.b64  	%rd917, %rd916, %rd915;
	shf.l.wrap.b32 	%r10168, %r10162, %r10163, 23;
	shf.l.wrap.b32 	%r10169, %r10163, %r10162, 23;
	mov.b64 	%rd918, {%r10169, %r10168};
	xor.b64  	%rd919, %rd917, %rd918;
	xor.b64  	%rd920, %rd879, %rd855;
	and.b64  	%rd921, %rd903, %rd920;
	xor.b64  	%rd922, %rd921, %rd855;
	add.s64 	%rd923, %rd831, %rd645;
	add.s64 	%rd924, %rd923, %rd2778;
	add.s64 	%rd925, %rd924, %rd922;
	add.s64 	%rd926, %rd925, %rd919;
	add.s64 	%rd927, %rd926, %rd842;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10170,%dummy}, %rd914;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10171}, %rd914;
	}
	shf.r.wrap.b32 	%r10172, %r10171, %r10170, 28;
	shf.r.wrap.b32 	%r10173, %r10170, %r10171, 28;
	mov.b64 	%rd928, {%r10173, %r10172};
	shf.l.wrap.b32 	%r10174, %r10170, %r10171, 30;
	shf.l.wrap.b32 	%r10175, %r10171, %r10170, 30;
	mov.b64 	%rd929, {%r10175, %r10174};
	xor.b64  	%rd930, %rd929, %rd928;
	shf.l.wrap.b32 	%r10176, %r10170, %r10171, 25;
	shf.l.wrap.b32 	%r10177, %r10171, %r10170, 25;
	mov.b64 	%rd931, {%r10177, %r10176};
	xor.b64  	%rd932, %rd930, %rd931;
	xor.b64  	%rd933, %rd914, %rd866;
	xor.b64  	%rd934, %rd914, %rd890;
	and.b64  	%rd935, %rd934, %rd933;
	xor.b64  	%rd936, %rd935, %rd914;
	add.s64 	%rd937, %rd926, %rd936;
	add.s64 	%rd938, %rd937, %rd932;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10178,%dummy}, %rd927;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10179}, %rd927;
	}
	shf.r.wrap.b32 	%r10180, %r10179, %r10178, 14;
	shf.r.wrap.b32 	%r10181, %r10178, %r10179, 14;
	mov.b64 	%rd939, {%r10181, %r10180};
	shf.r.wrap.b32 	%r10182, %r10179, %r10178, 18;
	shf.r.wrap.b32 	%r10183, %r10178, %r10179, 18;
	mov.b64 	%rd940, {%r10183, %r10182};
	xor.b64  	%rd941, %rd940, %rd939;
	shf.l.wrap.b32 	%r10184, %r10178, %r10179, 23;
	shf.l.wrap.b32 	%r10185, %r10179, %r10178, 23;
	mov.b64 	%rd942, {%r10185, %r10184};
	xor.b64  	%rd943, %rd941, %rd942;
	xor.b64  	%rd944, %rd903, %rd879;
	and.b64  	%rd945, %rd927, %rd944;
	xor.b64  	%rd946, %rd945, %rd879;
	add.s64 	%rd947, %rd855, %rd652;
	add.s64 	%rd948, %rd947, %rd2779;
	add.s64 	%rd949, %rd948, %rd946;
	add.s64 	%rd950, %rd949, %rd943;
	add.s64 	%rd951, %rd950, %rd866;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10186,%dummy}, %rd938;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10187}, %rd938;
	}
	shf.r.wrap.b32 	%r10188, %r10187, %r10186, 28;
	shf.r.wrap.b32 	%r10189, %r10186, %r10187, 28;
	mov.b64 	%rd952, {%r10189, %r10188};
	shf.l.wrap.b32 	%r10190, %r10186, %r10187, 30;
	shf.l.wrap.b32 	%r10191, %r10187, %r10186, 30;
	mov.b64 	%rd953, {%r10191, %r10190};
	xor.b64  	%rd954, %rd953, %rd952;
	shf.l.wrap.b32 	%r10192, %r10186, %r10187, 25;
	shf.l.wrap.b32 	%r10193, %r10187, %r10186, 25;
	mov.b64 	%rd955, {%r10193, %r10192};
	xor.b64  	%rd956, %rd954, %rd955;
	xor.b64  	%rd957, %rd938, %rd890;
	xor.b64  	%rd958, %rd938, %rd914;
	and.b64  	%rd959, %rd958, %rd957;
	xor.b64  	%rd960, %rd959, %rd938;
	add.s64 	%rd961, %rd950, %rd960;
	add.s64 	%rd962, %rd961, %rd956;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10194,%dummy}, %rd951;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10195}, %rd951;
	}
	shf.r.wrap.b32 	%r10196, %r10195, %r10194, 14;
	shf.r.wrap.b32 	%r10197, %r10194, %r10195, 14;
	mov.b64 	%rd963, {%r10197, %r10196};
	shf.r.wrap.b32 	%r10198, %r10195, %r10194, 18;
	shf.r.wrap.b32 	%r10199, %r10194, %r10195, 18;
	mov.b64 	%rd964, {%r10199, %r10198};
	xor.b64  	%rd965, %rd964, %rd963;
	shf.l.wrap.b32 	%r10200, %r10194, %r10195, 23;
	shf.l.wrap.b32 	%r10201, %r10195, %r10194, 23;
	mov.b64 	%rd966, {%r10201, %r10200};
	xor.b64  	%rd967, %rd965, %rd966;
	xor.b64  	%rd968, %rd927, %rd903;
	and.b64  	%rd969, %rd951, %rd968;
	xor.b64  	%rd970, %rd969, %rd903;
	add.s64 	%rd971, %rd879, %rd659;
	add.s64 	%rd972, %rd971, %rd2780;
	add.s64 	%rd973, %rd972, %rd970;
	add.s64 	%rd974, %rd973, %rd967;
	add.s64 	%rd975, %rd974, %rd890;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10202,%dummy}, %rd962;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10203}, %rd962;
	}
	shf.r.wrap.b32 	%r10204, %r10203, %r10202, 28;
	shf.r.wrap.b32 	%r10205, %r10202, %r10203, 28;
	mov.b64 	%rd976, {%r10205, %r10204};
	shf.l.wrap.b32 	%r10206, %r10202, %r10203, 30;
	shf.l.wrap.b32 	%r10207, %r10203, %r10202, 30;
	mov.b64 	%rd977, {%r10207, %r10206};
	xor.b64  	%rd978, %rd977, %rd976;
	shf.l.wrap.b32 	%r10208, %r10202, %r10203, 25;
	shf.l.wrap.b32 	%r10209, %r10203, %r10202, 25;
	mov.b64 	%rd979, {%r10209, %r10208};
	xor.b64  	%rd980, %rd978, %rd979;
	xor.b64  	%rd981, %rd962, %rd914;
	xor.b64  	%rd982, %rd962, %rd938;
	and.b64  	%rd983, %rd982, %rd981;
	xor.b64  	%rd984, %rd983, %rd962;
	add.s64 	%rd985, %rd974, %rd984;
	add.s64 	%rd986, %rd985, %rd980;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10210,%dummy}, %rd975;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10211}, %rd975;
	}
	shf.r.wrap.b32 	%r10212, %r10211, %r10210, 14;
	shf.r.wrap.b32 	%r10213, %r10210, %r10211, 14;
	mov.b64 	%rd987, {%r10213, %r10212};
	shf.r.wrap.b32 	%r10214, %r10211, %r10210, 18;
	shf.r.wrap.b32 	%r10215, %r10210, %r10211, 18;
	mov.b64 	%rd988, {%r10215, %r10214};
	xor.b64  	%rd989, %rd988, %rd987;
	shf.l.wrap.b32 	%r10216, %r10210, %r10211, 23;
	shf.l.wrap.b32 	%r10217, %r10211, %r10210, 23;
	mov.b64 	%rd990, {%r10217, %r10216};
	xor.b64  	%rd991, %rd989, %rd990;
	xor.b64  	%rd992, %rd951, %rd927;
	and.b64  	%rd993, %rd975, %rd992;
	xor.b64  	%rd994, %rd993, %rd927;
	add.s64 	%rd995, %rd903, %rd666;
	add.s64 	%rd996, %rd995, %rd2781;
	add.s64 	%rd997, %rd996, %rd994;
	add.s64 	%rd998, %rd997, %rd991;
	add.s64 	%rd999, %rd998, %rd914;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10218,%dummy}, %rd986;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10219}, %rd986;
	}
	shf.r.wrap.b32 	%r10220, %r10219, %r10218, 28;
	shf.r.wrap.b32 	%r10221, %r10218, %r10219, 28;
	mov.b64 	%rd1000, {%r10221, %r10220};
	shf.l.wrap.b32 	%r10222, %r10218, %r10219, 30;
	shf.l.wrap.b32 	%r10223, %r10219, %r10218, 30;
	mov.b64 	%rd1001, {%r10223, %r10222};
	xor.b64  	%rd1002, %rd1001, %rd1000;
	shf.l.wrap.b32 	%r10224, %r10218, %r10219, 25;
	shf.l.wrap.b32 	%r10225, %r10219, %r10218, 25;
	mov.b64 	%rd1003, {%r10225, %r10224};
	xor.b64  	%rd1004, %rd1002, %rd1003;
	xor.b64  	%rd1005, %rd986, %rd938;
	xor.b64  	%rd1006, %rd986, %rd962;
	and.b64  	%rd1007, %rd1006, %rd1005;
	xor.b64  	%rd1008, %rd1007, %rd986;
	add.s64 	%rd1009, %rd998, %rd1008;
	add.s64 	%rd1010, %rd1009, %rd1004;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10226,%dummy}, %rd999;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10227}, %rd999;
	}
	shf.r.wrap.b32 	%r10228, %r10227, %r10226, 14;
	shf.r.wrap.b32 	%r10229, %r10226, %r10227, 14;
	mov.b64 	%rd1011, {%r10229, %r10228};
	shf.r.wrap.b32 	%r10230, %r10227, %r10226, 18;
	shf.r.wrap.b32 	%r10231, %r10226, %r10227, 18;
	mov.b64 	%rd1012, {%r10231, %r10230};
	xor.b64  	%rd1013, %rd1012, %rd1011;
	shf.l.wrap.b32 	%r10232, %r10226, %r10227, 23;
	shf.l.wrap.b32 	%r10233, %r10227, %r10226, 23;
	mov.b64 	%rd1014, {%r10233, %r10232};
	xor.b64  	%rd1015, %rd1013, %rd1014;
	xor.b64  	%rd1016, %rd975, %rd951;
	and.b64  	%rd1017, %rd999, %rd1016;
	xor.b64  	%rd1018, %rd1017, %rd951;
	add.s64 	%rd1019, %rd927, %rd673;
	add.s64 	%rd1020, %rd1019, %rd2782;
	add.s64 	%rd1021, %rd1020, %rd1018;
	add.s64 	%rd1022, %rd1021, %rd1015;
	add.s64 	%rd1023, %rd1022, %rd938;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10234,%dummy}, %rd1010;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10235}, %rd1010;
	}
	shf.r.wrap.b32 	%r10236, %r10235, %r10234, 28;
	shf.r.wrap.b32 	%r10237, %r10234, %r10235, 28;
	mov.b64 	%rd1024, {%r10237, %r10236};
	shf.l.wrap.b32 	%r10238, %r10234, %r10235, 30;
	shf.l.wrap.b32 	%r10239, %r10235, %r10234, 30;
	mov.b64 	%rd1025, {%r10239, %r10238};
	xor.b64  	%rd1026, %rd1025, %rd1024;
	shf.l.wrap.b32 	%r10240, %r10234, %r10235, 25;
	shf.l.wrap.b32 	%r10241, %r10235, %r10234, 25;
	mov.b64 	%rd1027, {%r10241, %r10240};
	xor.b64  	%rd1028, %rd1026, %rd1027;
	xor.b64  	%rd1029, %rd1010, %rd962;
	xor.b64  	%rd1030, %rd1010, %rd986;
	and.b64  	%rd1031, %rd1030, %rd1029;
	xor.b64  	%rd1032, %rd1031, %rd1010;
	add.s64 	%rd1033, %rd1022, %rd1032;
	add.s64 	%rd1034, %rd1033, %rd1028;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10242,%dummy}, %rd1023;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10243}, %rd1023;
	}
	shf.r.wrap.b32 	%r10244, %r10243, %r10242, 14;
	shf.r.wrap.b32 	%r10245, %r10242, %r10243, 14;
	mov.b64 	%rd1035, {%r10245, %r10244};
	shf.r.wrap.b32 	%r10246, %r10243, %r10242, 18;
	shf.r.wrap.b32 	%r10247, %r10242, %r10243, 18;
	mov.b64 	%rd1036, {%r10247, %r10246};
	xor.b64  	%rd1037, %rd1036, %rd1035;
	shf.l.wrap.b32 	%r10248, %r10242, %r10243, 23;
	shf.l.wrap.b32 	%r10249, %r10243, %r10242, 23;
	mov.b64 	%rd1038, {%r10249, %r10248};
	xor.b64  	%rd1039, %rd1037, %rd1038;
	xor.b64  	%rd1040, %rd999, %rd975;
	and.b64  	%rd1041, %rd1023, %rd1040;
	xor.b64  	%rd1042, %rd1041, %rd975;
	add.s64 	%rd1043, %rd951, %rd685;
	add.s64 	%rd1044, %rd1043, %rd2783;
	add.s64 	%rd1045, %rd1044, %rd1042;
	add.s64 	%rd1046, %rd1045, %rd1039;
	add.s64 	%rd1047, %rd1046, %rd962;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10250,%dummy}, %rd1034;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10251}, %rd1034;
	}
	shf.r.wrap.b32 	%r10252, %r10251, %r10250, 28;
	shf.r.wrap.b32 	%r10253, %r10250, %r10251, 28;
	mov.b64 	%rd1048, {%r10253, %r10252};
	shf.l.wrap.b32 	%r10254, %r10250, %r10251, 30;
	shf.l.wrap.b32 	%r10255, %r10251, %r10250, 30;
	mov.b64 	%rd1049, {%r10255, %r10254};
	xor.b64  	%rd1050, %rd1049, %rd1048;
	shf.l.wrap.b32 	%r10256, %r10250, %r10251, 25;
	shf.l.wrap.b32 	%r10257, %r10251, %r10250, 25;
	mov.b64 	%rd1051, {%r10257, %r10256};
	xor.b64  	%rd1052, %rd1050, %rd1051;
	xor.b64  	%rd1053, %rd1034, %rd986;
	xor.b64  	%rd1054, %rd1034, %rd1010;
	and.b64  	%rd1055, %rd1054, %rd1053;
	xor.b64  	%rd1056, %rd1055, %rd1034;
	add.s64 	%rd1057, %rd1046, %rd1056;
	add.s64 	%rd1058, %rd1057, %rd1052;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10258,%dummy}, %rd1047;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10259}, %rd1047;
	}
	shf.r.wrap.b32 	%r10260, %r10259, %r10258, 14;
	shf.r.wrap.b32 	%r10261, %r10258, %r10259, 14;
	mov.b64 	%rd1059, {%r10261, %r10260};
	shf.r.wrap.b32 	%r10262, %r10259, %r10258, 18;
	shf.r.wrap.b32 	%r10263, %r10258, %r10259, 18;
	mov.b64 	%rd1060, {%r10263, %r10262};
	xor.b64  	%rd1061, %rd1060, %rd1059;
	shf.l.wrap.b32 	%r10264, %r10258, %r10259, 23;
	shf.l.wrap.b32 	%r10265, %r10259, %r10258, 23;
	mov.b64 	%rd1062, {%r10265, %r10264};
	xor.b64  	%rd1063, %rd1061, %rd1062;
	xor.b64  	%rd1064, %rd1023, %rd999;
	and.b64  	%rd1065, %rd1047, %rd1064;
	xor.b64  	%rd1066, %rd1065, %rd999;
	add.s64 	%rd1067, %rd975, %rd698;
	add.s64 	%rd1068, %rd1067, %rd2784;
	add.s64 	%rd1069, %rd1068, %rd1066;
	add.s64 	%rd1070, %rd1069, %rd1063;
	add.s64 	%rd1071, %rd1070, %rd986;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10266,%dummy}, %rd1058;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10267}, %rd1058;
	}
	shf.r.wrap.b32 	%r10268, %r10267, %r10266, 28;
	shf.r.wrap.b32 	%r10269, %r10266, %r10267, 28;
	mov.b64 	%rd1072, {%r10269, %r10268};
	shf.l.wrap.b32 	%r10270, %r10266, %r10267, 30;
	shf.l.wrap.b32 	%r10271, %r10267, %r10266, 30;
	mov.b64 	%rd1073, {%r10271, %r10270};
	xor.b64  	%rd1074, %rd1073, %rd1072;
	shf.l.wrap.b32 	%r10272, %r10266, %r10267, 25;
	shf.l.wrap.b32 	%r10273, %r10267, %r10266, 25;
	mov.b64 	%rd1075, {%r10273, %r10272};
	xor.b64  	%rd1076, %rd1074, %rd1075;
	xor.b64  	%rd1077, %rd1058, %rd1010;
	xor.b64  	%rd1078, %rd1058, %rd1034;
	and.b64  	%rd1079, %rd1078, %rd1077;
	xor.b64  	%rd1080, %rd1079, %rd1058;
	add.s64 	%rd1081, %rd1070, %rd1080;
	add.s64 	%rd1082, %rd1081, %rd1076;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10274,%dummy}, %rd685;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10275}, %rd685;
	}
	shf.r.wrap.b32 	%r10276, %r10275, %r10274, 19;
	shf.r.wrap.b32 	%r10277, %r10274, %r10275, 19;
	mov.b64 	%rd1083, {%r10277, %r10276};
	shf.l.wrap.b32 	%r10278, %r10274, %r10275, 3;
	shf.l.wrap.b32 	%r10279, %r10275, %r10274, 3;
	mov.b64 	%rd1084, {%r10279, %r10278};
	shr.u64 	%rd1085, %rd685, 6;
	xor.b64  	%rd1086, %rd1083, %rd1085;
	xor.b64  	%rd1087, %rd1086, %rd1084;
	shf.r.wrap.b32 	%r10280, %r9915, %r9914, 1;
	shf.r.wrap.b32 	%r10281, %r9914, %r9915, 1;
	mov.b64 	%rd1088, {%r10281, %r10280};
	shf.r.wrap.b32 	%r10282, %r9915, %r9914, 8;
	shf.r.wrap.b32 	%r10283, %r9914, %r9915, 8;
	mov.b64 	%rd1089, {%r10283, %r10282};
	shr.u64 	%rd1090, %rd568, 7;
	xor.b64  	%rd1091, %rd1088, %rd1090;
	xor.b64  	%rd1092, %rd1091, %rd1089;
	add.s64 	%rd1093, %rd556, %rd645;
	add.s64 	%rd1094, %rd1093, %rd1087;
	add.s64 	%rd1095, %rd1094, %rd1092;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10284,%dummy}, %rd698;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10285}, %rd698;
	}
	shf.r.wrap.b32 	%r10286, %r10285, %r10284, 19;
	shf.r.wrap.b32 	%r10287, %r10284, %r10285, 19;
	mov.b64 	%rd1096, {%r10287, %r10286};
	shf.l.wrap.b32 	%r10288, %r10284, %r10285, 3;
	shf.l.wrap.b32 	%r10289, %r10285, %r10284, 3;
	mov.b64 	%rd1097, {%r10289, %r10288};
	shr.u64 	%rd1098, %rd698, 6;
	xor.b64  	%rd1099, %rd1096, %rd1098;
	xor.b64  	%rd1100, %rd1099, %rd1097;
	shf.r.wrap.b32 	%r10290, %r9927, %r9926, 1;
	shf.r.wrap.b32 	%r10291, %r9926, %r9927, 1;
	mov.b64 	%rd1101, {%r10291, %r10290};
	shf.r.wrap.b32 	%r10292, %r9927, %r9926, 8;
	shf.r.wrap.b32 	%r10293, %r9926, %r9927, 8;
	mov.b64 	%rd1102, {%r10293, %r10292};
	shr.u64 	%rd1103, %rd580, 7;
	xor.b64  	%rd1104, %rd1101, %rd1103;
	xor.b64  	%rd1105, %rd1104, %rd1102;
	add.s64 	%rd1106, %rd568, %rd652;
	add.s64 	%rd1107, %rd1106, %rd1100;
	add.s64 	%rd1108, %rd1107, %rd1105;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10294,%dummy}, %rd1095;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10295}, %rd1095;
	}
	shf.r.wrap.b32 	%r10296, %r10295, %r10294, 19;
	shf.r.wrap.b32 	%r10297, %r10294, %r10295, 19;
	mov.b64 	%rd1109, {%r10297, %r10296};
	shf.l.wrap.b32 	%r10298, %r10294, %r10295, 3;
	shf.l.wrap.b32 	%r10299, %r10295, %r10294, 3;
	mov.b64 	%rd1110, {%r10299, %r10298};
	shr.u64 	%rd1111, %rd1095, 6;
	xor.b64  	%rd1112, %rd1109, %rd1111;
	xor.b64  	%rd1113, %rd1112, %rd1110;
	shf.r.wrap.b32 	%r10300, %r9939, %r9938, 1;
	shf.r.wrap.b32 	%r10301, %r9938, %r9939, 1;
	mov.b64 	%rd1114, {%r10301, %r10300};
	shf.r.wrap.b32 	%r10302, %r9939, %r9938, 8;
	shf.r.wrap.b32 	%r10303, %r9938, %r9939, 8;
	mov.b64 	%rd1115, {%r10303, %r10302};
	shr.u64 	%rd1116, %rd592, 7;
	xor.b64  	%rd1117, %rd1114, %rd1116;
	xor.b64  	%rd1118, %rd1117, %rd1115;
	add.s64 	%rd1119, %rd580, %rd659;
	add.s64 	%rd1120, %rd1119, %rd1113;
	add.s64 	%rd1121, %rd1120, %rd1118;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10304,%dummy}, %rd1108;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10305}, %rd1108;
	}
	shf.r.wrap.b32 	%r10306, %r10305, %r10304, 19;
	shf.r.wrap.b32 	%r10307, %r10304, %r10305, 19;
	mov.b64 	%rd1122, {%r10307, %r10306};
	shf.l.wrap.b32 	%r10308, %r10304, %r10305, 3;
	shf.l.wrap.b32 	%r10309, %r10305, %r10304, 3;
	mov.b64 	%rd1123, {%r10309, %r10308};
	shr.u64 	%rd1124, %rd1108, 6;
	xor.b64  	%rd1125, %rd1122, %rd1124;
	xor.b64  	%rd1126, %rd1125, %rd1123;
	shf.r.wrap.b32 	%r10310, %r9951, %r9950, 1;
	shf.r.wrap.b32 	%r10311, %r9950, %r9951, 1;
	mov.b64 	%rd1127, {%r10311, %r10310};
	shf.r.wrap.b32 	%r10312, %r9951, %r9950, 8;
	shf.r.wrap.b32 	%r10313, %r9950, %r9951, 8;
	mov.b64 	%rd1128, {%r10313, %r10312};
	shr.u64 	%rd1129, %rd604, 7;
	xor.b64  	%rd1130, %rd1127, %rd1129;
	xor.b64  	%rd1131, %rd1130, %rd1128;
	add.s64 	%rd1132, %rd592, %rd666;
	add.s64 	%rd1133, %rd1132, %rd1126;
	add.s64 	%rd1134, %rd1133, %rd1131;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10314,%dummy}, %rd1121;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10315}, %rd1121;
	}
	shf.r.wrap.b32 	%r10316, %r10315, %r10314, 19;
	shf.r.wrap.b32 	%r10317, %r10314, %r10315, 19;
	mov.b64 	%rd1135, {%r10317, %r10316};
	shf.l.wrap.b32 	%r10318, %r10314, %r10315, 3;
	shf.l.wrap.b32 	%r10319, %r10315, %r10314, 3;
	mov.b64 	%rd1136, {%r10319, %r10318};
	shr.u64 	%rd1137, %rd1121, 6;
	xor.b64  	%rd1138, %rd1135, %rd1137;
	xor.b64  	%rd1139, %rd1138, %rd1136;
	shf.r.wrap.b32 	%r10320, %r9957, %r9956, 1;
	shf.r.wrap.b32 	%r10321, %r9956, %r9957, 1;
	mov.b64 	%rd1140, {%r10321, %r10320};
	shf.r.wrap.b32 	%r10322, %r9957, %r9956, 8;
	shf.r.wrap.b32 	%r10323, %r9956, %r9957, 8;
	mov.b64 	%rd1141, {%r10323, %r10322};
	shr.u64 	%rd1142, %rd616, 7;
	xor.b64  	%rd1143, %rd1140, %rd1142;
	xor.b64  	%rd1144, %rd1143, %rd1141;
	add.s64 	%rd1145, %rd604, %rd673;
	add.s64 	%rd1146, %rd1145, %rd1139;
	add.s64 	%rd1147, %rd1146, %rd1144;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10324,%dummy}, %rd1134;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10325}, %rd1134;
	}
	shf.r.wrap.b32 	%r10326, %r10325, %r10324, 19;
	shf.r.wrap.b32 	%r10327, %r10324, %r10325, 19;
	mov.b64 	%rd1148, {%r10327, %r10326};
	shf.l.wrap.b32 	%r10328, %r10324, %r10325, 3;
	shf.l.wrap.b32 	%r10329, %r10325, %r10324, 3;
	mov.b64 	%rd1149, {%r10329, %r10328};
	shr.u64 	%rd1150, %rd1134, 6;
	xor.b64  	%rd1151, %rd1148, %rd1150;
	xor.b64  	%rd1152, %rd1151, %rd1149;
	shf.r.wrap.b32 	%r10330, %r9963, %r9962, 1;
	shf.r.wrap.b32 	%r10331, %r9962, %r9963, 1;
	mov.b64 	%rd1153, {%r10331, %r10330};
	shf.r.wrap.b32 	%r10332, %r9963, %r9962, 8;
	shf.r.wrap.b32 	%r10333, %r9962, %r9963, 8;
	mov.b64 	%rd1154, {%r10333, %r10332};
	shr.u64 	%rd1155, %rd624, 7;
	xor.b64  	%rd1156, %rd1153, %rd1155;
	xor.b64  	%rd1157, %rd1156, %rd1154;
	add.s64 	%rd1158, %rd616, %rd685;
	add.s64 	%rd1159, %rd1158, %rd1152;
	add.s64 	%rd1160, %rd1159, %rd1157;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10334,%dummy}, %rd1147;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10335}, %rd1147;
	}
	shf.r.wrap.b32 	%r10336, %r10335, %r10334, 19;
	shf.r.wrap.b32 	%r10337, %r10334, %r10335, 19;
	mov.b64 	%rd1161, {%r10337, %r10336};
	shf.l.wrap.b32 	%r10338, %r10334, %r10335, 3;
	shf.l.wrap.b32 	%r10339, %r10335, %r10334, 3;
	mov.b64 	%rd1162, {%r10339, %r10338};
	shr.u64 	%rd1163, %rd1147, 6;
	xor.b64  	%rd1164, %rd1161, %rd1163;
	xor.b64  	%rd1165, %rd1164, %rd1162;
	shf.r.wrap.b32 	%r10340, %r9969, %r9968, 1;
	shf.r.wrap.b32 	%r10341, %r9968, %r9969, 1;
	mov.b64 	%rd1166, {%r10341, %r10340};
	shf.r.wrap.b32 	%r10342, %r9969, %r9968, 8;
	shf.r.wrap.b32 	%r10343, %r9968, %r9969, 8;
	mov.b64 	%rd1167, {%r10343, %r10342};
	shr.u64 	%rd1168, %rd631, 7;
	xor.b64  	%rd1169, %rd1166, %rd1168;
	xor.b64  	%rd1170, %rd1169, %rd1167;
	add.s64 	%rd1171, %rd624, %rd698;
	add.s64 	%rd1172, %rd1171, %rd1165;
	add.s64 	%rd1173, %rd1172, %rd1170;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10344,%dummy}, %rd1160;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10345}, %rd1160;
	}
	shf.r.wrap.b32 	%r10346, %r10345, %r10344, 19;
	shf.r.wrap.b32 	%r10347, %r10344, %r10345, 19;
	mov.b64 	%rd1174, {%r10347, %r10346};
	shf.l.wrap.b32 	%r10348, %r10344, %r10345, 3;
	shf.l.wrap.b32 	%r10349, %r10345, %r10344, 3;
	mov.b64 	%rd1175, {%r10349, %r10348};
	shr.u64 	%rd1176, %rd1160, 6;
	xor.b64  	%rd1177, %rd1174, %rd1176;
	xor.b64  	%rd1178, %rd1177, %rd1175;
	shf.r.wrap.b32 	%r10350, %r9975, %r9974, 1;
	shf.r.wrap.b32 	%r10351, %r9974, %r9975, 1;
	mov.b64 	%rd1179, {%r10351, %r10350};
	shf.r.wrap.b32 	%r10352, %r9975, %r9974, 8;
	shf.r.wrap.b32 	%r10353, %r9974, %r9975, 8;
	mov.b64 	%rd1180, {%r10353, %r10352};
	shr.u64 	%rd1181, %rd638, 7;
	xor.b64  	%rd1182, %rd1179, %rd1181;
	xor.b64  	%rd1183, %rd1182, %rd1180;
	add.s64 	%rd1184, %rd1095, %rd631;
	add.s64 	%rd1185, %rd1184, %rd1178;
	add.s64 	%rd1186, %rd1185, %rd1183;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10354,%dummy}, %rd1173;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10355}, %rd1173;
	}
	shf.r.wrap.b32 	%r10356, %r10355, %r10354, 19;
	shf.r.wrap.b32 	%r10357, %r10354, %r10355, 19;
	mov.b64 	%rd1187, {%r10357, %r10356};
	shf.l.wrap.b32 	%r10358, %r10354, %r10355, 3;
	shf.l.wrap.b32 	%r10359, %r10355, %r10354, 3;
	mov.b64 	%rd1188, {%r10359, %r10358};
	shr.u64 	%rd1189, %rd1173, 6;
	xor.b64  	%rd1190, %rd1187, %rd1189;
	xor.b64  	%rd1191, %rd1190, %rd1188;
	shf.r.wrap.b32 	%r10360, %r9981, %r9980, 1;
	shf.r.wrap.b32 	%r10361, %r9980, %r9981, 1;
	mov.b64 	%rd1192, {%r10361, %r10360};
	shf.r.wrap.b32 	%r10362, %r9981, %r9980, 8;
	shf.r.wrap.b32 	%r10363, %r9980, %r9981, 8;
	mov.b64 	%rd1193, {%r10363, %r10362};
	shr.u64 	%rd1194, %rd645, 7;
	xor.b64  	%rd1195, %rd1192, %rd1194;
	xor.b64  	%rd1196, %rd1195, %rd1193;
	add.s64 	%rd1197, %rd1108, %rd638;
	add.s64 	%rd1198, %rd1197, %rd1191;
	add.s64 	%rd1199, %rd1198, %rd1196;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10364,%dummy}, %rd1186;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10365}, %rd1186;
	}
	shf.r.wrap.b32 	%r10366, %r10365, %r10364, 19;
	shf.r.wrap.b32 	%r10367, %r10364, %r10365, 19;
	mov.b64 	%rd1200, {%r10367, %r10366};
	shf.l.wrap.b32 	%r10368, %r10364, %r10365, 3;
	shf.l.wrap.b32 	%r10369, %r10365, %r10364, 3;
	mov.b64 	%rd1201, {%r10369, %r10368};
	shr.u64 	%rd1202, %rd1186, 6;
	xor.b64  	%rd1203, %rd1200, %rd1202;
	xor.b64  	%rd1204, %rd1203, %rd1201;
	shf.r.wrap.b32 	%r10370, %r9987, %r9986, 1;
	shf.r.wrap.b32 	%r10371, %r9986, %r9987, 1;
	mov.b64 	%rd1205, {%r10371, %r10370};
	shf.r.wrap.b32 	%r10372, %r9987, %r9986, 8;
	shf.r.wrap.b32 	%r10373, %r9986, %r9987, 8;
	mov.b64 	%rd1206, {%r10373, %r10372};
	shr.u64 	%rd1207, %rd652, 7;
	xor.b64  	%rd1208, %rd1205, %rd1207;
	xor.b64  	%rd1209, %rd1208, %rd1206;
	add.s64 	%rd1210, %rd1121, %rd645;
	add.s64 	%rd1211, %rd1210, %rd1204;
	add.s64 	%rd1212, %rd1211, %rd1209;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10374,%dummy}, %rd1199;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10375}, %rd1199;
	}
	shf.r.wrap.b32 	%r10376, %r10375, %r10374, 19;
	shf.r.wrap.b32 	%r10377, %r10374, %r10375, 19;
	mov.b64 	%rd1213, {%r10377, %r10376};
	shf.l.wrap.b32 	%r10378, %r10374, %r10375, 3;
	shf.l.wrap.b32 	%r10379, %r10375, %r10374, 3;
	mov.b64 	%rd1214, {%r10379, %r10378};
	shr.u64 	%rd1215, %rd1199, 6;
	xor.b64  	%rd1216, %rd1213, %rd1215;
	xor.b64  	%rd1217, %rd1216, %rd1214;
	shf.r.wrap.b32 	%r10380, %r9993, %r9992, 1;
	shf.r.wrap.b32 	%r10381, %r9992, %r9993, 1;
	mov.b64 	%rd1218, {%r10381, %r10380};
	shf.r.wrap.b32 	%r10382, %r9993, %r9992, 8;
	shf.r.wrap.b32 	%r10383, %r9992, %r9993, 8;
	mov.b64 	%rd1219, {%r10383, %r10382};
	shr.u64 	%rd1220, %rd659, 7;
	xor.b64  	%rd1221, %rd1218, %rd1220;
	xor.b64  	%rd1222, %rd1221, %rd1219;
	add.s64 	%rd1223, %rd1134, %rd652;
	add.s64 	%rd1224, %rd1223, %rd1217;
	add.s64 	%rd1225, %rd1224, %rd1222;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10384,%dummy}, %rd1212;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10385}, %rd1212;
	}
	shf.r.wrap.b32 	%r10386, %r10385, %r10384, 19;
	shf.r.wrap.b32 	%r10387, %r10384, %r10385, 19;
	mov.b64 	%rd1226, {%r10387, %r10386};
	shf.l.wrap.b32 	%r10388, %r10384, %r10385, 3;
	shf.l.wrap.b32 	%r10389, %r10385, %r10384, 3;
	mov.b64 	%rd1227, {%r10389, %r10388};
	shr.u64 	%rd1228, %rd1212, 6;
	xor.b64  	%rd1229, %rd1226, %rd1228;
	xor.b64  	%rd1230, %rd1229, %rd1227;
	shf.r.wrap.b32 	%r10390, %r9999, %r9998, 1;
	shf.r.wrap.b32 	%r10391, %r9998, %r9999, 1;
	mov.b64 	%rd1231, {%r10391, %r10390};
	shf.r.wrap.b32 	%r10392, %r9999, %r9998, 8;
	shf.r.wrap.b32 	%r10393, %r9998, %r9999, 8;
	mov.b64 	%rd1232, {%r10393, %r10392};
	shr.u64 	%rd1233, %rd666, 7;
	xor.b64  	%rd1234, %rd1231, %rd1233;
	xor.b64  	%rd1235, %rd1234, %rd1232;
	add.s64 	%rd1236, %rd1147, %rd659;
	add.s64 	%rd1237, %rd1236, %rd1230;
	add.s64 	%rd1238, %rd1237, %rd1235;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10394,%dummy}, %rd1225;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10395}, %rd1225;
	}
	shf.r.wrap.b32 	%r10396, %r10395, %r10394, 19;
	shf.r.wrap.b32 	%r10397, %r10394, %r10395, 19;
	mov.b64 	%rd1239, {%r10397, %r10396};
	shf.l.wrap.b32 	%r10398, %r10394, %r10395, 3;
	shf.l.wrap.b32 	%r10399, %r10395, %r10394, 3;
	mov.b64 	%rd1240, {%r10399, %r10398};
	shr.u64 	%rd1241, %rd1225, 6;
	xor.b64  	%rd1242, %rd1239, %rd1241;
	xor.b64  	%rd1243, %rd1242, %rd1240;
	shf.r.wrap.b32 	%r10400, %r10009, %r10008, 1;
	shf.r.wrap.b32 	%r10401, %r10008, %r10009, 1;
	mov.b64 	%rd1244, {%r10401, %r10400};
	shf.r.wrap.b32 	%r10402, %r10009, %r10008, 8;
	shf.r.wrap.b32 	%r10403, %r10008, %r10009, 8;
	mov.b64 	%rd1245, {%r10403, %r10402};
	shr.u64 	%rd1246, %rd673, 7;
	xor.b64  	%rd1247, %rd1244, %rd1246;
	xor.b64  	%rd1248, %rd1247, %rd1245;
	add.s64 	%rd1249, %rd1160, %rd666;
	add.s64 	%rd1250, %rd1249, %rd1243;
	add.s64 	%rd1251, %rd1250, %rd1248;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10404,%dummy}, %rd1238;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10405}, %rd1238;
	}
	shf.r.wrap.b32 	%r10406, %r10405, %r10404, 19;
	shf.r.wrap.b32 	%r10407, %r10404, %r10405, 19;
	mov.b64 	%rd1252, {%r10407, %r10406};
	shf.l.wrap.b32 	%r10408, %r10404, %r10405, 3;
	shf.l.wrap.b32 	%r10409, %r10405, %r10404, 3;
	mov.b64 	%rd1253, {%r10409, %r10408};
	shr.u64 	%rd1254, %rd1238, 6;
	xor.b64  	%rd1255, %rd1252, %rd1254;
	xor.b64  	%rd1256, %rd1255, %rd1253;
	shf.r.wrap.b32 	%r10410, %r10275, %r10274, 1;
	shf.r.wrap.b32 	%r10411, %r10274, %r10275, 1;
	mov.b64 	%rd1257, {%r10411, %r10410};
	shf.r.wrap.b32 	%r10412, %r10275, %r10274, 8;
	shf.r.wrap.b32 	%r10413, %r10274, %r10275, 8;
	mov.b64 	%rd1258, {%r10413, %r10412};
	shr.u64 	%rd1259, %rd685, 7;
	xor.b64  	%rd1260, %rd1257, %rd1259;
	xor.b64  	%rd1261, %rd1260, %rd1258;
	add.s64 	%rd1262, %rd1173, %rd673;
	add.s64 	%rd1263, %rd1262, %rd1256;
	add.s64 	%rd1264, %rd1263, %rd1261;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10414,%dummy}, %rd1251;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10415}, %rd1251;
	}
	shf.r.wrap.b32 	%r10416, %r10415, %r10414, 19;
	shf.r.wrap.b32 	%r10417, %r10414, %r10415, 19;
	mov.b64 	%rd1265, {%r10417, %r10416};
	shf.l.wrap.b32 	%r10418, %r10414, %r10415, 3;
	shf.l.wrap.b32 	%r10419, %r10415, %r10414, 3;
	mov.b64 	%rd1266, {%r10419, %r10418};
	shr.u64 	%rd1267, %rd1251, 6;
	xor.b64  	%rd1268, %rd1265, %rd1267;
	xor.b64  	%rd1269, %rd1268, %rd1266;
	shf.r.wrap.b32 	%r10420, %r10285, %r10284, 1;
	shf.r.wrap.b32 	%r10421, %r10284, %r10285, 1;
	mov.b64 	%rd1270, {%r10421, %r10420};
	shf.r.wrap.b32 	%r10422, %r10285, %r10284, 8;
	shf.r.wrap.b32 	%r10423, %r10284, %r10285, 8;
	mov.b64 	%rd1271, {%r10423, %r10422};
	shr.u64 	%rd1272, %rd698, 7;
	xor.b64  	%rd1273, %rd1270, %rd1272;
	xor.b64  	%rd1274, %rd1273, %rd1271;
	add.s64 	%rd1275, %rd1186, %rd685;
	add.s64 	%rd1276, %rd1275, %rd1269;
	add.s64 	%rd1277, %rd1276, %rd1274;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10424,%dummy}, %rd1264;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10425}, %rd1264;
	}
	shf.r.wrap.b32 	%r10426, %r10425, %r10424, 19;
	shf.r.wrap.b32 	%r10427, %r10424, %r10425, 19;
	mov.b64 	%rd1278, {%r10427, %r10426};
	shf.l.wrap.b32 	%r10428, %r10424, %r10425, 3;
	shf.l.wrap.b32 	%r10429, %r10425, %r10424, 3;
	mov.b64 	%rd1279, {%r10429, %r10428};
	shr.u64 	%rd1280, %rd1264, 6;
	xor.b64  	%rd1281, %rd1278, %rd1280;
	xor.b64  	%rd1282, %rd1281, %rd1279;
	shf.r.wrap.b32 	%r10430, %r10295, %r10294, 1;
	shf.r.wrap.b32 	%r10431, %r10294, %r10295, 1;
	mov.b64 	%rd1283, {%r10431, %r10430};
	shf.r.wrap.b32 	%r10432, %r10295, %r10294, 8;
	shf.r.wrap.b32 	%r10433, %r10294, %r10295, 8;
	mov.b64 	%rd1284, {%r10433, %r10432};
	shr.u64 	%rd1285, %rd1095, 7;
	xor.b64  	%rd1286, %rd1283, %rd1285;
	xor.b64  	%rd1287, %rd1286, %rd1284;
	add.s64 	%rd1288, %rd1199, %rd698;
	add.s64 	%rd1289, %rd1288, %rd1282;
	add.s64 	%rd1290, %rd1289, %rd1287;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10434,%dummy}, %rd1071;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10435}, %rd1071;
	}
	shf.r.wrap.b32 	%r10436, %r10435, %r10434, 14;
	shf.r.wrap.b32 	%r10437, %r10434, %r10435, 14;
	mov.b64 	%rd1291, {%r10437, %r10436};
	shf.r.wrap.b32 	%r10438, %r10435, %r10434, 18;
	shf.r.wrap.b32 	%r10439, %r10434, %r10435, 18;
	mov.b64 	%rd1292, {%r10439, %r10438};
	xor.b64  	%rd1293, %rd1292, %rd1291;
	shf.l.wrap.b32 	%r10440, %r10434, %r10435, 23;
	shf.l.wrap.b32 	%r10441, %r10435, %r10434, 23;
	mov.b64 	%rd1294, {%r10441, %r10440};
	xor.b64  	%rd1295, %rd1293, %rd1294;
	xor.b64  	%rd1296, %rd1023, %rd1047;
	and.b64  	%rd1297, %rd1296, %rd1071;
	xor.b64  	%rd1298, %rd1297, %rd1023;
	add.s64 	%rd1299, %rd1298, %rd999;
	add.s64 	%rd1300, %rd1299, %rd1095;
	add.s64 	%rd1301, %rd1300, %rd2785;
	add.s64 	%rd1302, %rd1301, %rd1295;
	add.s64 	%rd1303, %rd1302, %rd1010;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10442,%dummy}, %rd1082;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10443}, %rd1082;
	}
	shf.r.wrap.b32 	%r10444, %r10443, %r10442, 28;
	shf.r.wrap.b32 	%r10445, %r10442, %r10443, 28;
	mov.b64 	%rd1304, {%r10445, %r10444};
	shf.l.wrap.b32 	%r10446, %r10442, %r10443, 30;
	shf.l.wrap.b32 	%r10447, %r10443, %r10442, 30;
	mov.b64 	%rd1305, {%r10447, %r10446};
	xor.b64  	%rd1306, %rd1305, %rd1304;
	shf.l.wrap.b32 	%r10448, %r10442, %r10443, 25;
	shf.l.wrap.b32 	%r10449, %r10443, %r10442, 25;
	mov.b64 	%rd1307, {%r10449, %r10448};
	xor.b64  	%rd1308, %rd1306, %rd1307;
	xor.b64  	%rd1309, %rd1082, %rd1034;
	xor.b64  	%rd1310, %rd1082, %rd1058;
	and.b64  	%rd1311, %rd1310, %rd1309;
	xor.b64  	%rd1312, %rd1311, %rd1082;
	add.s64 	%rd1313, %rd1302, %rd1312;
	add.s64 	%rd1314, %rd1313, %rd1308;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10450,%dummy}, %rd1303;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10451}, %rd1303;
	}
	shf.r.wrap.b32 	%r10452, %r10451, %r10450, 14;
	shf.r.wrap.b32 	%r10453, %r10450, %r10451, 14;
	mov.b64 	%rd1315, {%r10453, %r10452};
	shf.r.wrap.b32 	%r10454, %r10451, %r10450, 18;
	shf.r.wrap.b32 	%r10455, %r10450, %r10451, 18;
	mov.b64 	%rd1316, {%r10455, %r10454};
	xor.b64  	%rd1317, %rd1316, %rd1315;
	shf.l.wrap.b32 	%r10456, %r10450, %r10451, 23;
	shf.l.wrap.b32 	%r10457, %r10451, %r10450, 23;
	mov.b64 	%rd1318, {%r10457, %r10456};
	xor.b64  	%rd1319, %rd1317, %rd1318;
	xor.b64  	%rd1320, %rd1047, %rd1071;
	and.b64  	%rd1321, %rd1303, %rd1320;
	xor.b64  	%rd1322, %rd1321, %rd1047;
	add.s64 	%rd1323, %rd1108, %rd1023;
	add.s64 	%rd1324, %rd1323, %rd2786;
	add.s64 	%rd1325, %rd1324, %rd1322;
	add.s64 	%rd1326, %rd1325, %rd1319;
	add.s64 	%rd1327, %rd1326, %rd1034;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10458,%dummy}, %rd1314;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10459}, %rd1314;
	}
	shf.r.wrap.b32 	%r10460, %r10459, %r10458, 28;
	shf.r.wrap.b32 	%r10461, %r10458, %r10459, 28;
	mov.b64 	%rd1328, {%r10461, %r10460};
	shf.l.wrap.b32 	%r10462, %r10458, %r10459, 30;
	shf.l.wrap.b32 	%r10463, %r10459, %r10458, 30;
	mov.b64 	%rd1329, {%r10463, %r10462};
	xor.b64  	%rd1330, %rd1329, %rd1328;
	shf.l.wrap.b32 	%r10464, %r10458, %r10459, 25;
	shf.l.wrap.b32 	%r10465, %r10459, %r10458, 25;
	mov.b64 	%rd1331, {%r10465, %r10464};
	xor.b64  	%rd1332, %rd1330, %rd1331;
	xor.b64  	%rd1333, %rd1314, %rd1058;
	xor.b64  	%rd1334, %rd1314, %rd1082;
	and.b64  	%rd1335, %rd1334, %rd1333;
	xor.b64  	%rd1336, %rd1335, %rd1314;
	add.s64 	%rd1337, %rd1326, %rd1336;
	add.s64 	%rd1338, %rd1337, %rd1332;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10466,%dummy}, %rd1327;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10467}, %rd1327;
	}
	shf.r.wrap.b32 	%r10468, %r10467, %r10466, 14;
	shf.r.wrap.b32 	%r10469, %r10466, %r10467, 14;
	mov.b64 	%rd1339, {%r10469, %r10468};
	shf.r.wrap.b32 	%r10470, %r10467, %r10466, 18;
	shf.r.wrap.b32 	%r10471, %r10466, %r10467, 18;
	mov.b64 	%rd1340, {%r10471, %r10470};
	xor.b64  	%rd1341, %rd1340, %rd1339;
	shf.l.wrap.b32 	%r10472, %r10466, %r10467, 23;
	shf.l.wrap.b32 	%r10473, %r10467, %r10466, 23;
	mov.b64 	%rd1342, {%r10473, %r10472};
	xor.b64  	%rd1343, %rd1341, %rd1342;
	xor.b64  	%rd1344, %rd1303, %rd1071;
	and.b64  	%rd1345, %rd1327, %rd1344;
	xor.b64  	%rd1346, %rd1345, %rd1071;
	add.s64 	%rd1347, %rd1121, %rd1047;
	add.s64 	%rd1348, %rd1347, %rd2787;
	add.s64 	%rd1349, %rd1348, %rd1346;
	add.s64 	%rd1350, %rd1349, %rd1343;
	add.s64 	%rd1351, %rd1350, %rd1058;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10474,%dummy}, %rd1338;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10475}, %rd1338;
	}
	shf.r.wrap.b32 	%r10476, %r10475, %r10474, 28;
	shf.r.wrap.b32 	%r10477, %r10474, %r10475, 28;
	mov.b64 	%rd1352, {%r10477, %r10476};
	shf.l.wrap.b32 	%r10478, %r10474, %r10475, 30;
	shf.l.wrap.b32 	%r10479, %r10475, %r10474, 30;
	mov.b64 	%rd1353, {%r10479, %r10478};
	xor.b64  	%rd1354, %rd1353, %rd1352;
	shf.l.wrap.b32 	%r10480, %r10474, %r10475, 25;
	shf.l.wrap.b32 	%r10481, %r10475, %r10474, 25;
	mov.b64 	%rd1355, {%r10481, %r10480};
	xor.b64  	%rd1356, %rd1354, %rd1355;
	xor.b64  	%rd1357, %rd1338, %rd1082;
	xor.b64  	%rd1358, %rd1338, %rd1314;
	and.b64  	%rd1359, %rd1358, %rd1357;
	xor.b64  	%rd1360, %rd1359, %rd1338;
	add.s64 	%rd1361, %rd1350, %rd1360;
	add.s64 	%rd1362, %rd1361, %rd1356;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10482,%dummy}, %rd1351;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10483}, %rd1351;
	}
	shf.r.wrap.b32 	%r10484, %r10483, %r10482, 14;
	shf.r.wrap.b32 	%r10485, %r10482, %r10483, 14;
	mov.b64 	%rd1363, {%r10485, %r10484};
	shf.r.wrap.b32 	%r10486, %r10483, %r10482, 18;
	shf.r.wrap.b32 	%r10487, %r10482, %r10483, 18;
	mov.b64 	%rd1364, {%r10487, %r10486};
	xor.b64  	%rd1365, %rd1364, %rd1363;
	shf.l.wrap.b32 	%r10488, %r10482, %r10483, 23;
	shf.l.wrap.b32 	%r10489, %r10483, %r10482, 23;
	mov.b64 	%rd1366, {%r10489, %r10488};
	xor.b64  	%rd1367, %rd1365, %rd1366;
	xor.b64  	%rd1368, %rd1327, %rd1303;
	and.b64  	%rd1369, %rd1351, %rd1368;
	xor.b64  	%rd1370, %rd1369, %rd1303;
	add.s64 	%rd1371, %rd1134, %rd1071;
	add.s64 	%rd1372, %rd1371, %rd2788;
	add.s64 	%rd1373, %rd1372, %rd1370;
	add.s64 	%rd1374, %rd1373, %rd1367;
	add.s64 	%rd1375, %rd1374, %rd1082;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10490,%dummy}, %rd1362;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10491}, %rd1362;
	}
	shf.r.wrap.b32 	%r10492, %r10491, %r10490, 28;
	shf.r.wrap.b32 	%r10493, %r10490, %r10491, 28;
	mov.b64 	%rd1376, {%r10493, %r10492};
	shf.l.wrap.b32 	%r10494, %r10490, %r10491, 30;
	shf.l.wrap.b32 	%r10495, %r10491, %r10490, 30;
	mov.b64 	%rd1377, {%r10495, %r10494};
	xor.b64  	%rd1378, %rd1377, %rd1376;
	shf.l.wrap.b32 	%r10496, %r10490, %r10491, 25;
	shf.l.wrap.b32 	%r10497, %r10491, %r10490, 25;
	mov.b64 	%rd1379, {%r10497, %r10496};
	xor.b64  	%rd1380, %rd1378, %rd1379;
	xor.b64  	%rd1381, %rd1362, %rd1314;
	xor.b64  	%rd1382, %rd1362, %rd1338;
	and.b64  	%rd1383, %rd1382, %rd1381;
	xor.b64  	%rd1384, %rd1383, %rd1362;
	add.s64 	%rd1385, %rd1374, %rd1384;
	add.s64 	%rd1386, %rd1385, %rd1380;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10498,%dummy}, %rd1375;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10499}, %rd1375;
	}
	shf.r.wrap.b32 	%r10500, %r10499, %r10498, 14;
	shf.r.wrap.b32 	%r10501, %r10498, %r10499, 14;
	mov.b64 	%rd1387, {%r10501, %r10500};
	shf.r.wrap.b32 	%r10502, %r10499, %r10498, 18;
	shf.r.wrap.b32 	%r10503, %r10498, %r10499, 18;
	mov.b64 	%rd1388, {%r10503, %r10502};
	xor.b64  	%rd1389, %rd1388, %rd1387;
	shf.l.wrap.b32 	%r10504, %r10498, %r10499, 23;
	shf.l.wrap.b32 	%r10505, %r10499, %r10498, 23;
	mov.b64 	%rd1390, {%r10505, %r10504};
	xor.b64  	%rd1391, %rd1389, %rd1390;
	xor.b64  	%rd1392, %rd1351, %rd1327;
	and.b64  	%rd1393, %rd1375, %rd1392;
	xor.b64  	%rd1394, %rd1393, %rd1327;
	add.s64 	%rd1395, %rd1303, %rd1147;
	add.s64 	%rd1396, %rd1395, %rd2789;
	add.s64 	%rd1397, %rd1396, %rd1394;
	add.s64 	%rd1398, %rd1397, %rd1391;
	add.s64 	%rd1399, %rd1398, %rd1314;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10506,%dummy}, %rd1386;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10507}, %rd1386;
	}
	shf.r.wrap.b32 	%r10508, %r10507, %r10506, 28;
	shf.r.wrap.b32 	%r10509, %r10506, %r10507, 28;
	mov.b64 	%rd1400, {%r10509, %r10508};
	shf.l.wrap.b32 	%r10510, %r10506, %r10507, 30;
	shf.l.wrap.b32 	%r10511, %r10507, %r10506, 30;
	mov.b64 	%rd1401, {%r10511, %r10510};
	xor.b64  	%rd1402, %rd1401, %rd1400;
	shf.l.wrap.b32 	%r10512, %r10506, %r10507, 25;
	shf.l.wrap.b32 	%r10513, %r10507, %r10506, 25;
	mov.b64 	%rd1403, {%r10513, %r10512};
	xor.b64  	%rd1404, %rd1402, %rd1403;
	xor.b64  	%rd1405, %rd1386, %rd1338;
	xor.b64  	%rd1406, %rd1386, %rd1362;
	and.b64  	%rd1407, %rd1406, %rd1405;
	xor.b64  	%rd1408, %rd1407, %rd1386;
	add.s64 	%rd1409, %rd1398, %rd1408;
	add.s64 	%rd1410, %rd1409, %rd1404;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10514,%dummy}, %rd1399;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10515}, %rd1399;
	}
	shf.r.wrap.b32 	%r10516, %r10515, %r10514, 14;
	shf.r.wrap.b32 	%r10517, %r10514, %r10515, 14;
	mov.b64 	%rd1411, {%r10517, %r10516};
	shf.r.wrap.b32 	%r10518, %r10515, %r10514, 18;
	shf.r.wrap.b32 	%r10519, %r10514, %r10515, 18;
	mov.b64 	%rd1412, {%r10519, %r10518};
	xor.b64  	%rd1413, %rd1412, %rd1411;
	shf.l.wrap.b32 	%r10520, %r10514, %r10515, 23;
	shf.l.wrap.b32 	%r10521, %r10515, %r10514, 23;
	mov.b64 	%rd1414, {%r10521, %r10520};
	xor.b64  	%rd1415, %rd1413, %rd1414;
	xor.b64  	%rd1416, %rd1375, %rd1351;
	and.b64  	%rd1417, %rd1399, %rd1416;
	xor.b64  	%rd1418, %rd1417, %rd1351;
	add.s64 	%rd1419, %rd1327, %rd1160;
	add.s64 	%rd1420, %rd1419, %rd2790;
	add.s64 	%rd1421, %rd1420, %rd1418;
	add.s64 	%rd1422, %rd1421, %rd1415;
	add.s64 	%rd1423, %rd1422, %rd1338;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10522,%dummy}, %rd1410;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10523}, %rd1410;
	}
	shf.r.wrap.b32 	%r10524, %r10523, %r10522, 28;
	shf.r.wrap.b32 	%r10525, %r10522, %r10523, 28;
	mov.b64 	%rd1424, {%r10525, %r10524};
	shf.l.wrap.b32 	%r10526, %r10522, %r10523, 30;
	shf.l.wrap.b32 	%r10527, %r10523, %r10522, 30;
	mov.b64 	%rd1425, {%r10527, %r10526};
	xor.b64  	%rd1426, %rd1425, %rd1424;
	shf.l.wrap.b32 	%r10528, %r10522, %r10523, 25;
	shf.l.wrap.b32 	%r10529, %r10523, %r10522, 25;
	mov.b64 	%rd1427, {%r10529, %r10528};
	xor.b64  	%rd1428, %rd1426, %rd1427;
	xor.b64  	%rd1429, %rd1410, %rd1362;
	xor.b64  	%rd1430, %rd1410, %rd1386;
	and.b64  	%rd1431, %rd1430, %rd1429;
	xor.b64  	%rd1432, %rd1431, %rd1410;
	add.s64 	%rd1433, %rd1422, %rd1432;
	add.s64 	%rd1434, %rd1433, %rd1428;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10530,%dummy}, %rd1423;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10531}, %rd1423;
	}
	shf.r.wrap.b32 	%r10532, %r10531, %r10530, 14;
	shf.r.wrap.b32 	%r10533, %r10530, %r10531, 14;
	mov.b64 	%rd1435, {%r10533, %r10532};
	shf.r.wrap.b32 	%r10534, %r10531, %r10530, 18;
	shf.r.wrap.b32 	%r10535, %r10530, %r10531, 18;
	mov.b64 	%rd1436, {%r10535, %r10534};
	xor.b64  	%rd1437, %rd1436, %rd1435;
	shf.l.wrap.b32 	%r10536, %r10530, %r10531, 23;
	shf.l.wrap.b32 	%r10537, %r10531, %r10530, 23;
	mov.b64 	%rd1438, {%r10537, %r10536};
	xor.b64  	%rd1439, %rd1437, %rd1438;
	xor.b64  	%rd1440, %rd1399, %rd1375;
	and.b64  	%rd1441, %rd1423, %rd1440;
	xor.b64  	%rd1442, %rd1441, %rd1375;
	add.s64 	%rd1443, %rd1351, %rd1173;
	add.s64 	%rd1444, %rd1443, %rd2791;
	add.s64 	%rd1445, %rd1444, %rd1442;
	add.s64 	%rd1446, %rd1445, %rd1439;
	add.s64 	%rd1447, %rd1446, %rd1362;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10538,%dummy}, %rd1434;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10539}, %rd1434;
	}
	shf.r.wrap.b32 	%r10540, %r10539, %r10538, 28;
	shf.r.wrap.b32 	%r10541, %r10538, %r10539, 28;
	mov.b64 	%rd1448, {%r10541, %r10540};
	shf.l.wrap.b32 	%r10542, %r10538, %r10539, 30;
	shf.l.wrap.b32 	%r10543, %r10539, %r10538, 30;
	mov.b64 	%rd1449, {%r10543, %r10542};
	xor.b64  	%rd1450, %rd1449, %rd1448;
	shf.l.wrap.b32 	%r10544, %r10538, %r10539, 25;
	shf.l.wrap.b32 	%r10545, %r10539, %r10538, 25;
	mov.b64 	%rd1451, {%r10545, %r10544};
	xor.b64  	%rd1452, %rd1450, %rd1451;
	xor.b64  	%rd1453, %rd1434, %rd1386;
	xor.b64  	%rd1454, %rd1434, %rd1410;
	and.b64  	%rd1455, %rd1454, %rd1453;
	xor.b64  	%rd1456, %rd1455, %rd1434;
	add.s64 	%rd1457, %rd1446, %rd1456;
	add.s64 	%rd1458, %rd1457, %rd1452;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10546,%dummy}, %rd1447;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10547}, %rd1447;
	}
	shf.r.wrap.b32 	%r10548, %r10547, %r10546, 14;
	shf.r.wrap.b32 	%r10549, %r10546, %r10547, 14;
	mov.b64 	%rd1459, {%r10549, %r10548};
	shf.r.wrap.b32 	%r10550, %r10547, %r10546, 18;
	shf.r.wrap.b32 	%r10551, %r10546, %r10547, 18;
	mov.b64 	%rd1460, {%r10551, %r10550};
	xor.b64  	%rd1461, %rd1460, %rd1459;
	shf.l.wrap.b32 	%r10552, %r10546, %r10547, 23;
	shf.l.wrap.b32 	%r10553, %r10547, %r10546, 23;
	mov.b64 	%rd1462, {%r10553, %r10552};
	xor.b64  	%rd1463, %rd1461, %rd1462;
	xor.b64  	%rd1464, %rd1423, %rd1399;
	and.b64  	%rd1465, %rd1447, %rd1464;
	xor.b64  	%rd1466, %rd1465, %rd1399;
	add.s64 	%rd1467, %rd1375, %rd1186;
	add.s64 	%rd1468, %rd1467, %rd2792;
	add.s64 	%rd1469, %rd1468, %rd1466;
	add.s64 	%rd1470, %rd1469, %rd1463;
	add.s64 	%rd1471, %rd1470, %rd1386;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10554,%dummy}, %rd1458;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10555}, %rd1458;
	}
	shf.r.wrap.b32 	%r10556, %r10555, %r10554, 28;
	shf.r.wrap.b32 	%r10557, %r10554, %r10555, 28;
	mov.b64 	%rd1472, {%r10557, %r10556};
	shf.l.wrap.b32 	%r10558, %r10554, %r10555, 30;
	shf.l.wrap.b32 	%r10559, %r10555, %r10554, 30;
	mov.b64 	%rd1473, {%r10559, %r10558};
	xor.b64  	%rd1474, %rd1473, %rd1472;
	shf.l.wrap.b32 	%r10560, %r10554, %r10555, 25;
	shf.l.wrap.b32 	%r10561, %r10555, %r10554, 25;
	mov.b64 	%rd1475, {%r10561, %r10560};
	xor.b64  	%rd1476, %rd1474, %rd1475;
	xor.b64  	%rd1477, %rd1458, %rd1410;
	xor.b64  	%rd1478, %rd1458, %rd1434;
	and.b64  	%rd1479, %rd1478, %rd1477;
	xor.b64  	%rd1480, %rd1479, %rd1458;
	add.s64 	%rd1481, %rd1470, %rd1480;
	add.s64 	%rd1482, %rd1481, %rd1476;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10562,%dummy}, %rd1471;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10563}, %rd1471;
	}
	shf.r.wrap.b32 	%r10564, %r10563, %r10562, 14;
	shf.r.wrap.b32 	%r10565, %r10562, %r10563, 14;
	mov.b64 	%rd1483, {%r10565, %r10564};
	shf.r.wrap.b32 	%r10566, %r10563, %r10562, 18;
	shf.r.wrap.b32 	%r10567, %r10562, %r10563, 18;
	mov.b64 	%rd1484, {%r10567, %r10566};
	xor.b64  	%rd1485, %rd1484, %rd1483;
	shf.l.wrap.b32 	%r10568, %r10562, %r10563, 23;
	shf.l.wrap.b32 	%r10569, %r10563, %r10562, 23;
	mov.b64 	%rd1486, {%r10569, %r10568};
	xor.b64  	%rd1487, %rd1485, %rd1486;
	xor.b64  	%rd1488, %rd1447, %rd1423;
	and.b64  	%rd1489, %rd1471, %rd1488;
	xor.b64  	%rd1490, %rd1489, %rd1423;
	add.s64 	%rd1491, %rd1399, %rd1199;
	add.s64 	%rd1492, %rd1491, %rd2793;
	add.s64 	%rd1493, %rd1492, %rd1490;
	add.s64 	%rd1494, %rd1493, %rd1487;
	add.s64 	%rd1495, %rd1494, %rd1410;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10570,%dummy}, %rd1482;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10571}, %rd1482;
	}
	shf.r.wrap.b32 	%r10572, %r10571, %r10570, 28;
	shf.r.wrap.b32 	%r10573, %r10570, %r10571, 28;
	mov.b64 	%rd1496, {%r10573, %r10572};
	shf.l.wrap.b32 	%r10574, %r10570, %r10571, 30;
	shf.l.wrap.b32 	%r10575, %r10571, %r10570, 30;
	mov.b64 	%rd1497, {%r10575, %r10574};
	xor.b64  	%rd1498, %rd1497, %rd1496;
	shf.l.wrap.b32 	%r10576, %r10570, %r10571, 25;
	shf.l.wrap.b32 	%r10577, %r10571, %r10570, 25;
	mov.b64 	%rd1499, {%r10577, %r10576};
	xor.b64  	%rd1500, %rd1498, %rd1499;
	xor.b64  	%rd1501, %rd1482, %rd1434;
	xor.b64  	%rd1502, %rd1482, %rd1458;
	and.b64  	%rd1503, %rd1502, %rd1501;
	xor.b64  	%rd1504, %rd1503, %rd1482;
	add.s64 	%rd1505, %rd1494, %rd1504;
	add.s64 	%rd1506, %rd1505, %rd1500;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10578,%dummy}, %rd1495;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10579}, %rd1495;
	}
	shf.r.wrap.b32 	%r10580, %r10579, %r10578, 14;
	shf.r.wrap.b32 	%r10581, %r10578, %r10579, 14;
	mov.b64 	%rd1507, {%r10581, %r10580};
	shf.r.wrap.b32 	%r10582, %r10579, %r10578, 18;
	shf.r.wrap.b32 	%r10583, %r10578, %r10579, 18;
	mov.b64 	%rd1508, {%r10583, %r10582};
	xor.b64  	%rd1509, %rd1508, %rd1507;
	shf.l.wrap.b32 	%r10584, %r10578, %r10579, 23;
	shf.l.wrap.b32 	%r10585, %r10579, %r10578, 23;
	mov.b64 	%rd1510, {%r10585, %r10584};
	xor.b64  	%rd1511, %rd1509, %rd1510;
	xor.b64  	%rd1512, %rd1471, %rd1447;
	and.b64  	%rd1513, %rd1495, %rd1512;
	xor.b64  	%rd1514, %rd1513, %rd1447;
	add.s64 	%rd1515, %rd1423, %rd1212;
	add.s64 	%rd1516, %rd1515, %rd2794;
	add.s64 	%rd1517, %rd1516, %rd1514;
	add.s64 	%rd1518, %rd1517, %rd1511;
	add.s64 	%rd1519, %rd1518, %rd1434;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10586,%dummy}, %rd1506;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10587}, %rd1506;
	}
	shf.r.wrap.b32 	%r10588, %r10587, %r10586, 28;
	shf.r.wrap.b32 	%r10589, %r10586, %r10587, 28;
	mov.b64 	%rd1520, {%r10589, %r10588};
	shf.l.wrap.b32 	%r10590, %r10586, %r10587, 30;
	shf.l.wrap.b32 	%r10591, %r10587, %r10586, 30;
	mov.b64 	%rd1521, {%r10591, %r10590};
	xor.b64  	%rd1522, %rd1521, %rd1520;
	shf.l.wrap.b32 	%r10592, %r10586, %r10587, 25;
	shf.l.wrap.b32 	%r10593, %r10587, %r10586, 25;
	mov.b64 	%rd1523, {%r10593, %r10592};
	xor.b64  	%rd1524, %rd1522, %rd1523;
	xor.b64  	%rd1525, %rd1506, %rd1458;
	xor.b64  	%rd1526, %rd1506, %rd1482;
	and.b64  	%rd1527, %rd1526, %rd1525;
	xor.b64  	%rd1528, %rd1527, %rd1506;
	add.s64 	%rd1529, %rd1518, %rd1528;
	add.s64 	%rd1530, %rd1529, %rd1524;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10594,%dummy}, %rd1519;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10595}, %rd1519;
	}
	shf.r.wrap.b32 	%r10596, %r10595, %r10594, 14;
	shf.r.wrap.b32 	%r10597, %r10594, %r10595, 14;
	mov.b64 	%rd1531, {%r10597, %r10596};
	shf.r.wrap.b32 	%r10598, %r10595, %r10594, 18;
	shf.r.wrap.b32 	%r10599, %r10594, %r10595, 18;
	mov.b64 	%rd1532, {%r10599, %r10598};
	xor.b64  	%rd1533, %rd1532, %rd1531;
	shf.l.wrap.b32 	%r10600, %r10594, %r10595, 23;
	shf.l.wrap.b32 	%r10601, %r10595, %r10594, 23;
	mov.b64 	%rd1534, {%r10601, %r10600};
	xor.b64  	%rd1535, %rd1533, %rd1534;
	xor.b64  	%rd1536, %rd1495, %rd1471;
	and.b64  	%rd1537, %rd1519, %rd1536;
	xor.b64  	%rd1538, %rd1537, %rd1471;
	add.s64 	%rd1539, %rd1447, %rd1225;
	add.s64 	%rd1540, %rd1539, %rd2795;
	add.s64 	%rd1541, %rd1540, %rd1538;
	add.s64 	%rd1542, %rd1541, %rd1535;
	add.s64 	%rd1543, %rd1542, %rd1458;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10602,%dummy}, %rd1530;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10603}, %rd1530;
	}
	shf.r.wrap.b32 	%r10604, %r10603, %r10602, 28;
	shf.r.wrap.b32 	%r10605, %r10602, %r10603, 28;
	mov.b64 	%rd1544, {%r10605, %r10604};
	shf.l.wrap.b32 	%r10606, %r10602, %r10603, 30;
	shf.l.wrap.b32 	%r10607, %r10603, %r10602, 30;
	mov.b64 	%rd1545, {%r10607, %r10606};
	xor.b64  	%rd1546, %rd1545, %rd1544;
	shf.l.wrap.b32 	%r10608, %r10602, %r10603, 25;
	shf.l.wrap.b32 	%r10609, %r10603, %r10602, 25;
	mov.b64 	%rd1547, {%r10609, %r10608};
	xor.b64  	%rd1548, %rd1546, %rd1547;
	xor.b64  	%rd1549, %rd1530, %rd1482;
	xor.b64  	%rd1550, %rd1530, %rd1506;
	and.b64  	%rd1551, %rd1550, %rd1549;
	xor.b64  	%rd1552, %rd1551, %rd1530;
	add.s64 	%rd1553, %rd1542, %rd1552;
	add.s64 	%rd1554, %rd1553, %rd1548;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10610,%dummy}, %rd1543;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10611}, %rd1543;
	}
	shf.r.wrap.b32 	%r10612, %r10611, %r10610, 14;
	shf.r.wrap.b32 	%r10613, %r10610, %r10611, 14;
	mov.b64 	%rd1555, {%r10613, %r10612};
	shf.r.wrap.b32 	%r10614, %r10611, %r10610, 18;
	shf.r.wrap.b32 	%r10615, %r10610, %r10611, 18;
	mov.b64 	%rd1556, {%r10615, %r10614};
	xor.b64  	%rd1557, %rd1556, %rd1555;
	shf.l.wrap.b32 	%r10616, %r10610, %r10611, 23;
	shf.l.wrap.b32 	%r10617, %r10611, %r10610, 23;
	mov.b64 	%rd1558, {%r10617, %r10616};
	xor.b64  	%rd1559, %rd1557, %rd1558;
	xor.b64  	%rd1560, %rd1519, %rd1495;
	and.b64  	%rd1561, %rd1543, %rd1560;
	xor.b64  	%rd1562, %rd1561, %rd1495;
	add.s64 	%rd1563, %rd1471, %rd1238;
	add.s64 	%rd1564, %rd1563, %rd2796;
	add.s64 	%rd1565, %rd1564, %rd1562;
	add.s64 	%rd1566, %rd1565, %rd1559;
	add.s64 	%rd1567, %rd1566, %rd1482;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10618,%dummy}, %rd1554;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10619}, %rd1554;
	}
	shf.r.wrap.b32 	%r10620, %r10619, %r10618, 28;
	shf.r.wrap.b32 	%r10621, %r10618, %r10619, 28;
	mov.b64 	%rd1568, {%r10621, %r10620};
	shf.l.wrap.b32 	%r10622, %r10618, %r10619, 30;
	shf.l.wrap.b32 	%r10623, %r10619, %r10618, 30;
	mov.b64 	%rd1569, {%r10623, %r10622};
	xor.b64  	%rd1570, %rd1569, %rd1568;
	shf.l.wrap.b32 	%r10624, %r10618, %r10619, 25;
	shf.l.wrap.b32 	%r10625, %r10619, %r10618, 25;
	mov.b64 	%rd1571, {%r10625, %r10624};
	xor.b64  	%rd1572, %rd1570, %rd1571;
	xor.b64  	%rd1573, %rd1554, %rd1506;
	xor.b64  	%rd1574, %rd1554, %rd1530;
	and.b64  	%rd1575, %rd1574, %rd1573;
	xor.b64  	%rd1576, %rd1575, %rd1554;
	add.s64 	%rd1577, %rd1566, %rd1576;
	add.s64 	%rd1578, %rd1577, %rd1572;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10626,%dummy}, %rd1567;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10627}, %rd1567;
	}
	shf.r.wrap.b32 	%r10628, %r10627, %r10626, 14;
	shf.r.wrap.b32 	%r10629, %r10626, %r10627, 14;
	mov.b64 	%rd1579, {%r10629, %r10628};
	shf.r.wrap.b32 	%r10630, %r10627, %r10626, 18;
	shf.r.wrap.b32 	%r10631, %r10626, %r10627, 18;
	mov.b64 	%rd1580, {%r10631, %r10630};
	xor.b64  	%rd1581, %rd1580, %rd1579;
	shf.l.wrap.b32 	%r10632, %r10626, %r10627, 23;
	shf.l.wrap.b32 	%r10633, %r10627, %r10626, 23;
	mov.b64 	%rd1582, {%r10633, %r10632};
	xor.b64  	%rd1583, %rd1581, %rd1582;
	xor.b64  	%rd1584, %rd1543, %rd1519;
	and.b64  	%rd1585, %rd1567, %rd1584;
	xor.b64  	%rd1586, %rd1585, %rd1519;
	add.s64 	%rd1587, %rd1495, %rd1251;
	add.s64 	%rd1588, %rd1587, %rd2797;
	add.s64 	%rd1589, %rd1588, %rd1586;
	add.s64 	%rd1590, %rd1589, %rd1583;
	add.s64 	%rd1591, %rd1590, %rd1506;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10634,%dummy}, %rd1578;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10635}, %rd1578;
	}
	shf.r.wrap.b32 	%r10636, %r10635, %r10634, 28;
	shf.r.wrap.b32 	%r10637, %r10634, %r10635, 28;
	mov.b64 	%rd1592, {%r10637, %r10636};
	shf.l.wrap.b32 	%r10638, %r10634, %r10635, 30;
	shf.l.wrap.b32 	%r10639, %r10635, %r10634, 30;
	mov.b64 	%rd1593, {%r10639, %r10638};
	xor.b64  	%rd1594, %rd1593, %rd1592;
	shf.l.wrap.b32 	%r10640, %r10634, %r10635, 25;
	shf.l.wrap.b32 	%r10641, %r10635, %r10634, 25;
	mov.b64 	%rd1595, {%r10641, %r10640};
	xor.b64  	%rd1596, %rd1594, %rd1595;
	xor.b64  	%rd1597, %rd1578, %rd1530;
	xor.b64  	%rd1598, %rd1578, %rd1554;
	and.b64  	%rd1599, %rd1598, %rd1597;
	xor.b64  	%rd1600, %rd1599, %rd1578;
	add.s64 	%rd1601, %rd1590, %rd1600;
	add.s64 	%rd1602, %rd1601, %rd1596;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10642,%dummy}, %rd1591;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10643}, %rd1591;
	}
	shf.r.wrap.b32 	%r10644, %r10643, %r10642, 14;
	shf.r.wrap.b32 	%r10645, %r10642, %r10643, 14;
	mov.b64 	%rd1603, {%r10645, %r10644};
	shf.r.wrap.b32 	%r10646, %r10643, %r10642, 18;
	shf.r.wrap.b32 	%r10647, %r10642, %r10643, 18;
	mov.b64 	%rd1604, {%r10647, %r10646};
	xor.b64  	%rd1605, %rd1604, %rd1603;
	shf.l.wrap.b32 	%r10648, %r10642, %r10643, 23;
	shf.l.wrap.b32 	%r10649, %r10643, %r10642, 23;
	mov.b64 	%rd1606, {%r10649, %r10648};
	xor.b64  	%rd1607, %rd1605, %rd1606;
	xor.b64  	%rd1608, %rd1567, %rd1543;
	and.b64  	%rd1609, %rd1591, %rd1608;
	xor.b64  	%rd1610, %rd1609, %rd1543;
	add.s64 	%rd1611, %rd1519, %rd1264;
	add.s64 	%rd1612, %rd1611, %rd2798;
	add.s64 	%rd1613, %rd1612, %rd1610;
	add.s64 	%rd1614, %rd1613, %rd1607;
	add.s64 	%rd1615, %rd1614, %rd1530;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10650,%dummy}, %rd1602;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10651}, %rd1602;
	}
	shf.r.wrap.b32 	%r10652, %r10651, %r10650, 28;
	shf.r.wrap.b32 	%r10653, %r10650, %r10651, 28;
	mov.b64 	%rd1616, {%r10653, %r10652};
	shf.l.wrap.b32 	%r10654, %r10650, %r10651, 30;
	shf.l.wrap.b32 	%r10655, %r10651, %r10650, 30;
	mov.b64 	%rd1617, {%r10655, %r10654};
	xor.b64  	%rd1618, %rd1617, %rd1616;
	shf.l.wrap.b32 	%r10656, %r10650, %r10651, 25;
	shf.l.wrap.b32 	%r10657, %r10651, %r10650, 25;
	mov.b64 	%rd1619, {%r10657, %r10656};
	xor.b64  	%rd1620, %rd1618, %rd1619;
	xor.b64  	%rd1621, %rd1602, %rd1554;
	xor.b64  	%rd1622, %rd1602, %rd1578;
	and.b64  	%rd1623, %rd1622, %rd1621;
	xor.b64  	%rd1624, %rd1623, %rd1602;
	add.s64 	%rd1625, %rd1614, %rd1624;
	add.s64 	%rd1626, %rd1625, %rd1620;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10658,%dummy}, %rd1615;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10659}, %rd1615;
	}
	shf.r.wrap.b32 	%r10660, %r10659, %r10658, 14;
	shf.r.wrap.b32 	%r10661, %r10658, %r10659, 14;
	mov.b64 	%rd1627, {%r10661, %r10660};
	shf.r.wrap.b32 	%r10662, %r10659, %r10658, 18;
	shf.r.wrap.b32 	%r10663, %r10658, %r10659, 18;
	mov.b64 	%rd1628, {%r10663, %r10662};
	xor.b64  	%rd1629, %rd1628, %rd1627;
	shf.l.wrap.b32 	%r10664, %r10658, %r10659, 23;
	shf.l.wrap.b32 	%r10665, %r10659, %r10658, 23;
	mov.b64 	%rd1630, {%r10665, %r10664};
	xor.b64  	%rd1631, %rd1629, %rd1630;
	xor.b64  	%rd1632, %rd1591, %rd1567;
	and.b64  	%rd1633, %rd1615, %rd1632;
	xor.b64  	%rd1634, %rd1633, %rd1567;
	add.s64 	%rd1635, %rd1543, %rd1277;
	add.s64 	%rd1636, %rd1635, %rd2799;
	add.s64 	%rd1637, %rd1636, %rd1634;
	add.s64 	%rd1638, %rd1637, %rd1631;
	add.s64 	%rd1639, %rd1638, %rd1554;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10666,%dummy}, %rd1626;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10667}, %rd1626;
	}
	shf.r.wrap.b32 	%r10668, %r10667, %r10666, 28;
	shf.r.wrap.b32 	%r10669, %r10666, %r10667, 28;
	mov.b64 	%rd1640, {%r10669, %r10668};
	shf.l.wrap.b32 	%r10670, %r10666, %r10667, 30;
	shf.l.wrap.b32 	%r10671, %r10667, %r10666, 30;
	mov.b64 	%rd1641, {%r10671, %r10670};
	xor.b64  	%rd1642, %rd1641, %rd1640;
	shf.l.wrap.b32 	%r10672, %r10666, %r10667, 25;
	shf.l.wrap.b32 	%r10673, %r10667, %r10666, 25;
	mov.b64 	%rd1643, {%r10673, %r10672};
	xor.b64  	%rd1644, %rd1642, %rd1643;
	xor.b64  	%rd1645, %rd1626, %rd1578;
	xor.b64  	%rd1646, %rd1626, %rd1602;
	and.b64  	%rd1647, %rd1646, %rd1645;
	xor.b64  	%rd1648, %rd1647, %rd1626;
	add.s64 	%rd1649, %rd1638, %rd1648;
	add.s64 	%rd1650, %rd1649, %rd1644;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10674,%dummy}, %rd1639;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10675}, %rd1639;
	}
	shf.r.wrap.b32 	%r10676, %r10675, %r10674, 14;
	shf.r.wrap.b32 	%r10677, %r10674, %r10675, 14;
	mov.b64 	%rd1651, {%r10677, %r10676};
	shf.r.wrap.b32 	%r10678, %r10675, %r10674, 18;
	shf.r.wrap.b32 	%r10679, %r10674, %r10675, 18;
	mov.b64 	%rd1652, {%r10679, %r10678};
	xor.b64  	%rd1653, %rd1652, %rd1651;
	shf.l.wrap.b32 	%r10680, %r10674, %r10675, 23;
	shf.l.wrap.b32 	%r10681, %r10675, %r10674, 23;
	mov.b64 	%rd1654, {%r10681, %r10680};
	xor.b64  	%rd1655, %rd1653, %rd1654;
	xor.b64  	%rd1656, %rd1615, %rd1591;
	and.b64  	%rd1657, %rd1639, %rd1656;
	xor.b64  	%rd1658, %rd1657, %rd1591;
	add.s64 	%rd1659, %rd1567, %rd1290;
	add.s64 	%rd1660, %rd1659, %rd2800;
	add.s64 	%rd1661, %rd1660, %rd1658;
	add.s64 	%rd1662, %rd1661, %rd1655;
	add.s64 	%rd1663, %rd1662, %rd1578;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10682,%dummy}, %rd1650;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10683}, %rd1650;
	}
	shf.r.wrap.b32 	%r10684, %r10683, %r10682, 28;
	shf.r.wrap.b32 	%r10685, %r10682, %r10683, 28;
	mov.b64 	%rd1664, {%r10685, %r10684};
	shf.l.wrap.b32 	%r10686, %r10682, %r10683, 30;
	shf.l.wrap.b32 	%r10687, %r10683, %r10682, 30;
	mov.b64 	%rd1665, {%r10687, %r10686};
	xor.b64  	%rd1666, %rd1665, %rd1664;
	shf.l.wrap.b32 	%r10688, %r10682, %r10683, 25;
	shf.l.wrap.b32 	%r10689, %r10683, %r10682, 25;
	mov.b64 	%rd1667, {%r10689, %r10688};
	xor.b64  	%rd1668, %rd1666, %rd1667;
	xor.b64  	%rd1669, %rd1650, %rd1602;
	xor.b64  	%rd1670, %rd1650, %rd1626;
	and.b64  	%rd1671, %rd1670, %rd1669;
	xor.b64  	%rd1672, %rd1671, %rd1650;
	add.s64 	%rd1673, %rd1662, %rd1672;
	add.s64 	%rd1674, %rd1673, %rd1668;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10690,%dummy}, %rd1277;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10691}, %rd1277;
	}
	shf.r.wrap.b32 	%r10692, %r10691, %r10690, 19;
	shf.r.wrap.b32 	%r10693, %r10690, %r10691, 19;
	mov.b64 	%rd1675, {%r10693, %r10692};
	shf.l.wrap.b32 	%r10694, %r10690, %r10691, 3;
	shf.l.wrap.b32 	%r10695, %r10691, %r10690, 3;
	mov.b64 	%rd1676, {%r10695, %r10694};
	shr.u64 	%rd1677, %rd1277, 6;
	xor.b64  	%rd1678, %rd1675, %rd1677;
	xor.b64  	%rd1679, %rd1678, %rd1676;
	shf.r.wrap.b32 	%r10696, %r10305, %r10304, 1;
	shf.r.wrap.b32 	%r10697, %r10304, %r10305, 1;
	mov.b64 	%rd1680, {%r10697, %r10696};
	shf.r.wrap.b32 	%r10698, %r10305, %r10304, 8;
	shf.r.wrap.b32 	%r10699, %r10304, %r10305, 8;
	mov.b64 	%rd1681, {%r10699, %r10698};
	shr.u64 	%rd1682, %rd1108, 7;
	xor.b64  	%rd1683, %rd1680, %rd1682;
	xor.b64  	%rd1684, %rd1683, %rd1681;
	add.s64 	%rd1685, %rd1095, %rd1212;
	add.s64 	%rd1686, %rd1685, %rd1679;
	add.s64 	%rd1687, %rd1686, %rd1684;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10700,%dummy}, %rd1290;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10701}, %rd1290;
	}
	shf.r.wrap.b32 	%r10702, %r10701, %r10700, 19;
	shf.r.wrap.b32 	%r10703, %r10700, %r10701, 19;
	mov.b64 	%rd1688, {%r10703, %r10702};
	shf.l.wrap.b32 	%r10704, %r10700, %r10701, 3;
	shf.l.wrap.b32 	%r10705, %r10701, %r10700, 3;
	mov.b64 	%rd1689, {%r10705, %r10704};
	shr.u64 	%rd1690, %rd1290, 6;
	xor.b64  	%rd1691, %rd1688, %rd1690;
	xor.b64  	%rd1692, %rd1691, %rd1689;
	shf.r.wrap.b32 	%r10706, %r10315, %r10314, 1;
	shf.r.wrap.b32 	%r10707, %r10314, %r10315, 1;
	mov.b64 	%rd1693, {%r10707, %r10706};
	shf.r.wrap.b32 	%r10708, %r10315, %r10314, 8;
	shf.r.wrap.b32 	%r10709, %r10314, %r10315, 8;
	mov.b64 	%rd1694, {%r10709, %r10708};
	shr.u64 	%rd1695, %rd1121, 7;
	xor.b64  	%rd1696, %rd1693, %rd1695;
	xor.b64  	%rd1697, %rd1696, %rd1694;
	add.s64 	%rd1698, %rd1108, %rd1225;
	add.s64 	%rd1699, %rd1698, %rd1692;
	add.s64 	%rd1700, %rd1699, %rd1697;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10710,%dummy}, %rd1687;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10711}, %rd1687;
	}
	shf.r.wrap.b32 	%r10712, %r10711, %r10710, 19;
	shf.r.wrap.b32 	%r10713, %r10710, %r10711, 19;
	mov.b64 	%rd1701, {%r10713, %r10712};
	shf.l.wrap.b32 	%r10714, %r10710, %r10711, 3;
	shf.l.wrap.b32 	%r10715, %r10711, %r10710, 3;
	mov.b64 	%rd1702, {%r10715, %r10714};
	shr.u64 	%rd1703, %rd1687, 6;
	xor.b64  	%rd1704, %rd1701, %rd1703;
	xor.b64  	%rd1705, %rd1704, %rd1702;
	shf.r.wrap.b32 	%r10716, %r10325, %r10324, 1;
	shf.r.wrap.b32 	%r10717, %r10324, %r10325, 1;
	mov.b64 	%rd1706, {%r10717, %r10716};
	shf.r.wrap.b32 	%r10718, %r10325, %r10324, 8;
	shf.r.wrap.b32 	%r10719, %r10324, %r10325, 8;
	mov.b64 	%rd1707, {%r10719, %r10718};
	shr.u64 	%rd1708, %rd1134, 7;
	xor.b64  	%rd1709, %rd1706, %rd1708;
	xor.b64  	%rd1710, %rd1709, %rd1707;
	add.s64 	%rd1711, %rd1121, %rd1238;
	add.s64 	%rd1712, %rd1711, %rd1705;
	add.s64 	%rd1713, %rd1712, %rd1710;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10720,%dummy}, %rd1700;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10721}, %rd1700;
	}
	shf.r.wrap.b32 	%r10722, %r10721, %r10720, 19;
	shf.r.wrap.b32 	%r10723, %r10720, %r10721, 19;
	mov.b64 	%rd1714, {%r10723, %r10722};
	shf.l.wrap.b32 	%r10724, %r10720, %r10721, 3;
	shf.l.wrap.b32 	%r10725, %r10721, %r10720, 3;
	mov.b64 	%rd1715, {%r10725, %r10724};
	shr.u64 	%rd1716, %rd1700, 6;
	xor.b64  	%rd1717, %rd1714, %rd1716;
	xor.b64  	%rd1718, %rd1717, %rd1715;
	shf.r.wrap.b32 	%r10726, %r10335, %r10334, 1;
	shf.r.wrap.b32 	%r10727, %r10334, %r10335, 1;
	mov.b64 	%rd1719, {%r10727, %r10726};
	shf.r.wrap.b32 	%r10728, %r10335, %r10334, 8;
	shf.r.wrap.b32 	%r10729, %r10334, %r10335, 8;
	mov.b64 	%rd1720, {%r10729, %r10728};
	shr.u64 	%rd1721, %rd1147, 7;
	xor.b64  	%rd1722, %rd1719, %rd1721;
	xor.b64  	%rd1723, %rd1722, %rd1720;
	add.s64 	%rd1724, %rd1134, %rd1251;
	add.s64 	%rd1725, %rd1724, %rd1718;
	add.s64 	%rd1726, %rd1725, %rd1723;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10730,%dummy}, %rd1713;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10731}, %rd1713;
	}
	shf.r.wrap.b32 	%r10732, %r10731, %r10730, 19;
	shf.r.wrap.b32 	%r10733, %r10730, %r10731, 19;
	mov.b64 	%rd1727, {%r10733, %r10732};
	shf.l.wrap.b32 	%r10734, %r10730, %r10731, 3;
	shf.l.wrap.b32 	%r10735, %r10731, %r10730, 3;
	mov.b64 	%rd1728, {%r10735, %r10734};
	shr.u64 	%rd1729, %rd1713, 6;
	xor.b64  	%rd1730, %rd1727, %rd1729;
	xor.b64  	%rd1731, %rd1730, %rd1728;
	shf.r.wrap.b32 	%r10736, %r10345, %r10344, 1;
	shf.r.wrap.b32 	%r10737, %r10344, %r10345, 1;
	mov.b64 	%rd1732, {%r10737, %r10736};
	shf.r.wrap.b32 	%r10738, %r10345, %r10344, 8;
	shf.r.wrap.b32 	%r10739, %r10344, %r10345, 8;
	mov.b64 	%rd1733, {%r10739, %r10738};
	shr.u64 	%rd1734, %rd1160, 7;
	xor.b64  	%rd1735, %rd1732, %rd1734;
	xor.b64  	%rd1736, %rd1735, %rd1733;
	add.s64 	%rd1737, %rd1147, %rd1264;
	add.s64 	%rd1738, %rd1737, %rd1731;
	add.s64 	%rd1739, %rd1738, %rd1736;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10740,%dummy}, %rd1726;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10741}, %rd1726;
	}
	shf.r.wrap.b32 	%r10742, %r10741, %r10740, 19;
	shf.r.wrap.b32 	%r10743, %r10740, %r10741, 19;
	mov.b64 	%rd1740, {%r10743, %r10742};
	shf.l.wrap.b32 	%r10744, %r10740, %r10741, 3;
	shf.l.wrap.b32 	%r10745, %r10741, %r10740, 3;
	mov.b64 	%rd1741, {%r10745, %r10744};
	shr.u64 	%rd1742, %rd1726, 6;
	xor.b64  	%rd1743, %rd1740, %rd1742;
	xor.b64  	%rd1744, %rd1743, %rd1741;
	shf.r.wrap.b32 	%r10746, %r10355, %r10354, 1;
	shf.r.wrap.b32 	%r10747, %r10354, %r10355, 1;
	mov.b64 	%rd1745, {%r10747, %r10746};
	shf.r.wrap.b32 	%r10748, %r10355, %r10354, 8;
	shf.r.wrap.b32 	%r10749, %r10354, %r10355, 8;
	mov.b64 	%rd1746, {%r10749, %r10748};
	shr.u64 	%rd1747, %rd1173, 7;
	xor.b64  	%rd1748, %rd1745, %rd1747;
	xor.b64  	%rd1749, %rd1748, %rd1746;
	add.s64 	%rd1750, %rd1160, %rd1277;
	add.s64 	%rd1751, %rd1750, %rd1744;
	add.s64 	%rd1752, %rd1751, %rd1749;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10750,%dummy}, %rd1739;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10751}, %rd1739;
	}
	shf.r.wrap.b32 	%r10752, %r10751, %r10750, 19;
	shf.r.wrap.b32 	%r10753, %r10750, %r10751, 19;
	mov.b64 	%rd1753, {%r10753, %r10752};
	shf.l.wrap.b32 	%r10754, %r10750, %r10751, 3;
	shf.l.wrap.b32 	%r10755, %r10751, %r10750, 3;
	mov.b64 	%rd1754, {%r10755, %r10754};
	shr.u64 	%rd1755, %rd1739, 6;
	xor.b64  	%rd1756, %rd1753, %rd1755;
	xor.b64  	%rd1757, %rd1756, %rd1754;
	shf.r.wrap.b32 	%r10756, %r10365, %r10364, 1;
	shf.r.wrap.b32 	%r10757, %r10364, %r10365, 1;
	mov.b64 	%rd1758, {%r10757, %r10756};
	shf.r.wrap.b32 	%r10758, %r10365, %r10364, 8;
	shf.r.wrap.b32 	%r10759, %r10364, %r10365, 8;
	mov.b64 	%rd1759, {%r10759, %r10758};
	shr.u64 	%rd1760, %rd1186, 7;
	xor.b64  	%rd1761, %rd1758, %rd1760;
	xor.b64  	%rd1762, %rd1761, %rd1759;
	add.s64 	%rd1763, %rd1173, %rd1290;
	add.s64 	%rd1764, %rd1763, %rd1757;
	add.s64 	%rd1765, %rd1764, %rd1762;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10760,%dummy}, %rd1752;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10761}, %rd1752;
	}
	shf.r.wrap.b32 	%r10762, %r10761, %r10760, 19;
	shf.r.wrap.b32 	%r10763, %r10760, %r10761, 19;
	mov.b64 	%rd1766, {%r10763, %r10762};
	shf.l.wrap.b32 	%r10764, %r10760, %r10761, 3;
	shf.l.wrap.b32 	%r10765, %r10761, %r10760, 3;
	mov.b64 	%rd1767, {%r10765, %r10764};
	shr.u64 	%rd1768, %rd1752, 6;
	xor.b64  	%rd1769, %rd1766, %rd1768;
	xor.b64  	%rd1770, %rd1769, %rd1767;
	shf.r.wrap.b32 	%r10766, %r10375, %r10374, 1;
	shf.r.wrap.b32 	%r10767, %r10374, %r10375, 1;
	mov.b64 	%rd1771, {%r10767, %r10766};
	shf.r.wrap.b32 	%r10768, %r10375, %r10374, 8;
	shf.r.wrap.b32 	%r10769, %r10374, %r10375, 8;
	mov.b64 	%rd1772, {%r10769, %r10768};
	shr.u64 	%rd1773, %rd1199, 7;
	xor.b64  	%rd1774, %rd1771, %rd1773;
	xor.b64  	%rd1775, %rd1774, %rd1772;
	add.s64 	%rd1776, %rd1687, %rd1186;
	add.s64 	%rd1777, %rd1776, %rd1770;
	add.s64 	%rd1778, %rd1777, %rd1775;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10770,%dummy}, %rd1765;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10771}, %rd1765;
	}
	shf.r.wrap.b32 	%r10772, %r10771, %r10770, 19;
	shf.r.wrap.b32 	%r10773, %r10770, %r10771, 19;
	mov.b64 	%rd1779, {%r10773, %r10772};
	shf.l.wrap.b32 	%r10774, %r10770, %r10771, 3;
	shf.l.wrap.b32 	%r10775, %r10771, %r10770, 3;
	mov.b64 	%rd1780, {%r10775, %r10774};
	shr.u64 	%rd1781, %rd1765, 6;
	xor.b64  	%rd1782, %rd1779, %rd1781;
	xor.b64  	%rd1783, %rd1782, %rd1780;
	shf.r.wrap.b32 	%r10776, %r10385, %r10384, 1;
	shf.r.wrap.b32 	%r10777, %r10384, %r10385, 1;
	mov.b64 	%rd1784, {%r10777, %r10776};
	shf.r.wrap.b32 	%r10778, %r10385, %r10384, 8;
	shf.r.wrap.b32 	%r10779, %r10384, %r10385, 8;
	mov.b64 	%rd1785, {%r10779, %r10778};
	shr.u64 	%rd1786, %rd1212, 7;
	xor.b64  	%rd1787, %rd1784, %rd1786;
	xor.b64  	%rd1788, %rd1787, %rd1785;
	add.s64 	%rd1789, %rd1700, %rd1199;
	add.s64 	%rd1790, %rd1789, %rd1783;
	add.s64 	%rd1791, %rd1790, %rd1788;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10780,%dummy}, %rd1778;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10781}, %rd1778;
	}
	shf.r.wrap.b32 	%r10782, %r10781, %r10780, 19;
	shf.r.wrap.b32 	%r10783, %r10780, %r10781, 19;
	mov.b64 	%rd1792, {%r10783, %r10782};
	shf.l.wrap.b32 	%r10784, %r10780, %r10781, 3;
	shf.l.wrap.b32 	%r10785, %r10781, %r10780, 3;
	mov.b64 	%rd1793, {%r10785, %r10784};
	shr.u64 	%rd1794, %rd1778, 6;
	xor.b64  	%rd1795, %rd1792, %rd1794;
	xor.b64  	%rd1796, %rd1795, %rd1793;
	shf.r.wrap.b32 	%r10786, %r10395, %r10394, 1;
	shf.r.wrap.b32 	%r10787, %r10394, %r10395, 1;
	mov.b64 	%rd1797, {%r10787, %r10786};
	shf.r.wrap.b32 	%r10788, %r10395, %r10394, 8;
	shf.r.wrap.b32 	%r10789, %r10394, %r10395, 8;
	mov.b64 	%rd1798, {%r10789, %r10788};
	shr.u64 	%rd1799, %rd1225, 7;
	xor.b64  	%rd1800, %rd1797, %rd1799;
	xor.b64  	%rd1801, %rd1800, %rd1798;
	add.s64 	%rd1802, %rd1713, %rd1212;
	add.s64 	%rd1803, %rd1802, %rd1796;
	add.s64 	%rd1804, %rd1803, %rd1801;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10790,%dummy}, %rd1791;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10791}, %rd1791;
	}
	shf.r.wrap.b32 	%r10792, %r10791, %r10790, 19;
	shf.r.wrap.b32 	%r10793, %r10790, %r10791, 19;
	mov.b64 	%rd1805, {%r10793, %r10792};
	shf.l.wrap.b32 	%r10794, %r10790, %r10791, 3;
	shf.l.wrap.b32 	%r10795, %r10791, %r10790, 3;
	mov.b64 	%rd1806, {%r10795, %r10794};
	shr.u64 	%rd1807, %rd1791, 6;
	xor.b64  	%rd1808, %rd1805, %rd1807;
	xor.b64  	%rd1809, %rd1808, %rd1806;
	shf.r.wrap.b32 	%r10796, %r10405, %r10404, 1;
	shf.r.wrap.b32 	%r10797, %r10404, %r10405, 1;
	mov.b64 	%rd1810, {%r10797, %r10796};
	shf.r.wrap.b32 	%r10798, %r10405, %r10404, 8;
	shf.r.wrap.b32 	%r10799, %r10404, %r10405, 8;
	mov.b64 	%rd1811, {%r10799, %r10798};
	shr.u64 	%rd1812, %rd1238, 7;
	xor.b64  	%rd1813, %rd1810, %rd1812;
	xor.b64  	%rd1814, %rd1813, %rd1811;
	add.s64 	%rd1815, %rd1726, %rd1225;
	add.s64 	%rd1816, %rd1815, %rd1809;
	add.s64 	%rd1817, %rd1816, %rd1814;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10800,%dummy}, %rd1804;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10801}, %rd1804;
	}
	shf.r.wrap.b32 	%r10802, %r10801, %r10800, 19;
	shf.r.wrap.b32 	%r10803, %r10800, %r10801, 19;
	mov.b64 	%rd1818, {%r10803, %r10802};
	shf.l.wrap.b32 	%r10804, %r10800, %r10801, 3;
	shf.l.wrap.b32 	%r10805, %r10801, %r10800, 3;
	mov.b64 	%rd1819, {%r10805, %r10804};
	shr.u64 	%rd1820, %rd1804, 6;
	xor.b64  	%rd1821, %rd1818, %rd1820;
	xor.b64  	%rd1822, %rd1821, %rd1819;
	shf.r.wrap.b32 	%r10806, %r10415, %r10414, 1;
	shf.r.wrap.b32 	%r10807, %r10414, %r10415, 1;
	mov.b64 	%rd1823, {%r10807, %r10806};
	shf.r.wrap.b32 	%r10808, %r10415, %r10414, 8;
	shf.r.wrap.b32 	%r10809, %r10414, %r10415, 8;
	mov.b64 	%rd1824, {%r10809, %r10808};
	shr.u64 	%rd1825, %rd1251, 7;
	xor.b64  	%rd1826, %rd1823, %rd1825;
	xor.b64  	%rd1827, %rd1826, %rd1824;
	add.s64 	%rd1828, %rd1739, %rd1238;
	add.s64 	%rd1829, %rd1828, %rd1822;
	add.s64 	%rd1830, %rd1829, %rd1827;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10810,%dummy}, %rd1817;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10811}, %rd1817;
	}
	shf.r.wrap.b32 	%r10812, %r10811, %r10810, 19;
	shf.r.wrap.b32 	%r10813, %r10810, %r10811, 19;
	mov.b64 	%rd1831, {%r10813, %r10812};
	shf.l.wrap.b32 	%r10814, %r10810, %r10811, 3;
	shf.l.wrap.b32 	%r10815, %r10811, %r10810, 3;
	mov.b64 	%rd1832, {%r10815, %r10814};
	shr.u64 	%rd1833, %rd1817, 6;
	xor.b64  	%rd1834, %rd1831, %rd1833;
	xor.b64  	%rd1835, %rd1834, %rd1832;
	shf.r.wrap.b32 	%r10816, %r10425, %r10424, 1;
	shf.r.wrap.b32 	%r10817, %r10424, %r10425, 1;
	mov.b64 	%rd1836, {%r10817, %r10816};
	shf.r.wrap.b32 	%r10818, %r10425, %r10424, 8;
	shf.r.wrap.b32 	%r10819, %r10424, %r10425, 8;
	mov.b64 	%rd1837, {%r10819, %r10818};
	shr.u64 	%rd1838, %rd1264, 7;
	xor.b64  	%rd1839, %rd1836, %rd1838;
	xor.b64  	%rd1840, %rd1839, %rd1837;
	add.s64 	%rd1841, %rd1752, %rd1251;
	add.s64 	%rd1842, %rd1841, %rd1835;
	add.s64 	%rd1843, %rd1842, %rd1840;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10820,%dummy}, %rd1830;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10821}, %rd1830;
	}
	shf.r.wrap.b32 	%r10822, %r10821, %r10820, 19;
	shf.r.wrap.b32 	%r10823, %r10820, %r10821, 19;
	mov.b64 	%rd1844, {%r10823, %r10822};
	shf.l.wrap.b32 	%r10824, %r10820, %r10821, 3;
	shf.l.wrap.b32 	%r10825, %r10821, %r10820, 3;
	mov.b64 	%rd1845, {%r10825, %r10824};
	shr.u64 	%rd1846, %rd1830, 6;
	xor.b64  	%rd1847, %rd1844, %rd1846;
	xor.b64  	%rd1848, %rd1847, %rd1845;
	shf.r.wrap.b32 	%r10826, %r10691, %r10690, 1;
	shf.r.wrap.b32 	%r10827, %r10690, %r10691, 1;
	mov.b64 	%rd1849, {%r10827, %r10826};
	shf.r.wrap.b32 	%r10828, %r10691, %r10690, 8;
	shf.r.wrap.b32 	%r10829, %r10690, %r10691, 8;
	mov.b64 	%rd1850, {%r10829, %r10828};
	shr.u64 	%rd1851, %rd1277, 7;
	xor.b64  	%rd1852, %rd1849, %rd1851;
	xor.b64  	%rd1853, %rd1852, %rd1850;
	add.s64 	%rd1854, %rd1765, %rd1264;
	add.s64 	%rd1855, %rd1854, %rd1848;
	add.s64 	%rd1856, %rd1855, %rd1853;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10830,%dummy}, %rd1843;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10831}, %rd1843;
	}
	shf.r.wrap.b32 	%r10832, %r10831, %r10830, 19;
	shf.r.wrap.b32 	%r10833, %r10830, %r10831, 19;
	mov.b64 	%rd1857, {%r10833, %r10832};
	shf.l.wrap.b32 	%r10834, %r10830, %r10831, 3;
	shf.l.wrap.b32 	%r10835, %r10831, %r10830, 3;
	mov.b64 	%rd1858, {%r10835, %r10834};
	shr.u64 	%rd1859, %rd1843, 6;
	xor.b64  	%rd1860, %rd1857, %rd1859;
	xor.b64  	%rd1861, %rd1860, %rd1858;
	shf.r.wrap.b32 	%r10836, %r10701, %r10700, 1;
	shf.r.wrap.b32 	%r10837, %r10700, %r10701, 1;
	mov.b64 	%rd1862, {%r10837, %r10836};
	shf.r.wrap.b32 	%r10838, %r10701, %r10700, 8;
	shf.r.wrap.b32 	%r10839, %r10700, %r10701, 8;
	mov.b64 	%rd1863, {%r10839, %r10838};
	shr.u64 	%rd1864, %rd1290, 7;
	xor.b64  	%rd1865, %rd1862, %rd1864;
	xor.b64  	%rd1866, %rd1865, %rd1863;
	add.s64 	%rd1867, %rd1778, %rd1277;
	add.s64 	%rd1868, %rd1867, %rd1861;
	add.s64 	%rd1869, %rd1868, %rd1866;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10840,%dummy}, %rd1856;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10841}, %rd1856;
	}
	shf.r.wrap.b32 	%r10842, %r10841, %r10840, 19;
	shf.r.wrap.b32 	%r10843, %r10840, %r10841, 19;
	mov.b64 	%rd1870, {%r10843, %r10842};
	shf.l.wrap.b32 	%r10844, %r10840, %r10841, 3;
	shf.l.wrap.b32 	%r10845, %r10841, %r10840, 3;
	mov.b64 	%rd1871, {%r10845, %r10844};
	shr.u64 	%rd1872, %rd1856, 6;
	xor.b64  	%rd1873, %rd1870, %rd1872;
	xor.b64  	%rd1874, %rd1873, %rd1871;
	shf.r.wrap.b32 	%r10846, %r10711, %r10710, 1;
	shf.r.wrap.b32 	%r10847, %r10710, %r10711, 1;
	mov.b64 	%rd1875, {%r10847, %r10846};
	shf.r.wrap.b32 	%r10848, %r10711, %r10710, 8;
	shf.r.wrap.b32 	%r10849, %r10710, %r10711, 8;
	mov.b64 	%rd1876, {%r10849, %r10848};
	shr.u64 	%rd1877, %rd1687, 7;
	xor.b64  	%rd1878, %rd1875, %rd1877;
	xor.b64  	%rd1879, %rd1878, %rd1876;
	add.s64 	%rd1880, %rd1791, %rd1290;
	add.s64 	%rd1881, %rd1880, %rd1874;
	add.s64 	%rd1882, %rd1881, %rd1879;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10850,%dummy}, %rd1663;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10851}, %rd1663;
	}
	shf.r.wrap.b32 	%r10852, %r10851, %r10850, 14;
	shf.r.wrap.b32 	%r10853, %r10850, %r10851, 14;
	mov.b64 	%rd1883, {%r10853, %r10852};
	shf.r.wrap.b32 	%r10854, %r10851, %r10850, 18;
	shf.r.wrap.b32 	%r10855, %r10850, %r10851, 18;
	mov.b64 	%rd1884, {%r10855, %r10854};
	xor.b64  	%rd1885, %rd1884, %rd1883;
	shf.l.wrap.b32 	%r10856, %r10850, %r10851, 23;
	shf.l.wrap.b32 	%r10857, %r10851, %r10850, 23;
	mov.b64 	%rd1886, {%r10857, %r10856};
	xor.b64  	%rd1887, %rd1885, %rd1886;
	xor.b64  	%rd1888, %rd1615, %rd1639;
	and.b64  	%rd1889, %rd1888, %rd1663;
	xor.b64  	%rd1890, %rd1889, %rd1615;
	add.s64 	%rd1891, %rd1890, %rd1591;
	add.s64 	%rd1892, %rd1891, %rd1687;
	add.s64 	%rd1893, %rd1892, %rd2801;
	add.s64 	%rd1894, %rd1893, %rd1887;
	add.s64 	%rd1895, %rd1894, %rd1602;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10858,%dummy}, %rd1674;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10859}, %rd1674;
	}
	shf.r.wrap.b32 	%r10860, %r10859, %r10858, 28;
	shf.r.wrap.b32 	%r10861, %r10858, %r10859, 28;
	mov.b64 	%rd1896, {%r10861, %r10860};
	shf.l.wrap.b32 	%r10862, %r10858, %r10859, 30;
	shf.l.wrap.b32 	%r10863, %r10859, %r10858, 30;
	mov.b64 	%rd1897, {%r10863, %r10862};
	xor.b64  	%rd1898, %rd1897, %rd1896;
	shf.l.wrap.b32 	%r10864, %r10858, %r10859, 25;
	shf.l.wrap.b32 	%r10865, %r10859, %r10858, 25;
	mov.b64 	%rd1899, {%r10865, %r10864};
	xor.b64  	%rd1900, %rd1898, %rd1899;
	xor.b64  	%rd1901, %rd1674, %rd1626;
	xor.b64  	%rd1902, %rd1674, %rd1650;
	and.b64  	%rd1903, %rd1902, %rd1901;
	xor.b64  	%rd1904, %rd1903, %rd1674;
	add.s64 	%rd1905, %rd1894, %rd1904;
	add.s64 	%rd1906, %rd1905, %rd1900;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10866,%dummy}, %rd1895;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10867}, %rd1895;
	}
	shf.r.wrap.b32 	%r10868, %r10867, %r10866, 14;
	shf.r.wrap.b32 	%r10869, %r10866, %r10867, 14;
	mov.b64 	%rd1907, {%r10869, %r10868};
	shf.r.wrap.b32 	%r10870, %r10867, %r10866, 18;
	shf.r.wrap.b32 	%r10871, %r10866, %r10867, 18;
	mov.b64 	%rd1908, {%r10871, %r10870};
	xor.b64  	%rd1909, %rd1908, %rd1907;
	shf.l.wrap.b32 	%r10872, %r10866, %r10867, 23;
	shf.l.wrap.b32 	%r10873, %r10867, %r10866, 23;
	mov.b64 	%rd1910, {%r10873, %r10872};
	xor.b64  	%rd1911, %rd1909, %rd1910;
	xor.b64  	%rd1912, %rd1639, %rd1663;
	and.b64  	%rd1913, %rd1895, %rd1912;
	xor.b64  	%rd1914, %rd1913, %rd1639;
	add.s64 	%rd1915, %rd1700, %rd1615;
	add.s64 	%rd1916, %rd1915, %rd2802;
	add.s64 	%rd1917, %rd1916, %rd1914;
	add.s64 	%rd1918, %rd1917, %rd1911;
	add.s64 	%rd1919, %rd1918, %rd1626;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10874,%dummy}, %rd1906;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10875}, %rd1906;
	}
	shf.r.wrap.b32 	%r10876, %r10875, %r10874, 28;
	shf.r.wrap.b32 	%r10877, %r10874, %r10875, 28;
	mov.b64 	%rd1920, {%r10877, %r10876};
	shf.l.wrap.b32 	%r10878, %r10874, %r10875, 30;
	shf.l.wrap.b32 	%r10879, %r10875, %r10874, 30;
	mov.b64 	%rd1921, {%r10879, %r10878};
	xor.b64  	%rd1922, %rd1921, %rd1920;
	shf.l.wrap.b32 	%r10880, %r10874, %r10875, 25;
	shf.l.wrap.b32 	%r10881, %r10875, %r10874, 25;
	mov.b64 	%rd1923, {%r10881, %r10880};
	xor.b64  	%rd1924, %rd1922, %rd1923;
	xor.b64  	%rd1925, %rd1906, %rd1650;
	xor.b64  	%rd1926, %rd1906, %rd1674;
	and.b64  	%rd1927, %rd1926, %rd1925;
	xor.b64  	%rd1928, %rd1927, %rd1906;
	add.s64 	%rd1929, %rd1918, %rd1928;
	add.s64 	%rd1930, %rd1929, %rd1924;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10882,%dummy}, %rd1919;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10883}, %rd1919;
	}
	shf.r.wrap.b32 	%r10884, %r10883, %r10882, 14;
	shf.r.wrap.b32 	%r10885, %r10882, %r10883, 14;
	mov.b64 	%rd1931, {%r10885, %r10884};
	shf.r.wrap.b32 	%r10886, %r10883, %r10882, 18;
	shf.r.wrap.b32 	%r10887, %r10882, %r10883, 18;
	mov.b64 	%rd1932, {%r10887, %r10886};
	xor.b64  	%rd1933, %rd1932, %rd1931;
	shf.l.wrap.b32 	%r10888, %r10882, %r10883, 23;
	shf.l.wrap.b32 	%r10889, %r10883, %r10882, 23;
	mov.b64 	%rd1934, {%r10889, %r10888};
	xor.b64  	%rd1935, %rd1933, %rd1934;
	xor.b64  	%rd1936, %rd1895, %rd1663;
	and.b64  	%rd1937, %rd1919, %rd1936;
	xor.b64  	%rd1938, %rd1937, %rd1663;
	add.s64 	%rd1939, %rd1713, %rd1639;
	add.s64 	%rd1940, %rd1939, %rd2803;
	add.s64 	%rd1941, %rd1940, %rd1938;
	add.s64 	%rd1942, %rd1941, %rd1935;
	add.s64 	%rd1943, %rd1942, %rd1650;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10890,%dummy}, %rd1930;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10891}, %rd1930;
	}
	shf.r.wrap.b32 	%r10892, %r10891, %r10890, 28;
	shf.r.wrap.b32 	%r10893, %r10890, %r10891, 28;
	mov.b64 	%rd1944, {%r10893, %r10892};
	shf.l.wrap.b32 	%r10894, %r10890, %r10891, 30;
	shf.l.wrap.b32 	%r10895, %r10891, %r10890, 30;
	mov.b64 	%rd1945, {%r10895, %r10894};
	xor.b64  	%rd1946, %rd1945, %rd1944;
	shf.l.wrap.b32 	%r10896, %r10890, %r10891, 25;
	shf.l.wrap.b32 	%r10897, %r10891, %r10890, 25;
	mov.b64 	%rd1947, {%r10897, %r10896};
	xor.b64  	%rd1948, %rd1946, %rd1947;
	xor.b64  	%rd1949, %rd1930, %rd1674;
	xor.b64  	%rd1950, %rd1930, %rd1906;
	and.b64  	%rd1951, %rd1950, %rd1949;
	xor.b64  	%rd1952, %rd1951, %rd1930;
	add.s64 	%rd1953, %rd1942, %rd1952;
	add.s64 	%rd1954, %rd1953, %rd1948;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10898,%dummy}, %rd1943;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10899}, %rd1943;
	}
	shf.r.wrap.b32 	%r10900, %r10899, %r10898, 14;
	shf.r.wrap.b32 	%r10901, %r10898, %r10899, 14;
	mov.b64 	%rd1955, {%r10901, %r10900};
	shf.r.wrap.b32 	%r10902, %r10899, %r10898, 18;
	shf.r.wrap.b32 	%r10903, %r10898, %r10899, 18;
	mov.b64 	%rd1956, {%r10903, %r10902};
	xor.b64  	%rd1957, %rd1956, %rd1955;
	shf.l.wrap.b32 	%r10904, %r10898, %r10899, 23;
	shf.l.wrap.b32 	%r10905, %r10899, %r10898, 23;
	mov.b64 	%rd1958, {%r10905, %r10904};
	xor.b64  	%rd1959, %rd1957, %rd1958;
	xor.b64  	%rd1960, %rd1919, %rd1895;
	and.b64  	%rd1961, %rd1943, %rd1960;
	xor.b64  	%rd1962, %rd1961, %rd1895;
	add.s64 	%rd1963, %rd1726, %rd1663;
	add.s64 	%rd1964, %rd1963, %rd2804;
	add.s64 	%rd1965, %rd1964, %rd1962;
	add.s64 	%rd1966, %rd1965, %rd1959;
	add.s64 	%rd1967, %rd1966, %rd1674;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10906,%dummy}, %rd1954;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10907}, %rd1954;
	}
	shf.r.wrap.b32 	%r10908, %r10907, %r10906, 28;
	shf.r.wrap.b32 	%r10909, %r10906, %r10907, 28;
	mov.b64 	%rd1968, {%r10909, %r10908};
	shf.l.wrap.b32 	%r10910, %r10906, %r10907, 30;
	shf.l.wrap.b32 	%r10911, %r10907, %r10906, 30;
	mov.b64 	%rd1969, {%r10911, %r10910};
	xor.b64  	%rd1970, %rd1969, %rd1968;
	shf.l.wrap.b32 	%r10912, %r10906, %r10907, 25;
	shf.l.wrap.b32 	%r10913, %r10907, %r10906, 25;
	mov.b64 	%rd1971, {%r10913, %r10912};
	xor.b64  	%rd1972, %rd1970, %rd1971;
	xor.b64  	%rd1973, %rd1954, %rd1906;
	xor.b64  	%rd1974, %rd1954, %rd1930;
	and.b64  	%rd1975, %rd1974, %rd1973;
	xor.b64  	%rd1976, %rd1975, %rd1954;
	add.s64 	%rd1977, %rd1966, %rd1976;
	add.s64 	%rd1978, %rd1977, %rd1972;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10914,%dummy}, %rd1967;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10915}, %rd1967;
	}
	shf.r.wrap.b32 	%r10916, %r10915, %r10914, 14;
	shf.r.wrap.b32 	%r10917, %r10914, %r10915, 14;
	mov.b64 	%rd1979, {%r10917, %r10916};
	shf.r.wrap.b32 	%r10918, %r10915, %r10914, 18;
	shf.r.wrap.b32 	%r10919, %r10914, %r10915, 18;
	mov.b64 	%rd1980, {%r10919, %r10918};
	xor.b64  	%rd1981, %rd1980, %rd1979;
	shf.l.wrap.b32 	%r10920, %r10914, %r10915, 23;
	shf.l.wrap.b32 	%r10921, %r10915, %r10914, 23;
	mov.b64 	%rd1982, {%r10921, %r10920};
	xor.b64  	%rd1983, %rd1981, %rd1982;
	xor.b64  	%rd1984, %rd1943, %rd1919;
	and.b64  	%rd1985, %rd1967, %rd1984;
	xor.b64  	%rd1986, %rd1985, %rd1919;
	add.s64 	%rd1987, %rd1895, %rd1739;
	add.s64 	%rd1988, %rd1987, %rd2805;
	add.s64 	%rd1989, %rd1988, %rd1986;
	add.s64 	%rd1990, %rd1989, %rd1983;
	add.s64 	%rd1991, %rd1990, %rd1906;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10922,%dummy}, %rd1978;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10923}, %rd1978;
	}
	shf.r.wrap.b32 	%r10924, %r10923, %r10922, 28;
	shf.r.wrap.b32 	%r10925, %r10922, %r10923, 28;
	mov.b64 	%rd1992, {%r10925, %r10924};
	shf.l.wrap.b32 	%r10926, %r10922, %r10923, 30;
	shf.l.wrap.b32 	%r10927, %r10923, %r10922, 30;
	mov.b64 	%rd1993, {%r10927, %r10926};
	xor.b64  	%rd1994, %rd1993, %rd1992;
	shf.l.wrap.b32 	%r10928, %r10922, %r10923, 25;
	shf.l.wrap.b32 	%r10929, %r10923, %r10922, 25;
	mov.b64 	%rd1995, {%r10929, %r10928};
	xor.b64  	%rd1996, %rd1994, %rd1995;
	xor.b64  	%rd1997, %rd1978, %rd1930;
	xor.b64  	%rd1998, %rd1978, %rd1954;
	and.b64  	%rd1999, %rd1998, %rd1997;
	xor.b64  	%rd2000, %rd1999, %rd1978;
	add.s64 	%rd2001, %rd1990, %rd2000;
	add.s64 	%rd2002, %rd2001, %rd1996;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10930,%dummy}, %rd1991;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10931}, %rd1991;
	}
	shf.r.wrap.b32 	%r10932, %r10931, %r10930, 14;
	shf.r.wrap.b32 	%r10933, %r10930, %r10931, 14;
	mov.b64 	%rd2003, {%r10933, %r10932};
	shf.r.wrap.b32 	%r10934, %r10931, %r10930, 18;
	shf.r.wrap.b32 	%r10935, %r10930, %r10931, 18;
	mov.b64 	%rd2004, {%r10935, %r10934};
	xor.b64  	%rd2005, %rd2004, %rd2003;
	shf.l.wrap.b32 	%r10936, %r10930, %r10931, 23;
	shf.l.wrap.b32 	%r10937, %r10931, %r10930, 23;
	mov.b64 	%rd2006, {%r10937, %r10936};
	xor.b64  	%rd2007, %rd2005, %rd2006;
	xor.b64  	%rd2008, %rd1967, %rd1943;
	and.b64  	%rd2009, %rd1991, %rd2008;
	xor.b64  	%rd2010, %rd2009, %rd1943;
	add.s64 	%rd2011, %rd1919, %rd1752;
	add.s64 	%rd2012, %rd2011, %rd2806;
	add.s64 	%rd2013, %rd2012, %rd2010;
	add.s64 	%rd2014, %rd2013, %rd2007;
	add.s64 	%rd2015, %rd2014, %rd1930;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10938,%dummy}, %rd2002;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10939}, %rd2002;
	}
	shf.r.wrap.b32 	%r10940, %r10939, %r10938, 28;
	shf.r.wrap.b32 	%r10941, %r10938, %r10939, 28;
	mov.b64 	%rd2016, {%r10941, %r10940};
	shf.l.wrap.b32 	%r10942, %r10938, %r10939, 30;
	shf.l.wrap.b32 	%r10943, %r10939, %r10938, 30;
	mov.b64 	%rd2017, {%r10943, %r10942};
	xor.b64  	%rd2018, %rd2017, %rd2016;
	shf.l.wrap.b32 	%r10944, %r10938, %r10939, 25;
	shf.l.wrap.b32 	%r10945, %r10939, %r10938, 25;
	mov.b64 	%rd2019, {%r10945, %r10944};
	xor.b64  	%rd2020, %rd2018, %rd2019;
	xor.b64  	%rd2021, %rd2002, %rd1954;
	xor.b64  	%rd2022, %rd2002, %rd1978;
	and.b64  	%rd2023, %rd2022, %rd2021;
	xor.b64  	%rd2024, %rd2023, %rd2002;
	add.s64 	%rd2025, %rd2014, %rd2024;
	add.s64 	%rd2026, %rd2025, %rd2020;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10946,%dummy}, %rd2015;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10947}, %rd2015;
	}
	shf.r.wrap.b32 	%r10948, %r10947, %r10946, 14;
	shf.r.wrap.b32 	%r10949, %r10946, %r10947, 14;
	mov.b64 	%rd2027, {%r10949, %r10948};
	shf.r.wrap.b32 	%r10950, %r10947, %r10946, 18;
	shf.r.wrap.b32 	%r10951, %r10946, %r10947, 18;
	mov.b64 	%rd2028, {%r10951, %r10950};
	xor.b64  	%rd2029, %rd2028, %rd2027;
	shf.l.wrap.b32 	%r10952, %r10946, %r10947, 23;
	shf.l.wrap.b32 	%r10953, %r10947, %r10946, 23;
	mov.b64 	%rd2030, {%r10953, %r10952};
	xor.b64  	%rd2031, %rd2029, %rd2030;
	xor.b64  	%rd2032, %rd1991, %rd1967;
	and.b64  	%rd2033, %rd2015, %rd2032;
	xor.b64  	%rd2034, %rd2033, %rd1967;
	add.s64 	%rd2035, %rd1943, %rd1765;
	add.s64 	%rd2036, %rd2035, %rd2807;
	add.s64 	%rd2037, %rd2036, %rd2034;
	add.s64 	%rd2038, %rd2037, %rd2031;
	add.s64 	%rd2039, %rd2038, %rd1954;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10954,%dummy}, %rd2026;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10955}, %rd2026;
	}
	shf.r.wrap.b32 	%r10956, %r10955, %r10954, 28;
	shf.r.wrap.b32 	%r10957, %r10954, %r10955, 28;
	mov.b64 	%rd2040, {%r10957, %r10956};
	shf.l.wrap.b32 	%r10958, %r10954, %r10955, 30;
	shf.l.wrap.b32 	%r10959, %r10955, %r10954, 30;
	mov.b64 	%rd2041, {%r10959, %r10958};
	xor.b64  	%rd2042, %rd2041, %rd2040;
	shf.l.wrap.b32 	%r10960, %r10954, %r10955, 25;
	shf.l.wrap.b32 	%r10961, %r10955, %r10954, 25;
	mov.b64 	%rd2043, {%r10961, %r10960};
	xor.b64  	%rd2044, %rd2042, %rd2043;
	xor.b64  	%rd2045, %rd2026, %rd1978;
	xor.b64  	%rd2046, %rd2026, %rd2002;
	and.b64  	%rd2047, %rd2046, %rd2045;
	xor.b64  	%rd2048, %rd2047, %rd2026;
	add.s64 	%rd2049, %rd2038, %rd2048;
	add.s64 	%rd2050, %rd2049, %rd2044;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10962,%dummy}, %rd2039;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10963}, %rd2039;
	}
	shf.r.wrap.b32 	%r10964, %r10963, %r10962, 14;
	shf.r.wrap.b32 	%r10965, %r10962, %r10963, 14;
	mov.b64 	%rd2051, {%r10965, %r10964};
	shf.r.wrap.b32 	%r10966, %r10963, %r10962, 18;
	shf.r.wrap.b32 	%r10967, %r10962, %r10963, 18;
	mov.b64 	%rd2052, {%r10967, %r10966};
	xor.b64  	%rd2053, %rd2052, %rd2051;
	shf.l.wrap.b32 	%r10968, %r10962, %r10963, 23;
	shf.l.wrap.b32 	%r10969, %r10963, %r10962, 23;
	mov.b64 	%rd2054, {%r10969, %r10968};
	xor.b64  	%rd2055, %rd2053, %rd2054;
	xor.b64  	%rd2056, %rd2015, %rd1991;
	and.b64  	%rd2057, %rd2039, %rd2056;
	xor.b64  	%rd2058, %rd2057, %rd1991;
	add.s64 	%rd2059, %rd1967, %rd1778;
	add.s64 	%rd2060, %rd2059, %rd2808;
	add.s64 	%rd2061, %rd2060, %rd2058;
	add.s64 	%rd2062, %rd2061, %rd2055;
	add.s64 	%rd2063, %rd2062, %rd1978;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10970,%dummy}, %rd2050;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10971}, %rd2050;
	}
	shf.r.wrap.b32 	%r10972, %r10971, %r10970, 28;
	shf.r.wrap.b32 	%r10973, %r10970, %r10971, 28;
	mov.b64 	%rd2064, {%r10973, %r10972};
	shf.l.wrap.b32 	%r10974, %r10970, %r10971, 30;
	shf.l.wrap.b32 	%r10975, %r10971, %r10970, 30;
	mov.b64 	%rd2065, {%r10975, %r10974};
	xor.b64  	%rd2066, %rd2065, %rd2064;
	shf.l.wrap.b32 	%r10976, %r10970, %r10971, 25;
	shf.l.wrap.b32 	%r10977, %r10971, %r10970, 25;
	mov.b64 	%rd2067, {%r10977, %r10976};
	xor.b64  	%rd2068, %rd2066, %rd2067;
	xor.b64  	%rd2069, %rd2050, %rd2002;
	xor.b64  	%rd2070, %rd2050, %rd2026;
	and.b64  	%rd2071, %rd2070, %rd2069;
	xor.b64  	%rd2072, %rd2071, %rd2050;
	add.s64 	%rd2073, %rd2062, %rd2072;
	add.s64 	%rd2074, %rd2073, %rd2068;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10978,%dummy}, %rd2063;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10979}, %rd2063;
	}
	shf.r.wrap.b32 	%r10980, %r10979, %r10978, 14;
	shf.r.wrap.b32 	%r10981, %r10978, %r10979, 14;
	mov.b64 	%rd2075, {%r10981, %r10980};
	shf.r.wrap.b32 	%r10982, %r10979, %r10978, 18;
	shf.r.wrap.b32 	%r10983, %r10978, %r10979, 18;
	mov.b64 	%rd2076, {%r10983, %r10982};
	xor.b64  	%rd2077, %rd2076, %rd2075;
	shf.l.wrap.b32 	%r10984, %r10978, %r10979, 23;
	shf.l.wrap.b32 	%r10985, %r10979, %r10978, 23;
	mov.b64 	%rd2078, {%r10985, %r10984};
	xor.b64  	%rd2079, %rd2077, %rd2078;
	xor.b64  	%rd2080, %rd2039, %rd2015;
	and.b64  	%rd2081, %rd2063, %rd2080;
	xor.b64  	%rd2082, %rd2081, %rd2015;
	add.s64 	%rd2083, %rd1991, %rd1791;
	add.s64 	%rd2084, %rd2083, %rd2809;
	add.s64 	%rd2085, %rd2084, %rd2082;
	add.s64 	%rd2086, %rd2085, %rd2079;
	add.s64 	%rd2087, %rd2086, %rd2002;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10986,%dummy}, %rd2074;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10987}, %rd2074;
	}
	shf.r.wrap.b32 	%r10988, %r10987, %r10986, 28;
	shf.r.wrap.b32 	%r10989, %r10986, %r10987, 28;
	mov.b64 	%rd2088, {%r10989, %r10988};
	shf.l.wrap.b32 	%r10990, %r10986, %r10987, 30;
	shf.l.wrap.b32 	%r10991, %r10987, %r10986, 30;
	mov.b64 	%rd2089, {%r10991, %r10990};
	xor.b64  	%rd2090, %rd2089, %rd2088;
	shf.l.wrap.b32 	%r10992, %r10986, %r10987, 25;
	shf.l.wrap.b32 	%r10993, %r10987, %r10986, 25;
	mov.b64 	%rd2091, {%r10993, %r10992};
	xor.b64  	%rd2092, %rd2090, %rd2091;
	xor.b64  	%rd2093, %rd2074, %rd2026;
	xor.b64  	%rd2094, %rd2074, %rd2050;
	and.b64  	%rd2095, %rd2094, %rd2093;
	xor.b64  	%rd2096, %rd2095, %rd2074;
	add.s64 	%rd2097, %rd2086, %rd2096;
	add.s64 	%rd2098, %rd2097, %rd2092;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r10994,%dummy}, %rd2087;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r10995}, %rd2087;
	}
	shf.r.wrap.b32 	%r10996, %r10995, %r10994, 14;
	shf.r.wrap.b32 	%r10997, %r10994, %r10995, 14;
	mov.b64 	%rd2099, {%r10997, %r10996};
	shf.r.wrap.b32 	%r10998, %r10995, %r10994, 18;
	shf.r.wrap.b32 	%r10999, %r10994, %r10995, 18;
	mov.b64 	%rd2100, {%r10999, %r10998};
	xor.b64  	%rd2101, %rd2100, %rd2099;
	shf.l.wrap.b32 	%r11000, %r10994, %r10995, 23;
	shf.l.wrap.b32 	%r11001, %r10995, %r10994, 23;
	mov.b64 	%rd2102, {%r11001, %r11000};
	xor.b64  	%rd2103, %rd2101, %rd2102;
	xor.b64  	%rd2104, %rd2063, %rd2039;
	and.b64  	%rd2105, %rd2087, %rd2104;
	xor.b64  	%rd2106, %rd2105, %rd2039;
	add.s64 	%rd2107, %rd2015, %rd1804;
	add.s64 	%rd2108, %rd2107, %rd2810;
	add.s64 	%rd2109, %rd2108, %rd2106;
	add.s64 	%rd2110, %rd2109, %rd2103;
	add.s64 	%rd2111, %rd2110, %rd2026;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11002,%dummy}, %rd2098;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11003}, %rd2098;
	}
	shf.r.wrap.b32 	%r11004, %r11003, %r11002, 28;
	shf.r.wrap.b32 	%r11005, %r11002, %r11003, 28;
	mov.b64 	%rd2112, {%r11005, %r11004};
	shf.l.wrap.b32 	%r11006, %r11002, %r11003, 30;
	shf.l.wrap.b32 	%r11007, %r11003, %r11002, 30;
	mov.b64 	%rd2113, {%r11007, %r11006};
	xor.b64  	%rd2114, %rd2113, %rd2112;
	shf.l.wrap.b32 	%r11008, %r11002, %r11003, 25;
	shf.l.wrap.b32 	%r11009, %r11003, %r11002, 25;
	mov.b64 	%rd2115, {%r11009, %r11008};
	xor.b64  	%rd2116, %rd2114, %rd2115;
	xor.b64  	%rd2117, %rd2098, %rd2050;
	xor.b64  	%rd2118, %rd2098, %rd2074;
	and.b64  	%rd2119, %rd2118, %rd2117;
	xor.b64  	%rd2120, %rd2119, %rd2098;
	add.s64 	%rd2121, %rd2110, %rd2120;
	add.s64 	%rd2122, %rd2121, %rd2116;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11010,%dummy}, %rd2111;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11011}, %rd2111;
	}
	shf.r.wrap.b32 	%r11012, %r11011, %r11010, 14;
	shf.r.wrap.b32 	%r11013, %r11010, %r11011, 14;
	mov.b64 	%rd2123, {%r11013, %r11012};
	shf.r.wrap.b32 	%r11014, %r11011, %r11010, 18;
	shf.r.wrap.b32 	%r11015, %r11010, %r11011, 18;
	mov.b64 	%rd2124, {%r11015, %r11014};
	xor.b64  	%rd2125, %rd2124, %rd2123;
	shf.l.wrap.b32 	%r11016, %r11010, %r11011, 23;
	shf.l.wrap.b32 	%r11017, %r11011, %r11010, 23;
	mov.b64 	%rd2126, {%r11017, %r11016};
	xor.b64  	%rd2127, %rd2125, %rd2126;
	xor.b64  	%rd2128, %rd2087, %rd2063;
	and.b64  	%rd2129, %rd2111, %rd2128;
	xor.b64  	%rd2130, %rd2129, %rd2063;
	add.s64 	%rd2131, %rd2039, %rd1817;
	add.s64 	%rd2132, %rd2131, %rd2811;
	add.s64 	%rd2133, %rd2132, %rd2130;
	add.s64 	%rd2134, %rd2133, %rd2127;
	add.s64 	%rd2135, %rd2134, %rd2050;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11018,%dummy}, %rd2122;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11019}, %rd2122;
	}
	shf.r.wrap.b32 	%r11020, %r11019, %r11018, 28;
	shf.r.wrap.b32 	%r11021, %r11018, %r11019, 28;
	mov.b64 	%rd2136, {%r11021, %r11020};
	shf.l.wrap.b32 	%r11022, %r11018, %r11019, 30;
	shf.l.wrap.b32 	%r11023, %r11019, %r11018, 30;
	mov.b64 	%rd2137, {%r11023, %r11022};
	xor.b64  	%rd2138, %rd2137, %rd2136;
	shf.l.wrap.b32 	%r11024, %r11018, %r11019, 25;
	shf.l.wrap.b32 	%r11025, %r11019, %r11018, 25;
	mov.b64 	%rd2139, {%r11025, %r11024};
	xor.b64  	%rd2140, %rd2138, %rd2139;
	xor.b64  	%rd2141, %rd2122, %rd2074;
	xor.b64  	%rd2142, %rd2122, %rd2098;
	and.b64  	%rd2143, %rd2142, %rd2141;
	xor.b64  	%rd2144, %rd2143, %rd2122;
	add.s64 	%rd2145, %rd2134, %rd2144;
	add.s64 	%rd2146, %rd2145, %rd2140;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11026,%dummy}, %rd2135;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11027}, %rd2135;
	}
	shf.r.wrap.b32 	%r11028, %r11027, %r11026, 14;
	shf.r.wrap.b32 	%r11029, %r11026, %r11027, 14;
	mov.b64 	%rd2147, {%r11029, %r11028};
	shf.r.wrap.b32 	%r11030, %r11027, %r11026, 18;
	shf.r.wrap.b32 	%r11031, %r11026, %r11027, 18;
	mov.b64 	%rd2148, {%r11031, %r11030};
	xor.b64  	%rd2149, %rd2148, %rd2147;
	shf.l.wrap.b32 	%r11032, %r11026, %r11027, 23;
	shf.l.wrap.b32 	%r11033, %r11027, %r11026, 23;
	mov.b64 	%rd2150, {%r11033, %r11032};
	xor.b64  	%rd2151, %rd2149, %rd2150;
	xor.b64  	%rd2152, %rd2111, %rd2087;
	and.b64  	%rd2153, %rd2135, %rd2152;
	xor.b64  	%rd2154, %rd2153, %rd2087;
	add.s64 	%rd2155, %rd2063, %rd1830;
	add.s64 	%rd2156, %rd2155, %rd2812;
	add.s64 	%rd2157, %rd2156, %rd2154;
	add.s64 	%rd2158, %rd2157, %rd2151;
	add.s64 	%rd2159, %rd2158, %rd2074;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11034,%dummy}, %rd2146;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11035}, %rd2146;
	}
	shf.r.wrap.b32 	%r11036, %r11035, %r11034, 28;
	shf.r.wrap.b32 	%r11037, %r11034, %r11035, 28;
	mov.b64 	%rd2160, {%r11037, %r11036};
	shf.l.wrap.b32 	%r11038, %r11034, %r11035, 30;
	shf.l.wrap.b32 	%r11039, %r11035, %r11034, 30;
	mov.b64 	%rd2161, {%r11039, %r11038};
	xor.b64  	%rd2162, %rd2161, %rd2160;
	shf.l.wrap.b32 	%r11040, %r11034, %r11035, 25;
	shf.l.wrap.b32 	%r11041, %r11035, %r11034, 25;
	mov.b64 	%rd2163, {%r11041, %r11040};
	xor.b64  	%rd2164, %rd2162, %rd2163;
	xor.b64  	%rd2165, %rd2146, %rd2098;
	xor.b64  	%rd2166, %rd2146, %rd2122;
	and.b64  	%rd2167, %rd2166, %rd2165;
	xor.b64  	%rd2168, %rd2167, %rd2146;
	add.s64 	%rd2169, %rd2158, %rd2168;
	add.s64 	%rd2170, %rd2169, %rd2164;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11042,%dummy}, %rd2159;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11043}, %rd2159;
	}
	shf.r.wrap.b32 	%r11044, %r11043, %r11042, 14;
	shf.r.wrap.b32 	%r11045, %r11042, %r11043, 14;
	mov.b64 	%rd2171, {%r11045, %r11044};
	shf.r.wrap.b32 	%r11046, %r11043, %r11042, 18;
	shf.r.wrap.b32 	%r11047, %r11042, %r11043, 18;
	mov.b64 	%rd2172, {%r11047, %r11046};
	xor.b64  	%rd2173, %rd2172, %rd2171;
	shf.l.wrap.b32 	%r11048, %r11042, %r11043, 23;
	shf.l.wrap.b32 	%r11049, %r11043, %r11042, 23;
	mov.b64 	%rd2174, {%r11049, %r11048};
	xor.b64  	%rd2175, %rd2173, %rd2174;
	xor.b64  	%rd2176, %rd2135, %rd2111;
	and.b64  	%rd2177, %rd2159, %rd2176;
	xor.b64  	%rd2178, %rd2177, %rd2111;
	add.s64 	%rd2179, %rd2087, %rd1843;
	add.s64 	%rd2180, %rd2179, %rd2813;
	add.s64 	%rd2181, %rd2180, %rd2178;
	add.s64 	%rd2182, %rd2181, %rd2175;
	add.s64 	%rd2183, %rd2182, %rd2098;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11050,%dummy}, %rd2170;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11051}, %rd2170;
	}
	shf.r.wrap.b32 	%r11052, %r11051, %r11050, 28;
	shf.r.wrap.b32 	%r11053, %r11050, %r11051, 28;
	mov.b64 	%rd2184, {%r11053, %r11052};
	shf.l.wrap.b32 	%r11054, %r11050, %r11051, 30;
	shf.l.wrap.b32 	%r11055, %r11051, %r11050, 30;
	mov.b64 	%rd2185, {%r11055, %r11054};
	xor.b64  	%rd2186, %rd2185, %rd2184;
	shf.l.wrap.b32 	%r11056, %r11050, %r11051, 25;
	shf.l.wrap.b32 	%r11057, %r11051, %r11050, 25;
	mov.b64 	%rd2187, {%r11057, %r11056};
	xor.b64  	%rd2188, %rd2186, %rd2187;
	xor.b64  	%rd2189, %rd2170, %rd2122;
	xor.b64  	%rd2190, %rd2170, %rd2146;
	and.b64  	%rd2191, %rd2190, %rd2189;
	xor.b64  	%rd2192, %rd2191, %rd2170;
	add.s64 	%rd2193, %rd2182, %rd2192;
	add.s64 	%rd2194, %rd2193, %rd2188;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11058,%dummy}, %rd2183;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11059}, %rd2183;
	}
	shf.r.wrap.b32 	%r11060, %r11059, %r11058, 14;
	shf.r.wrap.b32 	%r11061, %r11058, %r11059, 14;
	mov.b64 	%rd2195, {%r11061, %r11060};
	shf.r.wrap.b32 	%r11062, %r11059, %r11058, 18;
	shf.r.wrap.b32 	%r11063, %r11058, %r11059, 18;
	mov.b64 	%rd2196, {%r11063, %r11062};
	xor.b64  	%rd2197, %rd2196, %rd2195;
	shf.l.wrap.b32 	%r11064, %r11058, %r11059, 23;
	shf.l.wrap.b32 	%r11065, %r11059, %r11058, 23;
	mov.b64 	%rd2198, {%r11065, %r11064};
	xor.b64  	%rd2199, %rd2197, %rd2198;
	xor.b64  	%rd2200, %rd2159, %rd2135;
	and.b64  	%rd2201, %rd2183, %rd2200;
	xor.b64  	%rd2202, %rd2201, %rd2135;
	add.s64 	%rd2203, %rd2111, %rd1856;
	add.s64 	%rd2204, %rd2203, %rd2814;
	add.s64 	%rd2205, %rd2204, %rd2202;
	add.s64 	%rd2206, %rd2205, %rd2199;
	add.s64 	%rd2207, %rd2206, %rd2122;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11066,%dummy}, %rd2194;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11067}, %rd2194;
	}
	shf.r.wrap.b32 	%r11068, %r11067, %r11066, 28;
	shf.r.wrap.b32 	%r11069, %r11066, %r11067, 28;
	mov.b64 	%rd2208, {%r11069, %r11068};
	shf.l.wrap.b32 	%r11070, %r11066, %r11067, 30;
	shf.l.wrap.b32 	%r11071, %r11067, %r11066, 30;
	mov.b64 	%rd2209, {%r11071, %r11070};
	xor.b64  	%rd2210, %rd2209, %rd2208;
	shf.l.wrap.b32 	%r11072, %r11066, %r11067, 25;
	shf.l.wrap.b32 	%r11073, %r11067, %r11066, 25;
	mov.b64 	%rd2211, {%r11073, %r11072};
	xor.b64  	%rd2212, %rd2210, %rd2211;
	xor.b64  	%rd2213, %rd2194, %rd2146;
	xor.b64  	%rd2214, %rd2194, %rd2170;
	and.b64  	%rd2215, %rd2214, %rd2213;
	xor.b64  	%rd2216, %rd2215, %rd2194;
	add.s64 	%rd2217, %rd2206, %rd2216;
	add.s64 	%rd2218, %rd2217, %rd2212;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11074,%dummy}, %rd2207;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11075}, %rd2207;
	}
	shf.r.wrap.b32 	%r11076, %r11075, %r11074, 14;
	shf.r.wrap.b32 	%r11077, %r11074, %r11075, 14;
	mov.b64 	%rd2219, {%r11077, %r11076};
	shf.r.wrap.b32 	%r11078, %r11075, %r11074, 18;
	shf.r.wrap.b32 	%r11079, %r11074, %r11075, 18;
	mov.b64 	%rd2220, {%r11079, %r11078};
	xor.b64  	%rd2221, %rd2220, %rd2219;
	shf.l.wrap.b32 	%r11080, %r11074, %r11075, 23;
	shf.l.wrap.b32 	%r11081, %r11075, %r11074, 23;
	mov.b64 	%rd2222, {%r11081, %r11080};
	xor.b64  	%rd2223, %rd2221, %rd2222;
	xor.b64  	%rd2224, %rd2183, %rd2159;
	and.b64  	%rd2225, %rd2207, %rd2224;
	xor.b64  	%rd2226, %rd2225, %rd2159;
	add.s64 	%rd2227, %rd2135, %rd1869;
	add.s64 	%rd2228, %rd2227, %rd2815;
	add.s64 	%rd2229, %rd2228, %rd2226;
	add.s64 	%rd2230, %rd2229, %rd2223;
	add.s64 	%rd2231, %rd2230, %rd2146;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11082,%dummy}, %rd2218;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11083}, %rd2218;
	}
	shf.r.wrap.b32 	%r11084, %r11083, %r11082, 28;
	shf.r.wrap.b32 	%r11085, %r11082, %r11083, 28;
	mov.b64 	%rd2232, {%r11085, %r11084};
	shf.l.wrap.b32 	%r11086, %r11082, %r11083, 30;
	shf.l.wrap.b32 	%r11087, %r11083, %r11082, 30;
	mov.b64 	%rd2233, {%r11087, %r11086};
	xor.b64  	%rd2234, %rd2233, %rd2232;
	shf.l.wrap.b32 	%r11088, %r11082, %r11083, 25;
	shf.l.wrap.b32 	%r11089, %r11083, %r11082, 25;
	mov.b64 	%rd2235, {%r11089, %r11088};
	xor.b64  	%rd2236, %rd2234, %rd2235;
	xor.b64  	%rd2237, %rd2218, %rd2170;
	xor.b64  	%rd2238, %rd2218, %rd2194;
	and.b64  	%rd2239, %rd2238, %rd2237;
	xor.b64  	%rd2240, %rd2239, %rd2218;
	add.s64 	%rd2241, %rd2230, %rd2240;
	add.s64 	%rd2242, %rd2241, %rd2236;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11090,%dummy}, %rd2231;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11091}, %rd2231;
	}
	shf.r.wrap.b32 	%r11092, %r11091, %r11090, 14;
	shf.r.wrap.b32 	%r11093, %r11090, %r11091, 14;
	mov.b64 	%rd2243, {%r11093, %r11092};
	shf.r.wrap.b32 	%r11094, %r11091, %r11090, 18;
	shf.r.wrap.b32 	%r11095, %r11090, %r11091, 18;
	mov.b64 	%rd2244, {%r11095, %r11094};
	xor.b64  	%rd2245, %rd2244, %rd2243;
	shf.l.wrap.b32 	%r11096, %r11090, %r11091, 23;
	shf.l.wrap.b32 	%r11097, %r11091, %r11090, 23;
	mov.b64 	%rd2246, {%r11097, %r11096};
	xor.b64  	%rd2247, %rd2245, %rd2246;
	xor.b64  	%rd2248, %rd2207, %rd2183;
	and.b64  	%rd2249, %rd2231, %rd2248;
	xor.b64  	%rd2250, %rd2249, %rd2183;
	add.s64 	%rd2251, %rd2159, %rd1882;
	add.s64 	%rd2252, %rd2251, %rd2816;
	add.s64 	%rd2253, %rd2252, %rd2250;
	add.s64 	%rd2254, %rd2253, %rd2247;
	add.s64 	%rd2255, %rd2254, %rd2170;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11098,%dummy}, %rd2242;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11099}, %rd2242;
	}
	shf.r.wrap.b32 	%r11100, %r11099, %r11098, 28;
	shf.r.wrap.b32 	%r11101, %r11098, %r11099, 28;
	mov.b64 	%rd2256, {%r11101, %r11100};
	shf.l.wrap.b32 	%r11102, %r11098, %r11099, 30;
	shf.l.wrap.b32 	%r11103, %r11099, %r11098, 30;
	mov.b64 	%rd2257, {%r11103, %r11102};
	xor.b64  	%rd2258, %rd2257, %rd2256;
	shf.l.wrap.b32 	%r11104, %r11098, %r11099, 25;
	shf.l.wrap.b32 	%r11105, %r11099, %r11098, 25;
	mov.b64 	%rd2259, {%r11105, %r11104};
	xor.b64  	%rd2260, %rd2258, %rd2259;
	xor.b64  	%rd2261, %rd2242, %rd2194;
	xor.b64  	%rd2262, %rd2242, %rd2218;
	and.b64  	%rd2263, %rd2262, %rd2261;
	xor.b64  	%rd2264, %rd2263, %rd2242;
	add.s64 	%rd2265, %rd2254, %rd2264;
	add.s64 	%rd2266, %rd2265, %rd2260;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11106,%dummy}, %rd1869;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11107}, %rd1869;
	}
	shf.r.wrap.b32 	%r11108, %r11107, %r11106, 19;
	shf.r.wrap.b32 	%r11109, %r11106, %r11107, 19;
	mov.b64 	%rd2267, {%r11109, %r11108};
	shf.l.wrap.b32 	%r11110, %r11106, %r11107, 3;
	shf.l.wrap.b32 	%r11111, %r11107, %r11106, 3;
	mov.b64 	%rd2268, {%r11111, %r11110};
	shr.u64 	%rd2269, %rd1869, 6;
	xor.b64  	%rd2270, %rd2267, %rd2269;
	xor.b64  	%rd2271, %rd2270, %rd2268;
	shf.r.wrap.b32 	%r11112, %r10721, %r10720, 1;
	shf.r.wrap.b32 	%r11113, %r10720, %r10721, 1;
	mov.b64 	%rd2272, {%r11113, %r11112};
	shf.r.wrap.b32 	%r11114, %r10721, %r10720, 8;
	shf.r.wrap.b32 	%r11115, %r10720, %r10721, 8;
	mov.b64 	%rd2273, {%r11115, %r11114};
	shr.u64 	%rd2274, %rd1700, 7;
	xor.b64  	%rd2275, %rd2272, %rd2274;
	xor.b64  	%rd2276, %rd2275, %rd2273;
	add.s64 	%rd2277, %rd1687, %rd1804;
	add.s64 	%rd2278, %rd2277, %rd2271;
	add.s64 	%rd2279, %rd2278, %rd2276;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11116,%dummy}, %rd1882;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11117}, %rd1882;
	}
	shf.r.wrap.b32 	%r11118, %r11117, %r11116, 19;
	shf.r.wrap.b32 	%r11119, %r11116, %r11117, 19;
	mov.b64 	%rd2280, {%r11119, %r11118};
	shf.l.wrap.b32 	%r11120, %r11116, %r11117, 3;
	shf.l.wrap.b32 	%r11121, %r11117, %r11116, 3;
	mov.b64 	%rd2281, {%r11121, %r11120};
	shr.u64 	%rd2282, %rd1882, 6;
	xor.b64  	%rd2283, %rd2280, %rd2282;
	xor.b64  	%rd2284, %rd2283, %rd2281;
	shf.r.wrap.b32 	%r11122, %r10731, %r10730, 1;
	shf.r.wrap.b32 	%r11123, %r10730, %r10731, 1;
	mov.b64 	%rd2285, {%r11123, %r11122};
	shf.r.wrap.b32 	%r11124, %r10731, %r10730, 8;
	shf.r.wrap.b32 	%r11125, %r10730, %r10731, 8;
	mov.b64 	%rd2286, {%r11125, %r11124};
	shr.u64 	%rd2287, %rd1713, 7;
	xor.b64  	%rd2288, %rd2285, %rd2287;
	xor.b64  	%rd2289, %rd2288, %rd2286;
	add.s64 	%rd2290, %rd1700, %rd1817;
	add.s64 	%rd2291, %rd2290, %rd2284;
	add.s64 	%rd2292, %rd2291, %rd2289;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11126,%dummy}, %rd2279;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11127}, %rd2279;
	}
	shf.r.wrap.b32 	%r11128, %r11127, %r11126, 19;
	shf.r.wrap.b32 	%r11129, %r11126, %r11127, 19;
	mov.b64 	%rd2293, {%r11129, %r11128};
	shf.l.wrap.b32 	%r11130, %r11126, %r11127, 3;
	shf.l.wrap.b32 	%r11131, %r11127, %r11126, 3;
	mov.b64 	%rd2294, {%r11131, %r11130};
	shr.u64 	%rd2295, %rd2279, 6;
	xor.b64  	%rd2296, %rd2293, %rd2295;
	xor.b64  	%rd2297, %rd2296, %rd2294;
	shf.r.wrap.b32 	%r11132, %r10741, %r10740, 1;
	shf.r.wrap.b32 	%r11133, %r10740, %r10741, 1;
	mov.b64 	%rd2298, {%r11133, %r11132};
	shf.r.wrap.b32 	%r11134, %r10741, %r10740, 8;
	shf.r.wrap.b32 	%r11135, %r10740, %r10741, 8;
	mov.b64 	%rd2299, {%r11135, %r11134};
	shr.u64 	%rd2300, %rd1726, 7;
	xor.b64  	%rd2301, %rd2298, %rd2300;
	xor.b64  	%rd2302, %rd2301, %rd2299;
	add.s64 	%rd2303, %rd1713, %rd1830;
	add.s64 	%rd2304, %rd2303, %rd2297;
	add.s64 	%rd2305, %rd2304, %rd2302;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11136,%dummy}, %rd2292;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11137}, %rd2292;
	}
	shf.r.wrap.b32 	%r11138, %r11137, %r11136, 19;
	shf.r.wrap.b32 	%r11139, %r11136, %r11137, 19;
	mov.b64 	%rd2306, {%r11139, %r11138};
	shf.l.wrap.b32 	%r11140, %r11136, %r11137, 3;
	shf.l.wrap.b32 	%r11141, %r11137, %r11136, 3;
	mov.b64 	%rd2307, {%r11141, %r11140};
	shr.u64 	%rd2308, %rd2292, 6;
	xor.b64  	%rd2309, %rd2306, %rd2308;
	xor.b64  	%rd2310, %rd2309, %rd2307;
	shf.r.wrap.b32 	%r11142, %r10751, %r10750, 1;
	shf.r.wrap.b32 	%r11143, %r10750, %r10751, 1;
	mov.b64 	%rd2311, {%r11143, %r11142};
	shf.r.wrap.b32 	%r11144, %r10751, %r10750, 8;
	shf.r.wrap.b32 	%r11145, %r10750, %r10751, 8;
	mov.b64 	%rd2312, {%r11145, %r11144};
	shr.u64 	%rd2313, %rd1739, 7;
	xor.b64  	%rd2314, %rd2311, %rd2313;
	xor.b64  	%rd2315, %rd2314, %rd2312;
	add.s64 	%rd2316, %rd1726, %rd1843;
	add.s64 	%rd2317, %rd2316, %rd2310;
	add.s64 	%rd2318, %rd2317, %rd2315;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11146,%dummy}, %rd2305;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11147}, %rd2305;
	}
	shf.r.wrap.b32 	%r11148, %r11147, %r11146, 19;
	shf.r.wrap.b32 	%r11149, %r11146, %r11147, 19;
	mov.b64 	%rd2319, {%r11149, %r11148};
	shf.l.wrap.b32 	%r11150, %r11146, %r11147, 3;
	shf.l.wrap.b32 	%r11151, %r11147, %r11146, 3;
	mov.b64 	%rd2320, {%r11151, %r11150};
	shr.u64 	%rd2321, %rd2305, 6;
	xor.b64  	%rd2322, %rd2319, %rd2321;
	xor.b64  	%rd2323, %rd2322, %rd2320;
	shf.r.wrap.b32 	%r11152, %r10761, %r10760, 1;
	shf.r.wrap.b32 	%r11153, %r10760, %r10761, 1;
	mov.b64 	%rd2324, {%r11153, %r11152};
	shf.r.wrap.b32 	%r11154, %r10761, %r10760, 8;
	shf.r.wrap.b32 	%r11155, %r10760, %r10761, 8;
	mov.b64 	%rd2325, {%r11155, %r11154};
	shr.u64 	%rd2326, %rd1752, 7;
	xor.b64  	%rd2327, %rd2324, %rd2326;
	xor.b64  	%rd2328, %rd2327, %rd2325;
	add.s64 	%rd2329, %rd1739, %rd1856;
	add.s64 	%rd2330, %rd2329, %rd2323;
	add.s64 	%rd2331, %rd2330, %rd2328;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11156,%dummy}, %rd2318;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11157}, %rd2318;
	}
	shf.r.wrap.b32 	%r11158, %r11157, %r11156, 19;
	shf.r.wrap.b32 	%r11159, %r11156, %r11157, 19;
	mov.b64 	%rd2332, {%r11159, %r11158};
	shf.l.wrap.b32 	%r11160, %r11156, %r11157, 3;
	shf.l.wrap.b32 	%r11161, %r11157, %r11156, 3;
	mov.b64 	%rd2333, {%r11161, %r11160};
	shr.u64 	%rd2334, %rd2318, 6;
	xor.b64  	%rd2335, %rd2332, %rd2334;
	xor.b64  	%rd2336, %rd2335, %rd2333;
	shf.r.wrap.b32 	%r11162, %r10771, %r10770, 1;
	shf.r.wrap.b32 	%r11163, %r10770, %r10771, 1;
	mov.b64 	%rd2337, {%r11163, %r11162};
	shf.r.wrap.b32 	%r11164, %r10771, %r10770, 8;
	shf.r.wrap.b32 	%r11165, %r10770, %r10771, 8;
	mov.b64 	%rd2338, {%r11165, %r11164};
	shr.u64 	%rd2339, %rd1765, 7;
	xor.b64  	%rd2340, %rd2337, %rd2339;
	xor.b64  	%rd2341, %rd2340, %rd2338;
	add.s64 	%rd2342, %rd1752, %rd1869;
	add.s64 	%rd2343, %rd2342, %rd2336;
	add.s64 	%rd2344, %rd2343, %rd2341;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11166,%dummy}, %rd2331;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11167}, %rd2331;
	}
	shf.r.wrap.b32 	%r11168, %r11167, %r11166, 19;
	shf.r.wrap.b32 	%r11169, %r11166, %r11167, 19;
	mov.b64 	%rd2345, {%r11169, %r11168};
	shf.l.wrap.b32 	%r11170, %r11166, %r11167, 3;
	shf.l.wrap.b32 	%r11171, %r11167, %r11166, 3;
	mov.b64 	%rd2346, {%r11171, %r11170};
	shr.u64 	%rd2347, %rd2331, 6;
	xor.b64  	%rd2348, %rd2345, %rd2347;
	xor.b64  	%rd2349, %rd2348, %rd2346;
	shf.r.wrap.b32 	%r11172, %r10781, %r10780, 1;
	shf.r.wrap.b32 	%r11173, %r10780, %r10781, 1;
	mov.b64 	%rd2350, {%r11173, %r11172};
	shf.r.wrap.b32 	%r11174, %r10781, %r10780, 8;
	shf.r.wrap.b32 	%r11175, %r10780, %r10781, 8;
	mov.b64 	%rd2351, {%r11175, %r11174};
	shr.u64 	%rd2352, %rd1778, 7;
	xor.b64  	%rd2353, %rd2350, %rd2352;
	xor.b64  	%rd2354, %rd2353, %rd2351;
	add.s64 	%rd2355, %rd1765, %rd1882;
	add.s64 	%rd2356, %rd2355, %rd2349;
	add.s64 	%rd2357, %rd2356, %rd2354;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11176,%dummy}, %rd2344;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11177}, %rd2344;
	}
	shf.r.wrap.b32 	%r11178, %r11177, %r11176, 19;
	shf.r.wrap.b32 	%r11179, %r11176, %r11177, 19;
	mov.b64 	%rd2358, {%r11179, %r11178};
	shf.l.wrap.b32 	%r11180, %r11176, %r11177, 3;
	shf.l.wrap.b32 	%r11181, %r11177, %r11176, 3;
	mov.b64 	%rd2359, {%r11181, %r11180};
	shr.u64 	%rd2360, %rd2344, 6;
	xor.b64  	%rd2361, %rd2358, %rd2360;
	xor.b64  	%rd2362, %rd2361, %rd2359;
	shf.r.wrap.b32 	%r11182, %r10791, %r10790, 1;
	shf.r.wrap.b32 	%r11183, %r10790, %r10791, 1;
	mov.b64 	%rd2363, {%r11183, %r11182};
	shf.r.wrap.b32 	%r11184, %r10791, %r10790, 8;
	shf.r.wrap.b32 	%r11185, %r10790, %r10791, 8;
	mov.b64 	%rd2364, {%r11185, %r11184};
	shr.u64 	%rd2365, %rd1791, 7;
	xor.b64  	%rd2366, %rd2363, %rd2365;
	xor.b64  	%rd2367, %rd2366, %rd2364;
	add.s64 	%rd2368, %rd2279, %rd1778;
	add.s64 	%rd2369, %rd2368, %rd2362;
	add.s64 	%rd2370, %rd2369, %rd2367;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11186,%dummy}, %rd2357;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11187}, %rd2357;
	}
	shf.r.wrap.b32 	%r11188, %r11187, %r11186, 19;
	shf.r.wrap.b32 	%r11189, %r11186, %r11187, 19;
	mov.b64 	%rd2371, {%r11189, %r11188};
	shf.l.wrap.b32 	%r11190, %r11186, %r11187, 3;
	shf.l.wrap.b32 	%r11191, %r11187, %r11186, 3;
	mov.b64 	%rd2372, {%r11191, %r11190};
	shr.u64 	%rd2373, %rd2357, 6;
	xor.b64  	%rd2374, %rd2371, %rd2373;
	xor.b64  	%rd2375, %rd2374, %rd2372;
	shf.r.wrap.b32 	%r11192, %r10801, %r10800, 1;
	shf.r.wrap.b32 	%r11193, %r10800, %r10801, 1;
	mov.b64 	%rd2376, {%r11193, %r11192};
	shf.r.wrap.b32 	%r11194, %r10801, %r10800, 8;
	shf.r.wrap.b32 	%r11195, %r10800, %r10801, 8;
	mov.b64 	%rd2377, {%r11195, %r11194};
	shr.u64 	%rd2378, %rd1804, 7;
	xor.b64  	%rd2379, %rd2376, %rd2378;
	xor.b64  	%rd2380, %rd2379, %rd2377;
	add.s64 	%rd2381, %rd2292, %rd1791;
	add.s64 	%rd2382, %rd2381, %rd2375;
	add.s64 	%rd2383, %rd2382, %rd2380;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11196,%dummy}, %rd2370;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11197}, %rd2370;
	}
	shf.r.wrap.b32 	%r11198, %r11197, %r11196, 19;
	shf.r.wrap.b32 	%r11199, %r11196, %r11197, 19;
	mov.b64 	%rd2384, {%r11199, %r11198};
	shf.l.wrap.b32 	%r11200, %r11196, %r11197, 3;
	shf.l.wrap.b32 	%r11201, %r11197, %r11196, 3;
	mov.b64 	%rd2385, {%r11201, %r11200};
	shr.u64 	%rd2386, %rd2370, 6;
	xor.b64  	%rd2387, %rd2384, %rd2386;
	xor.b64  	%rd2388, %rd2387, %rd2385;
	shf.r.wrap.b32 	%r11202, %r10811, %r10810, 1;
	shf.r.wrap.b32 	%r11203, %r10810, %r10811, 1;
	mov.b64 	%rd2389, {%r11203, %r11202};
	shf.r.wrap.b32 	%r11204, %r10811, %r10810, 8;
	shf.r.wrap.b32 	%r11205, %r10810, %r10811, 8;
	mov.b64 	%rd2390, {%r11205, %r11204};
	shr.u64 	%rd2391, %rd1817, 7;
	xor.b64  	%rd2392, %rd2389, %rd2391;
	xor.b64  	%rd2393, %rd2392, %rd2390;
	add.s64 	%rd2394, %rd2305, %rd1804;
	add.s64 	%rd2395, %rd2394, %rd2388;
	add.s64 	%rd2396, %rd2395, %rd2393;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11206,%dummy}, %rd2383;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11207}, %rd2383;
	}
	shf.r.wrap.b32 	%r11208, %r11207, %r11206, 19;
	shf.r.wrap.b32 	%r11209, %r11206, %r11207, 19;
	mov.b64 	%rd2397, {%r11209, %r11208};
	shf.l.wrap.b32 	%r11210, %r11206, %r11207, 3;
	shf.l.wrap.b32 	%r11211, %r11207, %r11206, 3;
	mov.b64 	%rd2398, {%r11211, %r11210};
	shr.u64 	%rd2399, %rd2383, 6;
	xor.b64  	%rd2400, %rd2397, %rd2399;
	xor.b64  	%rd2401, %rd2400, %rd2398;
	shf.r.wrap.b32 	%r11212, %r10821, %r10820, 1;
	shf.r.wrap.b32 	%r11213, %r10820, %r10821, 1;
	mov.b64 	%rd2402, {%r11213, %r11212};
	shf.r.wrap.b32 	%r11214, %r10821, %r10820, 8;
	shf.r.wrap.b32 	%r11215, %r10820, %r10821, 8;
	mov.b64 	%rd2403, {%r11215, %r11214};
	shr.u64 	%rd2404, %rd1830, 7;
	xor.b64  	%rd2405, %rd2402, %rd2404;
	xor.b64  	%rd2406, %rd2405, %rd2403;
	add.s64 	%rd2407, %rd2318, %rd1817;
	add.s64 	%rd2408, %rd2407, %rd2401;
	add.s64 	%rd2409, %rd2408, %rd2406;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11216,%dummy}, %rd2396;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11217}, %rd2396;
	}
	shf.r.wrap.b32 	%r11218, %r11217, %r11216, 19;
	shf.r.wrap.b32 	%r11219, %r11216, %r11217, 19;
	mov.b64 	%rd2410, {%r11219, %r11218};
	shf.l.wrap.b32 	%r11220, %r11216, %r11217, 3;
	shf.l.wrap.b32 	%r11221, %r11217, %r11216, 3;
	mov.b64 	%rd2411, {%r11221, %r11220};
	shr.u64 	%rd2412, %rd2396, 6;
	xor.b64  	%rd2413, %rd2410, %rd2412;
	xor.b64  	%rd2414, %rd2413, %rd2411;
	shf.r.wrap.b32 	%r11222, %r10831, %r10830, 1;
	shf.r.wrap.b32 	%r11223, %r10830, %r10831, 1;
	mov.b64 	%rd2415, {%r11223, %r11222};
	shf.r.wrap.b32 	%r11224, %r10831, %r10830, 8;
	shf.r.wrap.b32 	%r11225, %r10830, %r10831, 8;
	mov.b64 	%rd2416, {%r11225, %r11224};
	shr.u64 	%rd2417, %rd1843, 7;
	xor.b64  	%rd2418, %rd2415, %rd2417;
	xor.b64  	%rd2419, %rd2418, %rd2416;
	add.s64 	%rd2420, %rd2331, %rd1830;
	add.s64 	%rd2421, %rd2420, %rd2414;
	add.s64 	%rd2422, %rd2421, %rd2419;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11226,%dummy}, %rd2409;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11227}, %rd2409;
	}
	shf.r.wrap.b32 	%r11228, %r11227, %r11226, 19;
	shf.r.wrap.b32 	%r11229, %r11226, %r11227, 19;
	mov.b64 	%rd2423, {%r11229, %r11228};
	shf.l.wrap.b32 	%r11230, %r11226, %r11227, 3;
	shf.l.wrap.b32 	%r11231, %r11227, %r11226, 3;
	mov.b64 	%rd2424, {%r11231, %r11230};
	shr.u64 	%rd2425, %rd2409, 6;
	xor.b64  	%rd2426, %rd2423, %rd2425;
	xor.b64  	%rd2427, %rd2426, %rd2424;
	shf.r.wrap.b32 	%r11232, %r10841, %r10840, 1;
	shf.r.wrap.b32 	%r11233, %r10840, %r10841, 1;
	mov.b64 	%rd2428, {%r11233, %r11232};
	shf.r.wrap.b32 	%r11234, %r10841, %r10840, 8;
	shf.r.wrap.b32 	%r11235, %r10840, %r10841, 8;
	mov.b64 	%rd2429, {%r11235, %r11234};
	shr.u64 	%rd2430, %rd1856, 7;
	xor.b64  	%rd2431, %rd2428, %rd2430;
	xor.b64  	%rd2432, %rd2431, %rd2429;
	add.s64 	%rd2433, %rd2344, %rd1843;
	add.s64 	%rd2434, %rd2433, %rd2427;
	add.s64 	%rd2435, %rd2434, %rd2432;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11236,%dummy}, %rd2255;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11237}, %rd2255;
	}
	shf.r.wrap.b32 	%r11238, %r11237, %r11236, 14;
	shf.r.wrap.b32 	%r11239, %r11236, %r11237, 14;
	mov.b64 	%rd2436, {%r11239, %r11238};
	shf.r.wrap.b32 	%r11240, %r11237, %r11236, 18;
	shf.r.wrap.b32 	%r11241, %r11236, %r11237, 18;
	mov.b64 	%rd2437, {%r11241, %r11240};
	xor.b64  	%rd2438, %rd2437, %rd2436;
	shf.l.wrap.b32 	%r11242, %r11236, %r11237, 23;
	shf.l.wrap.b32 	%r11243, %r11237, %r11236, 23;
	mov.b64 	%rd2439, {%r11243, %r11242};
	xor.b64  	%rd2440, %rd2438, %rd2439;
	xor.b64  	%rd2441, %rd2207, %rd2231;
	and.b64  	%rd2442, %rd2441, %rd2255;
	xor.b64  	%rd2443, %rd2442, %rd2207;
	add.s64 	%rd2444, %rd2443, %rd2183;
	add.s64 	%rd2445, %rd2444, %rd2279;
	add.s64 	%rd2446, %rd2445, %rd2817;
	add.s64 	%rd2447, %rd2446, %rd2440;
	add.s64 	%rd2448, %rd2447, %rd2194;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11244,%dummy}, %rd2266;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11245}, %rd2266;
	}
	shf.r.wrap.b32 	%r11246, %r11245, %r11244, 28;
	shf.r.wrap.b32 	%r11247, %r11244, %r11245, 28;
	mov.b64 	%rd2449, {%r11247, %r11246};
	shf.l.wrap.b32 	%r11248, %r11244, %r11245, 30;
	shf.l.wrap.b32 	%r11249, %r11245, %r11244, 30;
	mov.b64 	%rd2450, {%r11249, %r11248};
	xor.b64  	%rd2451, %rd2450, %rd2449;
	shf.l.wrap.b32 	%r11250, %r11244, %r11245, 25;
	shf.l.wrap.b32 	%r11251, %r11245, %r11244, 25;
	mov.b64 	%rd2452, {%r11251, %r11250};
	xor.b64  	%rd2453, %rd2451, %rd2452;
	xor.b64  	%rd2454, %rd2266, %rd2218;
	xor.b64  	%rd2455, %rd2266, %rd2242;
	and.b64  	%rd2456, %rd2455, %rd2454;
	xor.b64  	%rd2457, %rd2456, %rd2266;
	add.s64 	%rd2458, %rd2447, %rd2457;
	add.s64 	%rd2459, %rd2458, %rd2453;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11252,%dummy}, %rd2448;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11253}, %rd2448;
	}
	shf.r.wrap.b32 	%r11254, %r11253, %r11252, 14;
	shf.r.wrap.b32 	%r11255, %r11252, %r11253, 14;
	mov.b64 	%rd2460, {%r11255, %r11254};
	shf.r.wrap.b32 	%r11256, %r11253, %r11252, 18;
	shf.r.wrap.b32 	%r11257, %r11252, %r11253, 18;
	mov.b64 	%rd2461, {%r11257, %r11256};
	xor.b64  	%rd2462, %rd2461, %rd2460;
	shf.l.wrap.b32 	%r11258, %r11252, %r11253, 23;
	shf.l.wrap.b32 	%r11259, %r11253, %r11252, 23;
	mov.b64 	%rd2463, {%r11259, %r11258};
	xor.b64  	%rd2464, %rd2462, %rd2463;
	xor.b64  	%rd2465, %rd2231, %rd2255;
	and.b64  	%rd2466, %rd2448, %rd2465;
	xor.b64  	%rd2467, %rd2466, %rd2231;
	add.s64 	%rd2468, %rd2292, %rd2207;
	add.s64 	%rd2469, %rd2468, %rd2818;
	add.s64 	%rd2470, %rd2469, %rd2467;
	add.s64 	%rd2471, %rd2470, %rd2464;
	add.s64 	%rd2472, %rd2471, %rd2218;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11260,%dummy}, %rd2459;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11261}, %rd2459;
	}
	shf.r.wrap.b32 	%r11262, %r11261, %r11260, 28;
	shf.r.wrap.b32 	%r11263, %r11260, %r11261, 28;
	mov.b64 	%rd2473, {%r11263, %r11262};
	shf.l.wrap.b32 	%r11264, %r11260, %r11261, 30;
	shf.l.wrap.b32 	%r11265, %r11261, %r11260, 30;
	mov.b64 	%rd2474, {%r11265, %r11264};
	xor.b64  	%rd2475, %rd2474, %rd2473;
	shf.l.wrap.b32 	%r11266, %r11260, %r11261, 25;
	shf.l.wrap.b32 	%r11267, %r11261, %r11260, 25;
	mov.b64 	%rd2476, {%r11267, %r11266};
	xor.b64  	%rd2477, %rd2475, %rd2476;
	xor.b64  	%rd2478, %rd2459, %rd2242;
	xor.b64  	%rd2479, %rd2459, %rd2266;
	and.b64  	%rd2480, %rd2479, %rd2478;
	xor.b64  	%rd2481, %rd2480, %rd2459;
	add.s64 	%rd2482, %rd2471, %rd2481;
	add.s64 	%rd2483, %rd2482, %rd2477;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11268,%dummy}, %rd2472;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11269}, %rd2472;
	}
	shf.r.wrap.b32 	%r11270, %r11269, %r11268, 14;
	shf.r.wrap.b32 	%r11271, %r11268, %r11269, 14;
	mov.b64 	%rd2484, {%r11271, %r11270};
	shf.r.wrap.b32 	%r11272, %r11269, %r11268, 18;
	shf.r.wrap.b32 	%r11273, %r11268, %r11269, 18;
	mov.b64 	%rd2485, {%r11273, %r11272};
	xor.b64  	%rd2486, %rd2485, %rd2484;
	shf.l.wrap.b32 	%r11274, %r11268, %r11269, 23;
	shf.l.wrap.b32 	%r11275, %r11269, %r11268, 23;
	mov.b64 	%rd2487, {%r11275, %r11274};
	xor.b64  	%rd2488, %rd2486, %rd2487;
	xor.b64  	%rd2489, %rd2448, %rd2255;
	and.b64  	%rd2490, %rd2472, %rd2489;
	xor.b64  	%rd2491, %rd2490, %rd2255;
	add.s64 	%rd2492, %rd2305, %rd2231;
	add.s64 	%rd2493, %rd2492, %rd2819;
	add.s64 	%rd2494, %rd2493, %rd2491;
	add.s64 	%rd2495, %rd2494, %rd2488;
	add.s64 	%rd2496, %rd2495, %rd2242;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11276,%dummy}, %rd2483;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11277}, %rd2483;
	}
	shf.r.wrap.b32 	%r11278, %r11277, %r11276, 28;
	shf.r.wrap.b32 	%r11279, %r11276, %r11277, 28;
	mov.b64 	%rd2497, {%r11279, %r11278};
	shf.l.wrap.b32 	%r11280, %r11276, %r11277, 30;
	shf.l.wrap.b32 	%r11281, %r11277, %r11276, 30;
	mov.b64 	%rd2498, {%r11281, %r11280};
	xor.b64  	%rd2499, %rd2498, %rd2497;
	shf.l.wrap.b32 	%r11282, %r11276, %r11277, 25;
	shf.l.wrap.b32 	%r11283, %r11277, %r11276, 25;
	mov.b64 	%rd2500, {%r11283, %r11282};
	xor.b64  	%rd2501, %rd2499, %rd2500;
	xor.b64  	%rd2502, %rd2483, %rd2266;
	xor.b64  	%rd2503, %rd2483, %rd2459;
	and.b64  	%rd2504, %rd2503, %rd2502;
	xor.b64  	%rd2505, %rd2504, %rd2483;
	add.s64 	%rd2506, %rd2495, %rd2505;
	add.s64 	%rd2507, %rd2506, %rd2501;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11284,%dummy}, %rd2496;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11285}, %rd2496;
	}
	shf.r.wrap.b32 	%r11286, %r11285, %r11284, 14;
	shf.r.wrap.b32 	%r11287, %r11284, %r11285, 14;
	mov.b64 	%rd2508, {%r11287, %r11286};
	shf.r.wrap.b32 	%r11288, %r11285, %r11284, 18;
	shf.r.wrap.b32 	%r11289, %r11284, %r11285, 18;
	mov.b64 	%rd2509, {%r11289, %r11288};
	xor.b64  	%rd2510, %rd2509, %rd2508;
	shf.l.wrap.b32 	%r11290, %r11284, %r11285, 23;
	shf.l.wrap.b32 	%r11291, %r11285, %r11284, 23;
	mov.b64 	%rd2511, {%r11291, %r11290};
	xor.b64  	%rd2512, %rd2510, %rd2511;
	xor.b64  	%rd2513, %rd2472, %rd2448;
	and.b64  	%rd2514, %rd2496, %rd2513;
	xor.b64  	%rd2515, %rd2514, %rd2448;
	add.s64 	%rd2516, %rd2318, %rd2255;
	add.s64 	%rd2517, %rd2516, %rd2820;
	add.s64 	%rd2518, %rd2517, %rd2515;
	add.s64 	%rd2519, %rd2518, %rd2512;
	add.s64 	%rd2520, %rd2519, %rd2266;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11292,%dummy}, %rd2507;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11293}, %rd2507;
	}
	shf.r.wrap.b32 	%r11294, %r11293, %r11292, 28;
	shf.r.wrap.b32 	%r11295, %r11292, %r11293, 28;
	mov.b64 	%rd2521, {%r11295, %r11294};
	shf.l.wrap.b32 	%r11296, %r11292, %r11293, 30;
	shf.l.wrap.b32 	%r11297, %r11293, %r11292, 30;
	mov.b64 	%rd2522, {%r11297, %r11296};
	xor.b64  	%rd2523, %rd2522, %rd2521;
	shf.l.wrap.b32 	%r11298, %r11292, %r11293, 25;
	shf.l.wrap.b32 	%r11299, %r11293, %r11292, 25;
	mov.b64 	%rd2524, {%r11299, %r11298};
	xor.b64  	%rd2525, %rd2523, %rd2524;
	xor.b64  	%rd2526, %rd2507, %rd2459;
	xor.b64  	%rd2527, %rd2507, %rd2483;
	and.b64  	%rd2528, %rd2527, %rd2526;
	xor.b64  	%rd2529, %rd2528, %rd2507;
	add.s64 	%rd2530, %rd2519, %rd2529;
	add.s64 	%rd2531, %rd2530, %rd2525;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11300,%dummy}, %rd2520;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11301}, %rd2520;
	}
	shf.r.wrap.b32 	%r11302, %r11301, %r11300, 14;
	shf.r.wrap.b32 	%r11303, %r11300, %r11301, 14;
	mov.b64 	%rd2532, {%r11303, %r11302};
	shf.r.wrap.b32 	%r11304, %r11301, %r11300, 18;
	shf.r.wrap.b32 	%r11305, %r11300, %r11301, 18;
	mov.b64 	%rd2533, {%r11305, %r11304};
	xor.b64  	%rd2534, %rd2533, %rd2532;
	shf.l.wrap.b32 	%r11306, %r11300, %r11301, 23;
	shf.l.wrap.b32 	%r11307, %r11301, %r11300, 23;
	mov.b64 	%rd2535, {%r11307, %r11306};
	xor.b64  	%rd2536, %rd2534, %rd2535;
	xor.b64  	%rd2537, %rd2496, %rd2472;
	and.b64  	%rd2538, %rd2520, %rd2537;
	xor.b64  	%rd2539, %rd2538, %rd2472;
	add.s64 	%rd2540, %rd2448, %rd2331;
	add.s64 	%rd2541, %rd2540, %rd2821;
	add.s64 	%rd2542, %rd2541, %rd2539;
	add.s64 	%rd2543, %rd2542, %rd2536;
	add.s64 	%rd2544, %rd2543, %rd2459;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11308,%dummy}, %rd2531;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11309}, %rd2531;
	}
	shf.r.wrap.b32 	%r11310, %r11309, %r11308, 28;
	shf.r.wrap.b32 	%r11311, %r11308, %r11309, 28;
	mov.b64 	%rd2545, {%r11311, %r11310};
	shf.l.wrap.b32 	%r11312, %r11308, %r11309, 30;
	shf.l.wrap.b32 	%r11313, %r11309, %r11308, 30;
	mov.b64 	%rd2546, {%r11313, %r11312};
	xor.b64  	%rd2547, %rd2546, %rd2545;
	shf.l.wrap.b32 	%r11314, %r11308, %r11309, 25;
	shf.l.wrap.b32 	%r11315, %r11309, %r11308, 25;
	mov.b64 	%rd2548, {%r11315, %r11314};
	xor.b64  	%rd2549, %rd2547, %rd2548;
	xor.b64  	%rd2550, %rd2531, %rd2483;
	xor.b64  	%rd2551, %rd2531, %rd2507;
	and.b64  	%rd2552, %rd2551, %rd2550;
	xor.b64  	%rd2553, %rd2552, %rd2531;
	add.s64 	%rd2554, %rd2543, %rd2553;
	add.s64 	%rd2555, %rd2554, %rd2549;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11316,%dummy}, %rd2544;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11317}, %rd2544;
	}
	shf.r.wrap.b32 	%r11318, %r11317, %r11316, 14;
	shf.r.wrap.b32 	%r11319, %r11316, %r11317, 14;
	mov.b64 	%rd2556, {%r11319, %r11318};
	shf.r.wrap.b32 	%r11320, %r11317, %r11316, 18;
	shf.r.wrap.b32 	%r11321, %r11316, %r11317, 18;
	mov.b64 	%rd2557, {%r11321, %r11320};
	xor.b64  	%rd2558, %rd2557, %rd2556;
	shf.l.wrap.b32 	%r11322, %r11316, %r11317, 23;
	shf.l.wrap.b32 	%r11323, %r11317, %r11316, 23;
	mov.b64 	%rd2559, {%r11323, %r11322};
	xor.b64  	%rd2560, %rd2558, %rd2559;
	xor.b64  	%rd2561, %rd2520, %rd2496;
	and.b64  	%rd2562, %rd2544, %rd2561;
	xor.b64  	%rd2563, %rd2562, %rd2496;
	add.s64 	%rd2564, %rd2472, %rd2344;
	add.s64 	%rd2565, %rd2564, %rd2822;
	add.s64 	%rd2566, %rd2565, %rd2563;
	add.s64 	%rd2567, %rd2566, %rd2560;
	add.s64 	%rd2568, %rd2567, %rd2483;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11324,%dummy}, %rd2555;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11325}, %rd2555;
	}
	shf.r.wrap.b32 	%r11326, %r11325, %r11324, 28;
	shf.r.wrap.b32 	%r11327, %r11324, %r11325, 28;
	mov.b64 	%rd2569, {%r11327, %r11326};
	shf.l.wrap.b32 	%r11328, %r11324, %r11325, 30;
	shf.l.wrap.b32 	%r11329, %r11325, %r11324, 30;
	mov.b64 	%rd2570, {%r11329, %r11328};
	xor.b64  	%rd2571, %rd2570, %rd2569;
	shf.l.wrap.b32 	%r11330, %r11324, %r11325, 25;
	shf.l.wrap.b32 	%r11331, %r11325, %r11324, 25;
	mov.b64 	%rd2572, {%r11331, %r11330};
	xor.b64  	%rd2573, %rd2571, %rd2572;
	xor.b64  	%rd2574, %rd2555, %rd2507;
	xor.b64  	%rd2575, %rd2555, %rd2531;
	and.b64  	%rd2576, %rd2575, %rd2574;
	xor.b64  	%rd2577, %rd2576, %rd2555;
	add.s64 	%rd2578, %rd2567, %rd2577;
	add.s64 	%rd2579, %rd2578, %rd2573;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11332,%dummy}, %rd2568;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11333}, %rd2568;
	}
	shf.r.wrap.b32 	%r11334, %r11333, %r11332, 14;
	shf.r.wrap.b32 	%r11335, %r11332, %r11333, 14;
	mov.b64 	%rd2580, {%r11335, %r11334};
	shf.r.wrap.b32 	%r11336, %r11333, %r11332, 18;
	shf.r.wrap.b32 	%r11337, %r11332, %r11333, 18;
	mov.b64 	%rd2581, {%r11337, %r11336};
	xor.b64  	%rd2582, %rd2581, %rd2580;
	shf.l.wrap.b32 	%r11338, %r11332, %r11333, 23;
	shf.l.wrap.b32 	%r11339, %r11333, %r11332, 23;
	mov.b64 	%rd2583, {%r11339, %r11338};
	xor.b64  	%rd2584, %rd2582, %rd2583;
	xor.b64  	%rd2585, %rd2544, %rd2520;
	and.b64  	%rd2586, %rd2568, %rd2585;
	xor.b64  	%rd2587, %rd2586, %rd2520;
	add.s64 	%rd2588, %rd2496, %rd2357;
	add.s64 	%rd2589, %rd2588, %rd2823;
	add.s64 	%rd2590, %rd2589, %rd2587;
	add.s64 	%rd2591, %rd2590, %rd2584;
	add.s64 	%rd2592, %rd2591, %rd2507;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11340,%dummy}, %rd2579;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11341}, %rd2579;
	}
	shf.r.wrap.b32 	%r11342, %r11341, %r11340, 28;
	shf.r.wrap.b32 	%r11343, %r11340, %r11341, 28;
	mov.b64 	%rd2593, {%r11343, %r11342};
	shf.l.wrap.b32 	%r11344, %r11340, %r11341, 30;
	shf.l.wrap.b32 	%r11345, %r11341, %r11340, 30;
	mov.b64 	%rd2594, {%r11345, %r11344};
	xor.b64  	%rd2595, %rd2594, %rd2593;
	shf.l.wrap.b32 	%r11346, %r11340, %r11341, 25;
	shf.l.wrap.b32 	%r11347, %r11341, %r11340, 25;
	mov.b64 	%rd2596, {%r11347, %r11346};
	xor.b64  	%rd2597, %rd2595, %rd2596;
	xor.b64  	%rd2598, %rd2579, %rd2531;
	xor.b64  	%rd2599, %rd2579, %rd2555;
	and.b64  	%rd2600, %rd2599, %rd2598;
	xor.b64  	%rd2601, %rd2600, %rd2579;
	add.s64 	%rd2602, %rd2591, %rd2601;
	add.s64 	%rd2603, %rd2602, %rd2597;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11348,%dummy}, %rd2592;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11349}, %rd2592;
	}
	shf.r.wrap.b32 	%r11350, %r11349, %r11348, 14;
	shf.r.wrap.b32 	%r11351, %r11348, %r11349, 14;
	mov.b64 	%rd2604, {%r11351, %r11350};
	shf.r.wrap.b32 	%r11352, %r11349, %r11348, 18;
	shf.r.wrap.b32 	%r11353, %r11348, %r11349, 18;
	mov.b64 	%rd2605, {%r11353, %r11352};
	xor.b64  	%rd2606, %rd2605, %rd2604;
	shf.l.wrap.b32 	%r11354, %r11348, %r11349, 23;
	shf.l.wrap.b32 	%r11355, %r11349, %r11348, 23;
	mov.b64 	%rd2607, {%r11355, %r11354};
	xor.b64  	%rd2608, %rd2606, %rd2607;
	xor.b64  	%rd2609, %rd2568, %rd2544;
	and.b64  	%rd2610, %rd2592, %rd2609;
	xor.b64  	%rd2611, %rd2610, %rd2544;
	add.s64 	%rd2612, %rd2520, %rd2370;
	add.s64 	%rd2613, %rd2612, %rd2824;
	add.s64 	%rd2614, %rd2613, %rd2611;
	add.s64 	%rd2615, %rd2614, %rd2608;
	add.s64 	%rd2616, %rd2615, %rd2531;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11356,%dummy}, %rd2603;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11357}, %rd2603;
	}
	shf.r.wrap.b32 	%r11358, %r11357, %r11356, 28;
	shf.r.wrap.b32 	%r11359, %r11356, %r11357, 28;
	mov.b64 	%rd2617, {%r11359, %r11358};
	shf.l.wrap.b32 	%r11360, %r11356, %r11357, 30;
	shf.l.wrap.b32 	%r11361, %r11357, %r11356, 30;
	mov.b64 	%rd2618, {%r11361, %r11360};
	xor.b64  	%rd2619, %rd2618, %rd2617;
	shf.l.wrap.b32 	%r11362, %r11356, %r11357, 25;
	shf.l.wrap.b32 	%r11363, %r11357, %r11356, 25;
	mov.b64 	%rd2620, {%r11363, %r11362};
	xor.b64  	%rd2621, %rd2619, %rd2620;
	xor.b64  	%rd2622, %rd2603, %rd2555;
	xor.b64  	%rd2623, %rd2603, %rd2579;
	and.b64  	%rd2624, %rd2623, %rd2622;
	xor.b64  	%rd2625, %rd2624, %rd2603;
	add.s64 	%rd2626, %rd2615, %rd2625;
	add.s64 	%rd2627, %rd2626, %rd2621;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11364,%dummy}, %rd2616;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11365}, %rd2616;
	}
	shf.r.wrap.b32 	%r11366, %r11365, %r11364, 14;
	shf.r.wrap.b32 	%r11367, %r11364, %r11365, 14;
	mov.b64 	%rd2628, {%r11367, %r11366};
	shf.r.wrap.b32 	%r11368, %r11365, %r11364, 18;
	shf.r.wrap.b32 	%r11369, %r11364, %r11365, 18;
	mov.b64 	%rd2629, {%r11369, %r11368};
	xor.b64  	%rd2630, %rd2629, %rd2628;
	shf.l.wrap.b32 	%r11370, %r11364, %r11365, 23;
	shf.l.wrap.b32 	%r11371, %r11365, %r11364, 23;
	mov.b64 	%rd2631, {%r11371, %r11370};
	xor.b64  	%rd2632, %rd2630, %rd2631;
	xor.b64  	%rd2633, %rd2592, %rd2568;
	and.b64  	%rd2634, %rd2616, %rd2633;
	xor.b64  	%rd2635, %rd2634, %rd2568;
	add.s64 	%rd2636, %rd2544, %rd2383;
	add.s64 	%rd2637, %rd2636, %rd2825;
	add.s64 	%rd2638, %rd2637, %rd2635;
	add.s64 	%rd2639, %rd2638, %rd2632;
	add.s64 	%rd2640, %rd2639, %rd2555;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11372,%dummy}, %rd2627;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11373}, %rd2627;
	}
	shf.r.wrap.b32 	%r11374, %r11373, %r11372, 28;
	shf.r.wrap.b32 	%r11375, %r11372, %r11373, 28;
	mov.b64 	%rd2641, {%r11375, %r11374};
	shf.l.wrap.b32 	%r11376, %r11372, %r11373, 30;
	shf.l.wrap.b32 	%r11377, %r11373, %r11372, 30;
	mov.b64 	%rd2642, {%r11377, %r11376};
	xor.b64  	%rd2643, %rd2642, %rd2641;
	shf.l.wrap.b32 	%r11378, %r11372, %r11373, 25;
	shf.l.wrap.b32 	%r11379, %r11373, %r11372, 25;
	mov.b64 	%rd2644, {%r11379, %r11378};
	xor.b64  	%rd2645, %rd2643, %rd2644;
	xor.b64  	%rd2646, %rd2627, %rd2579;
	xor.b64  	%rd2647, %rd2627, %rd2603;
	and.b64  	%rd2648, %rd2647, %rd2646;
	xor.b64  	%rd2649, %rd2648, %rd2627;
	add.s64 	%rd2650, %rd2639, %rd2649;
	add.s64 	%rd2651, %rd2650, %rd2645;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11380,%dummy}, %rd2640;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11381}, %rd2640;
	}
	shf.r.wrap.b32 	%r11382, %r11381, %r11380, 14;
	shf.r.wrap.b32 	%r11383, %r11380, %r11381, 14;
	mov.b64 	%rd2652, {%r11383, %r11382};
	shf.r.wrap.b32 	%r11384, %r11381, %r11380, 18;
	shf.r.wrap.b32 	%r11385, %r11380, %r11381, 18;
	mov.b64 	%rd2653, {%r11385, %r11384};
	xor.b64  	%rd2654, %rd2653, %rd2652;
	shf.l.wrap.b32 	%r11386, %r11380, %r11381, 23;
	shf.l.wrap.b32 	%r11387, %r11381, %r11380, 23;
	mov.b64 	%rd2655, {%r11387, %r11386};
	xor.b64  	%rd2656, %rd2654, %rd2655;
	xor.b64  	%rd2657, %rd2616, %rd2592;
	and.b64  	%rd2658, %rd2640, %rd2657;
	xor.b64  	%rd2659, %rd2658, %rd2592;
	add.s64 	%rd2660, %rd2568, %rd2396;
	add.s64 	%rd2661, %rd2660, %rd2826;
	add.s64 	%rd2662, %rd2661, %rd2659;
	add.s64 	%rd2663, %rd2662, %rd2656;
	add.s64 	%rd2664, %rd2663, %rd2579;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11388,%dummy}, %rd2651;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11389}, %rd2651;
	}
	shf.r.wrap.b32 	%r11390, %r11389, %r11388, 28;
	shf.r.wrap.b32 	%r11391, %r11388, %r11389, 28;
	mov.b64 	%rd2665, {%r11391, %r11390};
	shf.l.wrap.b32 	%r11392, %r11388, %r11389, 30;
	shf.l.wrap.b32 	%r11393, %r11389, %r11388, 30;
	mov.b64 	%rd2666, {%r11393, %r11392};
	xor.b64  	%rd2667, %rd2666, %rd2665;
	shf.l.wrap.b32 	%r11394, %r11388, %r11389, 25;
	shf.l.wrap.b32 	%r11395, %r11389, %r11388, 25;
	mov.b64 	%rd2668, {%r11395, %r11394};
	xor.b64  	%rd2669, %rd2667, %rd2668;
	xor.b64  	%rd2670, %rd2651, %rd2603;
	xor.b64  	%rd2671, %rd2651, %rd2627;
	and.b64  	%rd2672, %rd2671, %rd2670;
	xor.b64  	%rd2673, %rd2672, %rd2651;
	add.s64 	%rd2674, %rd2663, %rd2673;
	add.s64 	%rd2675, %rd2674, %rd2669;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11396,%dummy}, %rd2664;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11397}, %rd2664;
	}
	shf.r.wrap.b32 	%r11398, %r11397, %r11396, 14;
	shf.r.wrap.b32 	%r11399, %r11396, %r11397, 14;
	mov.b64 	%rd2676, {%r11399, %r11398};
	shf.r.wrap.b32 	%r11400, %r11397, %r11396, 18;
	shf.r.wrap.b32 	%r11401, %r11396, %r11397, 18;
	mov.b64 	%rd2677, {%r11401, %r11400};
	xor.b64  	%rd2678, %rd2677, %rd2676;
	shf.l.wrap.b32 	%r11402, %r11396, %r11397, 23;
	shf.l.wrap.b32 	%r11403, %r11397, %r11396, 23;
	mov.b64 	%rd2679, {%r11403, %r11402};
	xor.b64  	%rd2680, %rd2678, %rd2679;
	xor.b64  	%rd2681, %rd2640, %rd2616;
	and.b64  	%rd2682, %rd2664, %rd2681;
	xor.b64  	%rd2683, %rd2682, %rd2616;
	add.s64 	%rd2684, %rd2592, %rd2409;
	add.s64 	%rd2685, %rd2684, %rd2827;
	add.s64 	%rd2686, %rd2685, %rd2683;
	add.s64 	%rd2687, %rd2686, %rd2680;
	add.s64 	%rd2688, %rd2687, %rd2603;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11404,%dummy}, %rd2675;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11405}, %rd2675;
	}
	shf.r.wrap.b32 	%r11406, %r11405, %r11404, 28;
	shf.r.wrap.b32 	%r11407, %r11404, %r11405, 28;
	mov.b64 	%rd2689, {%r11407, %r11406};
	shf.l.wrap.b32 	%r11408, %r11404, %r11405, 30;
	shf.l.wrap.b32 	%r11409, %r11405, %r11404, 30;
	mov.b64 	%rd2690, {%r11409, %r11408};
	xor.b64  	%rd2691, %rd2690, %rd2689;
	shf.l.wrap.b32 	%r11410, %r11404, %r11405, 25;
	shf.l.wrap.b32 	%r11411, %r11405, %r11404, 25;
	mov.b64 	%rd2692, {%r11411, %r11410};
	xor.b64  	%rd2693, %rd2691, %rd2692;
	xor.b64  	%rd2694, %rd2675, %rd2627;
	xor.b64  	%rd2695, %rd2675, %rd2651;
	and.b64  	%rd2696, %rd2695, %rd2694;
	xor.b64  	%rd2697, %rd2696, %rd2675;
	add.s64 	%rd2698, %rd2687, %rd2697;
	add.s64 	%rd2699, %rd2698, %rd2693;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11412,%dummy}, %rd2688;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11413}, %rd2688;
	}
	shf.r.wrap.b32 	%r11414, %r11413, %r11412, 14;
	shf.r.wrap.b32 	%r11415, %r11412, %r11413, 14;
	mov.b64 	%rd2700, {%r11415, %r11414};
	shf.r.wrap.b32 	%r11416, %r11413, %r11412, 18;
	shf.r.wrap.b32 	%r11417, %r11412, %r11413, 18;
	mov.b64 	%rd2701, {%r11417, %r11416};
	xor.b64  	%rd2702, %rd2701, %rd2700;
	shf.l.wrap.b32 	%r11418, %r11412, %r11413, 23;
	shf.l.wrap.b32 	%r11419, %r11413, %r11412, 23;
	mov.b64 	%rd2703, {%r11419, %r11418};
	xor.b64  	%rd2704, %rd2702, %rd2703;
	xor.b64  	%rd2705, %rd2664, %rd2640;
	and.b64  	%rd2706, %rd2688, %rd2705;
	xor.b64  	%rd2707, %rd2706, %rd2640;
	add.s64 	%rd2708, %rd2616, %rd2422;
	add.s64 	%rd2709, %rd2708, %rd2828;
	add.s64 	%rd2710, %rd2709, %rd2707;
	add.s64 	%rd2711, %rd2710, %rd2704;
	add.s64 	%rd2712, %rd2711, %rd2627;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11420,%dummy}, %rd2699;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11421}, %rd2699;
	}
	shf.r.wrap.b32 	%r11422, %r11421, %r11420, 28;
	shf.r.wrap.b32 	%r11423, %r11420, %r11421, 28;
	mov.b64 	%rd2713, {%r11423, %r11422};
	shf.l.wrap.b32 	%r11424, %r11420, %r11421, 30;
	shf.l.wrap.b32 	%r11425, %r11421, %r11420, 30;
	mov.b64 	%rd2714, {%r11425, %r11424};
	xor.b64  	%rd2715, %rd2714, %rd2713;
	shf.l.wrap.b32 	%r11426, %r11420, %r11421, 25;
	shf.l.wrap.b32 	%r11427, %r11421, %r11420, 25;
	mov.b64 	%rd2716, {%r11427, %r11426};
	xor.b64  	%rd2717, %rd2715, %rd2716;
	xor.b64  	%rd2718, %rd2699, %rd2651;
	xor.b64  	%rd2719, %rd2699, %rd2675;
	and.b64  	%rd2720, %rd2719, %rd2718;
	xor.b64  	%rd2721, %rd2720, %rd2699;
	add.s64 	%rd2722, %rd2711, %rd2721;
	add.s64 	%rd2723, %rd2722, %rd2717;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11428,%dummy}, %rd2712;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11429}, %rd2712;
	}
	shf.r.wrap.b32 	%r11430, %r11429, %r11428, 14;
	shf.r.wrap.b32 	%r11431, %r11428, %r11429, 14;
	mov.b64 	%rd2724, {%r11431, %r11430};
	shf.r.wrap.b32 	%r11432, %r11429, %r11428, 18;
	shf.r.wrap.b32 	%r11433, %r11428, %r11429, 18;
	mov.b64 	%rd2725, {%r11433, %r11432};
	xor.b64  	%rd2726, %rd2725, %rd2724;
	shf.l.wrap.b32 	%r11434, %r11428, %r11429, 23;
	shf.l.wrap.b32 	%r11435, %r11429, %r11428, 23;
	mov.b64 	%rd2727, {%r11435, %r11434};
	xor.b64  	%rd2728, %rd2726, %rd2727;
	xor.b64  	%rd2729, %rd2688, %rd2664;
	and.b64  	%rd2730, %rd2712, %rd2729;
	xor.b64  	%rd2731, %rd2730, %rd2664;
	add.s64 	%rd2732, %rd2640, %rd2435;
	add.s64 	%rd2733, %rd2732, %rd2829;
	add.s64 	%rd2734, %rd2733, %rd2731;
	add.s64 	%rd2735, %rd2734, %rd2728;
	add.s64 	%rd91, %rd2735, %rd2651;
	{
	.reg .b32 %dummy;
	mov.b64 	{%r11436,%dummy}, %rd2723;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r11437}, %rd2723;
	}
	shf.r.wrap.b32 	%r11438, %r11437, %r11436, 28;
	shf.r.wrap.b32 	%r11439, %r11436, %r11437, 28;
	mov.b64 	%rd2736, {%r11439, %r11438};
	shf.l.wrap.b32 	%r11440, %r11436, %r11437, 30;
	shf.l.wrap.b32 	%r11441, %r11437, %r11436, 30;
	mov.b64 	%rd2737, {%r11441, %r11440};
	xor.b64  	%rd2738, %rd2737, %rd2736;
	shf.l.wrap.b32 	%r11442, %r11436, %r11437, 25;
	shf.l.wrap.b32 	%r11443, %r11437, %r11436, 25;
	mov.b64 	%rd2739, {%r11443, %r11442};
	xor.b64  	%rd2740, %rd2738, %rd2739;
	xor.b64  	%rd2741, %rd2723, %rd2675;
	xor.b64  	%rd2742, %rd2723, %rd2699;
	and.b64  	%rd2743, %rd2742, %rd2741;
	xor.b64  	%rd2744, %rd2743, %rd2723;
	add.s64 	%rd2745, %rd2735, %rd2744;
	add.s64 	%rd92, %rd2745, %rd2740;
	cvt.u32.u64	%r11444, %rd91;
	setp.ne.s32	%p815, %r11444, %r26;
	@%p815 bra 	BB6_1079;

	shr.u64 	%rd2746, %rd92, 32;
	cvt.u32.u64	%r11445, %rd2746;
	shr.u64 	%rd2747, %rd91, 32;
	cvt.u32.u64	%r11446, %rd2747;
	setp.eq.s32	%p816, %r11446, %r27;
	cvt.u32.u64	%r11447, %rd92;
	setp.eq.s32	%p817, %r11447, %r28;
	and.pred  	%p818, %p816, %p817;
	setp.eq.s32	%p819, %r11445, %r29;
	and.pred  	%p820, %p818, %p819;
	@!%p820 bra 	BB6_1079;
	bra.uni 	BB6_1075;

BB6_1075:
	ld.param.u64 	%rd2830, [m01710_s04_param_16];
	mul.wide.u32 	%rd2748, %r1730, 4;
	add.s64 	%rd2749, %rd2830, %rd2748;
	atom.global.add.u32 	%r11448, [%rd2749], 1;
	setp.ne.s32	%p821, %r11448, 0;
	@%p821 bra 	BB6_1079;

	ld.param.u32 	%r11455, [m01710_s04_param_31];
	atom.global.add.u32 	%r1725, [%rd99], 1;
	setp.lt.u32	%p822, %r1725, %r11455;
	@%p822 bra 	BB6_1078;
	bra.uni 	BB6_1077;

BB6_1078:
	ld.param.u64 	%rd2831, [m01710_s04_param_14];
	ld.param.u32 	%r11454, [m01710_s04_param_27];
	mov.u32 	%r11453, 0;
	mul.wide.u32 	%rd2750, %r1725, 32;
	add.s64 	%rd2751, %rd2831, %rd2750;
	st.global.v2.u32 	[%rd2751], {%r11454, %r11453};
	st.global.u32 	[%rd2751+8], %r1730;
	st.global.u32 	[%rd2751+24], %r11456;
	st.global.u64 	[%rd2751+16], %rd21;
	bra.uni 	BB6_1079;

BB6_1077:
	atom.global.add.u32 	%r11449, [%rd99], -1;

BB6_1079:
	ld.param.u32 	%r11451, [m01710_s04_param_30];
	add.s32 	%r11456, %r11456, 1;
	setp.lt.u32	%p823, %r11456, %r11451;
	@%p823 bra 	BB6_3;

BB6_1080:
	ret;
}

	// .globl	m01710_s08
.entry m01710_s08(
	.param .u64 .ptr .global .align 4 m01710_s08_param_0,
	.param .u64 .ptr .const .align 4 m01710_s08_param_1,
	.param .u64 .ptr .global .align 4 m01710_s08_param_2,
	.param .u64 .ptr .global .align 4 m01710_s08_param_3,
	.param .u64 .ptr .global .align 1 m01710_s08_param_4,
	.param .u64 .ptr .global .align 1 m01710_s08_param_5,
	.param .u64 .ptr .global .align 4 m01710_s08_param_6,
	.param .u64 .ptr .global .align 4 m01710_s08_param_7,
	.param .u64 .ptr .global .align 4 m01710_s08_param_8,
	.param .u64 .ptr .global .align 4 m01710_s08_param_9,
	.param .u64 .ptr .global .align 4 m01710_s08_param_10,
	.param .u64 .ptr .global .align 4 m01710_s08_param_11,
	.param .u64 .ptr .global .align 4 m01710_s08_param_12,
	.param .u64 .ptr .global .align 4 m01710_s08_param_13,
	.param .u64 .ptr .global .align 8 m01710_s08_param_14,
	.param .u64 .ptr .global .align 4 m01710_s08_param_15,
	.param .u64 .ptr .global .align 4 m01710_s08_param_16,
	.param .u64 .ptr .global .align 4 m01710_s08_param_17,
	.param .u64 .ptr .global .align 1 m01710_s08_param_18,
	.param .u64 .ptr .global .align 4 m01710_s08_param_19,
	.param .u64 .ptr .global .align 4 m01710_s08_param_20,
	.param .u64 .ptr .global .align 4 m01710_s08_param_21,
	.param .u64 .ptr .global .align 4 m01710_s08_param_22,
	.param .u64 .ptr .global .align 4 m01710_s08_param_23,
	.param .u32 m01710_s08_param_24,
	.param .u32 m01710_s08_param_25,
	.param .u32 m01710_s08_param_26,
	.param .u32 m01710_s08_param_27,
	.param .u32 m01710_s08_param_28,
	.param .u32 m01710_s08_param_29,
	.param .u32 m01710_s08_param_30,
	.param .u32 m01710_s08_param_31,
	.param .u32 m01710_s08_param_32,
	.param .u32 m01710_s08_param_33,
	.param .u64 m01710_s08_param_34
)
{



	ret;
}

	// .globl	m01710_s16
.entry m01710_s16(
	.param .u64 .ptr .global .align 4 m01710_s16_param_0,
	.param .u64 .ptr .const .align 4 m01710_s16_param_1,
	.param .u64 .ptr .global .align 4 m01710_s16_param_2,
	.param .u64 .ptr .global .align 4 m01710_s16_param_3,
	.param .u64 .ptr .global .align 1 m01710_s16_param_4,
	.param .u64 .ptr .global .align 1 m01710_s16_param_5,
	.param .u64 .ptr .global .align 4 m01710_s16_param_6,
	.param .u64 .ptr .global .align 4 m01710_s16_param_7,
	.param .u64 .ptr .global .align 4 m01710_s16_param_8,
	.param .u64 .ptr .global .align 4 m01710_s16_param_9,
	.param .u64 .ptr .global .align 4 m01710_s16_param_10,
	.param .u64 .ptr .global .align 4 m01710_s16_param_11,
	.param .u64 .ptr .global .align 4 m01710_s16_param_12,
	.param .u64 .ptr .global .align 4 m01710_s16_param_13,
	.param .u64 .ptr .global .align 8 m01710_s16_param_14,
	.param .u64 .ptr .global .align 4 m01710_s16_param_15,
	.param .u64 .ptr .global .align 4 m01710_s16_param_16,
	.param .u64 .ptr .global .align 4 m01710_s16_param_17,
	.param .u64 .ptr .global .align 1 m01710_s16_param_18,
	.param .u64 .ptr .global .align 4 m01710_s16_param_19,
	.param .u64 .ptr .global .align 4 m01710_s16_param_20,
	.param .u64 .ptr .global .align 4 m01710_s16_param_21,
	.param .u64 .ptr .global .align 4 m01710_s16_param_22,
	.param .u64 .ptr .global .align 4 m01710_s16_param_23,
	.param .u32 m01710_s16_param_24,
	.param .u32 m01710_s16_param_25,
	.param .u32 m01710_s16_param_26,
	.param .u32 m01710_s16_param_27,
	.param .u32 m01710_s16_param_28,
	.param .u32 m01710_s16_param_29,
	.param .u32 m01710_s16_param_30,
	.param .u32 m01710_s16_param_31,
	.param .u32 m01710_s16_param_32,
	.param .u32 m01710_s16_param_33,
	.param .u64 m01710_s16_param_34
)
{



	ret;
}


  